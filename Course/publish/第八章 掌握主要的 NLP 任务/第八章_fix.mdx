# ç¬¬å…«ç«  æŒæ¡ä¸»è¦çš„ NLP ä»»åŠ¡ 

åœ¨ç¬¬å››ç« ï¼Œä½ äº†è§£äº†å¦‚ä½•å¾®è°ƒæ–‡æœ¬åˆ†ç±»æ¨¡å‹ã€‚åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†å¤„ç†ä»¥ä¸‹å¸¸è§çš„ NLP ä»»åŠ¡ï¼š

- Token åˆ†ç±»
- æ©ç è¯­è¨€å»ºæ¨¡ï¼ˆå¦‚ BERTï¼‰
- æ–‡æœ¬æ‘˜è¦
- ç¿»è¯‘
- å› æœè¯­è¨€å»ºæ¨¡é¢„è®­ç»ƒï¼ˆå¦‚ GPT-2ï¼‰
- é—®ç­”

{#if fw === 'pt'}

ä¸ºæ­¤ï¼Œä½ éœ€è¦å……åˆ†åˆ©ç”¨åœ¨ç¬¬å››ç« ä¸­å­¦åˆ°çš„æœ‰å…³ `Trainer` API å’Œ Accelerate åº“çš„çŸ¥è¯†ï¼Œä»¥åŠåœ¨ç¬¬å…­ç« ä¸­å­¦åˆ°çš„ Datasets åº“å’Œç¬¬ä¸ƒç« ä¸­å­¦åˆ°çš„ Tokenizers åº“çš„çŸ¥è¯†ã€‚æˆ‘ä»¬è¿˜ä¼šåƒåœ¨ç¬¬äº”ç« ä¸­é‚£æ ·å°†ç»“æœä¸Šä¼ åˆ° Model Hubï¼Œæ‰€ä»¥è¿™çœŸçš„æ˜¯æ‰€æœ‰æ‰€å­¦å†…å®¹èä¼šè´¯é€šçš„ä¸€ç« ï¼

æœ¬ç« çš„æ¯ä¸ªéƒ¨åˆ†éƒ½å¯ä»¥ç‹¬ç«‹é˜…è¯»ï¼Œå®ƒä»¬å°†å‘ä½ å±•ç¤ºå¦‚ä½•ä½¿ç”¨ `Trainer` API æˆ–è‡ªå·±çš„è®­ç»ƒå¾ªç¯æ¥è®­ç»ƒæ¨¡å‹ï¼ŒåŒæ—¶ä½¿ç”¨ Accelerate åŠ é€Ÿã€‚ä½ å¯ä»¥éšæ„è·³è¿‡å…¶ä¸­ä»»æ„éƒ¨åˆ†ï¼Œé‡ç‚¹å…³æ³¨ä½ æœ€æ„Ÿå…´è¶£çš„éƒ¨åˆ†ï¼š `Trainer` API éå¸¸é€‚åˆå¾®è°ƒï¼ˆfine-tuningï¼‰æˆ–è®­ç»ƒæ¨¡å‹ï¼Œè€Œæ— éœ€æ‹…å¿ƒèƒŒåçš„ç»†èŠ‚ï¼Œè€Œä½¿ç”¨ `Accelerate` çš„è®­ç»ƒå¾ªç¯å°†ä½¿ä½ æ›´å®¹æ˜“è‡ªå®šä¹‰æ‰€éœ€çš„ä»»ä½•ç»“æ„ã€‚

{:else}

ä¸ºæ­¤ï¼Œä½ éœ€è¦å……åˆ†åˆ©ç”¨åœ¨ç¬¬å››ç« ä¸­å­¦åˆ°çš„æœ‰å…³ `Keras` APIã€ç¬¬å…­ç« ä¸­çš„ Datasets åº“ä»¥åŠç¬¬ä¸ƒç« ä¸­çš„ Tokenizers åº“çš„æ‰€æœ‰çŸ¥è¯†ã€‚æˆ‘ä»¬è¿˜ä¼šåƒåœ¨ç¬¬äº”ç« ä¸­é‚£æ ·å°†ç»“æœä¸Šä¼ åˆ° Model Hubï¼Œæ‰€ä»¥è¿™çœŸçš„æ˜¯æ‰€æœ‰æ‰€å­¦å†…å®¹èä¼šè´¯é€šçš„ä¸€ç« ï¼

æœ¬ç« æ¯ä¸ªéƒ¨åˆ†éƒ½å¯ä»¥ç‹¬ç«‹é˜…è¯»ã€‚

{/if}

<div custom-style="Tip-green">

å¦‚æœä½ æŒ‰é¡ºåºé˜…è¯»è¿™äº›éƒ¨åˆ†ï¼Œä½ ä¼šæ³¨æ„åˆ°å®ƒä»¬åœ¨ä»£ç å’Œæè¿°ä¸Šæœ‰è®¸å¤šç›¸ä¼¼ä¹‹å¤„ã€‚è¿™ç§é‡å¤æ˜¯æœ‰æ„ä¸ºä¹‹çš„ï¼Œè®©ä½ å¯ä»¥éšæ—¶é’»ç ”ï¼ˆæˆ–ç¨åå†å›çœ‹ï¼‰ä»»ä½•ä½ æ„Ÿå…´è¶£çš„ä»»åŠ¡å¹¶æ‰¾åˆ°ä¸€ä¸ªå®Œæ•´çš„å¯è¿è¡Œç¤ºä¾‹ã€‚

</div>


## 8.1 Token åˆ†ç±» 

æˆ‘ä»¬å°†é¦–å…ˆæ¢è®¨çš„åº”ç”¨æ˜¯ Token åˆ†ç±»ã€‚è¿™ä¸ªé€šç”¨ä»»åŠ¡æ¶µç›–äº†æ‰€æœ‰å¯ä»¥è¡¨è¿°ä¸ºâ€œç»™å¥å­ä¸­çš„è¯æˆ–å­—è´´ä¸Šæ ‡ç­¾â€çš„é—®é¢˜ï¼Œä¾‹å¦‚ï¼š

- **å®ä½“å‘½åè¯†åˆ« ï¼ˆNERï¼‰**ï¼šæ‰¾å‡ºå¥å­ä¸­çš„å®ä½“ï¼ˆå¦‚äººç‰©ã€åœ°ç‚¹æˆ–ç»„ç»‡ï¼‰ã€‚è¿™å¯ä»¥é€šè¿‡ä¸ºæ¯ä¸ªå®ä½“æˆ–â€œæ— å®ä½“â€æŒ‡å®šä¸€ä¸ªç±»åˆ«çš„æ ‡ç­¾ã€‚
- **è¯æ€§æ ‡æ³¨ ï¼ˆPOSï¼‰**ï¼šå°†å¥å­ä¸­çš„æ¯ä¸ªå•è¯æ ‡è®°ä¸ºå¯¹åº”äºç‰¹å®šçš„è¯æ€§ï¼ˆå¦‚åè¯ã€åŠ¨è¯ã€å½¢å®¹è¯ç­‰ï¼‰ã€‚
- **åˆ†å—ï¼ˆchunkingï¼‰**ï¼šæ‰¾å‡ºå±äºåŒä¸€å®ä½“çš„ tokens è¿™ä¸ªä»»åŠ¡ï¼ˆå¯ä»¥ä¸è¯æ€§æ ‡æ³¨æˆ–å‘½åå®ä½“è¯†åˆ«ç»“åˆï¼‰å¯ä»¥è¢«æè¿°ä¸ºå°†ä½äºå—å¼€å¤´çš„ token èµ‹äºˆä¸€ä¸ªæ ‡ç­¾ï¼ˆé€šå¸¸æ˜¯ â€œ `B-` â€ ï¼ˆBeginï¼‰ï¼‰ï¼Œå°†ä½äºå—å†…çš„ tokens èµ‹äºˆå¦ä¸€ä¸ªæ ‡ç­¾ï¼ˆé€šå¸¸æ˜¯ â€œ `I-` â€ï¼ˆinnerï¼‰ï¼‰ï¼Œå°†ä¸å±äºä»»ä½•å—çš„ tokens èµ‹äºˆç¬¬ä¸‰ä¸ªæ ‡ç­¾ï¼ˆé€šå¸¸æ˜¯ â€œ `O` â€ ï¼ˆouterï¼‰ï¼‰ã€‚

å½“ç„¶ï¼Œè¿˜æœ‰å¾ˆå¤šå…¶ä»–ç±»å‹çš„ token åˆ†ç±»é—®é¢˜ï¼›è¿™äº›åªæ˜¯å‡ ä¸ªæœ‰ä»£è¡¨æ€§çš„ä¾‹å­ã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†åœ¨ NER ä»»åŠ¡ä¸Šå¾®è°ƒæ¨¡å‹ ï¼ˆBERTï¼‰ï¼Œç„¶åè¯¥æ¨¡å‹å°†èƒ½å¤Ÿè®¡ç®—å¦‚ä¸‹é¢„æµ‹ï¼š

![NER çš„ä¸€ä¸ªæ¼”ç¤ºä¾‹å­](./assets/ner-example.jpg)

![æ¨¡å‹çš„è¯„ä¼°ç»“æœ](./assets/model-eval-bert-finetuned-ner.png)

ä½ å¯ä»¥ [åœ¨è¿™é‡Œ](https://huggingface.co/huggingface-course/bert-finetuned-ner?text=My+name+is+Sylvain+and+I+work+at+Hugging+Face+in+Brooklyn)(https://huggingface.co/huggingface-course/bert-finetuned-ner?text=My+name+is+Sylvain+and+I+work+at+Hugging+Face+in+Brooklyn) ã€‚æ‰¾åˆ°æˆ‘ä»¬å°†è®­ç»ƒå¹¶ä¸Šä¼ åˆ° Hub çš„æ¨¡å‹ï¼Œå¯ä»¥å°è¯•è¾“å…¥ä¸€äº›å¥å­çœ‹çœ‹æ¨¡å‹çš„é¢„æµ‹ç»“æœã€‚

### å‡†å¤‡æ•°æ® 

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªé€‚åˆ token åˆ†ç±»çš„æ•°æ®é›†ã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ [CoNLL-2003 æ•°æ®é›†](https://huggingface.co/datasets/conll2003)(https://huggingface.co/datasets/conll2003) ï¼Œè¯¥æ•°æ®é›†åŒ…å«äº†æ¥è‡ªè·¯é€ç¤¾çš„æ–°é—»æ•…äº‹ã€‚

<div custom-style="Tip-green">

ğŸ’¡ åªè¦ä½ çš„æ•°æ®é›†ç”±å¸¦æœ‰ç›¸åº”æ ‡ç­¾çš„åˆ†è¯æ–‡æœ¬ç»„æˆï¼Œä½ å°±èƒ½å¤Ÿå°†è¿™é‡Œæè¿°çš„æ•°æ®å¤„ç†è¿‡ç¨‹åº”ç”¨åˆ°ä½ è‡ªå·±çš„æ•°æ®é›†ã€‚å¦‚æœéœ€è¦å¤ä¹ å¦‚ä½•åœ¨ `Dataset` ä¸­åŠ è½½è‡ªå®šä¹‰æ•°æ®é›†ï¼Œè¯·å¤ä¹ ç¬¬å…­ç« ã€‚

</div>

#### CoNLL-2003 æ•°æ®é›† 

è¦åŠ è½½ CoNLL-2003 æ•°æ®é›†ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨Datasets åº“çš„ `load_dataset()` æ–¹æ³•ï¼š

```python
from datasets import load_dataset

raw_datasets = load_dataset("conll2003")
```

è¿™å°†ä¸‹è½½å¹¶ç¼“å­˜æ•°æ®é›†ï¼Œå°±åƒå’Œæˆ‘ä»¬åœ¨ç¬¬å››ç« åŠ è½½ GLUE MRPC æ•°æ®é›†ä¸€æ ·ã€‚æŸ¥çœ‹è¿™ä¸ªå¯¹è±¡å¯ä»¥è®©æˆ‘ä»¬çœ‹åˆ°å­˜åœ¨å“ªäº›åˆ—ï¼Œä»¥åŠè®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†ä¹‹é—´æ˜¯å¦‚ä½•åˆ†å‰²çš„ï¼š

```python
raw_datasets
```

```python
DatasetDict({
    train: Dataset({
        features: ['chunk_tags', 'id', 'ner_tags', 'pos_tags', 'tokens'],
        num_rows: 14041
    })
    validation: Dataset({
        features: ['chunk_tags', 'id', 'ner_tags', 'pos_tags', 'tokens'],
        num_rows: 3250
    })
    test: Dataset({
        features: ['chunk_tags', 'id', 'ner_tags', 'pos_tags', 'tokens'],
        num_rows: 3453
    })
})
```

çœ‹åˆ°æ•°æ®é›†åŒ…å«äº†æˆ‘ä»¬ä¹‹å‰æåˆ°çš„ä¸‰é¡¹ä»»åŠ¡çš„æ ‡ç­¾ï¼šå‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ã€è¯æ€§æ ‡æ³¨ï¼ˆPOSï¼‰ä»¥åŠåˆ†å—ï¼ˆchunkingï¼‰ã€‚è¿™ä¸ªæ•°æ®é›†ä¸å…¶ä»–æ•°æ®é›†çš„ä¸€ä¸ªæ˜¾è‘—åŒºåˆ«åœ¨äºï¼Œè¾“å…¥æ–‡æœ¬å¹¶éä»¥å¥å­æˆ–æ–‡æ¡£çš„å½¢å¼å‘ˆç°ï¼Œè€Œæ˜¯ä»¥å•è¯åˆ—è¡¨çš„å½¢å¼ï¼ˆæœ€åä¸€åˆ—è¢«ç§°ä¸º `tokens` ï¼Œä¸è¿‡ `tokens` åˆ—ä¿å­˜çš„è¿˜æ˜¯å•è¯ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œè¿™äº›é¢„å…ˆåˆ†è¯çš„è¾“å…¥ä»éœ€è¦ç»è¿‡ tokenizer è¿›è¡Œå­è¯åˆ†è¯å¤„ç†ï¼‰ã€‚

æˆ‘ä»¬æ¥çœ‹çœ‹è®­ç»ƒé›†çš„ç¬¬ä¸€ä¸ªå…ƒç´ ï¼š

```python
raw_datasets["train"][0]["tokens"]
```

```python
['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']
```

ç”±äºæˆ‘ä»¬è¦è¿›è¡Œå‘½åå®ä½“è¯†åˆ«ï¼Œè®©æˆ‘ä»¬æŸ¥çœ‹ä¸€ä¸‹ NER æ ‡ç­¾ï¼š

```python
raw_datasets["train"][0]["ner_tags"]
```

```python
[3, 0, 7, 0, 0, 0, 7, 0, 0]
```

è¿™äº›æ˜¯ä¸ºè®­ç»ƒå‡†å¤‡çš„æ•´æ•°ç±»åˆ«æ ‡ç­¾ï¼Œä½†å½“æˆ‘ä»¬æƒ³è¦æ£€æŸ¥æ•°æ®æ—¶ï¼Œå®ƒä»¬ä¸æ˜¯å¾ˆç›´è§‚ã€‚å°±åƒåœ¨æ–‡æœ¬åˆ†ç±»ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æŸ¥çœ‹æ•°æ®é›†çš„ `features` å±æ€§æ¥è®¿é—®è¿™äº›æ•´æ•°å’Œæ ‡ç­¾åç§°ä¹‹é—´çš„å¯¹åº”å…³ç³»ï¼š

```python
ner_feature = raw_datasets["train"].features["ner_tags"]
ner_feature
```

```python
Sequence(feature=ClassLabel(num_classes=9, names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], names_file=None, id=None), length=-1, id=None)
```

å› æ­¤ï¼Œè¿™ä¸€åˆ—åŒ…å«çš„å…ƒç´ æ˜¯ `ClassLabels` çš„åºåˆ—ã€‚åºåˆ—å…ƒç´ çš„ç±»å‹åœ¨ `ner_feature` çš„ `feature` ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æŸ¥çœ‹è¯¥ `feature` çš„ `names` å±æ€§æ¥è®¿é—®æ ‡ç­¾åç§°çš„åˆ—è¡¨ï¼š

```python
label_names = ner_feature.feature.names
label_names
```

```python
['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']
```

æˆ‘ä»¬åœ¨ç¬¬ä¸ƒç« ï¼Œæ·±å…¥ç ”ç©¶ `token-classification` ç®¡é“æ—¶å·²ç»çœ‹åˆ°äº†è¿™äº›æ ‡ç­¾ æˆ‘ä»¬åœ¨è¿™é‡Œè¿›è¡Œä¸€ä¸ªå¿«é€Ÿçš„å›é¡¾ï¼š

- `O` è¡¨ç¤ºè¿™ä¸ªè¯ä¸å¯¹åº”ä»»ä½•å®ä½“ã€‚
- `B-PER` / `I-PER` æ„å‘³ç€è¿™ä¸ªè¯å¯¹åº”äºäººåå®ä½“çš„å¼€å¤´/å†…éƒ¨ã€‚
- `B-ORG` / `I-ORG` çš„æ„æ€æ˜¯è¿™ä¸ªè¯å¯¹åº”äºç»„ç»‡åç§°å®ä½“çš„å¼€å¤´/å†…éƒ¨ã€‚
- `B-LOC` / `I-LOC` æŒ‡çš„æ˜¯æ˜¯è¿™ä¸ªè¯å¯¹åº”äºåœ°åå®ä½“çš„å¼€å¤´/å†…éƒ¨ã€‚
- `B-MISC` / `I-MISC` è¡¨ç¤ºè¯¥è¯å¯¹åº”äºä¸€ä¸ªæ‚é¡¹å®ä½“çš„å¼€å¤´/å†…éƒ¨ã€‚

ç°åœ¨è§£ç æˆ‘ä»¬ä¹‹å‰çœ‹åˆ°çš„æ ‡ç­¾ï¼Œæˆ‘ä»¬å¾—åˆ°ä»¥ä¸‹çš„è¾“å‡ºï¼š

```python
words = raw_datasets["train"][0]["tokens"]
labels = raw_datasets["train"][0]["ner_tags"]
line1 = ""
line2 = ""
for word, label in zip(words, labels):
    full_label = label_names[label]
    max_length = max(len(word), len(full_label))
    line1 += word + " " * (max_length - len(word) + 1)
    line2 += full_label + " " * (max_length - len(full_label) + 1)

print(line1)
print(line2)
```

```python
'EU    rejects German call to boycott British lamb .'
'B-ORG O       B-MISC O    O  O       B-MISC  O    O'
```

æˆ‘ä»¬è¿˜å¯ä»¥æŸ¥çœ‹è®­ç»ƒé›†ä¸­ç´¢å¼•ä¸º 4 çš„å…ƒç´ ï¼Œå®ƒæ˜¯ä¸€ä¸ªåŒæ—¶åŒ…å« `B-` å’Œ `I-` æ ‡ç­¾çš„ä¾‹å­ï¼š

```python
'Germany \'s representative to the European Union \'s veterinary committee Werner Zwingmann said on Wednesday consumers should buy sheepmeat from countries other than Britain until the scientific advice was clearer .'
'B-LOC   O  O              O  O   B-ORG    I-ORG O  O          O         B-PER  I-PER     O    O  O         O         O      O   O         O    O         O     O    B-LOC   O     O   O          O      O   O       O'
```

æ­£å¦‚æˆ‘ä»¬åœ¨ä¸Šé¢çš„è¾“å‡ºä¸­æ‰€çœ‹åˆ°çš„ï¼Œè·¨è¶Šä¸¤ä¸ªå•è¯çš„å®ä½“ï¼Œå¦‚â€œEuropean Unionâ€å’Œâ€œWerner Zwingmannâ€ï¼Œæ¨¡å‹æŠŠç¬¬ä¸€ä¸ªå•è¯æ ‡æ³¨ä¸ºäº† B- æ ‡ç­¾ï¼Œä¸ºç¬¬äºŒä¸ªå•è¯æ ‡è®°ä¸ºäº† I- æ ‡ç­¾ã€‚

<div custom-style="Tip-green">

âœï¸ **è½®åˆ°ä½ äº†ï¼** ä½¿ç”¨è¯æ€§æ ‡æ³¨ ï¼ˆPOSï¼‰æˆ–åˆ†å—ï¼ˆchunkingï¼‰è¯†åˆ«åŒä¸€ä¸ªå¥å­ï¼ŒæŸ¥çœ‹è¾“å‡ºçš„ç»“æœã€‚

</div>

#### å¤„ç†æ•°æ® 

åƒå¾€å¸¸ä¸€æ ·ï¼Œæˆ‘ä»¬çš„æ–‡æœ¬éœ€è¦è½¬æ¢ä¸º Token IDï¼Œç„¶åæ¨¡å‹æ‰èƒ½ç†è§£å®ƒä»¬ã€‚æ­£å¦‚æˆ‘ä»¬åœ¨ç¬¬ä¸ƒç« æ‰€å­¦çš„é‚£æ ·ã€‚ä¸è¿‡åœ¨ tokens åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œä¸€ä¸ªå¾ˆå¤§çš„åŒºåˆ«æ˜¯æˆ‘ä»¬æœ‰ pre-tokenized çš„è¾“å…¥ã€‚å¹¸è¿çš„æ˜¯ï¼Œtokenizer API å¯ä»¥å¾ˆå®¹æ˜“åœ°å¤„ç†è¿™ä¸ªé—®é¢˜ï¼›æˆ‘ä»¬åªéœ€è¦é€šè¿‡ä¸€ä¸ªç‰¹æ®Šçš„æ ‡å¿—å‘Šè¯‰ tokenizerã€‚

é¦–å…ˆï¼Œè®©æˆ‘ä»¬åˆ›å»º `tokenizer` å¯¹è±¡ã€‚å¦‚å‰æ‰€è¿°ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ BERT é¢„è®­ç»ƒæ¨¡å‹ï¼Œå› æ­¤æˆ‘ä»¬å°†ä»ä¸‹è½½å¹¶ç¼“å­˜å…³è”çš„ tokenizer å¼€å§‹ï¼š

```python
from transformers import AutoTokenizer

model_checkpoint = "bert-base-cased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
```

ä½ å¯ä»¥æ›´æ¢æŠŠ `model_checkpoint` æ›´æ¢ä¸º [Hub](https://huggingface.co/models)(https://huggingface.co/models) ä¸Šä»»ä½•ä½ å–œæ¬¢çš„å…¶ä»–å‹å·ï¼Œæˆ–ä½¿ç”¨ä½ æœ¬åœ°ä¿å­˜çš„é¢„è®­ç»ƒæ¨¡å‹å’Œ tokenizerã€‚å”¯ä¸€çš„é™åˆ¶æ˜¯ tokenizer éœ€è¦ç”± Tokenizers åº“æ”¯æŒï¼Œå¹¶ä¸”æœ‰ä¸€ä¸ªâ€œå¿«é€Ÿâ€ç‰ˆæœ¬å¯ç”¨ã€‚ä½ å¯ä»¥åœ¨ [è¿™å¼ å¤§è¡¨](https://huggingface.co/transformers/#supported-frameworks)(https://huggingface.co/transformers/#supported-frameworks) ä¸Šçœ‹åˆ°æ‰€æœ‰å¸¦æœ‰å¿«é€Ÿç‰ˆæœ¬çš„æ¶æ„ï¼Œæˆ–è€…æ£€æŸ¥ä½ å¯ä»¥é€šè¿‡æŸ¥çœ‹å®ƒ `is_fast` å±æ€§æ¥æ£€æµ‹æ­£åœ¨ä½¿ç”¨çš„ `tokenizer` å¯¹è±¡æ˜¯å¦ç”± Tokenizers æ”¯æŒï¼š

```python
tokenizer.is_fast
```

```python
True
```

æˆ‘ä»¬å¯ä»¥åƒå¾€å¸¸ä¸€æ ·ä½¿ç”¨æˆ‘ä»¬çš„ `tokenizer` å¯¹é¢„å…ˆåˆ†è¯çš„è¾“å…¥è¿›è¡Œå­è¯åˆ†è¯ï¼Œåªéœ€é¢å¤–æ·»åŠ  `is_split_into_words=True` ï¼š

```python
inputs = tokenizer(raw_datasets["train"][0]["tokens"], is_split_into_words=True)
inputs.tokens()
```

```python
['[CLS]', 'EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'la', '##mb', '.', '[SEP]']
```

å¦‚æˆ‘ä»¬æ‰€è§ï¼Œtokenizer æ·»åŠ äº†æ¨¡å‹ä½¿ç”¨çš„ç‰¹æ®Š tokensï¼ˆåœ¨å¼€å§‹çš„ `[CLS]` ï¼Œåœ¨ç»“æŸçš„ `[SEP]` ï¼‰ï¼Œå¹¶ä¸”å¤§éƒ¨åˆ†å•è¯ä¿æŒä¸å˜ã€‚ç„¶è€Œï¼Œå•è¯ `lamb` è¢«åˆ†è¯ä¸ºä¸¤ä¸ªå­è¯ï¼Œ `la` å’Œ `##mb` ã€‚è¿™å¯¼è‡´äº†è¾“å…¥å’Œæ ‡ç­¾ä¹‹é—´çš„ä¸åŒ¹é…ï¼šæ ‡ç­¾åˆ—è¡¨åªæœ‰ 9 ä¸ªå…ƒç´ ï¼Œè€Œæˆ‘ä»¬çš„è¾“å…¥ç°åœ¨æœ‰ 12 ä¸ª tokensã€‚è§£å†³ç‰¹æ®Š tokens çš„é—®é¢˜å¾ˆå®¹æ˜“ï¼ˆæˆ‘ä»¬çŸ¥é“å®ƒä»¬åœ¨å¼€å§‹å’Œç»“æŸçš„ä½ç½®ï¼‰ï¼Œä½†æˆ‘ä»¬è¿˜éœ€è¦ç¡®ä¿æˆ‘ä»¬å°†æ‰€æœ‰çš„æ ‡ç­¾ä¸æ­£ç¡®çš„è¯å¯¹é½ã€‚

å¹¸è¿çš„æ˜¯ï¼Œç”±äºæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯å¿«é€Ÿ tokens å› æ­¤æˆ‘ä»¬å¯ä»¥ä½¿ç”¨Tokenizers è¶…èƒ½åŠ›ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬å¯ä»¥è½»æ¾åœ°å°†æ¯ä¸ª token æ˜ å°„åˆ°å…¶ç›¸åº”çš„å•è¯ï¼ˆå¦‚ç¬¬ä¸ƒç« ä¸­æ‰€å­¦ï¼‰ï¼š

```python
inputs.word_ids()
```

```python
[None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, None]
```

åªéœ€è¦ä¸€ç‚¹ç‚¹å·¥ä½œï¼Œæˆ‘ä»¬å°±å¯ä»¥æ‰©å±•æˆ‘ä»¬çš„æ ‡ç­¾åˆ—è¡¨æ¥åŒ¹é… tokensã€‚æˆ‘ä»¬å°†æ·»åŠ çš„ç¬¬ä¸€æ¡è§„åˆ™æ˜¯ï¼Œç‰¹æ®Š tokens çš„æ ‡ç­¾æ˜¯ `-100` ã€‚è¿™æ˜¯å› ä¸ºé»˜è®¤æƒ…å†µä¸‹ï¼Œ `-100` ä¼šè¢«æˆ‘ä»¬çš„æŸå¤±å‡½æ•°ï¼ˆäº¤å‰ç†µï¼‰å¿½ç•¥ã€‚ç„¶åï¼Œæ¯ä¸ª token å¾—åˆ°çš„æ ‡ç­¾ä¸å…¶æ‰€åœ¨çš„è¯çš„å¼€å§‹çš„ token ç›¸åŒï¼Œå› ä¸ºå®ƒä»¬æ˜¯åŒä¸€å®ä½“çš„ä¸€éƒ¨åˆ†ã€‚å› æ­¤å¯¹äºè¯å†…éƒ¨ä½†ä¸åœ¨å¼€å§‹ä½ç½®çš„ tokensï¼Œæˆ‘ä»¬å°† `B-` æ›¿æ¢ä¸º `I-` ï¼ˆå› ä¸ºè¯¥ token ä¸æ˜¯å®ä½“çš„å¼€å§‹ï¼‰ï¼š

```python
def align_labels_with_tokens(labels, word_ids):
    new_labels = []
    current_word = None
    for word_id in word_ids:
        if word_id != current_word:
            # æ–°å•è¯çš„å¼€å§‹!
            current_word = word_id
            label = -100 if word_id is None else labels[word_id]
            new_labels.append(label)
        elif word_id is None:
            # ç‰¹æ®Šçš„token
            new_labels.append(-100)
        else:
            # ä¸å‰ä¸€ä¸ª tokens ç±»å‹ç›¸åŒçš„å•è¯
            label = labels[word_id]
            # å¦‚æœæ ‡ç­¾æ˜¯ B-XXX æˆ‘ä»¬å°†å…¶æ›´æ”¹ä¸º I-XXX
            if label % 2 == 1:
                label += 1
            new_labels.append(label)

    return new_labels
```

è®©æˆ‘ä»¬åœ¨æˆ‘ä»¬çš„ç¬¬ä¸€å¥è¯ä¸Šè¯•ä¸€è¯•ï¼š

```python
labels = raw_datasets["train"][0]["ner_tags"]
word_ids = inputs.word_ids()
print(labels)
print(align_labels_with_tokens(labels, word_ids))
```

```python
[3, 0, 7, 0, 0, 0, 7, 0, 0]
[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100]
```

æ­£å¦‚æˆ‘ä»¬æ‰€çœ‹åˆ°çš„ï¼Œæˆ‘ä»¬çš„å‡½æ•°ä¸ºå¼€å¤´å’Œç»“å°¾çš„ä¸¤ä¸ªç‰¹æ®Š tokens æ·»åŠ äº† `-100` ï¼Œå¹¶ä¸ºåˆ‡åˆ†æˆä¸¤ä¸ª tokens çš„å•è¯æ·»åŠ äº†ä¸€ä¸ªæ–°çš„ `0` ã€‚

<div custom-style="Tip-green">

âœï¸ **è½®åˆ°ä½ äº†ï¼** æœ‰äº›ç ”ç©¶äººå‘˜æ›´å–œæ¬¢æ¯ä¸ªå•è¯åªåˆ†é…ä¸€ä¸ªæ ‡ç­¾ï¼Œå¯¹è¯¥å•è¯å…¶ä»–éƒ¨åˆ†åˆ†é… `-100` ã€‚è¿™æ˜¯ä¸ºäº†é¿å…é‚£äº›åˆ†è§£æˆè®¸å¤šå­å­è¯çš„é•¿å•è¯å¯¹æŸå¤±ä½œå‡ºè¿‡é‡çš„è´¡çŒ®ã€‚è¯·æŒ‰ç…§è¿™ä¸ªè§„åˆ™ï¼Œæ”¹å˜ä¹‹å‰çš„å‡½æ•°ï¼Œä½¿æ ‡ç­¾ä¸ inputs ID å¯¹é½ã€‚

</div>

ä¸ºäº†é¢„å¤„ç†æˆ‘ä»¬çš„æ•´ä¸ªæ•°æ®é›†ï¼Œæˆ‘ä»¬éœ€è¦å¯¹æ‰€æœ‰è¾“å…¥è¿›è¡Œ tokenizeï¼Œå¹¶ä½¿ç”¨ `align_labels_with_tokens()` å‡½æ•°å¤„ç†æ‰€æœ‰æ ‡ç­¾ã€‚ä¸ºäº†å……åˆ†åˆ©ç”¨æˆ‘ä»¬å¿«é€Ÿ tokenizer çš„é€Ÿåº¦ï¼Œæœ€å¥½æ˜¯åŒæ—¶å¯¹å¤§é‡æ–‡æœ¬ tokenizeï¼Œæ‰€ä»¥æˆ‘ä»¬å°†ç¼–å†™ä¸€ä¸ªå¤„ç†ä¸€ç»„ç¤ºä¾‹çš„å‡½æ•°ï¼Œå¹¶ä½¿ç”¨å¸¦æœ‰ `batched=True` é€‰é¡¹çš„ `Dataset.map()` æ–¹æ³•ã€‚ä¸æˆ‘ä»¬ä¹‹å‰çš„ç¤ºä¾‹å”¯ä¸€ä¸åŒçš„æ˜¯ï¼Œå½“ tokenizer çš„è¾“å…¥æ˜¯æ–‡æœ¬åˆ—è¡¨ï¼ˆæˆ–ç¤ºä¾‹ä¸­å•è¯çš„åˆ—è¡¨çš„åˆ—è¡¨ï¼‰æ—¶ï¼Œ `word_ids()` å‡½æ•°éœ€è¦è·å–æˆ‘ä»¬ç¤ºä¾‹ä¸­å•è¯ ID çš„ç´¢å¼•ï¼Œæ‰€ä»¥æˆ‘ä»¬ä¹Ÿæ·»åŠ äº†è¿™ä¸ªåŠŸèƒ½ï¼š

```python
def tokenize_and_align_labels(examples):
    tokenized_inputs = tokenizer(
        examples["tokens"], truncation=True, is_split_into_words=True
    )
    all_labels = examples["ner_tags"]
    new_labels = []
    for i, labels in enumerate(all_labels):
        word_ids = tokenized_inputs.word_ids(i)
        new_labels.append(align_labels_with_tokens(labels, word_ids))

    tokenized_inputs["labels"] = new_labels
    return tokenized_inputs
```

æ³¨æ„ï¼Œæˆ‘ä»¬è¿˜æ²¡æœ‰å¡«å……æˆ‘ä»¬çš„è¾“å…¥ï¼Œæˆ‘ä»¬å°†åœ¨ç¨åè¿›è¡Œï¼Œå³åœ¨ä½¿ç”¨æ•°æ®æ•´ç†å™¨åˆ›å»º batch æ—¶ã€‚

æˆ‘ä»¬ç°åœ¨å¯ä»¥ä¸€æ¬¡æ€§ä½¿ç”¨æ‰€æœ‰é¢„å¤„ç†å¤„ç†æ•°æ®é›†çš„å…¶ä»–éƒ¨åˆ†ï¼š

```python
tokenized_datasets = raw_datasets.map(
    tokenize_and_align_labels,
    batched=True,
    remove_columns=raw_datasets["train"].column_names,
)
```

æˆ‘ä»¬å·²ç»å®Œæˆäº†æœ€å›°éš¾çš„éƒ¨åˆ†ï¼ç°åœ¨æ•°æ®å·²ç»ç»è¿‡äº†é¢„å¤„ç†ï¼Œå®é™…çš„è®­ç»ƒè¿‡ç¨‹å°†ä¼šä¸æˆ‘ä»¬åœ¨ç¬¬å››ç« æ‰€åšçš„å¾ˆç›¸ä¼¼ã€‚

### ä½¿ç”¨ Trainer API å¾®è°ƒæ¨¡å‹ 

ä½¿ç”¨ `Trainer` / `Keras` çš„å®é™…ä»£ç ä¼šå’Œä»¥å‰éå¸¸ç›¸ä¼¼ï¼Œå”¯ä¸€çš„å˜åŒ–æ˜¯å¦‚ä½•å°†æ•°æ®æ•´ç†æˆ batch ä»¥åŠè¯„ä¼°è®¡ç®—å‡½æ•°çš„å˜åŒ–ã€‚

#### æ•´ç†æ•°æ® 

æˆ‘ä»¬ä¸èƒ½åƒç¬¬å››ç« é‚£æ ·åªä½¿ç”¨ä¸€ä¸ª `DataCollatorWithPadding` ï¼Œå› ä¸ºé‚£æ ·åªä¼šå¡«å……è¾“å…¥ï¼ˆinputs IDã€æ³¨æ„æ©ç å’Œ tokens ç±»å‹ IDï¼‰ã€‚åœ¨è¿™é‡Œæˆ‘ä»¬åº”è¯¥å¯¹æ ‡ç­¾ä¹Ÿä½¿ç”¨ä¸è¾“å…¥å®Œå…¨ç›¸åŒçš„æ–¹å¼å¡«å……ï¼Œä»¥ä¿è¯å®ƒä»¬çš„å¤§å°ç›¸åŒã€‚æˆ‘ä»¬å°†ä½¿ç”¨ `-100` è¿›è¡Œå¡«å……ï¼Œä»¥ä¾¿åœ¨æŸå¤±è®¡ç®—ä¸­å¿½ç•¥ç›¸åº”çš„é¢„æµ‹ã€‚

è¿™äº›å¯ä»¥ç”± [`DataCollatorForTokenClassification`](https://huggingface.co/transformers/main_classes/data_collator.html#datacollatorfortokenclassification)(https://huggingface.co/transformers/main_classes/data_collator.html#datacollatorfortokenclassification) å®ç°ã€‚å®ƒæ˜¯ä¸€ä¸ªå¸¦æœ‰å¡«å……åŠŸèƒ½çš„æ•°æ®æ•´ç†å™¨ï¼Œä½¿ç”¨æ—¶åªéœ€è¦ä¼ å…¥ç”¨äºé¢„å¤„ç†è¾“å…¥çš„ `tokenizer` ï¼š

{#if fw === 'pt'}

```python
#####Pytorch}
from transformers import DataCollatorForTokenClassification

data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)####end
```

{:else}

```python
#####TensorFlow}
from transformers import DataCollatorForTokenClassification

data_collator = DataCollatorForTokenClassification(
    tokenizer=tokenizer, return_tensors="tf"
)####end
```

{/if}

ä¸ºäº†åœ¨å‡ ä¸ªæ ·æœ¬ä¸Šæµ‹è¯•è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨è®­ç»ƒé›†ä¸­çš„å‡ ä¸ªç¤ºä¾‹ä¸Šè°ƒç”¨å®ƒï¼š

```python
batch = data_collator([tokenized_datasets["train"][i] for i in range(2)])
batch["labels"]
```

```python
tensor([[-100,    3,    0,    7,    0,    0,    0,    7,    0,    0,    0, -100],
        [-100,    1,    2, -100, -100, -100, -100, -100, -100, -100, -100, -100]])
```

è®©æˆ‘ä»¬å°†å…¶ä¸æ•°æ®é›†ä¸­ç¬¬ä¸€ä¸ªå’Œç¬¬äºŒä¸ªå…ƒç´ çš„æ ‡ç­¾è¿›è¡Œæ¯”è¾ƒï¼š

```python
for i in range(2):
    print(tokenized_datasets["train"][i]["labels"])
```

```python
[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100]
[-100, 1, 2, -100]
```

{#if fw === 'pt'}

æ­£å¦‚æˆ‘ä»¬æ‰€çœ‹åˆ°çš„ï¼Œç¬¬äºŒç»„æ ‡ç­¾çš„é•¿åº¦å·²ç»ä½¿ç”¨ `-100` å¡«å……åˆ°ä¸ç¬¬ä¸€ç»„æ ‡ç­¾ç›¸åŒã€‚

{:else}

æˆ‘ä»¬çš„æ•°æ®æ•´ç†å™¨å·²å‡†å¤‡å°±ç»ªï¼ç°åœ¨è®©æˆ‘ä»¬ä½¿ç”¨ `to_tf_dataset()` æ–¹æ³•åˆ›å»ºä¸€ä¸ª `tf.data.Dataset` ã€‚ä½ è¿˜å¯ä»¥ä½¿ç”¨ `model.prepare_tf_dataset()` æ¥ä½¿ç”¨æ›´å°‘çš„æ¨¡æ¿ä»£ç æ¥å®Œæˆæ­¤æ“ä½œâ€”â€”ä½ å°†åœ¨æœ¬ç« çš„å…¶ä»–å°èŠ‚ä¸­çœ‹åˆ°è¿™ä¸€ç‚¹ã€‚
```python
#####TensorFlow}
tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "labels", "token_type_ids"],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=16,
)

tf_eval_dataset = tokenized_datasets["validation"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "labels", "token_type_ids"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=16,
)####end
```

ä¸‹ä¸€ç«™ï¼šæ¨¡å‹æœ¬èº«ã€‚

{/if}

{#if fw === 'tf'}

#### å®šä¹‰æ¨¡å‹

ç”±äºæˆ‘ä»¬æ­£åœ¨ç ”ç©¶ Token åˆ†ç±»é—®é¢˜ï¼Œå› æ­¤æˆ‘ä»¬å°†ä½¿ç”¨ `TFAutoModelForTokenClassification` ç±»ã€‚å®šä¹‰æ­¤æ¨¡å‹æ—¶è¦è®°å¾—ä¼ é€’æˆ‘ä»¬æ ‡ç­¾çš„æ•°é‡ï¼Œæœ€ç®€å•æ–¹æ³•æ˜¯å°†è¯¥æ•°å­—ä¼ é€’ç»™ `num_labels` å‚æ•°ï¼Œä½†æ˜¯å¦‚æœæˆ‘ä»¬æƒ³è¦ä¸€ä¸ªå°±åƒæˆ‘ä»¬åœ¨æœ¬èŠ‚å¼€å¤´çœ‹åˆ°çš„é‚£æ ·çš„æ¨ç†å°éƒ¨ä»¶ï¼Œæœ€å¥½è®¾ç½®æ­£ç¡®çš„æ ‡ç­¾å¯¹åº”å…³ç³»ã€‚

åº”è¯¥ç”±ä¸¤ä¸ªå­—å…¸ `id2label` å’Œ `label2id` æ¥è®¾ç½®å®ƒä»¬ï¼Œè¿™ä¸¤ä¸ªå­—å…¸åŒ…å«ä» ID åˆ°æ ‡ç­¾çš„æ˜ å°„ä»¥åŠåå‘çš„æ˜ å°„ï¼š

```python
#####TensorFlow}
id2label = {str(i): label for i, label in enumerate(label_names)}
label2id = {v: k for k, v in id2label.items()}####end
```

ç°åœ¨æˆ‘ä»¬åªéœ€å°†å®ƒä»¬ä¼ é€’ç»™ `TFAutoModelForTokenClassification.from_pretrained()` æ–¹æ³•ï¼Œå®ƒä»¬å°±ä¼šè¢«ä¿å­˜åœ¨æ¨¡å‹çš„é…ç½®ä¸­ï¼Œç„¶åè¢«æ­£ç¡®åœ°ä¿å­˜å’Œä¸Šä¼ åˆ° Hubï¼š

```python
#####TensorFlow}
from transformers import TFAutoModelForTokenClassification

model = TFAutoModelForTokenClassification.from_pretrained(
    model_checkpoint,
    id2label=id2label,
    label2id=label2id,
)####end
```

å°±åƒæˆ‘ä»¬åœ¨ç¬¬å››ç« ä¸­å®šä¹‰ `TFAutoModelForTokenClassification` ç±»ä¸€æ · åˆ›å»ºæ¨¡å‹ä¼šå‘å‡ºä¸€ä¸ªè­¦å‘Šï¼Œæç¤ºä¸€äº›æƒé‡æœªè¢«ä½¿ç”¨ï¼ˆæ¥è‡ªé¢„è®­ç»ƒå¤´çš„æƒé‡ï¼‰å’Œä¸€äº›å…¶ä»–æƒé‡è¢«éšæœºåˆå§‹åŒ–ï¼ˆæ¥è‡ªæ–° Token åˆ†ç±»å¤´çš„æƒé‡ï¼‰ï¼Œæˆ‘ä»¬å°†è¦è®­ç»ƒè¿™äº›æƒé‡ã€‚æˆ‘ä»¬å°†åœ¨ä¸€ä¼šå„¿è¿›è¡Œè®­ç»ƒï¼Œä½†é¦–å…ˆè®©æˆ‘ä»¬ç¡®è®¤ä¸€ä¸‹æˆ‘ä»¬çš„æ¨¡å‹æ˜¯å¦å…·æœ‰æ­£ç¡®çš„æ ‡ç­¾æ•°é‡ï¼š

```python
#####TensorFlow}
model.config.num_labels####end
```

```python
#####TensorFlow}
9####end
```

<div custom-style="Tip-yellow">

âš ï¸ å¦‚æœä½ çš„æ¨¡å‹çš„æ ‡ç­¾æ•°é‡é”™è¯¯ï¼Œé‚£ä¹ˆåœ¨åé¢è°ƒç”¨ `model.fit()` æ—¶ï¼Œä½ ä¼šå¾—åˆ°ä¸€ä¸ªæ™¦æ¶©çš„é”™è¯¯ã€‚è¿™å¯èƒ½ä¼šä»¤äººçƒ¦æ¼ï¼Œæ‰€ä»¥ç¡®ä¿ä½ åšäº†è¿™ä¸ªæ£€æŸ¥ï¼Œç¡®è®¤ç¡®è®¤ä½ çš„æ ‡ç­¾æ•°é‡æ˜¯æ­£ç¡®ã€‚

</div>

#### å¾®è°ƒæ¨¡å‹

ç°åœ¨ï¼Œæˆ‘ä»¬å·²å‡†å¤‡å¥½è®­ç»ƒæ¨¡å‹äº†ï¼ä¸è¿‡ï¼Œæˆ‘ä»¬é¦–å…ˆè¦åšä¸¤ä»¶äº‹ï¼šåº”è¯¥ç™»å½•åˆ° Hugging Face å¹¶å®šä¹‰æˆ‘ä»¬çš„è®­ç»ƒè¶…å‚æ•°ã€‚å¦‚æœä½ åœ¨ notebook ä¸Šå·¥ä½œï¼Œæœ‰ä¸€ä¸ªä¾¿æ·å‡½æ•°å¯ä»¥å¸®åŠ©ä½ åšåˆ°è¿™ä¸€ç‚¹ï¼š

```python
#####TensorFlow}
from huggingface_hub import notebook_login

notebook_login()####end
```

è¿™å°†æ˜¾ç¤ºä¸€ä¸ªå°éƒ¨ä»¶ï¼Œä½ å¯ä»¥åœ¨å…¶ä¸­è¾“å…¥ä½ çš„ Hugging Face è´¦å·å’Œå¯†ç ã€‚

å¦‚æœä½ ä¸æ˜¯åœ¨ notebook ä¸Šå·¥ä½œï¼Œåªéœ€åœ¨ç»ˆç«¯ä¸­è¾“å…¥ä»¥ä¸‹è¡Œï¼š

```python
#####TensorFlow}
huggingface-cli login####end
```

ç™»å½•åï¼Œæˆ‘ä»¬å¯ä»¥å‡†å¤‡ç¼–è¯‘æˆ‘ä»¬æ¨¡å‹æ‰€éœ€è¦çš„æ‰€æœ‰é…ç½®ã€‚Transformers æä¾›äº†ä¸€ä¸ªä¾¿æ·çš„ `create_optimizer()` å‡½æ•°ï¼Œå®ƒå°†ç»™ä½ ä¸€ä¸ªå¸¦æœ‰é€‚å½“çš„æƒé‡è¡°å‡å’Œå­¦ä¹ ç‡è¡°å‡è®¾ç½®çš„ `AdamW` ä¼˜åŒ–å™¨ï¼Œä¸å†…ç½®çš„ `Adam` ä¼˜åŒ–å™¨ç›¸ä¼¼ï¼Œä¸è¿‡è¿™ä¸¤ä¸ªä¼˜åŒ–æŠ€å·§éƒ½å°†æé«˜æ¨¡å‹çš„æ€§èƒ½ï¼š

```python
#####TensorFlow}
from transformers import create_optimizer
import tensorflow as tf

## åœ¨æ··åˆç²¾åº¦ float16 ä¸­è¿›è¡Œè®­ç»ƒ
tf.keras.mixed_precision.set_global_policy("mixed_float16")

# è®­ç»ƒæ­¥æ•°æ˜¯æ•°æ®é›†ä¸­çš„æ ·æœ¬æ•°é‡,é™¤ä»¥ batch å¤§å°,ç„¶åä¹˜ä»¥æ€»çš„ epoch æ•°ã€‚
# æ³¨æ„è¿™é‡Œçš„ tf_train_dataset æ˜¯ batch å½¢å¼çš„ tf.data.Dataset,
# è€Œä¸æ˜¯åŸå§‹çš„ Hugging Face Dataset ,æ‰€ä»¥ä½¿ç”¨ len() è®¡ç®—å®ƒçš„é•¿åº¦å·²ç»æ˜¯ num_samples // batch_sizeã€‚
num_epochs = 3
num_train_steps = len(tf_train_dataset) * num_epochs

optimizer, schedule = create_optimizer(
    init_lr=2e-5,
    num_warmup_steps=0,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)
model.compile(optimizer=optimizer)####end
```

è¯·æ³¨æ„ï¼Œæˆ‘ä»¬æ²¡æœ‰ç»™ `compile()` æä¾› `loss` å‚æ•°ã€‚è¿™æ˜¯å› ä¸ºæ¨¡å‹å®é™…ä¸Šå¯ä»¥å†…éƒ¨è®¡ç®—æŸå¤± â€”â€” å¦‚æœä½ ç¼–è¯‘æ—¶æ²¡æœ‰æä¾›æŸå¤±çš„è®¡ç®—æ–¹æ³•å¹¶åœ¨è¾“å…¥å­—å…¸ä¸­æä¾›æ ‡ç­¾ï¼ˆå°±åƒæˆ‘ä»¬åœ¨æ•°æ®é›†ä¸­æ‰€åšçš„é‚£æ ·ï¼‰ï¼Œé‚£ä¹ˆæ¨¡å‹å°†ä½¿ç”¨å†…éƒ¨é»˜è®¤çš„ `loss` è®¡ç®—æ–¹æ³•è¿›è¡Œè®­ç»ƒï¼Œè¿™å–å†³äºä½ é€‰æ‹©çš„ä»»åŠ¡å’Œæ¨¡å‹ç±»å‹ã€‚

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å®šä¹‰ä¸€ä¸ª `PushToHubCallback` å›è°ƒå‡½æ•°ï¼Œä»¥ä¾¿åœ¨è®­ç»ƒæœŸé—´å°†æ¨¡å‹ä¸Šä¼ åˆ° Hubï¼Œå¹¶ä½¿ç”¨è¯¥å›è°ƒæ¥æ‹Ÿåˆæ¨¡å‹ï¼š

```python
#####TensorFlow}
from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(output_dir="bert-finetuned-ner", tokenizer=tokenizer)

model.fit(
    tf_train_dataset,
    validation_data=tf_eval_dataset,
    callbacks=[callback],
    epochs=num_epochs,
)####end
```

ä½ å¯ä»¥ä½¿ç”¨ `hub_model_id` å‚æ•°æŒ‡å®šä½ æƒ³è¦æ¨é€çš„ä»“åº“çš„å…¨åï¼ˆç‰¹åˆ«æ˜¯ï¼Œå¦‚æœä½ éœ€è¦æ¨é€ç»™æŸä¸ªç»„ç»‡ï¼Œå°±å¿…é¡»ä½¿ç”¨è¿™ä¸ªå‚æ•°ï¼‰ã€‚ä¾‹å¦‚ï¼Œå½“æˆ‘ä»¬å°†æ¨¡å‹æ¨é€åˆ° [`huggingface-course` ç»„ç»‡](https://huggingface.co/huggingface-course)(https://huggingface.co/huggingface-course)(https://huggingface.co/huggingface-course) æ—¶ï¼Œæˆ‘ä»¬æ·»åŠ äº† `hub_model_id="huggingface-course/bert-finetuned-ner"` ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œä½¿ç”¨çš„ä»“åº“å°†ä¿å­˜åœ¨ä½ çš„è´¦æˆ·ä¹‹å†…ï¼Œå¹¶ä»¥ä½ è®¾ç½®çš„è¾“å‡ºç›®å½•å‘½åï¼Œä¾‹å¦‚ `"cool_huggingface_user/bert-finetuned-ner"` ã€‚

<div custom-style="Tip-green">

ğŸ’¡ å¦‚æœä½ ä½¿ç”¨çš„è¾“å‡ºç›®å½•å·²ç»å­˜åœ¨ï¼Œé‚£ä¹ˆå®ƒéœ€è¦æ˜¯ä½ æƒ³æ¨é€åˆ° hub çš„ä»“åº“çš„æœ¬åœ°å…‹éš†ã€‚å¦‚æœä¸æ˜¯ï¼Œä½ åœ¨è°ƒç”¨ `model.fit()` æ—¶å°†æ”¶åˆ°ä¸€ä¸ªé”™è¯¯ï¼Œå¹¶éœ€è¦è®¾ç½®ä¸€ä¸ªæ–°çš„åå­—ã€‚

</div>

è¯·æ³¨æ„ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ¯æ¬¡ä¿å­˜æ¨¡å‹æ—¶ï¼ˆè¿™é‡Œæ˜¯æ¯ä¸ª epoochï¼‰ï¼Œå®ƒéƒ½ä¼šåœ¨åå°ä¸Šä¼ åˆ° Hubã€‚è¿™æ ·ï¼Œå¦‚æœ‰å¿…è¦ï¼Œä½ å°†èƒ½å¤Ÿåœ¨å¦ä¸€å°æœºå™¨ä¸Šç»§ç»­ä½ çš„è®­ç»ƒã€‚

åˆ°æ­¤é˜¶æ®µï¼Œä½ å¯ä»¥åœ¨æ¨¡å‹ Hub ä¸Šä½¿ç”¨æ¨ç†å°éƒ¨ä»¶æ¥æµ‹è¯•ä½ çš„æ¨¡å‹å¹¶ä¸ä½ çš„æœ‹å‹åˆ†äº«ã€‚ä½ å·²ç»æˆåŠŸåœ°åœ¨ä¸€ä¸ª tokens åˆ†ç±»ä»»åŠ¡ä¸Šå¾®è°ƒäº†ä¸€ä¸ªæ¨¡å‹â€”â€”æ­å–œä½ ï¼ä½†æˆ‘ä»¬çš„æ¨¡å‹çœŸçš„å¥½å—ï¼Ÿæˆ‘ä»¬åº”è¯¥æ‰¾å‡ºä¸€äº›æŒ‡æ ‡æ¥å¯¹æ¨¡å‹è¿›è¯„ä¼°ã€‚

{/if}

#### è¯„ä¼°æŒ‡æ ‡ 

{#if fw === 'pt'}

è¦è®© `Trainer` åœ¨æ¯ä¸ªå‘¨æœŸè®¡ç®—ä¸€ä¸ªæŒ‡æ ‡ï¼Œæˆ‘ä»¬éœ€è¦å®šä¹‰ä¸€ä¸ª `compute_metrics()` å‡½æ•°ï¼Œè¯¥å‡½æ•°çš„è¾“å…¥æ˜¯é¢„æµ‹å€¼å’Œæ ‡ç­¾çš„æ•°ç»„ï¼Œå¹¶è¿”å›å¸¦æœ‰æŒ‡æ ‡åç§°å’Œå€¼çš„å­—å…¸ã€‚

ç”¨äºè¯„ä¼° Token åˆ†ç±»é¢„æµ‹çš„ä¼ ç»Ÿæ¡†æ¶æ˜¯ [*seqeval*](https://github.com/chakki-works/seqeval)(https://github.com/chakki-works/seqeval)(https://github.com/chakki-works/seqeval) ã€‚è¦ä½¿ç”¨æ­¤æŒ‡æ ‡ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦å®‰è£… seqeval åº“ï¼š

```python
#####Pytorch}
!pip install seqeval####end
```

ç„¶åæˆ‘ä»¬å¯ä»¥é€šè¿‡ `evaluate.load()` å‡½æ•°åŠ è½½å®ƒï¼Œå°±åƒæˆ‘ä»¬åœ¨ç¬¬å››ç« ä¸­æ‰€åšçš„é‚£æ ·ï¼š

{:else}

ç”¨äºè¯„ä¼° Token åˆ†ç±»é¢„æµ‹çš„ä¼ ç»Ÿæ¡†æ¶æ˜¯ [*seqeval*](https://github.com/chakki-works/seqeval)(https://github.com/chakki-works/seqeval)(https://github.com/chakki-works/seqeval) ã€‚è¦ä½¿ç”¨æ­¤æŒ‡æ ‡ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦å®‰è£… seqeval åº“ï¼š

```python
#####TensorFlow}
!pip install seqeval####end
```

ç„¶åæˆ‘ä»¬å¯ä»¥é€šè¿‡ `evaluate.load()` å‡½æ•°åŠ è½½å®ƒï¼Œå°±åƒæˆ‘ä»¬åœ¨ç¬¬å››ç« ä¸­æ‰€åšçš„é‚£æ ·ï¼š

{/if}

```python
import evaluate

metric = evaluate.load("seqeval")
```

è¿™ä¸ªæŒ‡æ ‡å¹¶ä¸åƒæ ‡å‡†çš„ç²¾åº¦é‚£æ ·ï¼šå®ƒéœ€è¦å­—ç¬¦ä¸²å½¢å¼çš„æ ‡ç­¾åˆ—è¡¨è€Œä¸æ˜¯æ•´æ•°ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦åœ¨å°†å®ƒä»¬ä¼ é€’ç»™æŒ‡æ ‡ä¹‹å‰è§£ç é¢„æµ‹å€¼å’Œæ ‡ç­¾ã€‚è®©æˆ‘ä»¬çœ‹çœ‹å®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å°†è·å–æˆ‘ä»¬ç¬¬ä¸€ä¸ªè®­ç»ƒæ ·æœ¬çš„æ ‡ç­¾ï¼š

```python
labels = raw_datasets["train"][0]["ner_tags"]
labels = [label_names[i] for i in labels]
labels
```

```python
['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']
```

ç„¶åæˆ‘ä»¬å¯ä»¥é€šè¿‡æ›´æ”¹ç´¢å¼• 2 å¤„çš„å€¼æ¥ä¸ºè¿™äº›æ ‡ç­¾åˆ›å»ºå‡çš„é¢„æµ‹å€¼ï¼š

```python
predictions = labels.copy()
predictions[2] = "O"
metric.compute(predictions=[predictions], references=[labels])
```

è¯·æ³¨æ„ï¼Œè¯¥æŒ‡æ ‡çš„è¾“å…¥æ˜¯é¢„æµ‹åˆ—è¡¨ï¼ˆä¸ä»…ä»…æ˜¯ä¸€ä¸ªï¼‰å’Œæ ‡ç­¾åˆ—è¡¨ã€‚è¿™æ˜¯è¾“å‡ºï¼š

```python
{'MISC': {'precision': 1.0, 'recall': 0.5, 'f1': 0.67, 'number': 2},
 'ORG': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},
 'overall_precision': 1.0,
 'overall_recall': 0.67,
 'overall_f1': 0.8,
 'overall_accuracy': 0.89}
```

{#if fw === 'pt'}

å®ƒè¿”å›å¾ˆå¤šä¿¡æ¯ï¼æˆ‘ä»¬è·å¾—æ¯ä¸ªå•ç‹¬å®ä½“ä»¥åŠæ•´ä½“çš„å‡†ç¡®ç‡ã€å¬å›ç‡å’Œ F1 åˆ†æ•°ã€‚å¯¹äºæˆ‘ä»¬çš„åº¦é‡è®¡ç®—ï¼Œæˆ‘ä»¬å°†åªä¿ç•™æ€»åˆ†ï¼Œä½†æ˜¯ä½ å¯ä»¥è‡ªç”±åœ°è°ƒæ•´ `compute_metrics()` å‡½æ•°è¿”å›ä½ æƒ³è¦æŸ¥çœ‹çš„æ‰€æœ‰æŒ‡æ ‡ã€‚ `compute_metrics()` å‡½æ•°é¦–å…ˆå– logits çš„ argmaxï¼Œå°†å®ƒä»¬è½¬æ¢ä¸ºé¢„æµ‹å€¼ï¼ˆé€šå¸¸æƒ…å†µä¸‹ï¼Œlogits å’Œæ¦‚ç‡çš„é¡ºåºæ˜¯ç›¸åŒï¼Œæ‰€ä»¥æˆ‘ä»¬ä¸éœ€è¦ä½¿ç”¨ softmaxï¼‰ã€‚ç„¶åæˆ‘ä»¬éœ€è¦å°†æ ‡ç­¾å’Œé¢„æµ‹å€¼éƒ½ä»æ•´æ•°è½¬æ¢ä¸ºå­—ç¬¦ä¸²ã€‚æˆ‘ä»¬åˆ é™¤æ‰€æœ‰æ ‡ç­¾ä¸º `-100` çš„å€¼ï¼Œç„¶åå°†ç»“æœä¼ é€’ç»™ `metric.compute()` æ–¹æ³•ï¼š

```python
#####Pytorch}
import numpy as np

def compute_metrics(eval_preds):
    logits, labels = eval_preds
    predictions = np.argmax(logits, axis=-1)

    # åˆ é™¤å¿½ç•¥çš„ç´¢å¼•(ç‰¹æ®Š tokens )å¹¶è½¬æ¢ä¸ºæ ‡ç­¾
    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]
    true_predictions = [
        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)
    return {
        "precision": all_metrics["overall_precision"],
        "recall": all_metrics["overall_recall"],
        "f1": all_metrics["overall_f1"],
        "accuracy": all_metrics["overall_accuracy"],
    }####end
```

ç°åœ¨å·²ç»å®Œæˆäº†ï¼Œæˆ‘ä»¬å‡ ä¹å¯ä»¥å¼€å§‹å®šä¹‰æˆ‘ä»¬çš„ `Trainer` äº†ã€‚æˆ‘ä»¬åªéœ€è¦ä¸€ä¸ª `model` å¾®è°ƒï¼

{:else}

å®ƒè¿”å›äº†å¤§é‡çš„ä¿¡æ¯ï¼æˆ‘ä»¬è·å¾—æ¯ä¸ªå•ç‹¬å®ä½“ä»¥åŠæ•´ä½“çš„å‡†ç¡®ç‡ã€å¬å›ç‡å’Œ F1 åˆ†æ•°ã€‚ç°åœ¨ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å¦‚æœæˆ‘ä»¬å°è¯•ä½¿ç”¨æˆ‘ä»¬å®é™…çš„æ¨¡å‹é¢„æµ‹æ¥è®¡ç®—ä¸€äº›çœŸå®çš„åˆ†æ•°ä¼šå‘ç”Ÿä»€ä¹ˆã€‚

TensorFlow ä¸å–œæ¬¢æŠŠæˆ‘ä»¬çš„é¢„æµ‹æ‹¼æ¥åœ¨ä¸€èµ·ï¼Œå› ä¸ºè¿™æ ·ä¼šå¯¼è‡´åºåˆ—é•¿åº¦ä¸ç»Ÿä¸€ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬ä¸èƒ½ä»…ä»…ç›´æ¥ä½¿ç”¨ `model.predict()` â€”â€” ä½†è¿™å¹¶ä¸èƒ½é˜»æ­¢æˆ‘ä»¬ã€‚æˆ‘ä»¬å°†é€æ‰¹è·å–ä¸€äº›é¢„æµ‹å¹¶åœ¨è¿›è¡Œçš„è¿‡ç¨‹ä¸­å°†å®ƒä»¬æ‹¼æ¥æˆä¸€ä¸ªå¤§çš„åˆ—è¡¨ï¼Œåˆ é™¤è¡¨ç¤º masking/padding çš„ `-100` tokens ç„¶ååœ¨æœ€åçš„åˆ—è¡¨ä¸Šè®¡ç®—åº¦é‡å€¼ï¼š

```python
#####TensorFlow}
import numpy as np

all_predictions = []
all_labels = []
for batch in tf_eval_dataset:
    logits = model.predict_on_batch(batch)["logits"]
    labels = batch["labels"]
    predictions = np.argmax(logits, axis=-1)
    for prediction, label in zip(predictions, labels):
        for predicted_idx, label_idx in zip(prediction, label):
            if label_idx == -100:
                continue
            all_predictions.append(label_names[predicted_idx])
            all_labels.append(label_names[label_idx])
metric.compute(predictions=[all_predictions], references=[all_labels])####end
```

```python
#####TensorFlow}
{'LOC': {'precision': 0.91, 'recall': 0.92, 'f1': 0.91, 'number': 1668},
 'MISC': {'precision': 0.70, 'recall': 0.79, 'f1': 0.74, 'number': 702},
 'ORG': {'precision': 0.85, 'recall': 0.90, 'f1': 0.88, 'number': 1661},
 'PER': {'precision': 0.95, 'recall': 0.95, 'f1': 0.95, 'number': 1617},
 'overall_precision': 0.87,
 'overall_recall': 0.91,
 'overall_f1': 0.89,
 'overall_accuracy': 0.97}####end
```

ä¸æˆ‘ä»¬çš„æ¨¡å‹ç›¸æ¯”ï¼Œä½ çš„æ¨¡å‹çš„è¡¨ç°å¦‚ä½•ï¼Ÿå¦‚æœä½ è·å¾—ç±»ä¼¼çš„æ•°å­—ï¼Œé‚£ä¹ˆä½ çš„è®­ç»ƒå°±æˆåŠŸäº†ï¼

{/if}

{#if fw === 'pt'}

#### å®šä¹‰æ¨¡å‹

ç”±äºæˆ‘ä»¬æ­£åœ¨ç ”ç©¶ Token åˆ†ç±»é—®é¢˜ï¼Œå› æ­¤æˆ‘ä»¬å°†ä½¿ç”¨ `AutoModelForTokenClassification` ç±»ã€‚å®šä¹‰æ­¤æ¨¡å‹æ—¶è¦è®°å¾—ä¼ é€’æˆ‘ä»¬æ ‡ç­¾çš„æ•°é‡ï¼Œæœ€ç®€å•æ–¹æ³•æ˜¯å°†è¯¥æ•°å­—ä¼ é€’ç»™ `num_labels` å‚æ•°ï¼Œä½†æ˜¯å¦‚æœæˆ‘ä»¬æƒ³è¦ä¸€ä¸ªå°±åƒæˆ‘ä»¬åœ¨æœ¬èŠ‚å¼€å¤´çœ‹åˆ°çš„é‚£æ ·çš„æ¨ç†å°éƒ¨ä»¶ï¼Œæœ€å¥½è®¾ç½®æ­£ç¡®çš„æ ‡ç­¾å¯¹åº”å…³ç³»ã€‚

åº”è¯¥ç”±ä¸¤ä¸ªå­—å…¸ `id2label` å’Œ `label2id` æ¥è®¾ç½®å®ƒä»¬ï¼Œè¿™ä¸¤ä¸ªå­—å…¸åŒ…å«ä» ID åˆ°æ ‡ç­¾çš„æ˜ å°„ä»¥åŠåå‘çš„æ˜ å°„ï¼š

```python
#####Pytorch}
id2label = {str(i): label for i, label in enumerate(label_names)}
label2id = {v: k for k, v in id2label.items()}####end
```

ç°åœ¨æˆ‘ä»¬åªéœ€å°†å®ƒä»¬ä¼ é€’ç»™ `AutoModelForTokenClassification.from_pretrained()` æ–¹æ³•ï¼Œå®ƒä»¬å°±ä¼šè¢«ä¿å­˜åœ¨æ¨¡å‹çš„é…ç½®ä¸­ï¼Œç„¶åè¢«æ­£ç¡®åœ°ä¿å­˜å’Œä¸Šä¼ åˆ° Hubï¼š

```python
#####Pytorch}
from transformers import AutoModelForTokenClassification

model = AutoModelForTokenClassification.from_pretrained(
    model_checkpoint,
    id2label=id2label,
    label2id=label2id,
)####end
```

å°±åƒæˆ‘ä»¬åœ¨ç¬¬å››ç« ä¸­å®šä¹‰ `AutoModelForSequenceClassification` ç±»ä¸€æ · åˆ›å»ºæ¨¡å‹ä¼šå‘å‡ºä¸€ä¸ªè­¦å‘Šï¼Œæç¤ºä¸€äº›æƒé‡æœªè¢«ä½¿ç”¨ï¼ˆæ¥è‡ªé¢„è®­ç»ƒå¤´çš„æƒé‡ï¼‰å’Œä¸€äº›å…¶ä»–æƒé‡è¢«éšæœºåˆå§‹åŒ–ï¼ˆæ¥è‡ªæ–° Token åˆ†ç±»å¤´çš„æƒé‡ï¼‰ï¼Œæˆ‘ä»¬å°†è¦è®­ç»ƒè¿™äº›æƒé‡ã€‚æˆ‘ä»¬å°†åœ¨ä¸€ä¼šå„¿è¿›è¡Œè®­ç»ƒï¼Œä½†é¦–å…ˆè®©æˆ‘ä»¬ç¡®è®¤ä¸€ä¸‹æˆ‘ä»¬çš„æ¨¡å‹æ˜¯å¦å…·æœ‰æ­£ç¡®çš„æ ‡ç­¾æ•°é‡ï¼š

```python
#####Pytorch}
model.config.num_labels####end
```

```python
#####Pytorch}
9####end
```

<div custom-style="Tip-yellow">

âš ï¸ å¦‚æœä½ çš„æ¨¡å‹çš„æ ‡ç­¾æ•°é‡é”™è¯¯ï¼Œé‚£ä¹ˆåœ¨åé¢è°ƒç”¨ `Trainer.train()` æ—¶ï¼Œä½ ä¼šå¾—åˆ°ä¸€ä¸ªæ™¦æ¶©çš„é”™è¯¯ï¼ˆç±»ä¼¼äºâ€œCUDA errorï¼šdevice-side assert triggeredâ€ï¼‰ã€‚è¿™å¯èƒ½ä¼šä»¤äººçƒ¦æ¼ï¼Œæ‰€ä»¥ç¡®ä¿ä½ åšäº†è¿™ä¸ªæ£€æŸ¥ï¼Œç¡®è®¤ä½ çš„æ ‡ç­¾æ•°é‡æ˜¯æ­£ç¡®ã€‚

</div>

#### å¾®è°ƒæ¨¡å‹

æˆ‘ä»¬ç°åœ¨å‡†å¤‡å¥½è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹äº†ï¼åœ¨å®šä¹‰æˆ‘ä»¬çš„ `Trainer` ä¹‹å‰ï¼Œæˆ‘ä»¬åªéœ€è¦åšæœ€åä¸¤ä»¶äº‹ï¼šç™»å½• Hugging Face å¹¶å®šä¹‰æˆ‘ä»¬çš„è®­ç»ƒå‚æ•°ã€‚å¦‚æœä½ åœ¨ notebook ä¸Šå·¥ä½œï¼Œæœ‰ä¸€ä¸ªæ–¹ä¾¿çš„åŠŸèƒ½å¯ä»¥å¸®åŠ©ä½ ï¼š

```python
#####Pytorch}
from huggingface_hub import notebook_login

notebook_login()####end
```
ç°åœ¨ï¼Œæˆ‘ä»¬å·²å‡†å¤‡å¥½è®­ç»ƒæ¨¡å‹äº†ï¼ä¸è¿‡ï¼Œæˆ‘ä»¬é¦–å…ˆè¦åšä¸¤ä»¶äº‹ï¼šåº”è¯¥ç™»å½•åˆ° Hugging Face å¹¶å®šä¹‰æˆ‘ä»¬çš„è®­ç»ƒè¶…å‚æ•°ã€‚å¦‚æœä½ åœ¨ notebook ä¸Šå·¥ä½œï¼Œæœ‰ä¸€ä¸ªä¾¿åˆ©å‡½æ•°å¯ä»¥å¸®åŠ©ä½ åšåˆ°è¿™ä¸€ç‚¹ï¼š

```python
#####Pytorch}
huggingface-cli login####end
```

ç™»å½•åï¼Œæˆ‘ä»¬å°±å¯ä»¥å®šä¹‰æˆ‘ä»¬çš„ `TrainingArguments` ï¼š

```python
#####Pytorch}
from transformers import TrainingArguments

args = TrainingArguments(
    "bert-finetuned-ner",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
    push_to_hub=True,
)####end
```

ä½ å·²ç»å¯¹å¤§å¤šæ•°å†…å®¹æœ‰æ‰€äº†è§£äº†ï¼šæˆ‘ä»¬è®¾ç½®äº†ä¸€äº›è¶…å‚æ•°ï¼ˆå¦‚å­¦ä¹ ç‡ã€è®­ç»ƒçš„è½®æ•°å’Œæƒé‡è¡°å‡ï¼‰ï¼Œå¹¶æŒ‡å®š `push_to_hub=True` ï¼Œä»¥è¡¨ç¤ºæˆ‘ä»¬å¸Œæœ›åœ¨æ¯ä¸ªè®­ç»ƒè½®æ¬¡ç»“æŸæ—¶ä¿å­˜å¹¶è¯„ä¼°æ¨¡å‹ï¼Œå¹¶å¸Œæœ›å°†ç»“æœä¸Šä¼ åˆ°æ¨¡å‹ä¸­å¿ƒã€‚æ³¨æ„ï¼Œä½ å¯ä»¥é€šè¿‡ `hub_model_id` å‚æ•°æŒ‡å®šä½ æƒ³æ¨é€çš„ä»“åº“çš„åç§°ï¼ˆç‰¹åˆ«æ˜¯ï¼Œå¦‚æœä½ éœ€è¦æ¨é€ç»™æŸä¸ªç»„ç»‡ï¼Œå°±å¿…é¡»ä½¿ç”¨è¿™ä¸ªå‚æ•°ï¼‰ã€‚ä¾‹å¦‚ï¼Œå½“æˆ‘ä»¬å°†æ¨¡å‹æ¨é€åˆ° [`huggingface-course` ç»„ç»‡](https://huggingface.co/huggingface-course)(https://huggingface.co/huggingface-course)(https://huggingface.co/huggingface-course) æ—¶ï¼Œæˆ‘ä»¬åœ¨ `TrainingArguments` ä¸­æ·»åŠ äº† `hub_model_id="huggingface-course/bert-finetuned-ner"` ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œä½¿ç”¨çš„ä»“åº“å°†ä¿å­˜åœ¨ä½ çš„è´¦æˆ·ä¹‹å†…ï¼Œå¹¶ä»¥ä½ è®¾ç½®çš„è¾“å‡ºç›®å½•å‘½åï¼Œæ‰€ä»¥åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œä»“åº“çš„åœ°å€æ˜¯ `"sgugger/bert-finetuned-ner"` ã€‚

<div custom-style="Tip-green">

ğŸ’¡ å¦‚æœä½ ä½¿ç”¨çš„è¾“å‡ºç›®å½•å·²ç»å­˜åœ¨ï¼Œé‚£ä¹ˆå®ƒéœ€è¦æ˜¯ä½ æƒ³æ¨é€åˆ° hub çš„ä»“åº“çš„æœ¬åœ°å…‹éš†ã€‚å¦‚æœä¸æ˜¯ï¼Œä½ å°†åœ¨å£°æ˜ `Trainer` æ—¶é‡åˆ°é”™è¯¯ï¼Œå¹¶éœ€è¦è®¾ç½®ä¸€ä¸ªæ–°çš„åå­—ã€‚

</div>

æœ€åï¼Œæˆ‘ä»¬å°†æ‰€æœ‰å†…å®¹ä¼ é€’ç»™ `Trainer` å¹¶å¯åŠ¨è®­ç»ƒï¼š

```python
#####Pytorch}
from transformers import Trainer

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    tokenizer=tokenizer,
)
trainer.train()####end
```

è¯·æ³¨æ„ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ¯æ¬¡ä¿å­˜æ¨¡å‹æ—¶ï¼ˆè¿™é‡Œæ˜¯æ¯ä¸ª epoochï¼‰ï¼Œå®ƒéƒ½ä¼šåœ¨åå°ä¸Šä¼ åˆ° Hubã€‚è¿™æ ·ï¼Œå¦‚æœ‰å¿…è¦ï¼Œä½ å°†èƒ½å¤Ÿåœ¨å¦ä¸€å°æœºå™¨ä¸Šç»§ç»­ä½ çš„è®­ç»ƒã€‚

è®­ç»ƒå®Œæˆåï¼Œæˆ‘ä»¬ä½¿ç”¨ `push_to_hub()` ä¸Šä¼ æ¨¡å‹çš„æœ€æ–°ç‰ˆæœ¬

```python
#####Pytorch}
trainer.push_to_hub(commit_message="Training complete")####end
```

å¦‚æœä½ æƒ³æ£€æŸ¥ä¸€ä¸‹æ˜¯å¦ä¸Šä¼ æˆåŠŸï¼Œè¿™ä¸ªå‘½ä»¤ä¼šè¿”å›åˆšåˆšæ‰§è¡Œçš„æäº¤çš„ URLï¼š

```python
#####Pytorch}
'https://huggingface.co/sgugger/bert-finetuned-ner/commit/26ab21e5b1568f9afeccdaed2d8715f571d786ed'####end
```

åŒæ—¶ `Trainer` è¿˜åˆ›å»ºå¹¶ä¸Šä¼ äº†ä¸€å¼ åŒ…å«æ‰€æœ‰è¯„ä¼°ç»“æœçš„æ¨¡å‹å¡ã€‚åˆ°æ­¤é˜¶æ®µï¼Œä½ å¯ä»¥åœ¨æ¨¡å‹ Hub ä¸Šä½¿ç”¨æ¨ç†å°éƒ¨ä»¶æ¥æµ‹è¯•ä½ çš„æ¨¡å‹å¹¶ä¸ä½ çš„æœ‹å‹åˆ†äº«ã€‚ä½ å·²ç»æˆåŠŸåœ°åœ¨ä¸€ä¸ª tokens åˆ†ç±»ä»»åŠ¡ä¸Šå¾®è°ƒäº†ä¸€ä¸ªæ¨¡å‹â€”â€”æ­å–œä½ ï¼

å¦‚æœä½ æƒ³æ›´æ·±å…¥åœ°äº†è§£è®­ç»ƒå¾ªç¯ï¼Œæˆ‘ä»¬ç°åœ¨å°†å‘ä½ å±•ç¤ºå¦‚ä½•ä½¿ç”¨ Accelerate åšåŒæ ·çš„äº‹æƒ…ã€‚

### è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ 

ç°åœ¨æˆ‘ä»¬æ¥çœ‹çœ‹å®Œæ•´çš„è®­ç»ƒå¾ªç¯ï¼Œè¿™æ ·ä½ å°±å¯ä»¥è½»æ¾åœ°å®šåˆ¶ä½ éœ€è¦çš„éƒ¨åˆ†ã€‚å®ƒä¸æˆ‘ä»¬åœ¨ç¬¬å››ç« ä¸­æ‰€åšçš„å†…å®¹å¾ˆç›¸ä¼¼ï¼Œä½†å¯¹è¯„ä¼°éƒ¨åˆ†æœ‰ä¸€äº›æ”¹åŠ¨ã€‚

#### åšå¥½è®­ç»ƒå‰çš„å‡†å¤‡ 

é¦–å…ˆæˆ‘ä»¬éœ€è¦ä¸ºæˆ‘ä»¬çš„æ•°æ®é›†æ„å»º `DataLoader` ã€‚æˆ‘ä»¬å°† `data_collator` è¾“å…¥ `collate_fn` å‚æ•°å¹¶æ‰“ä¹±è®­ç»ƒé›†ï¼Œä½†ä¸æ‰“ä¹±éªŒè¯é›†ï¼š

```python
#####Pytorch}
from torch.utils.data import DataLoader

train_dataloader = DataLoader(
    tokenized_datasets["train"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], collate_fn=data_collator, batch_size=8
)####end
```

æ¥ä¸‹æ¥æˆ‘ä»¬é‡æ–°å®ä¾‹åŒ–æˆ‘ä»¬çš„æ¨¡å‹ï¼Œä»¥ç¡®ä¿æˆ‘ä»¬ä¸æ˜¯ç»§ç»­ä¹‹å‰çš„å¾®è°ƒï¼Œè€Œæ˜¯é‡æ–°å¼€å§‹ä» BERT é¢„è®­ç»ƒæ¨¡å‹ï¼š

```python
#####Pytorch}
model = AutoModelForTokenClassification.from_pretrained(
    model_checkpoint,
    id2label=id2label,
    label2id=label2id,
)####end
```

ç„¶åæˆ‘ä»¬éœ€è¦ä¸€ä¸ªä¼˜åŒ–å™¨ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ç»å…¸ `AdamW` ï¼Œå®ƒç±»ä¼¼äº `Adam` ï¼Œä½†åœ¨æƒé‡è¡°å‡çš„æ–¹å¼ä¸Šè¿›è¡Œäº†æ”¹è¿›ï¼š

```python
#####Pytorch}
from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr=2e-5)####end
```

å½“æˆ‘ä»¬æ‹¥æœ‰äº†æ‰€æœ‰è¿™äº›å¯¹è±¡ï¼Œæˆ‘ä»¬å°±å¯ä»¥å°†å®ƒä»¬å‘é€åˆ° `accelerator.prepare()` æ–¹æ³•ï¼š

```python
#####Pytorch}
from accelerate import Accelerator

accelerator = Accelerator()
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)####end
```

<div custom-style="Tip-red">

ğŸš¨ å¦‚æœä½ æ­£åœ¨ TPU ä¸Šè®­ç»ƒï¼Œä½ éœ€è¦å°†ä¸Šé¢å•å…ƒæ ¼å¼€å§‹çš„æ‰€æœ‰ä»£ç ç§»åŠ¨åˆ°ä¸€ä¸ªä¸“é—¨çš„è®­ç»ƒå‡½æ•°ä¸­ã€‚æ›´å¤šè¯¦æƒ…è¯·è§ç¬¬å››ç« ã€‚

</div>

ç°åœ¨æˆ‘ä»¬å·²ç»å°†æˆ‘ä»¬çš„ `train_dataloader` ä¼ é€’ç»™äº† `accelerator.prepare()` ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `len()` æ¥è®¡ç®—è®­ç»ƒæ­¥éª¤çš„æ•°é‡ã€‚è¯·è®°ä½ï¼Œæˆ‘ä»¬åº”è¯¥åœ¨å‡†å¤‡å¥½ `dataloader` åå†ä½¿ç”¨ `len()` ï¼Œå› ä¸ºæ”¹åŠ¨ `dataloader` ä¼šæ”¹å˜å…¶é•¿åº¦ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªä»å­¦ä¹ ç‡è¡°å‡åˆ° 0 çš„ç»å…¸çº¿æ€§å­¦ä¹ ç‡è°ƒåº¦ï¼š

```python
#####Pytorch}
from transformers import get_scheduler

num_train_epochs = 3
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)####end
```

æœ€åï¼Œä¸ºäº†å°†æˆ‘ä»¬çš„æ¨¡å‹æ¨é€åˆ° Hubï¼Œæˆ‘ä»¬éœ€è¦åœ¨ä¸€ä¸ªå·¥ä½œæ–‡ä»¶å¤¹ä¸­åˆ›å»ºä¸€ä¸ª `Repository` å¯¹è±¡ã€‚å¦‚æœä½ è¿˜æ²¡æœ‰ç™»å½•çš„è¯ï¼Œé¦–å…ˆéœ€è¦ç™»å½•åˆ° Hugging Faceï¼Œæˆ‘ä»¬å°†æ ¹æ®æˆ‘ä»¬æƒ³ç»™æˆ‘ä»¬çš„æ¨¡å‹çš„æ¨¡å‹ ID æ¥ç¡®å®šä»“åº“åç§°ï¼ˆä½ å¯ä»¥ä½¿ç”¨ä½ å–œæ¬¢çš„åå­—æ›¿æ¢ `repo_name` ï¼›å®ƒåªéœ€è¦åŒ…å«ä½ çš„ç”¨æˆ·åï¼Œå¯ä»¥ä½¿ç”¨ `get_full_repo_name()` å‡½æ•°çš„æŸ¥çœ‹ç›®å‰çš„ `repo_name` ï¼‰ï¼š

```python
#####Pytorch}
from huggingface_hub import Repository, get_full_repo_name

model_name = "bert-finetuned-ner-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name####end
```

```python
#####Pytorch}
'sgugger/bert-finetuned-ner-accelerate'####end
```

ç„¶åæˆ‘ä»¬å¯ä»¥å°†è¯¥ä»“åº“å…‹éš†åˆ°æœ¬åœ°æ–‡ä»¶å¤¹ä¸­ã€‚å¦‚æœå®ƒå·²ç»å­˜åœ¨ï¼Œè¿™ä¸ªæœ¬åœ°æ–‡ä»¶å¤¹åº”è¯¥æ˜¯æˆ‘ä»¬æ­£åœ¨ä½¿ç”¨çš„ä»“åº“çš„ç°æœ‰å…‹éš†ï¼š

```python
#####Pytorch}
output_dir = "bert-finetuned-ner-accelerate"
repo = Repository(output_dir, clone_from=repo_name)####end
```

æˆ‘ä»¬ç°åœ¨å¯ä»¥é€šè¿‡è°ƒç”¨ `repo.push_to_hub()` æ–¹æ³•ä¸Šä¼ ä¿å­˜åœ¨ `output_dir` ä¸­çš„æ‰€æœ‰å†…å®¹ã€‚è¿™å°†å¸®åŠ©æˆ‘ä»¬åœ¨æ¯ä¸ªè®­ç»ƒå‘¨æœŸç»“æŸæ—¶ä¸Šä¼ ä¸­é—´æ¨¡å‹ã€‚

#### è®­ç»ƒå¾ªç¯ 

æˆ‘ä»¬ç°åœ¨å‡†å¤‡ç¼–å†™å®Œæ•´çš„è®­ç»ƒå¾ªç¯ã€‚ä¸ºäº†ç®€åŒ–å…¶è¯„ä¼°éƒ¨åˆ†ï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ª `postprocess()` å‡½æ•°ï¼Œè¯¥å‡½æ•°æ¥æ”¶æ¨¡å‹çš„é¢„æµ‹å’Œæ ‡ç­¾ï¼Œå¹¶å°†å®ƒä»¬è½¬æ¢ä¸ºå­—ç¬¦ä¸²åˆ—è¡¨ï¼Œä¹Ÿå°±æ˜¯æˆ‘ä»¬çš„ `metric` ï¼ˆè¯„ä¼°å‡½æ•°ï¼‰å¯¹è±¡éœ€è¦çš„è¾“å…¥æ ¼å¼ã€‚

```python
#####Pytorch}
def postprocess(predictions, labels):
    predictions = predictions.detach().cpu().clone().numpy()
    labels = labels.detach().cpu().clone().numpy()

    # åˆ é™¤å¿½ç•¥çš„ç´¢å¼•(ç‰¹æ®Š tokens )å¹¶è½¬æ¢ä¸ºæ ‡ç­¾
    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]
    true_predictions = [
        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    return true_labels, true_predictions####end
```

ç„¶åæˆ‘ä»¬å¯ä»¥ç¼–å†™è®­ç»ƒå¾ªç¯ã€‚åœ¨å®šä¹‰ä¸€ä¸ªè¿›åº¦æ¡æ¥è·Ÿè¸ªè®­ç»ƒçš„è¿›è¡Œåï¼Œå¾ªç¯åˆ†ä¸ºä¸‰ä¸ªéƒ¨åˆ†ï¼š

- è®­ç»ƒæœ¬èº«ï¼Œè¿™æ˜¯ç»å…¸çš„è¿­ä»£è¿‡ç¨‹ï¼Œå³åœ¨ `train_dataloader` ä¸Šè¿›è¡Œè¿­ä»£ï¼Œåœ¨æ¨¡å‹ä¸Šå‰å‘ä¼ æ’­ï¼Œç„¶ååå‘ä¼ é€’å’Œä¼˜åŒ–å‚æ•°
- è¯„ä¼°ï¼Œåœ¨è·å–æ¨¡å‹åœ¨ä¸€ä¸ª batch ä¸Šçš„è¾“å‡ºä¹‹åï¼Œè¿™é‡Œæœ‰ä¸€ä¸ªéœ€è¦æ³¨æ„çš„åœ°æ–¹ï¼šç”±äºä¸¤ä¸ªè¿›ç¨‹å¯èƒ½å·²å°†è¾“å…¥å’Œæ ‡ç­¾å¡«å……åˆ°ä¸åŒçš„å½¢çŠ¶ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨ `accelerator.pad_across_processes()` ä½¿é¢„æµ‹å’Œæ ‡ç­¾åœ¨è°ƒç”¨ `gather()` æ–¹æ³•ä¹‹å‰å…·æœ‰ç›¸åŒçš„å½¢çŠ¶ã€‚å¦‚æœæˆ‘ä»¬ä¸è¿™æ ·åšï¼Œè¯„ä¼°å¾ªç¯å°†ä¼šå‡ºé”™æˆ–æ— é™æœŸæŒ‚èµ·ã€‚ç„¶åæˆ‘ä»¬å°†ç»“æœå‘é€åˆ° `metric.add_batch()` ï¼Œå¹¶åœ¨è¯„ä¼°å¾ªç¯ç»“æŸæ—¶è°ƒç”¨ `metric.compute()` ã€‚
- ä¿å­˜å’Œä¸Šä¼ ï¼Œé¦–å…ˆä¿å­˜æ¨¡å‹å’Œ tokenizer ç„¶åè°ƒç”¨ `repo.push_to_hub()` ã€‚æ³¨æ„ï¼Œæˆ‘ä»¬ä½¿ç”¨å‚æ•° `blocking=False` æ¥å‘Šè¯‰ Hub åº“åœ¨ä¸€ä¸ªå¼‚æ­¥è¿›ç¨‹ä¸­æ¨é€ã€‚è¿™æ ·ï¼Œåœ¨è®­ç»ƒæ—¶ï¼Œè¿™ä¸ªæŒ‡ä»¤åœ¨åå°å°†æ¨¡å‹å’Œ tokenizer æ¨é€åˆ° hubã€‚

ä»¥ä¸‹æ˜¯å®Œæ•´çš„è®­ç»ƒå¾ªç¯ä»£ç ï¼š

```python
#####Pytorch}
from tqdm.auto import tqdm
import torch

progress_bar = tqdm(range(num_training_steps))

for epoch in range(num_train_epochs):
    # è®­ç»ƒ
    model.train()
    for batch in train_dataloader:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

    # è¯„ä¼°
    model.eval()
    for batch in eval_dataloader:
        with torch.no_grad():
            outputs = model(**batch)

        predictions = outputs.logits.argmax(dim=-1)
        labels = batch["labels"]

        # å¡«å……æ¨¡å‹çš„é¢„æµ‹å’Œæ ‡ç­¾åæ‰èƒ½è°ƒç”¨ gathere()
        predictions = accelerator.pad_across_processes(predictions, dim=1, pad_index=-100)
        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)

        predictions_gathered = accelerator.gather(predictions)
        labels_gathered = accelerator.gather(labels)

        true_predictions, true_labels = postprocess(predictions_gathered, labels_gathered)
        metric.add_batch(predictions=true_predictions, references=true_labels)

    results = metric.compute()
    print(
        f"epoch {epoch}:",
        {
            key: results[f"overall_{key}"]
            for key in ["precision", "recall", "f1", "accuracy"]
        },
    )

    # ä¿å­˜å¹¶ä¸Šä¼ 
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
    if accelerator.is_main_process:
        tokenizer.save_pretrained(output_dir)
        repo.push_to_hub(
            commit_message=f"Training in progress epoch {epoch}", blocking=False
        )####end
```

æœè¿™æ˜¯ä½ ç¬¬ä¸€æ¬¡çœ‹åˆ°ç”¨ Accelerate ä¿å­˜çš„æ¨¡å‹ï¼Œè®©æˆ‘ä»¬èŠ±ç‚¹æ—¶é—´æ¥äº†è§£ä¸€ä¸‹è¿™ä¸ªè¿‡ç¨‹ä¸­çš„ä¸‰è¡Œä»£ç ï¼š

```python
#####Pytorch}
accelerator.wait_for_everyone()
unwrapped_model = accelerator.unwrap_model(model)
unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)####end
```

ç¬¬ä¸€è¡Œæ˜¯ä¸è¨€è‡ªæ˜çš„ï¼šå®ƒå‘Šè¯‰æ‰€æœ‰çš„è¿›ç¨‹ç­‰å¾…ï¼Œç›´åˆ°æ‰€æœ‰çš„è¿›ç¨‹éƒ½å¤„äºé‚£ä¸ªé˜¶æ®µå†ç»§ç»­ï¼ˆé˜»å¡ï¼‰ã€‚è¿™æ˜¯ä¸ºäº†ç¡®ä¿åœ¨ä¿å­˜ä¹‹å‰ï¼Œæˆ‘ä»¬åœ¨æ¯ä¸ªè¿›ç¨‹ä¸­éƒ½æœ‰ç›¸åŒçš„æ¨¡å‹ã€‚ç„¶åè·å– `unwrapped_model` ï¼Œå®ƒå°±æ˜¯æˆ‘ä»¬å®šä¹‰çš„åŸºæœ¬æ¨¡å‹ã€‚ `accelerator.prepare()` æ–¹æ³•ä¼šä¸ºäº†åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­å·¥ä½œè€Œå¯¹æ¨¡å‹è¿›è¡Œäº†ä¸€äº›ä¿®æ”¹ï¼Œæ‰€ä»¥å®ƒä¸å†æœ‰ `save_pretraining()` æ–¹æ³•ï¼›ä½¿ç”¨ `accelerator.unwrap_model()` æ–¹æ³•å¯ä»¥æ’¤é”€å¯¹æ¨¡å‹çš„æ›´æ”¹ã€‚æœ€åï¼Œæˆ‘ä»¬è°ƒç”¨ `save_pretraining()` ï¼Œå¹¶æŒ‡å®š `accelerator.save()` ä½œä¸º `save_function` è€Œä¸æ˜¯ `torch.save()` ã€‚

å®Œæˆè¿™äº›æ“ä½œåï¼Œä½ åº”è¯¥æ‹¥æœ‰ä¸€ä¸ªä¸ `Trainer` è®­ç»ƒå‡ºçš„æ¨¡å‹ç»“æœç›¸å½“ç±»ä¼¼çš„æ¨¡å‹ã€‚ä½ å¯ä»¥åœ¨ [huggingface-course/bert-finetuned-ner-accelerate](https://huggingface.co/huggingface-course/bert-finetuned-ner-accelerate)(https://huggingface.co/huggingface-course/bert-finetuned-ner-accelerate) æŸ¥çœ‹æˆ‘ä»¬ä½¿ç”¨è¿™äº›ä»£ç è®­ç»ƒçš„æ¨¡å‹ã€‚å¦‚æœä½ æƒ³åœ¨è®­ç»ƒå¾ªç¯ä¸­æµ‹è¯•ä»»ä½•è°ƒæ•´ï¼Œä½ å¯ä»¥ç›´æ¥é€šè¿‡ç¼–è¾‘ä¸Šé¢æ˜¾ç¤ºçš„ä»£ç æ¥å®ç°å®ƒä»¬ï¼

{/if}

### ä½¿ç”¨å¾®è°ƒæ¨¡å‹ 

æˆ‘ä»¬å·²ç»å‘ä½ å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨æˆ‘ä»¬åœ¨æ¨¡å‹ä¸­å¿ƒå¾®è°ƒçš„æ¨¡å‹å’Œæ¨ç†å°éƒ¨ä»¶ã€‚åœ¨æœ¬åœ°ä½¿ç”¨ `pipeline` æ¥ä½¿ç”¨å®ƒï¼Œä½ åªéœ€è¦æŒ‡å®šæ­£ç¡®çš„æ¨¡å‹æ ‡ç­¾ï¼š

```python
from transformers import pipeline

## å°†æ­¤æ›¿æ¢ä¸ºä½ è‡ªå·±çš„ checkpoint
model_checkpoint = "huggingface-course/bert-finetuned-ner"
token_classifier = pipeline(
    "token-classification", model=model_checkpoint, aggregation_strategy="simple"
)
token_classifier("My name is Sylvain and I work at Hugging Face in Brooklyn.")
```

```python
[{'entity_group': 'PER', 'score': 0.9988506, 'word': 'Sylvain', 'start': 11, 'end': 18},
 {'entity_group': 'ORG', 'score': 0.9647625, 'word': 'Hugging Face', 'start': 33, 'end': 45},
 {'entity_group': 'LOC', 'score': 0.9986118, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

å¤ªæ£’äº†ï¼æˆ‘ä»¬çš„æ¨¡å‹ä¸æ­¤ç®¡é“çš„é»˜è®¤æ¨¡å‹ä¸€æ ·æœ‰æ•ˆï¼


## 8.2 å¾®è°ƒæ©ç è¯­è¨€æ¨¡å‹ï¼ˆmasked language modelï¼‰ 

å¯¹äºè®¸å¤šæ¶‰åŠ Transformer æ¨¡å‹çš„ NLP ç¨‹åºï¼Œä½ å¯ä»¥ç®€å•åœ°ä» Hugging Face Hub ä¸­è·å–ä¸€ä¸ªé¢„è®­ç»ƒçš„æ¨¡å‹ï¼Œç„¶åç›´æ¥åœ¨ä½ çš„æ•°æ®ä¸Šå¯¹å…¶è¿›è¡Œå¾®è°ƒï¼Œä»¥å®Œæˆæ‰‹å¤´çš„ä»»åŠ¡ã€‚åªè¦ç”¨äºé¢„è®­ç»ƒçš„è¯­æ–™åº“ä¸ç”¨äºå¾®è°ƒçš„è¯­æ–™åº“æ²¡æœ‰å¤ªå¤§åŒºåˆ«ï¼Œè¿ç§»å­¦ä¹ é€šå¸¸ä¼šäº§ç”Ÿå¾ˆå¥½çš„ç»“æœã€‚

ä½†æ˜¯ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œä½ éœ€è¦å…ˆå¾®è°ƒæ•°æ®ä¸Šçš„è¯­è¨€æ¨¡å‹ï¼Œç„¶åå†è®­ç»ƒç‰¹å®šäºä»»åŠ¡çš„ headã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ çš„æ•°æ®é›†åŒ…å«æ³•å¾‹åˆåŒæˆ–ç§‘å­¦æ–‡ç« ï¼Œåƒ BERT è¿™æ ·çš„æ™®é€š Transformer æ¨¡å‹é€šå¸¸ä¼šå°†ä½ è¯­æ–™åº“ä¸­çš„ç‰¹å®šé¢†åŸŸè¯è§†ä¸ºç¨€æœ‰ tokens ç»“æœæ€§èƒ½å¯èƒ½ä¸å°½å¦‚äººæ„ã€‚é€šè¿‡åœ¨ç‰¹å®šé¢†åŸŸå†…æ•°æ®ä¸Šå¾®è°ƒè¯­è¨€æ¨¡å‹ï¼Œä½ å¯ä»¥æé«˜è®¸å¤šä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ï¼Œè¿™æ„å‘³ç€ä½ é€šå¸¸åªéœ€æ‰§è¡Œä¸€æ¬¡æ­¤æ­¥éª¤ï¼

è¿™ç§åœ¨ç‰¹å®šé¢†åŸŸå†…æ•°æ®ä¸Šå¾®è°ƒé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„è¿‡ç¨‹é€šå¸¸ç§°ä¸º `é¢†åŸŸé€‚åº”ï¼ˆdomain adaptationï¼‰` ã€‚å®ƒäº 2018 å¹´ç”± [ULMFiT](https://arxiv.org/abs/1801.06146)(https://arxiv.org/abs/1801.06146) æ¨å¹¿ï¼Œè¿™æ˜¯ä½¿è¿ç§»å­¦ä¹ çœŸæ­£é€‚ç”¨äº NLP çš„é¦–æ‰¹ç¥ç»æ¶æ„ä¹‹ä¸€ ï¼ˆåŸºäº LSTMï¼‰ã€‚ä¸‹å›¾æ˜¾ç¤ºäº†ä½¿ç”¨ ULMFiT è¿›è¡Œé¢†åŸŸè‡ªé€‚åº”çš„ç¤ºä¾‹ï¼›åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†åšç±»ä¼¼çš„äº‹æƒ…ï¼Œä½†ä½¿ç”¨çš„æ˜¯ Transformer è€Œä¸æ˜¯ LSTMï¼

![ULMFiT](./assets/ulmfit.png "ULMFiT.")

åœ¨æœ¬èŠ‚ç»“æŸæ—¶ï¼Œä½ å°†åœ¨ Hub ä¸Šæ‹¥æœ‰ä¸€ä¸ª [æ©ç è¯­è¨€æ¨¡å‹(masked language model)](https://huggingface.co/huggingface-course/distilbert-base-uncased-finetuned-imdb?text=This+is+a+great+%5BMASK%5D)(https://huggingface.co/huggingface-course/distilbert-base-uncased-finetuned-imdb?text=This+is+a+great+%5BMASK%5D) ï¼Œè¯¥æ¨¡å‹å¯ä»¥è‡ªåŠ¨å®Œæˆå¥å­ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

![æ©ç è¯­è¨€æ¨¡å‹ çš„ä¸€ä¸ªæ¼”ç¤ºä¾‹å­](./assets/mask-language-model-example.jpg)

è®©æˆ‘ä»¬å¼€å§‹å§ï¼

<div custom-style="Tip-green">

ğŸ™‹ å¦‚æœä½ å¯¹â€œæ©ç è¯­è¨€å»ºæ¨¡â€å’Œâ€œé¢„è®­ç»ƒæ¨¡å‹â€è¿™ä¸¤ä¸ªæœ¯è¯­æ„Ÿåˆ°é™Œç”Ÿï¼Œè¯·æŸ¥çœ‹ç¬¬äºŒç« ï¼Œæˆ‘ä»¬åœ¨å…¶ä¸­è§£é‡Šäº†æ‰€æœ‰è¿™äº›æ ¸å¿ƒæ¦‚å¿µï¼

</div>

### é€‰æ‹©ç”¨äºæ©ç è¯­è¨€å»ºæ¨¡çš„é¢„è®­ç»ƒæ¨¡å‹ 

é¦–å…ˆï¼Œè®©æˆ‘ä»¬ä¸ºæ©ç è¯­è¨€å»ºæ¨¡é€‰æ‹©ä¸€ä¸ªåˆé€‚çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚å¦‚ä»¥ä¸‹å±å¹•æˆªå›¾æ‰€ç¤ºï¼Œä½ å¯ä»¥é€šè¿‡åœ¨ [Hugging Face Hub](https://huggingface.co/models?pipeline_tag=fill-mask&sort=downloads)(https://huggingface.co/models?pipeline_tag=fill-mask&sort=downloads) ä¸Šé€‰æ‹©â€œFill-Maskâ€è¿‡æ»¤å™¨ï¼š

![Hub models](./assets/mlm-models.png "Hub models.")
å°½ç®¡ BERT å’Œ RoBERTa ç³»åˆ—æ¨¡å‹çš„ä¸‹è½½é‡æœ€å¤§ï¼Œä½†æˆ‘ä»¬å°†ä½¿ç”¨åä¸º [DistilBERT](https://huggingface.co/distilbert-base-uncased)(https://huggingface.co/distilbert-base-uncased) çš„æ¨¡å‹ã€‚å®ƒå¯ä»¥æ›´å¿«åœ°è®­ç»ƒï¼Œè€Œä¸”å¯¹ä¸‹æ¸¸æ€§èƒ½å‡ ä¹æ²¡æœ‰æŸå¤±ã€‚è¿™ä¸ªæ¨¡å‹ä½¿ç”¨ä¸€ç§ç§°ä¸º [`çŸ¥è¯†è’¸é¦ï¼ˆknowledge distillationï¼‰`](https://en.wikipedia.org/wiki/Knowledge_distillation)(https://en.wikipedia.org/wiki/Knowledge_distillation) çš„ç‰¹æ®ŠæŠ€æœ¯è¿›è¡Œè®­ç»ƒï¼Œå…¶ä¸­ä½¿ç”¨åƒ BERT è¿™æ ·çš„å¤§å‹â€œæ•™å¸ˆæ¨¡å‹â€æ¥æŒ‡å¯¼å‚æ•°å°‘å¾—å¤šçš„â€œå­¦ç”Ÿæ¨¡å‹â€çš„è®­ç»ƒã€‚åœ¨æœ¬èŠ‚ä¸­å¯¹çŸ¥è¯†è’¸é¦ç»†èŠ‚çš„è§£é‡Šä¼šä½¿æˆ‘ä»¬ç¦»é¢˜å¤ªè¿œï¼Œä½†å¦‚æœä½ æœ‰å…´è¶£ï¼Œå¯ä»¥é˜…è¯» [`ä½¿ç”¨ Transformers è¿›è¡Œè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNatural Language Processing with Transformersï¼‰`](https://www.oreilly.com/library/view/natural-language-processing/9781098136789/)(https://www.oreilly.com/library/view/natural-language-processing/9781098136789/) ï¼ˆä¿—ç§° Transformers æ•™ç§‘ä¹¦ï¼‰ç›¸å…³å†…å®¹ã€‚

{#if fw === 'pt'}

è®©æˆ‘ä»¬ç»§ç»­ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `AutoModelForMaskedLM` ç±»ä¸‹è½½ DistilBERTï¼š

```python
#####Pytorch}
from transformers import AutoModelForMaskedLM

model_checkpoint = "distilbert-base-uncased"
model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)####end
```

æˆ‘ä»¬å¯ä»¥é€šè¿‡è°ƒç”¨ `num_parameters()` æ–¹æ³•æŸ¥çœ‹æ¨¡å‹æœ‰å¤šå°‘å‚æ•°ï¼š

```python
#####Pytorch}
distilbert_num_parameters = model.num_parameters() / 1_000_000
print(f"'>>> DistilBERT number of parameters: {round(distilbert_num_parameters)}M'")
print(f"'>>> BERT number of parameters: 110M'")####end
```

```python
#####Pytorch}
'>>> DistilBERT number of parameters: 67M'
'>>> BERT number of parameters: 110M'####end
```

{:else}

è®©æˆ‘ä»¬ç»§ç»­ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `TFAutoModelForMaskedLM` ç±»ä¸‹è½½ DistilBERTï¼š

```python
#####TensorFlow}
from transformers import TFAutoModelForMaskedLM

model_checkpoint = "distilbert-base-uncased"
model = TFAutoModelForMaskedLM.from_pretrained(model_checkpoint)####end
```

æˆ‘ä»¬å¯ä»¥é€šè¿‡è°ƒç”¨ `summary()` æ–¹æ³•æŸ¥çœ‹æ¨¡å‹æœ‰å¤šå°‘å‚æ•°ï¼š

```python
#####TensorFlow}
model(model.dummy_inputs)  # æ„å»ºæ¨¡å‹
model.summary()####end
```

```python
#####TensorFlow}
Model: "tf_distil_bert_for_masked_lm"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 distilbert (TFDistilBertMai  multiple                 66362880  
 nLayer)                                                         
                                                                 
 vocab_transform (Dense)     multiple                  590592    
                                                                 
 vocab_layer_norm (LayerNorm  multiple                 1536      
 alization)                                                      
                                                                 
 vocab_projector (TFDistilBe  multiple                 23866170  
 rtLMHead)                                                       
                                                                 
=================================================================
Total params: 66,985,530
Trainable params: 66,985,530
Non-trainable params: 0
_________________________________________________________________####end
```

{/if}

DistilBERT å¤§çº¦æœ‰ 6700 ä¸‡ä¸ªå‚æ•°ï¼Œå¤§çº¦æ˜¯ BERT base æ¨¡å‹çš„äºŒåˆ†ä¹‹ä¸€ï¼Œè¿™å¤§è‡´æ„å‘³ç€è®­ç»ƒçš„é€Ÿåº¦æé«˜äº†ä¸¤å€ â€”â€” éå¸¸æ£’ï¼ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹å¯¹äºä¸‹é¢çš„ä¸€å°éƒ¨åˆ†æ–‡æœ¬ï¼Œè¿™ä¸ªæ¨¡å‹æœ€æœ‰å¯èƒ½é¢„æµ‹ä»€ä¹ˆï¼š

```python
text = "This is a great [MASK]."
```

ä½œä¸ºäººç±»ï¼Œæˆ‘ä»¬å¯ä»¥æƒ³è±¡ `[MASK]` token æœ‰å¾ˆå¤šå¯èƒ½æ€§ï¼Œä¾‹å¦‚ â€œdayâ€ã€â€œrideâ€ æˆ–è€… â€œpaintingâ€ã€‚å¯¹äºé¢„è®­ç»ƒæ¨¡å‹ï¼Œé¢„æµ‹å–å†³äºæ¨¡å‹æ‰€è®­ç»ƒçš„è¯­æ–™åº“ï¼Œå› ä¸ºå®ƒä¼šå­¦ä¹ è·å–æ•°æ®ä¸­å­˜åœ¨çš„ç»Ÿè®¡æ¨¡å¼ã€‚ä¸ BERT ä¸€æ ·ï¼ŒDistilBERT åœ¨ [English Wikipedia](https://huggingface.co/datasets/wikipedia)(https://huggingface.co/datasets/wikipedia) å’Œ [BookCorpus](https://huggingface.co/datasets/bookcorpus)(https://huggingface.co/datasets/bookcorpus) æ•°æ®é›†ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œæ‰€ä»¥æˆ‘ä»¬æœŸæœ›å¯¹ `[MASK]` çš„é¢„æµ‹èƒ½å¤Ÿåæ˜ è¿™äº›é¢†åŸŸã€‚ä¸ºäº†é¢„æµ‹ `[MASK]` ï¼Œæˆ‘ä»¬éœ€è¦ DistilBERT çš„ tokenizer æ¥å¤„ç†æ¨¡å‹çš„è¾“å…¥ï¼Œæ‰€ä»¥è®©æˆ‘ä»¬ä¹Ÿä» Hub ä¸‹è½½å®ƒï¼š

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
```

æœ‰äº† tokenizer å’Œæ¨¡å‹ï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥å°†æˆ‘ä»¬çš„ç¤ºä¾‹æ–‡æœ¬ä¼ é€’ç»™æ¨¡å‹ï¼Œæå– logitsï¼Œå¹¶æ‰“å°å‡ºå‰ 5 ä¸ªå€™é€‰è¯ï¼š

{#if fw === 'pt'}

```python
#####Pytorch}
import torch

inputs = tokenizer(text, return_tensors="pt")
token_logits = model(**inputs).logits
## æ‰¾åˆ° [MASK] çš„ä½ç½®å¹¶æå–å…¶ logits
mask_token_index = torch.where(inputs["input_ids"] == tokenizer.mask_token_id)[1]
mask_token_logits = token_logits[0, mask_token_index, :]
## é€‰æ‹©å…·æœ‰æœ€é«˜ logits çš„ [MASK] å€™é€‰è¯
top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()

for token in top_5_tokens:
    print(f"'>>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}'")####end
```

{:else}

```python
#####TensorFlow}
import numpy as np
import tensorflow as tf

inputs = tokenizer(text, return_tensors="np")
token_logits = model(**inputs).logits
## æ‰¾åˆ° [MASK] çš„ä½ç½®å¹¶æå–å…¶ logits
mask_token_index = np.argwhere(inputs["input_ids"] == tokenizer.mask_token_id)[0, 1]
mask_token_logits = token_logits[0, mask_token_index, :]
## é€‰æ‹©å…·æœ‰æœ€é«˜ logits çš„ [MASK] å€™é€‰è¯
## é€šè¿‡åœ¨ argsort å‰å¯¹æ•°ç»„å–è´Ÿ,æ¥å¾—åˆ°æœ€å¤§çš„ logits
top_5_tokens = np.argsort(-mask_token_logits)[:5].tolist()

for token in top_5_tokens:
    print(f">>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}")####end
```

{/if}

```python
'>>> This is a great deal.'
'>>> This is a great success.'
'>>> This is a great adventure.'
'>>> This is a great idea.'
'>>> This is a great feat.'
```

æˆ‘ä»¬å¯ä»¥ä»è¾“å‡ºä¸­çœ‹åˆ°ï¼Œæ¨¡å‹çš„é¢„æµ‹çš„æ˜¯æ—¥å¸¸æœ¯è¯­ï¼Œè¿™å¯èƒ½å¹¶ä¸å¥‡æ€ªï¼Œè€ƒè™‘åˆ°è‹±è¯­ç»´åŸºç™¾ç§‘çš„æ•°æ®é›†åŸºç¡€ã€‚ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•å°†è¿™ä¸ªé¢†åŸŸæ”¹å˜æˆç¨å¾®æ›´åŠ ç‹¬ç‰¹â€”â€”é«˜åº¦ä¸¤æåˆ†åŒ–çš„ç”µå½±è¯„è®ºï¼

### æ•°æ®é›† 

ä¸ºäº†å±•ç¤ºé¢†åŸŸé€‚åº”æ€§ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨è‘—åçš„ [å¤§å‹ç”µå½±è¯„è®ºæ•°æ®é›†(Large Movie Review Dataset)](https://huggingface.co/datasets/imdb)(https://huggingface.co/datasets/imdb) ï¼ˆæˆ–è€…ç®€ç§°ä¸º IMDbï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç”µå½±è¯„è®ºè¯­æ–™åº“ï¼Œé€šå¸¸ç”¨äºå¯¹æƒ…æ„Ÿåˆ†ææ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚é€šè¿‡åœ¨è¿™ä¸ªè¯­æ–™åº“ä¸Šå¯¹ DistilBERT è¿›è¡Œå¾®è°ƒï¼Œæˆ‘ä»¬æœŸæœ›è¯­è¨€æ¨¡å‹ä¼šä»å…¶é¢„è®­ç»ƒçš„ç»´åŸºç™¾ç§‘çš„äº‹å®æ€§æ•°æ®ï¼Œé€‚åº”åˆ°æ›´ä¸»è§‚çš„ç”µå½±è¯„è®ºçš„é¢†åŸŸã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨Datasets ä¸­çš„ `load_dataset()` å‡½æ•°ä» Hugging Face ä¸­è·å–æ•°æ®ï¼š

```python
from datasets import load_dataset

imdb_dataset = load_dataset("imdb")
imdb_dataset
```

```python
DatasetDict({
    train: Dataset({
        features: ['text', 'label'],
        num_rows: 25000
    })
    test: Dataset({
        features: ['text', 'label'],
        num_rows: 25000
    })
    unsupervised: Dataset({
        features: ['text', 'label'],
        num_rows: 50000
    })
})
```

æˆ‘ä»¬å¯ä»¥çœ‹åˆ° `train` å’Œ `test` åˆ†åˆ«åŒ…å«äº† 25,000 æ¡è¯„è®ºï¼Œè¿˜æœ‰ä¸€ä¸ªæ²¡æœ‰çš„æ ‡ç­¾çš„ `unsupervisedï¼ˆæ— ç›‘ç£ï¼‰` éƒ¨åˆ†åŒ…å« 50,000 æ¡è¯„è®ºã€‚æ¥ä¸‹æ¥è®©æˆ‘ä»¬çœ‹ä¸€äº›ç¤ºä¾‹ï¼Œæ¥äº†è§£ä¸€ä¸‹æˆ‘ä»¬æ­£åœ¨å¤„ç†çš„æ–‡æœ¬ç±»å‹ã€‚æ­£å¦‚æˆ‘ä»¬åœ¨æœ¬è¯¾ç¨‹çš„å‰å‡ ç« ä¸­æ‰€åšçš„é‚£æ ·ï¼Œæˆ‘ä»¬å°†æŠŠ `Dataset.shuffle()` å‡½æ•°é“¾æ¥åˆ° `Dataset.select()` å‡½æ•°åˆ›å»ºéšæœºæ ·æœ¬ï¼š

```python
sample = imdb_dataset["train"].shuffle(seed=42).select(range(3))

for row in sample:
    print(f"\n'>>> Review: {row['text']}'")
    print(f"'>>> Label: {row['label']}'")
```

```python

'>>> Review: This is your typical Priyadarshan movie--a bunch of loony characters out on some silly mission. His signature climax has the entire cast of the film coming together and fighting each other in some crazy moshpit over hidden money. Whether it is a winning lottery ticket in Malamaal Weekly, black money in Hera Pheri, "kodokoo" in Phir Hera Pheri, etc., etc., the director is becoming ridiculously predictable. Don\'t get me wrong; as clichÃ©d and preposterous his movies may be, I usually end up enjoying the comedy. However, in most his previous movies there has actually been some good humor, (Hungama and Hera Pheri being noteworthy ones). Now, the hilarity of his films is fading as he is using the same formula over and over again.<br /><br />Songs are good. Tanushree Datta looks awesome. Rajpal Yadav is irritating, and Tusshar is not a whole lot better. Kunal Khemu is OK, and Sharman Joshi is the best.'
'>>> Label: 0'

'>>> Review: Okay, the story makes no sense, the characters lack any dimensionally, the best dialogue is ad-libs about the low quality of movie, the cinematography is dismal, and only editing saves a bit of the muddle, but Sam" Peckinpah directed the film. Somehow, his direction is not enough. For those who appreciate Peckinpah and his great work, this movie is a disappointment. Even a great cast cannot redeem the time the viewer wastes with this minimal effort.<br /><br />The proper response to the movie is the contempt that the director San Peckinpah, James Caan, Robert Duvall, Burt Young, Bo Hopkins, Arthur Hill, and even Gig Young bring to their work. Watch the great Peckinpah films. Skip this mess.'
'>>> Label: 0'

'>>> Review: I saw this movie at the theaters when I was about 6 or 7 years old. I loved it then, and have recently come to own a VHS version. <br /><br />My 4 and 6 year old children love this movie and have been asking again and again to watch it. <br /><br />I have enjoyed watching it again too. Though I have to admit it is not as good on a little TV.<br /><br />I do not have older children so I do not know what they would think of it. <br /><br />The songs are very cute. My daughter keeps singing them over and over.<br /><br />Hope this helps.'
'>>> Label: 1'
```

æ˜¯çš„ï¼Œè¿™äº›è‚¯å®šæ˜¯ç”µå½±è¯„è®ºï¼Œå¦‚æœä½ å¹´é¾„è¶³å¤Ÿå¤§ï¼Œä½ ç”šè‡³å¯èƒ½ä¼šç†è§£ä¸Šæ¬¡è¯„è®ºä¸­å…³äºæ‹¥æœ‰ VHS ç‰ˆæœ¬çš„è¯„è®ºğŸ˜œï¼è™½ç„¶è¯­è¨€æ¨¡å‹ä¸éœ€è¦é¢„å…ˆæ ‡æ³¨å¥½çš„æ ‡ç­¾ï¼Œä½†æˆ‘ä»¬å·²ç»å¯ä»¥çœ‹åˆ°æ•°æ®é›†å…¶å®åŒ…å«äº†æ ‡ç­¾ï¼Œ `0` ä»£è¡¨è´Ÿé¢è¯„è®ºï¼Œ `1` ä»£è¡¨æ­£é¢è¯„è®ºã€‚

<div custom-style="Tip-green">

âœï¸ **è¯•ä¸€è¯•ï¼** åˆ›å»ºä¸€ä¸ª `unsupervised` éƒ¨åˆ†çš„éšæœºæ ·æœ¬ï¼Œå¹¶éªŒè¯å…¶æ ‡ç­¾æ—¢ä¸æ˜¯ `0` ä¹Ÿä¸æ˜¯ `1` ã€‚å½“ä½ è¿™æ ·åšæ—¶ï¼Œä½ ä¹Ÿå¯ä»¥æ£€æŸ¥ `train` å’Œ `test` éƒ¨åˆ†çš„æ ‡ç­¾ç¡®å®æ˜¯ `0` æˆ– `1` â€”â€” è¿™æ˜¯æ¯ä¸ª NLP å®è·µè€…åœ¨å¼€å§‹æ–°é¡¹ç›®æ—¶åº”è¯¥è¿›è¡Œçš„æœ‰ç”¨çš„åˆç†çš„æ£€æŸ¥ï¼

</div>

ç°åœ¨æˆ‘ä»¬å·²ç»å¿«é€Ÿæµè§ˆäº†ä¸€ä¸‹æ•°æ®ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬è¦æ·±å…¥å‡†å¤‡è¿™äº›æ•°æ®ä»¥ä¾›è¿›è¡Œæ©ç è¯­è¨€å»ºæ¨¡ã€‚å¦‚æˆ‘ä»¬æ‰€è§ï¼Œä¸æˆ‘ä»¬åœ¨ç¬¬å››ç« çœ‹åˆ°çš„åºåˆ—åˆ†ç±»ä»»åŠ¡ç›¸æ¯”ï¼Œè¿™é‡Œéœ€è¦é‡‡å–ä¸€äº›é¢å¤–çš„æ­¥éª¤ã€‚è®©æˆ‘ä»¬å¼€å§‹å§ï¼

### é¢„å¤„ç†æ•°æ® 

å¯¹äºè‡ªå›å½’å’Œæ©ç è¯­è¨€å»ºæ¨¡ï¼Œå¸¸è§çš„é¢„å¤„ç†æ­¥éª¤æ˜¯å°†æ‰€æœ‰çš„ç¤ºä¾‹è¿æ¥èµ·æ¥ï¼Œç„¶åå°†æ•´ä¸ªè¯­æ–™åº“åˆ‡å‰²ä¸ºç­‰å¤§å°çš„å—ã€‚è¿™ä¸æˆ‘ä»¬é€šå¸¸çš„åšæ³•æœ‰å¾ˆå¤§çš„ä¸åŒï¼Œæˆ‘ä»¬é€šå¸¸åªæ˜¯å¯¹å•ä¸ªçš„ç¤ºä¾‹è¿›è¡Œ tokenizeã€‚ä¸ºä»€ä¹ˆè¦å°†æ‰€æœ‰çš„ç¤ºä¾‹è¿æ¥åœ¨ä¸€èµ·å‘¢ï¼ŸåŸå› æ˜¯å¦‚æœå•ä¸ªç¤ºä¾‹å¤ªé•¿ï¼Œå¯èƒ½ä¼šè¢«æˆªæ–­ï¼Œè¿™ä¼šå¯¼è‡´æˆ‘ä»¬å¤±å»å¯èƒ½å¯¹è¯­è¨€å»ºæ¨¡ä»»åŠ¡æœ‰ç”¨çš„ä¿¡æ¯ï¼

å› æ­¤ï¼Œæˆ‘ä»¬é¦–å…ˆåƒå¾€å¸¸ä¸€æ ·å¯¹è¯­æ–™åº“è¿›è¡Œ tokenize å¤„ç†ï¼Œä½†æ˜¯ä¸åœ¨ tokenizer ä¸­è®¾ç½® `truncation=True` é€‰é¡¹ã€‚å¦‚æœæˆ‘ä»¬æœ‰å¯ä»¥ä½¿ç”¨å¿«é€Ÿ tokenizerï¼ˆå¦‚ç¬¬ä¸ƒç« ä¸­æ‰€è¿°ï¼‰ï¼Œæˆ‘ä»¬è¿˜å°†è·å–å•è¯çš„ IDï¼Œå› ä¸ºåé¢æˆ‘ä»¬éœ€è¦ç”¨åˆ°å®ƒä»¬æ¥è¿›è¡Œæ•´è¯æ©ç ã€‚æˆ‘ä»¬å°†æŠŠè¿™ä¸ªè¿‡ç¨‹å°è£…åœ¨ä¸€ä¸ªç®€å•çš„å‡½æ•°ä¸­ï¼Œå¹¶åˆ é™¤ text å’Œ label åˆ—ï¼Œå› ä¸ºæˆ‘ä»¬ä¸å†éœ€è¦å®ƒä»¬ã€‚

```python
def tokenize_function(examples):
    result = tokenizer(examples["text"])
    if tokenizer.is_fast:
        result["word_ids"] = [result.word_ids(i) for i in range(len(result["input_ids"]))]
    return result

## ä½¿ç”¨ batched=True æ¥æ¿€æ´»å¿«é€Ÿå¤šçº¿ç¨‹!
tokenized_datasets = imdb_dataset.map(
    tokenize_function, batched=True, remove_columns=["text", "label"]
)
tokenized_datasets
```

```python
DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'input_ids', 'word_ids'],
        num_rows: 25000
    })
    test: Dataset({
        features: ['attention_mask', 'input_ids', 'word_ids'],
        num_rows: 25000
    })
    unsupervised: Dataset({
        features: ['attention_mask', 'input_ids', 'word_ids'],
        num_rows: 50000
    })
})
```

ç”±äº DistilBERT æ˜¯ä¸€ä¸ªç±»ä¼¼ BERT çš„æ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ç¼–ç åçš„æ–‡æœ¬åŒ…å«äº†æˆ‘ä»¬åœ¨ä¹‹å‰ç« èŠ‚ä¸­çœ‹åˆ°çš„ `input_ids` å’Œ `attention_mask` ï¼Œä»¥åŠæˆ‘ä»¬æ·»åŠ çš„ `word_ids` ã€‚

ç°åœ¨æˆ‘ä»¬å·²ç»å¯¹ç”µå½±è¯„è®ºè¿›è¡Œäº† tokenizeï¼Œä¸‹ä¸€æ­¥æ˜¯å°†å®ƒä»¬å…¨éƒ¨ç»„åˆåœ¨ä¸€èµ·å¹¶å°†ç»“æœåˆ†å‰²æˆå—ã€‚ä½†æ˜¯ï¼Œè¿™äº›å—åº”è¯¥æœ‰å¤šå¤§å‘¢ï¼Ÿè¿™æœ€ç»ˆå°†å–å†³äºä½ å¯ä»¥ä½¿ç”¨çš„ GPU å†…å­˜å¤§å°ï¼Œä½†ä¸€ä¸ªå¥½çš„èµ·ç‚¹æ˜¯æŸ¥çœ‹æ¨¡å‹çš„æœ€å¤§ä¸Šä¸‹æ–‡å¤§å°ã€‚è¿™å¯ä»¥é€šè¿‡æŸ¥çœ‹ tokenizer çš„ `model_max_length` å±æ€§æ¥æ¨æ–­ï¼š

```python
tokenizer.model_max_length
```

```python
512
```

è¯¥å€¼æ¥è‡ªäºä¸ checkpoint ç›¸å…³è”çš„ `tokenizer_config.json` æ–‡ä»¶ï¼›åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¸Šä¸‹æ–‡å¤§å°æ˜¯ 512 ä¸ª tokens å°±åƒ BERT ä¸€æ ·ã€‚

<div custom-style="Tip-green">

âœï¸ **è¯•è¯•çœ‹ï¼** ä¸€äº› Transformer æ¨¡å‹ï¼Œä¾‹å¦‚ [BigBird](https://huggingface.co/google/bigbird-roberta-base)(https://huggingface.co/google/bigbird-roberta-base) å’Œ [Longformer](hf.co/allenai/longformer-base-4096)(hf.co/allenai/longformer-base-4096) ï¼Œå®ƒä»¬å…·æœ‰æ¯” BERT å’Œå…¶ä»–æ—©æœŸ Transformer æ¨¡å‹æ›´é•¿çš„ä¸Šä¸‹æ–‡é•¿åº¦ã€‚é€‰æ‹©ä¸€ä¸ª checkpoint å®ä¾‹åŒ– tokenizer å¹¶éªŒè¯ `model_max_length` æ˜¯å¦ä¸æ¨¡å‹å¡ä¸Šå¼•ç”¨çš„å†…å®¹ä¸€è‡´ã€‚

</div>

å› æ­¤ï¼Œä»¥ä¾¿åœ¨åƒ Google Colab é‚£æ ·çš„ GPU ä¸Šè¿è¡Œæˆ‘ä»¬çš„å®éªŒï¼Œæˆ‘ä»¬ä¼šé€‰æ‹©ä¸€ä¸ªç¨å°ä¸€ç‚¹ã€å¯ä»¥æ”¾å…¥å†…å­˜ä¸­çš„åˆ†å—å°ºå¯¸ï¼š

```python
chunk_size = 128
```

<div custom-style="Tip-green">

æ³¨æ„ï¼Œåœ¨å®é™…åº”ç”¨åœºæ™¯ä¸­ï¼Œä½¿ç”¨å°çš„å—å°ºå¯¸å¯èƒ½ä¼šæœ‰ä¸åˆ©çš„å½±å“ï¼Œæ‰€ä»¥ä½ åº”è¯¥é€‰æ‹©ä¸€ä¸ªä¸ä½ å°†è¦ä½¿ç”¨æ¨¡å‹çš„ç”¨ä¾‹ç›¸åŒ¹é…çš„å¤§å°ã€‚

</div>

ç°åœ¨æ¥åˆ°äº†æœ‰è¶£çš„éƒ¨åˆ†ã€‚ä¸ºäº†å±•ç¤ºå¦‚ä½•æŠŠè¿™äº›ç¤ºä¾‹è¿æ¥åœ¨ä¸€ï¼Œæˆ‘ä»¬ä»åˆ†è¯åçš„è®­ç»ƒé›†ä¸­å–å‡ºå‡ ä¸ªè¯„è®ºï¼Œå¹¶æ‰“å°å‡ºæ¯ä¸ªè¯„è®ºçš„ token æ•°é‡ï¼š

```python
## åˆ‡ç‰‡ä¼šä¸ºæ¯ä¸ªç‰¹å¾ç”Ÿæˆä¸€ä¸ªåˆ—è¡¨çš„åˆ—è¡¨
tokenized_samples = tokenized_datasets["train"][:3]

for idx, sample in enumerate(tokenized_samples["input_ids"]):
    print(f"'>>> Review {idx} length: {len(sample)}'")
```

```python
'>>> Review 0 length: 200'
'>>> Review 1 length: 559'
'>>> Review 2 length: 192'
```

ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ä¸€ä¸ªç®€å•çš„å­—å…¸æ¨å¯¼å¼å°†æ‰€æœ‰è¿™äº›ç¤ºä¾‹è¿æ¥åœ¨ä¸€èµ·ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```python
concatenated_examples = {
    k: sum(tokenized_samples[k], []) for k in tokenized_samples.keys()
}
total_length = len(concatenated_examples["input_ids"])
print(f"'>>> Concatenated reviews length: {total_length}'")
```

```python
'>>> Concatenated reviews length: 951'
```

å¾ˆæ£’ï¼Œæ€»é•¿åº¦è®¡ç®—å‡ºæ¥äº† â€”â€” ç°åœ¨ï¼Œè®©æˆ‘ä»¬å°†è¿æ¥çš„è¯„è®ºæ‹†åˆ†ä¸ºå¤§å°ä¸º `block_size` çš„å—ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬è¿­ä»£äº† `concatenated_examples` ä¸­çš„ç‰¹å¾ï¼Œå¹¶ä½¿ç”¨åˆ—è¡¨æ¨å¯¼å¼åˆ›å»ºæ¯ä¸ªç‰¹å¾çš„åˆ‡ç‰‡ã€‚ç»“æœæ˜¯ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«æ¯ä¸ªç‰¹å¾å¯¹åº”çš„å—ï¼š

```python
chunks = {
    k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]
    for k, t in concatenated_examples.items()
}

for chunk in chunks["input_ids"]:
    print(f"'>>> Chunk length: {len(chunk)}'")
```

```python
'>>> Chunk length: 128'
'>>> Chunk length: 128'
'>>> Chunk length: 128'
'>>> Chunk length: 128'
'>>> Chunk length: 128'
'>>> Chunk length: 128'
'>>> Chunk length: 128'
'>>> Chunk length: 55'
```

æ­£å¦‚ä½ åœ¨è¿™ä¸ªä¾‹å­ä¸­çœ‹åˆ°çš„ï¼Œæœ€åä¸€ä¸ªå—é€šå¸¸ä¼šå°äºæœ€å¤§å—å¤§å°ã€‚æœ‰ä¸¤ç§ä¸»è¦çš„ç­–ç•¥æ¥å¤„ç†è¿™ä¸ªé—®é¢˜ï¼š

* å¦‚æœæœ€åä¸€ä¸ªå—å°äº `chunk_size` ï¼Œå°±ä¸¢å¼ƒã€‚
* å¡«å……æœ€åä¸€ä¸ªå—ï¼Œç›´åˆ°å…¶é•¿åº¦ç­‰äº `chunk_size` ã€‚

æˆ‘ä»¬å°†åœ¨è¿™é‡Œé‡‡ç”¨ç¬¬ä¸€ç§æ–¹æ³•ï¼Œå› æ­¤è®©æˆ‘ä»¬å°†ä¸Šè¿°æ‰€æœ‰é€»è¾‘åŒ…è£…åœ¨ä¸€ä¸ªå‡½æ•°ä¸­ï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥å°†å…¶åº”ç”¨äºæˆ‘ä»¬çš„å·²åˆ†è¯æ•°æ®é›†ä¸Šï¼š

```python
def group_texts(examples):
    # æ‹¼æ¥æ‰€æœ‰çš„æ–‡æœ¬
    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}
    # è®¡ç®—æ‹¼æ¥æ–‡æœ¬çš„é•¿åº¦
    total_length = len(concatenated_examples[list(examples.keys())[0]])
    # å¦‚æœæœ€åä¸€ä¸ªå—å°äº chunk_size,æˆ‘ä»¬å°†å…¶ä¸¢å¼ƒ
    total_length = (total_length // chunk_size) * chunk_size
    # æŒ‰æœ€å¤§é•¿åº¦åˆ†å—
    result = {
        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]
        for k, t in concatenated_examples.items()
    }
    # åˆ›å»ºä¸€ä¸ªæ–°çš„ labels åˆ—
    result["labels"] = result["input_ids"].copy()
    return result
```

æ³¨æ„ï¼Œåœ¨ `group_texts()` çš„æœ€åä¸€æ­¥ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªæ–°çš„ `labels` åˆ—ï¼Œå®ƒæ˜¯é€šè¿‡å¤åˆ¶ `input_ids` åˆ—å½¢æˆçš„ã€‚è¿™æ˜¯å› ä¸ºåœ¨æ©ç è¯­è¨€æ¨¡å‹ä¸­ï¼Œç›®æ ‡æ˜¯é¢„æµ‹è¾“å…¥æ‰¹ä¸­éšæœºæ©ç çš„ token é€šè¿‡åˆ›å»º `labels` åˆ—ï¼Œæˆ‘ä»¬ä¿å­˜äº†è®©æˆ‘ä»¬çš„è¯­è¨€æ¨¡å‹ä»ä¸­å­¦ä¹  `[Mask]` çš„ç­”æ¡ˆã€‚

ç°åœ¨ï¼Œè®©æˆ‘ä»¬ä½¿ç”¨æˆ‘ä»¬å¼ºå¤§çš„ `Dataset.map()` å‡½æ•°å°† `group_texts()` åº”ç”¨åˆ°æˆ‘ä»¬çš„å·²åˆ†è¯æ•°æ®é›†ä¸Šï¼š

```python
lm_datasets = tokenized_datasets.map(group_texts, batched=True)
lm_datasets
```

```python
DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'input_ids', 'labels', 'word_ids'],
        num_rows: 61289
    })
    test: Dataset({
        features: ['attention_mask', 'input_ids', 'labels', 'word_ids'],
        num_rows: 59905
    })
    unsupervised: Dataset({
        features: ['attention_mask', 'input_ids', 'labels', 'word_ids'],
        num_rows: 122963
    })
})
```

é€šè¿‡å¯¹æ–‡æœ¬è¿›è¡Œåˆ†ç»„å’Œå—åˆ†æï¼Œæˆ‘ä»¬å¾—åˆ°çš„ä¾‹å­æ¯”åŸæ¥çš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„ 25000 ä¸ªä¾‹å­è¦å¤šå¾—å¤šã€‚è¿™æ˜¯å› ä¸ºæˆ‘ä»¬ç°åœ¨æœ‰äº†æ¶‰åŠè·¨è¶ŠåŸå§‹è¯­æ–™åº“ä¸­å¤šä¸ªä¾‹å­çš„è¿ç»­æ ‡è®°çš„ä¾‹å­ã€‚ä½ å¯ä»¥é€šè¿‡åœ¨å…¶ä¸­ä¸€ä¸ªå—ä¸­æŸ¥æ‰¾ç‰¹æ®Šçš„ `[SEP]` å’Œ `[CLS]` tokens æ¥æ¸…æ™°åœ°çœ‹åˆ°è¿™ä¸€ç‚¹ï¼š

```python
tokenizer.decode(lm_datasets["train"][1]["input_ids"])
```

```python
".... at.......... high. a classic line : inspector : i'm here to sack one of your teachers. student : welcome to bromwell high. i expect that many adults of my age think that bromwell high is far fetched. what a pity that it isn't! [SEP] [CLS] homelessness ( or houselessness as george carlin stated ) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. most people think of the homeless"
```

åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œä½ å¯ä»¥çœ‹åˆ°ä¸¤ä¸ªé‡å çš„ç”µå½±è¯„è®ºï¼Œä¸€ä¸ªå…³äºé«˜ä¸­ç”µå½±ï¼Œå¦ä¸€ä¸ªå…³äºæ— å®¶å¯å½’çš„é—®é¢˜ã€‚è®©æˆ‘ä»¬ä¹Ÿæ£€æŸ¥ä¸€ä¸‹æ©ç è¯­è¨€æ¨¡å‹çš„æ ‡ç­¾æ˜¯ä»€ä¹ˆæ ·çš„ï¼š

```python
tokenizer.decode(lm_datasets["train"][1]["labels"])
```

```python
".... at.......... high. a classic line : inspector : i'm here to sack one of your teachers. student : welcome to bromwell high. i expect that many adults of my age think that bromwell high is far fetched. what a pity that it isn't! [SEP] [CLS] homelessness ( or houselessness as george carlin stated ) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. most people think of the homeless"
```

æ­£å¦‚æˆ‘ä»¬ä¸Šé¢çš„ `group_texts()` å‡½æ•°æ‰€é¢„æœŸçš„é‚£æ ·ï¼Œè¿™çœ‹èµ·æ¥ä¸è§£ç çš„ `input_ids` å®Œå…¨ç›¸åŒ â€”â€” ä½†æ˜¯è¦æ€ä¹ˆæ ·æ‰èƒ½è®©æˆ‘ä»¬çš„çš„æ¨¡å‹å¯ä»¥å­¦ä¹ åˆ°ä¸€äº›ä¸œè¥¿å‘¢ï¼Ÿæˆ‘ä»¬ç¼ºå°‘ä¸€ä¸ªå…³é”®çš„æ­¥éª¤ï¼šåœ¨è¾“å…¥ä¸­éšæœºæ’å…¥ `[MASK]` tokenï¼è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•åœ¨å¾®è°ƒæœŸé—´ä½¿ç”¨ç‰¹æ®Šçš„æ•°æ®æ”¶é›†å™¨æ¥å®æ—¶å®Œæˆè¿™ä¸ªæ­¥éª¤ã€‚

### ä½¿ç”¨ `Trainer` API å¾®è°ƒ DistilBERT 

å¾®è°ƒæ©ç è¯­è¨€æ¨¡å‹å‡ ä¹ä¸å¾®è°ƒåºåˆ—åˆ†ç±»æ¨¡å‹ç›¸åŒï¼Œå°±åƒæˆ‘ä»¬åœ¨ç¬¬å››ç« æ‰€åšçš„é‚£æ ·ã€‚å”¯ä¸€çš„åŒºåˆ«æ˜¯æˆ‘ä»¬éœ€è¦ä¸€ä¸ªç‰¹æ®Šçš„æ•°æ®æ•´ç†å™¨ï¼Œå®ƒå¯ä»¥éšæœºå±è”½æ¯æ‰¹æ–‡æœ¬ä¸­çš„ä¸€äº› tokens å¹¸è¿çš„æ˜¯ï¼ŒTransformers ä¸ºè¿™é¡¹ä»»åŠ¡å‡†å¤‡äº†ä¸“ç”¨çš„ `DataCollatorForLanguageModeling` ã€‚æˆ‘ä»¬åªéœ€è¦å°† tokenizer å’Œä¸€ä¸ª `mlm_probability` å‚æ•°ï¼ˆæŒ‡å®šæ©ç›– tokens çš„æ¯”ä¾‹ï¼‰ä¼ é€’ç»™å®ƒã€‚æˆ‘ä»¬å°†é€‰æ‹© 15ï¼…ï¼Œè¿™æ˜¯ BERT æ‰€ä½¿ç”¨çš„æ•°é‡ï¼Œä¹Ÿæ˜¯æ–‡çŒ®ä¸­å¸¸è§çš„é€‰æ‹©ï¼š

```python
from transformers import DataCollatorForLanguageModeling

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)
```

ä¸ºäº†äº†è§£éšæœºæ©ç çš„å·¥ä½œåŸç†ï¼Œè®©æˆ‘ä»¬æŠŠä¸€äº›ä¾‹å­è¾“å…¥åˆ°æ•°æ®æ•´ç†å™¨ã€‚ç”±äºæ•°æ®æ•´ç†å™¨æœŸæœ›æ¥æ”¶ä¸€ä¸ªå­—å…¸åˆ—è¡¨ï¼Œå…¶ä¸­æ¯ä¸ªå­—å…¸è¡¨ç¤ºä¸€æ®µè¿ç»­æ–‡æœ¬çš„å—ï¼Œæ‰€ä»¥æˆ‘ä»¬é¦–å…ˆéå†æ•°æ®é›†ï¼Œç„¶åå°†æ‰¹é‡çš„æ•°æ®è¾“å…¥åˆ°æ•´ç†å™¨ã€‚åœ¨è¿™ä¸ªæ•°æ®æ•´ç†å™¨ä¸­ï¼Œæˆ‘ä»¬åˆ é™¤äº† `word_ids` è¿™ä¸ªé”®ï¼Œå› ä¸ºå®ƒä¸éœ€è¦è¿™ä¸ªé”®ã€‚

```python
samples = [lm_datasets["train"][i] for i in range(2)]
for sample in samples:
    _ = sample.pop("word_ids")

for chunk in data_collator(samples)["input_ids"]:
    print(f"\n'>>> {tokenizer.decode(chunk)}'")
```

```python
'>>> [CLS] bromwell [MASK] is a cartoon comedy. it ran at the same [MASK] as some other [MASK] about school life, [MASK] as " teachers ". [MASK] [MASK] [MASK] in the teaching [MASK] lead [MASK] to believe that bromwell high\'[MASK] satire is much closer to reality than is " teachers ". the scramble [MASK] [MASK] financially, the [MASK]ful students whogn [MASK] right through [MASK] pathetic teachers\'pomp, the pettiness of the whole situation, distinction remind me of the schools i knew and their students. when i saw [MASK] episode in [MASK] a student repeatedly tried to burn down the school, [MASK] immediately recalled. [MASK]...'

'>>> .... at.. [MASK]... [MASK]... high. a classic line plucked inspector : i\'[MASK] here to [MASK] one of your [MASK]. student : welcome to bromwell [MASK]. i expect that many adults of my age think that [MASK]mwell [MASK] is [MASK] fetched. what a pity that it isn\'t! [SEP] [CLS] [MASK]ness ( or [MASK]lessness as george å®‡in stated )å…¬ been an issue for years but never [MASK] plan to help those on the street that were once considered human [MASK] did everything from going to school, [MASK], [MASK] vote for the matter. most people think [MASK] the homeless'
```

å¾ˆæ£’ï¼ŒæˆåŠŸäº†ï¼æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œ `[MASK]` tokens å·²éšæœºæ’å…¥æˆ‘ä»¬æ–‡æœ¬ä¸­çš„ä¸åŒä½ç½®ã€‚è¿™äº›å°†æ˜¯æˆ‘ä»¬çš„æ¨¡å‹åœ¨è®­ç»ƒæœŸé—´å¿…é¡»é¢„æµ‹çš„ tokens  â€”â€” æ•°æ®æ•´ç†å™¨çš„ç¾å¦™ä¹‹å¤„åœ¨äºï¼Œå®ƒä¼šåœ¨æ¯ä¸ª batch ä¸­éšæœºæ’å…¥ `[MASK]` ï¼

<div custom-style="Tip-green">

âœï¸ **è¯•ä¸€è¯•ï¼** è¿è¡Œä¸Šé¢çš„ä»£ç ç‰‡æ®µå‡ æ¬¡ï¼Œäº²çœ¼çœ‹çœ‹éšæœºé®è”½çš„è¿‡ç¨‹ï¼ä¹Ÿå¯ä»¥ç”¨ `tokenizer.convert_ids_to_tokens()` æ›¿æ¢ `tokenizer.decode()` æ–¹æ³•ï¼Œçœ‹çœ‹åªæŠŠä¸€ä¸ªç»™å®šå•è¯çš„å•ä¸ª token é®è”½ï¼Œè€Œä¿æŒè¿™ä¸ªå•è¯å…¶ä»– tokens ä¸å˜çš„æ•ˆæœã€‚

</div>

{#if fw === 'pt'}

éšæœºæ©ç çš„ä¸€ä¸ªå‰¯ä½œç”¨æ˜¯ï¼Œå½“ä½¿ç”¨ `Trainer` æ—¶ï¼Œæˆ‘ä»¬çš„è¯„ä¼°æŒ‡æ ‡å°†ä¸æ˜¯ç¡®å®šæ€§çš„ï¼Œå› ä¸ºæˆ‘ä»¬ä¼šå¯¹è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä½¿ç”¨ç›¸åŒçš„æ•°æ®æ•´ç†å™¨ã€‚ç¨åæˆ‘ä»¬å°†çœ‹åˆ°ï¼Œå½“æˆ‘ä»¬ä½¿ç”¨ Accelerate è¿›è¡Œå¾®è°ƒæ—¶ï¼Œæˆ‘ä»¬å°†å¦‚ä½•åˆ©ç”¨è‡ªå®šä¹‰è¯„ä¼°å¾ªç¯çš„çµæ´»æ€§æ¥å†»ç»“éšæœºæ€§ã€‚

{/if}

åœ¨ä¸ºæ©ç è¯­è¨€å»ºæ¨¡è®­ç»ƒæ¨¡å‹æ—¶ï¼Œå¯ä»¥ä½¿ç”¨çš„ä¸€ç§æŠ€æœ¯æ˜¯ä¸€æ¬¡é®è”½æ•´ä¸ªå•è¯ï¼Œè€Œä¸ä»…ä»…æ˜¯å•ä¸ªçš„ tokens è¿™ç§æ–¹æ³•ç§°ä¸º `å…¨è¯å±è”½ï¼ˆwhole word maskingï¼‰` ã€‚å¦‚æœæˆ‘ä»¬æƒ³ä½¿ç”¨å…¨è¯å±è”½ï¼ˆwhole word maskingï¼‰ï¼Œæˆ‘ä»¬éœ€è¦è‡ªå·±æ„å»ºä¸€ä¸ªæ•°æ®æ•´ç†å™¨ã€‚æ•°æ®æ•´ç†å™¨åªæ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œå®ƒæ¥å—ä¸€ä¸ªæ ·æœ¬åˆ—è¡¨å¹¶å°†å®ƒä»¬è½¬æ¢ä¸ºä¸€ä¸ª batchï¼Œæ‰€ä»¥ç°åœ¨è®©æˆ‘ä»¬è¿™æ ·åšå§ï¼æˆ‘ä»¬å°†ä½¿ç”¨å…ˆå‰è®¡ç®—çš„å•è¯ IDï¼Œæ„å»ºä¸€ä¸ªå•è¯ç´¢å¼•å’Œç›¸åº” token ä¹‹é—´çš„æ˜ å°„ï¼Œç„¶åéšæœºå†³å®šé®è”½å“ªäº›å•è¯ï¼Œå¹¶ä½¿ç”¨è¿™ç§æ–¹æ³•å¯¹è¾“å…¥è¿›è¡Œé®è”½ã€‚è¯·æ³¨æ„ï¼Œé™¤äº†ä¸æ©ç å¯¹åº”çš„æ ‡ç­¾å¤–ï¼Œæ‰€æœ‰å…¶ä»–çš„æ ‡ç­¾å‡ä¸º `-100` ã€‚

{#if fw === 'pt'}

```python
#####Pytorch}
import collections
import numpy as np

from transformers import default_data_collator

wwm_probability = 0.2

def whole_word_masking_data_collator(features):
    for feature in features:
        word_ids = feature.pop("word_ids")

        # åˆ›å»ºä¸€ä¸ªå•è¯ä¸å¯¹åº” token ç´¢å¼•ä¹‹é—´çš„æ˜ å°„
        mapping = collections.defaultdict(list)
        current_word_index = -1
        current_word = None
        for idx, word_id in enumerate(word_ids):
            if word_id is not None:
                if word_id != current_word:
                    current_word = word_id
                    current_word_index += 1
                mapping[current_word_index].append(idx)

        # éšæœºé®è”½å•è¯
        mask = np.random.binomial(1, wwm_probability, (len(mapping),))
        input_ids = feature["input_ids"]
        labels = feature["labels"]
        new_labels = [-100] * len(labels)
        for word_id in np.where(mask)[0]:
            word_id = word_id.item()
            for idx in mapping[word_id]:
                new_labels[idx] = labels[idx]
                input_ids[idx] = tokenizer.mask_token_id
        feature["labels"] = new_labels

    return default_data_collator(features)####end
```

{:else}

```python
#####TensorFlow}
import collections
import numpy as np

from transformers.data.data_collator import tf_default_data_collator

wwm_probability = 0.2

def whole_word_masking_data_collator(features):
    for feature in features:
        word_ids = feature.pop("word_ids")

        # åˆ›å»ºä¸€ä¸ªå•è¯ä¸å¯¹åº” token ç´¢å¼•ä¹‹é—´çš„æ˜ å°„
        mapping = collections.defaultdict(list)
        current_word_index = -1
        current_word = None
        for idx, word_id in enumerate(word_ids):
            if word_id is not None:
                if word_id != current_word:
                    current_word = word_id
                    current_word_index += 1
                mapping[current_word_index].append(idx)

        # éšæœºé®è”½å•è¯
        mask = np.random.binomial(1, wwm_probability, (len(mapping),))
        input_ids = feature["input_ids"]
        labels = feature["labels"]
        new_labels = [-100] * len(labels)
        for word_id in np.where(mask)[0]:
            word_id = word_id.item()
            for idx in mapping[word_id]:
                new_labels[idx] = labels[idx]
                input_ids[idx] = tokenizer.mask_token_id
        feature["labels"] = new_labels

    return tf_default_data_collator(features)####end
```

{/if}

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨å’Œä¹‹å‰ç›¸åŒçš„æ ·æœ¬ä¸Šè¯•è¯•å®ƒï¼š

```python
samples = [lm_datasets["train"][i] for i in range(2)]
batch = whole_word_masking_data_collator(samples)

for chunk in batch["input_ids"]:
    print(f"\n'>>> {tokenizer.decode(chunk)}'")
```

```python
'>>> [CLS] bromwell high is a cartoon comedy [MASK] it ran at the same time as some other programs about school life, such as " teachers ". my 35 years in the teaching profession lead me to believe that bromwell high\'s satire is much closer to reality than is " teachers ". the scramble to survive financially, the insightful students who can see right through their pathetic teachers\'pomp, the pettiness of the whole situation, all remind me of the schools i knew and their students. when i saw the episode in which a student repeatedly tried to burn down the school, i immediately recalled.....'

'>>> .... [MASK] [MASK] [MASK] [MASK]....... high. a classic line : inspector : i\'m here to sack one of your teachers. student : welcome to bromwell high. i expect that many adults of my age think that bromwell high is far fetched. what a pity that it isn\'t! [SEP] [CLS] homelessness ( or houselessness as george carlin stated ) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. most people think of the homeless'
```

<div custom-style="Tip-green">

âœï¸ **è¯•è¯•çœ‹ï¼** å¤šæ¬¡è¿è¡Œä¸Šé¢çš„ä»£ç ç‰‡æ®µï¼Œäº²çœ¼çœ‹çœ‹éšæœºé®è”½çš„æ•ˆæœï¼ä¹Ÿå¯ä»¥å°† `tokenizer.decode()` æ–¹æ³•æ›¿æ¢ä¸º `tokenizer.convert_ids_to_tokens()` ï¼Œå¯ä»¥è§‚å¯Ÿåˆ°ç»™å®šå•è¯çš„ tokens æ€»æ˜¯è¢«ä¸€èµ·é®è”½ã€‚

</div>

ç°åœ¨æˆ‘ä»¬æœ‰äº†ä¸¤ä¸ªæ•°æ®æ•´ç†å™¨ï¼Œå‰©ä¸‹çš„å¾®è°ƒæ­¥éª¤éƒ½æ˜¯æ ‡å‡†çš„ã€‚å¦‚æœä½ åœ¨ Google Colab ä¸Šè¿è¡Œå¹¶ä¸”æ²¡æœ‰å¹¸è¿åœ°å¾—åˆ°ç¥ç§˜çš„ P100 GPUğŸ˜­ï¼Œé‚£ä¹ˆè®­ç»ƒå¯èƒ½ä¼šéœ€è¦ä¸€äº›æ—¶é—´ï¼Œæ‰€ä»¥æˆ‘ä»¬é¦–å…ˆå°†è®­ç»ƒé›†çš„å¤§å°é™ä½åˆ°å‡ åƒä¸ªä¾‹å­ã€‚ä¸ç”¨æ‹…å¿ƒï¼Œæˆ‘ä»¬ä»ç„¶å¯ä»¥å¾—åˆ°ä¸€ä¸ªç›¸å½“ä¸é”™çš„è¯­è¨€æ¨¡å‹ï¼åœ¨ Datasets ä¸­å¿«é€Ÿä¸‹é‡‡æ ·æ•°æ®é›†çš„æ–¹æ³•æ˜¯ä½¿ç”¨æˆ‘ä»¬åœ¨ç¬¬å…­ç« ä¸­çœ‹åˆ°çš„ `Dataset.train_test_split()` å‡½æ•°ï¼š

```python
train_size = 10_000
test_size = int(0.1 * train_size)

downsampled_dataset = lm_datasets["train"].train_test_split(
    train_size=train_size, test_size=test_size, seed=42
)
downsampled_dataset
```

```python
DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'input_ids', 'labels', 'word_ids'],
        num_rows: 10000
    })
    test: Dataset({
        features: ['attention_mask', 'input_ids', 'labels', 'word_ids'],
        num_rows: 1000
    })
})
```

è¿™ä¼šè‡ªåŠ¨åˆ›å»ºæ–°çš„ `train` å’Œ `test` æ•°æ®é›†ï¼Œè®­ç»ƒé›†å¤§å°è®¾ç½®ä¸º 10,000 ä¸ªç¤ºä¾‹ï¼ŒéªŒè¯è®¾ç½®ä¸ºå…¶ä¸­çš„ 10ï¼… â€”â€” å¦‚æœä½ æœ‰ä¸€ä¸ªå¼ºå¤§çš„ GPUï¼Œå¯ä»¥è‡ªç”±å¢åŠ è¿™ä¸ªæ¯”ä¾‹ï¼æˆ‘ä»¬æ¥ä¸‹æ¥è¦åšçš„äº‹æƒ…æ˜¯ç™»å½• Hugging Face Hubã€‚å¦‚æœä½ åœ¨ Notebook ä¸­è¿è¡Œè¿™æ®µä»£ç ï¼Œä½ å¯ä»¥é€šè¿‡ä»¥ä¸‹çš„å·¥å…·å‡½æ•°è¿›è¡Œç™»å½•ï¼š

```python
from huggingface_hub import notebook_login

notebook_login()
```

å®ƒå°†æ˜¾ç¤ºä¸€ä¸ªå°éƒ¨ä»¶ï¼Œåœ¨å…¶ä¸­ä½ å¯ä»¥è¾“å…¥ä½ çš„å‡­æ®ã€‚æˆ–è€…ï¼Œä½ ä¹Ÿå¯ä»¥åœ¨ä½ æœ€å–œæ¬¢çš„ç»ˆç«¯ä¸­è¾“å…¥æŒ‡ä»¤ï¼š

```python
huggingface-cli login
```

ç„¶ååœ¨é‚£é‡Œç™»å½•ã€‚

{#if fw === 'tf'}

ç™»å½•åï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºæˆ‘ä»¬çš„ `tf.data` æ•°æ®é›†ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ `prepare_tf_dataset()` æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä¼šä½¿ç”¨æˆ‘ä»¬çš„æ¨¡å‹è‡ªåŠ¨æ¨æ–­å“ªäº›åˆ—åº”è¿›å…¥æ•°æ®é›†ã€‚å¦‚æœä½ æƒ³å‡†ç¡®æ§åˆ¶è¦ä½¿ç”¨çš„åˆ—ï¼Œå¯ä»¥æ”¹ç”¨ `Dataset.to_tf_dataset()` æ–¹æ³•ã€‚ä¸ºäº†ç®€å•èµ·è§ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œåªä½¿ç”¨æ ‡å‡†æ•°æ®æ•´ç†å™¨ï¼Œä½†ä½ ä¹Ÿå¯ä»¥å°è¯•å…¨è¯å±è”½ï¼ˆwhole word maskingï¼‰æ•´ç†å™¨ï¼Œå¹¶ä½œä¸ºä¸€ä¸ªç»ƒä¹ æ¯”è¾ƒç»“æœï¼š

```python
#####TensorFlow}
tf_train_dataset = model.prepare_tf_dataset(
    downsampled_dataset["train"],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=32,
)

tf_eval_dataset = model.prepare_tf_dataset(
    downsampled_dataset["test"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=32,
)####end
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬è®¾ç½®æˆ‘ä»¬çš„è®­ç»ƒè¶…å‚æ•°å¹¶ç¼–è¯‘æˆ‘ä»¬çš„æ¨¡å‹ã€‚æˆ‘ä»¬ä½¿ç”¨Transformers åº“çš„ `create_optimizer()` å‡½æ•°ï¼Œå®ƒæä¾›äº†ä¸€ä¸ªå¸¦æœ‰çº¿æ€§å­¦ä¹ ç‡è¡°å‡çš„ `AdamW` ä¼˜åŒ–å™¨ã€‚æˆ‘ä»¬è¿˜ä½¿ç”¨æ¨¡å‹å†…ç½®çš„æŸå¤±ï¼Œè¿™æ˜¯åœ¨ `compile()` çš„å‚æ•°ä¸­æ²¡æœ‰æŒ‡å®šæŸå¤±æ—¶çš„é»˜è®¤è®¾ç½®ï¼Œå¹¶å°†è®­ç»ƒç²¾åº¦è®¾ä¸º `"mixed_float16"` ã€‚æ³¨æ„ï¼Œå¦‚æœä½ ä½¿ç”¨çš„æ˜¯ Colab GPU æˆ–è€…å…¶ä»–ä¸æ”¯æŒåŠ é€Ÿçš„ float16 çš„ GPUï¼Œä½ å¯èƒ½åº”è¯¥æ³¨é‡Šæ‰è¿™ä¸€è¡Œã€‚

å¦å¤–ï¼Œæˆ‘ä»¬è®¾ç½®äº†ä¸€ä¸ª `PushToHubCallback` ï¼Œå®ƒå°†åœ¨æ¯ä¸ª epoch åå°†æ¨¡å‹ä¿å­˜åˆ° Hubã€‚ä½ å¯ä»¥ä½¿ç”¨ `hub_model_id` å‚æ•°æŒ‡å®šä½ æƒ³æ¨é€åˆ°çš„ä»“åº“çš„åç§°ï¼ˆå¦‚æœä½ æƒ³æŠŠå®ƒæ¨é€åˆ°ä¸€ä¸ªç»„ç»‡ï¼Œä½ å¿…é¡»ä½¿ç”¨è¿™ä¸ªå‚æ•°ï¼‰ã€‚ä¾‹å¦‚ï¼Œè¦å°†æ¨¡å‹æ¨é€åˆ° [`huggingface-course` ç»„ç»‡](https://huggingface.co/huggingface-course)(https://huggingface.co/huggingface-course)(https://huggingface.co/huggingface-course) ï¼Œæˆ‘ä»¬æ·»åŠ äº† `hub_model_id="huggingface-course/distilbert-finetuned-imdb"` ã€‚åœ¨é»˜è®¤çš„æƒ…å†µä¸‹ï¼Œæ¨¡å‹çš„ä»“åº“å°†ä¿å­˜åœ¨ä½ çš„è´¦æˆ·ä¸­ï¼Œå¹¶ä»¥ä½ è®¾ç½®çš„è¾“å‡ºç›®å½•å‘½åï¼Œæ‰€ä»¥åœ¨æˆ‘ä»¬çš„ç¤ºä¾‹ä¸­ï¼Œå®ƒå°†æ˜¯ `"lewtun/distilbert-finetuned-imdb"` ã€‚

```python
#####TensorFlow}
from transformers import create_optimizer
from transformers.keras_callbacks import PushToHubCallback
import tensorflow as tf

num_train_steps = len(tf_train_dataset)
optimizer, schedule = create_optimizer(
    init_lr=2e-5,
    num_warmup_steps=1_000,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)
model.compile(optimizer=optimizer)

## ä½¿ç”¨ float16 ç²¾åº¦è¿›è¡Œæ··åˆç²¾åº¦è®­ç»ƒ
tf.keras.mixed_precision.set_global_policy("mixed_float16")

model_name = model_checkpoint.split("/")[-1]
callback = PushToHubCallback(
    output_dir=f"{model_name}-finetuned-imdb", tokenizer=tokenizer
)####end
```

æˆ‘ä»¬ç°åœ¨å·²ç»å‡†å¤‡å¥½è¿è¡Œ `model.fit()` äº† â€”â€” ä½†åœ¨æ­¤ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å…ˆç®€å•åœ°çœ‹çœ‹ `å›°æƒ‘åº¦ï¼ˆperplexityï¼‰` ï¼Œå®ƒæ˜¯ä¸€ç§å¸¸ç”¨çš„è¯„ä¼°è¯­è¨€æ¨¡å‹æ€§èƒ½çš„æŒ‡æ ‡ã€‚

{:else}

ç™»é™†åï¼Œæˆ‘ä»¬å¯ä»¥æŒ‡å®š `Trainer` çš„å‚æ•°ï¼š

```python
#####Pytorch}
from transformers import TrainingArguments

batch_size = 64
##åœ¨æ¯ä¸ª epoch è¾“å‡ºè®­ç»ƒçš„ loss
logging_steps = len(downsampled_dataset["train"]) // batch_size
model_name = model_checkpoint.split("/")[-1]

training_args = TrainingArguments(
    output_dir=f"{model_name}-finetuned-imdb",
    overwrite_output_dir=True,
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    weight_decay=0.01,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    push_to_hub=True,
    fp16=True,
    logging_steps=logging_steps,
)####end
```

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬è°ƒæ•´äº†ä¸€äº›é»˜è®¤é€‰é¡¹ï¼ŒåŒ…æ‹¬ `logging_steps` ï¼Œä»¥ç¡®ä¿æˆ‘ä»¬å¯ä»¥è·Ÿè¸ªæ¯ä¸ª epoch çš„è®­ç»ƒæŸå¤±ã€‚æˆ‘ä»¬è¿˜ä½¿ç”¨äº† `fp16=True` æ¥å®ç°æ··åˆç²¾åº¦è®­ç»ƒï¼Œä»è€Œè¿›ä¸€æ­¥æé«˜è®­ç»ƒé€Ÿåº¦ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œ `Trainer` å°†åˆ é™¤æ¨¡å‹çš„ `forward()` æ–¹æ³•ä¸­æœªä½¿ç”¨çš„åˆ—ã€‚è¿™æ„å‘³ç€ï¼Œå¦‚æœä½ ä½¿ç”¨å…¨è¯å±è”½ï¼ˆwhole word maskingï¼‰æ•°æ®æ•´ç†å™¨ï¼Œä½ è¿˜éœ€è¦è®¾ç½® `remove_unused_columns=False` ï¼Œä»¥ç¡®ä¿æˆ‘ä»¬ä¸ä¼šåœ¨è®­ç»ƒæœŸé—´ä¸¢å¤± `word_ids` åˆ—ã€‚

è¯·æ³¨æ„ï¼Œä½ å¯ä»¥ä½¿ç”¨ `hub_model_id` å‚æ•°æŒ‡å®šä½ æƒ³æ¨é€åˆ°çš„ä»“åº“çš„åç§°ï¼ˆå¦‚æœä½ æƒ³æŠŠå®ƒæ¨é€åˆ°ä¸€ä¸ªç»„ç»‡ï¼Œå°±å¿…é¡»ä½¿ç”¨è¿™ä¸ªå‚æ•°ï¼‰ã€‚ä¾‹å¦‚ï¼Œå½“æˆ‘ä»¬å°†æ¨¡å‹æ¨é€åˆ° [`huggingface-course` ç»„ç»‡](https://huggingface.co/huggingface-course)(https://huggingface.co/huggingface-course)(https://huggingface.co/huggingface-course) æ—¶ï¼Œæˆ‘ä»¬åœ¨ `TrainingArguments` ä¸­æ·»åŠ äº† `hub_model_id="huggingface-course/distilbert-finetuned-imdb"` ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œä½¿ç”¨çš„ä»“åº“å°†ä¿å­˜åœ¨ä½ çš„è´¦æˆ·ä¸­å¹¶ä»¥ä½ è®¾ç½®çš„è¾“å‡ºç›®å½•å‘½åï¼Œå› æ­¤åœ¨æˆ‘ä»¬çš„ç¤ºä¾‹ä¸­ï¼Œå®ƒå°†æ˜¯ `"lewtun/distilbert-finetuned-imdb"` ã€‚

ç°åœ¨ï¼Œæˆ‘ä»¬æ‹¥æœ‰äº†åˆå§‹åŒ– `Trainer` æ‰€éœ€çš„æ‰€æœ‰è¦ç´ ã€‚è¿™é‡Œæˆ‘ä»¬åªä½¿ç”¨äº†æ ‡å‡†çš„ `data_collator` ï¼Œä½†ä½ å¯ä»¥å°è¯•ä½¿ç”¨å…¨è¯å±è”½ä½œä¸ºæ•°æ®æ•´ç†å™¨çš„ä¸€ä¸ªç»ƒä¹ ï¼Œå¹¶å°†ç»“æœè¿›è¡Œæ¯”è¾ƒï¼š

```python
#####Pytorch}
from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=downsampled_dataset["train"],
    eval_dataset=downsampled_dataset["test"],
    data_collator=data_collator,
    tokenizer=tokenizer,
)####end
```

æˆ‘ä»¬ç°åœ¨å‡†å¤‡è¿è¡Œ `trainer.train()` â€”â€” ä½†åœ¨æ­¤ä¹‹å‰è®©æˆ‘ä»¬ç®€è¦åœ°çœ‹ä¸€ä¸‹ `å›°æƒ‘åº¦ï¼ˆperplexityï¼‰` ï¼Œè¿™æ˜¯è¯„ä¼°è¯­è¨€æ¨¡å‹æ€§èƒ½çš„å¸¸ç”¨æŒ‡æ ‡ã€‚

{/if}

#### è¯­è¨€æ¨¡å‹çš„å›°æƒ‘åº¦ï¼ˆperplexityï¼‰ 

ä¸æ–‡æœ¬åˆ†ç±»æˆ–é—®ç­”ç­‰å…¶ä»–ä»»åŠ¡ä¸åŒï¼Œåœ¨è¿™äº›ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬ä¼šå¾—åˆ°ä¸€ä¸ªå¸¦æ ‡ç­¾çš„è¯­æ–™åº“è¿›è¡Œè®­ç»ƒï¼Œè€Œè¯­è¨€å»ºæ¨¡åˆ™æ²¡æœ‰ä»»ä½•æ˜ç¡®çš„æ ‡ç­¾ã€‚é‚£ä¹ˆæˆ‘ä»¬å¦‚ä½•ç¡®å®šä»€ä¹ˆæ˜¯å¥½çš„è¯­è¨€æ¨¡å‹å‘¢ï¼Ÿå°±åƒæ‰‹æœºä¸­çš„è‡ªåŠ¨æ›´æ­£åŠŸèƒ½ä¸€æ ·ï¼Œä¸€ä¸ªå¥½çš„è¯­è¨€æ¨¡å‹æ˜¯ä¸ºè¯­æ³•æ­£ç¡®çš„å¥å­åˆ†é…é«˜æ¦‚ç‡ï¼Œä¸ºæ— æ„ä¹‰çš„å¥å­åˆ†é…ä½æ¦‚ç‡ã€‚ä¸ºäº†ç»™ä½ ä¸€ä¸ªæ›´ç›´è§‚æ„Ÿå—ï¼Œä½ å¯ä»¥åœ¨ç½‘ä¸Šæ‰¾åˆ°ä¸€æ•´å¥—â€œè‡ªåŠ¨æ›´æ­£å¤±è´¥â€çš„ä¾‹å­ã€‚å…¶ä¸­ï¼Œäººä»¬çš„æ‰‹æœºä¸­çš„æ¨¡å‹äº§ç”Ÿäº†ä¸€äº›ç›¸å½“æœ‰è¶£ï¼ˆå¹¶ä¸”å¸¸å¸¸ä¸å¦¥å½“ï¼‰çš„è‡ªåŠ¨ç”Ÿæˆçš„ç»“æœï¼

{#if fw === 'pt'}

å¦‚æœæˆ‘ä»¬çš„æµ‹è¯•é›†ä¸»è¦ç”±è¯­æ³•æ­£ç¡®çš„å¥å­ç»„æˆï¼Œé‚£ä¹ˆè¡¡é‡æˆ‘ä»¬è¯­è¨€æ¨¡å‹è´¨é‡çš„ä¸€ç§æ–¹å¼å°±æ˜¯è®¡ç®—å®ƒç»™æµ‹è¯•é›†ä¸­æ‰€æœ‰å¥å­çš„ä¸‹ä¸€ä¸ªè¯åˆ†é…çš„æ¦‚ç‡ã€‚é«˜æ¦‚ç‡è¡¨ç¤ºæ¨¡å‹å¯¹æœªè§è¿‡çš„ä¾‹å­ä¸æ„Ÿåˆ°â€œæƒŠè®¶â€æˆ–â€œå›°æƒ‘â€ï¼Œè¿™è¡¨æ˜å®ƒå·²ç»å­¦ä¹ äº†è¯­è¨€çš„åŸºæœ¬è¯­æ³•æ¨¡å¼ã€‚å›°æƒ‘åº¦æœ‰å¾ˆå¤šç§æ•°å­¦å®šä¹‰ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨çš„å®šä¹‰æ˜¯äº¤å‰ç†µæŸå¤±çš„æŒ‡æ•°ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `Trainer.evaluate()` å‡½æ•°è®¡ç®—æµ‹è¯•é›†ä¸Šçš„äº¤å‰ç†µæŸå¤±ï¼Œç„¶åå–ç»“æœçš„æŒ‡æ•°ï¼Œæ¥è®¡ç®—æˆ‘ä»¬é¢„è®­ç»ƒæ¨¡å‹çš„å›°æƒ‘åº¦ï¼š

```python
#####Pytorch}
import math

eval_results = trainer.evaluate()
print(f">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}")####end
```

{:else}

å¦‚æœæˆ‘ä»¬çš„æµ‹è¯•é›†ä¸»è¦ç”±è¯­æ³•æ­£ç¡®çš„å¥å­ç»„æˆï¼Œé‚£ä¹ˆè¡¡é‡æˆ‘ä»¬è¯­è¨€æ¨¡å‹è´¨é‡çš„ä¸€ç§æ–¹å¼å°±æ˜¯è®¡ç®—å®ƒç»™æµ‹è¯•é›†ä¸­æ‰€æœ‰å¥å­çš„ä¸‹ä¸€ä¸ªè¯åˆ†é…çš„æ¦‚ç‡ã€‚é«˜æ¦‚ç‡è¡¨ç¤ºæ¨¡å‹å¯¹æœªè§è¿‡çš„ä¾‹å­ä¸æ„Ÿåˆ°â€œæƒŠè®¶â€æˆ–â€œå›°æƒ‘â€ï¼Œè¿™è¡¨æ˜å®ƒå·²ç»å­¦ä¹ äº†è¯­è¨€çš„åŸºæœ¬è¯­æ³•æ¨¡å¼ã€‚å›°æƒ‘åº¦æœ‰å¾ˆå¤šç§æ•°å­¦å®šä¹‰ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨çš„å®šä¹‰æ˜¯äº¤å‰ç†µæŸå¤±çš„æŒ‡æ•°ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `model.evaluate()` å‡½æ•°è®¡ç®—æµ‹è¯•é›†ä¸Šçš„äº¤å‰ç†µæŸå¤±ï¼Œç„¶åå–ç»“æœçš„æŒ‡æ•°ï¼Œæ¥è®¡ç®—æˆ‘ä»¬é¢„è®­ç»ƒæ¨¡å‹çš„å›°æƒ‘åº¦ï¼š

```python
#####TensorFlow}
import math

eval_loss = model.evaluate(tf_eval_dataset)
print(f"Perplexity: {math.exp(eval_loss):.2f}")####end
```

{/if}

```python
>>> Perplexity: 21.75
```

è¾ƒä½çš„å›°æƒ‘åº¦åˆ†æ•°æ„å‘³ç€æ›´å¥½çš„è¯­è¨€æ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œæˆ‘ä»¬çš„åˆå§‹æ¨¡å‹çš„å€¼ç›¸å½“å¤§ã€‚è®©æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬æ˜¯å¦å¯ä»¥é€šè¿‡å¾®è°ƒæ¥é™ä½å®ƒï¼ä¸ºæ­¤ï¼Œæˆ‘ä»¬é¦–å…ˆè¿è¡Œè®­ç»ƒå¾ªç¯ï¼š

{#if fw === 'pt'}

```python
#####Pytorch}
trainer.train()####end
```

{:else}

```python
#####TensorFlow}
model.fit(tf_train_dataset, validation_data=tf_eval_dataset, callbacks=[callback])####end
```

{/if}

ç„¶ååƒä¹‹å‰é‚£æ ·è®¡ç®—æµ‹è¯•é›†ä¸Šçš„ç»“æœå›°æƒ‘åº¦ï¼š

{#if fw === 'pt'}

```python
#####Pytorch}
eval_results = trainer.evaluate()
print(f">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}")####end
```

{:else}

```python
#####TensorFlow}
eval_loss = model.evaluate(tf_eval_dataset)
print(f"Perplexity: {math.exp(eval_loss):.2f}")####end
```

{/if}

```python
>>> Perplexity: 11.32
```

å¤ªæ£’äº†â€”â€”å›°æƒ‘åº¦æ˜¾è‘—é™ä½ï¼Œè¿™å‘Šè¯‰æˆ‘ä»¬æ¨¡å‹å·²ç»å­¦ä¹ åˆ°äº†ç”µå½±è¯„è®ºé¢†åŸŸçš„ä¸€äº›çŸ¥è¯†ï¼

{#if fw === 'pt'}

ä¸€æ—¦è®­ç»ƒå®Œæˆï¼Œæˆ‘ä»¬å¯ä»¥å°†å¸¦æœ‰è®­ç»ƒä¿¡æ¯çš„æ¨¡å‹å¡ç‰‡æ¨é€åˆ° Hubï¼ˆcheckpoint åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å°±å·²ç»ä¿å­˜äº†ï¼‰ï¼š

```python
#####Pytorch}
trainer.push_to_hub()####end
```

{/if}

<div custom-style="Tip-green">

âœï¸ **è½®åˆ°ä½ äº†ï¼** å°†æ•°æ®æ•´ç†å™¨æ”¹ä¸ºå…¨è¯å±è”½çš„æ•°æ®æ•´ç†å™¨åè¿è¡Œä¸Šé¢çš„è®­ç»ƒã€‚ä½ èƒ½å¾—åˆ°æ›´å¥½çš„ç»“æœå—ï¼Ÿ

</div>

{#if fw === 'pt'} 

åœ¨æˆ‘ä»¬çš„ä½¿ç”¨æ¡ˆä¾‹ä¸­ï¼Œæˆ‘ä»¬ä¸éœ€è¦å¯¹è®­ç»ƒå¾ªç¯åšä»»ä½•ç‰¹æ®Šçš„å¤„ç†ï¼Œä½†åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œä½ å¯èƒ½éœ€è¦å®ç°ä¸€äº›è‡ªå®šä¹‰é€»è¾‘ã€‚å¯¹äºè¿™äº›åº”ç”¨ï¼Œä½ å¯ä»¥ä½¿ç”¨ Accelerate â€”â€” è®©æˆ‘ä»¬çœ‹ä¸€çœ‹ï¼

### ä½¿ç”¨ Accelerate å¾®è°ƒ DistilBERT 

æ­£å¦‚æˆ‘ä»¬åœ¨ `Trainer` ä¸­æ‰€çœ‹åˆ°çš„ï¼Œå¾®è°ƒä¸€ä¸ªè¢«æ©è”½çš„è¯­è¨€æ¨¡å‹ä¸ç¬¬å››ç« ä¸­çš„æ–‡æœ¬åˆ†ç±»ç¤ºä¾‹éå¸¸ç›¸ä¼¼ã€‚äº‹å®ä¸Šï¼Œå”¯ä¸€çš„ä¸åŒä¹‹å¤„æ˜¯ä½¿ç”¨äº†ä¸€ä¸ªç‰¹æ®Šçš„æ•°æ®æ•´ç†å™¨ï¼Œæˆ‘ä»¬å·²ç»åœ¨æœ¬èŠ‚çš„å‰é¢è®¨è®ºè¿‡è¿™ä¸ªé—®é¢˜äº†ï¼
ç„¶è€Œï¼Œæˆ‘ä»¬æ³¨æ„åˆ° `DataCollatorForLanguageModeling` åœ¨æ¯æ¬¡è¯„ä¼°æ—¶ä¹Ÿä¼šè¿›è¡Œéšæœºé®ç½©ï¼Œå› æ­¤æˆ‘ä»¬åœ¨æ¯æ¬¡è®­ç»ƒè¿è¡Œä¸­éƒ½ä¼šçœ‹åˆ°å›°æƒ‘åº¦å¾—åˆ†æœ‰äº›æ³¢åŠ¨ã€‚æ¶ˆé™¤è¿™ç§éšæœºæ€§çš„ä¸€ç§æ–¹æ³•æ˜¯åœ¨æ•´ä¸ªæµ‹è¯•é›†ä¸Š `ä»…è¿›è¡Œä¸€æ¬¡` é®ç½©ï¼Œç„¶ååœ¨è¯„ä¼°è¿‡ç¨‹ä¸­ä½¿ç”¨Transformers ä¸­çš„é»˜è®¤æ•°æ®æ•´ç†å™¨æ¥æ”¶é›† batchã€‚ä¸ºäº†äº†è§£å…¶å·¥ä½œåŸç†ï¼Œè®©æˆ‘ä»¬å®ç°ä¸€ä¸ªç®€å•çš„å‡½æ•°ï¼Œç±»ä¼¼äºæˆ‘ä»¬ç¬¬ä¸€æ¬¡ä½¿ç”¨ `DataCollatorForLanguageModeling` æ—¶è¿›è¡Œé®ç½©çš„æ–¹å¼ï¼š

```python
#####Pytorch}
def insert_random_mask(batch):
    features = [dict(zip(batch, t)) for t in zip(*batch.values())]
    masked_inputs = data_collator(features)
    # ä¸ºæ•°æ®é›†ä¸­çš„æ¯ä¸€åˆ—åˆ›å»ºä¸€ä¸ªæ–°çš„"masked"åˆ—
    return {"masked_" + k: v.numpy() for k, v in masked_inputs.items()}####end
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†ä¸Šè¿°å‡½æ•°åº”ç”¨åˆ°æµ‹è¯•é›†ï¼Œå¹¶å»é™¤æœªè¿›è¡Œé®ç½©çš„åˆ—ï¼Œè¿™æ ·å°±å¯ä»¥ç”¨é®ç½©è¿‡çš„åˆ—æ¥æ›¿æ¢å®ƒä»¬ã€‚ä½ å¯ä»¥é€šè¿‡å°†ä¸Šè¿° `data_collator` æ›¿æ¢ä¸ºé€‚å½“çš„æ•°æ®æ•´ç†å™¨æ¥å®ç°å…¨è¯é®ç½©ã€‚å¦‚æœä½ è¿™æ ·åšäº†ï¼Œé‚£ä¹ˆä½ åº”è¯¥å»é™¤ä»¥ä¸‹ä»£ç ä¸­çš„ç¬¬ä¸€è¡Œï¼š

```python
#####Pytorch}
downsampled_dataset = downsampled_dataset.remove_columns(["word_ids"])
eval_dataset = downsampled_dataset["test"].map(
    insert_random_mask,
    batched=True,
    remove_columns=downsampled_dataset["test"].column_names,
)
eval_dataset = eval_dataset.rename_columns(
    {
        "masked_input_ids": "input_ids",
        "masked_attention_mask": "attention_mask",
        "masked_labels": "labels",
    }
)####end
```

ç„¶åæˆ‘ä»¬å¯ä»¥åƒå¾€å¸¸ä¸€æ ·è®¾ç½® DataLoaderï¼Œä½†æˆ‘ä»¬å°†ä½¿ç”¨Transformers ä¸­çš„ `default_data_collator` ï¼š

```python
#####Pytorch}
from torch.utils.data import DataLoader
from transformers import default_data_collator

batch_size = 64
train_dataloader = DataLoader(
    downsampled_dataset["train"],
    shuffle=True,
    batch_size=batch_size,
    collate_fn=data_collator,
)
eval_dataloader = DataLoader(
    eval_dataset, batch_size=batch_size, collate_fn=default_data_collator
)####end
```

ä»è¿™é‡Œå¼€å§‹ï¼Œæˆ‘ä»¬å°†éµå¾ªAccelerate çš„æ ‡å‡†æ­¥éª¤ã€‚ç¬¬ä¸€ä¸ªä»»åŠ¡æ˜¯é‡æ–°åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼š

```python
#####Pytorch}
model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)####end
```

ç„¶åæˆ‘ä»¬éœ€è¦æŒ‡å®šä¼˜åŒ–å™¨ï¼›æˆ‘ä»¬å°†ä½¿ç”¨æ ‡å‡†çš„ `AdamW` ï¼š

```python
#####Pytorch}
from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr=5e-5)####end
```

æœ‰äº†è¿™äº›å¯¹è±¡ï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥ç”¨ `Accelerator` å¯¹è±¡å‡†å¤‡å¥½ä¸€åˆ‡ï¼Œä»¥è¿›è¡Œè®­ç»ƒï¼š

```python
#####Pytorch}
from accelerate import Accelerator

accelerator = Accelerator()
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)####end
```

ç°åœ¨æˆ‘ä»¬çš„æ¨¡å‹ã€ä¼˜åŒ–å™¨å’Œ DataLoader éƒ½é…ç½®å¥½äº†ï¼Œæˆ‘ä»¬å¯ä»¥æŒ‰ç…§ä»¥ä¸‹æ–¹å¼è®¾ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼š

```python
#####Pytorch}
from transformers import get_scheduler

num_train_epochs = 3
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)####end
```

åœ¨å¼€å§‹è®­ç»ƒä¹‹å‰ï¼Œæˆ‘ä»¬è¿˜éœ€è¦åšçš„æœ€åä¸€ä»¶äº‹å°±æ˜¯åœ¨ Hugging Face Hub ä¸Šåˆ›å»ºä¸€ä¸ªæ¨¡å‹ä»“åº“ï¼æˆ‘ä»¬å¯ä»¥ä½¿ç”¨Hub åº“ï¼Œç”Ÿæˆæˆ‘ä»¬ä»“åº“çš„å…¨åï¼š

```python
#####Pytorch}
from huggingface_hub import get_full_repo_name

model_name = "distilbert-base-uncased-finetuned-imdb-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name####end
```

```python
#####Pytorch}
'lewtun/distilbert-base-uncased-finetuned-imdb-accelerate'####end
```

ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨Hub çš„ `Repository` ç±»åˆ›å»ºå¹¶å…‹éš†ä»“åº“ï¼š

```python
#####Pytorch}
from huggingface_hub import Repository

output_dir = model_name
repo = Repository(output_dir, clone_from=repo_name)####end
```

å®Œæˆåï¼Œåªéœ€å†™å‡ºå®Œæ•´çš„è®­ç»ƒå’Œè¯„ä¼°å¾ªç¯å³å¯ï¼š

```python
#####Pytorch}
from tqdm.auto import tqdm
import torch
import math

progress_bar = tqdm(range(num_training_steps))

for epoch in range(num_train_epochs):
    # è®­ç»ƒ
    model.train()
    for batch in train_dataloader:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

    # è¯„ä¼°
    model.eval()
    losses = []
    for step, batch in enumerate(eval_dataloader):
        with torch.no_grad():
            outputs = model(**batch)

        loss = outputs.loss
        losses.append(accelerator.gather(loss.repeat(batch_size)))

    losses = torch.cat(losses)
    losses = losses[: len(eval_dataset)]
    try:
        perplexity = math.exp(torch.mean(losses))
    except OverflowError:
        perplexity = float("inf")

    print(f">>> Epoch {epoch}: Perplexity: {perplexity}")

    # ä¿å­˜å¹¶ä¸Šä¼ 
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
    if accelerator.is_main_process:
        tokenizer.save_pretrained(output_dir)
        repo.push_to_hub(
            commit_message=f"Training in progress epoch {epoch}", blocking=False
        )####end
```

```python
#####Pytorch}
>>> Epoch 0: Perplexity: 11.397545307900472
>>> Epoch 1: Perplexity: 10.904909330983092
>>> Epoch 2: Perplexity: 10.729503505340409####end
```

å¾ˆæ£’ï¼Œæˆ‘ä»¬å·²ç»èƒ½å¤Ÿè¯„ä¼°æ¯ä¸ª epoch çš„å›°æƒ‘åº¦ï¼Œå¹¶ç¡®ä¿å¤šæ¬¡è¿è¡Œå¯ä»¥å¤ç°ï¼

{/if}

### ä½¿ç”¨æˆ‘ä»¬å¾®è°ƒçš„æ¨¡å‹ 

ä½ å¯ä»¥ä½¿ç”¨ Hub ä¸Šçš„æ¨¡å‹éƒ¨ä»¶æˆ–è€…åœ¨æœ¬åœ°ä½¿ç”¨Transformers çš„ `pipeline` ä¸å¾®è°ƒæ¨¡å‹è¿›è¡Œäº¤äº’ã€‚è®©æˆ‘ä»¬ä½¿ç”¨åè€…é€šè¿‡ `fill-mask` pipeline ä¸‹è½½æˆ‘ä»¬çš„æ¨¡å‹ï¼š

```python
from transformers import pipeline

mask_filler = pipeline(
    "fill-mask", model="huggingface-course/distilbert-base-uncased-finetuned-imdb"
)
```

ç„¶åæˆ‘ä»¬å¯ä»¥å°†æ–‡æœ¬â€œThis is a great [MASK]â€æä¾›ç»™ pipelineï¼Œçœ‹çœ‹å‰ 5 ä¸ªé¢„æµ‹æ˜¯ä»€ä¹ˆï¼š

```python
preds = mask_filler(text)

for pred in preds:
    print(f">>> {pred['sequence']}")
```

```python
'>>> this is a great movie.'
'>>> this is a great film.'
'>>> this is a great story.'
'>>> this is a great movies.'
'>>> this is a great character.'
```

Niceï¼â€”â€” æˆ‘ä»¬çš„æ¨¡å‹æ˜¾ç„¶å·²ç»è°ƒæ•´äº†å®ƒçš„æƒé‡æ¥é¢„æµ‹ä¸ç”µå½±æ›´å¯†åˆ‡ç›¸å…³çš„è¯ï¼

è¿™æ ‡å¿—ç€æˆ‘ä»¬ç¬¬ä¸€æ¬¡è®­ç»ƒè¯­è¨€æ¨¡å‹çš„å®éªŒçš„ç»“æŸã€‚åœ¨ç¬¬ 6 èŠ‚ä¸­ï¼Œä½ å°†å­¦ä¹ å¦‚ä½•ä»å¤´å¼€å§‹è®­ç»ƒä¸€ä¸ªè‡ªåŠ¨å›å½’æ¨¡å‹ï¼Œæ¯”å¦‚ GPT-2ï¼›å¦‚æœä½ æƒ³çœ‹çœ‹å¦‚ä½•é¢„è®­ç»ƒä½ è‡ªå·±çš„ Transformer æ¨¡å‹ï¼Œå°±èµ¶å¿«å»é‚£é‡Œçœ‹çœ‹å§ï¼

<div custom-style="Tip-green">

âœï¸ **è¯•è¯•çœ‹ï¼** ä¸ºäº†é‡åŒ–é¢†åŸŸé€‚åº”çš„å¥½å¤„ï¼Œåˆ†åˆ«ä½¿ç”¨é¢„è®­ç»ƒå’Œå¾®è°ƒçš„ DistilBERT checkpoint å’Œ IMDb æ ‡ç­¾æ¥å¾®è°ƒä¸€ä¸ªåˆ†ç±»å™¨ï¼Œå¹¶å¯¹æ¯”ä¸€ä¸‹è¿™ä¸ªä¸¤ä¸ª checkpoint çš„å·®å¼‚ã€‚å¦‚æœä½ éœ€è¦å¤ä¹ æ–‡æœ¬åˆ†ç±»çš„çŸ¥è¯†ï¼Œè¯·æŸ¥çœ‹ç¬¬å››ç« ã€‚
</div>


## 8.3 ç¿»è¯‘ 

ç°åœ¨è®©æˆ‘ä»¬æ·±å…¥ç ”ç©¶ç¿»è¯‘ã€‚è¿™æ˜¯å¦ä¸€ä¸ª [sequence-to-sequence ä»»åŠ¡](/course/chapter1/7)(/course/chapter1/7) ï¼Œè¿™æ„å‘³ç€è¿™æ˜¯ä¸€ä¸ªå¯ä»¥è¡¨è¿°ä¸ºä»ä¸€ä¸ªåºåˆ—åˆ°å¦ä¸€ä¸ªåºåˆ—çš„é—®é¢˜ã€‚ä»è¿™ä¸ªæ„ä¹‰ä¸Šè¯´ï¼Œè¿™ä¸ªé—®é¢˜éå¸¸ç±»ä¼¼ [æ–‡æœ¬æ‘˜è¦](/course/chapter7/6)(/course/chapter7/6) ï¼Œå¹¶ä¸”ä½ å¯ä»¥å°†æˆ‘ä»¬å°†åœ¨æ­¤å¤„å­¦ä¹ åˆ°çš„ä¸€äº›å†…å®¹è¿ç§»åˆ°å…¶ä»–çš„åºåˆ—åˆ°åºåˆ—é—®é¢˜ï¼Œä¾‹å¦‚ï¼š

- **é£æ ¼è¿ç§»** åˆ›å»ºä¸€ä¸ªæ¨¡å‹å°†æŸç§é£æ ¼è¿ç§»åˆ°ä¸€æ®µæ–‡æœ¬ï¼ˆä¾‹å¦‚ï¼Œæ­£å¼çš„é£æ ¼è¿ç§»åˆ°ä¼‘é—²çš„é£æ ¼ï¼Œæˆ–èå£«æ¯”äºšè‹±è¯­åˆ°ç°ä»£è‹±è¯­ï¼‰ã€‚
- **ç”Ÿæˆé—®é¢˜çš„å›ç­”** åˆ›å»ºä¸€ä¸ªæ¨¡å‹ï¼Œåœ¨ç»™å®šä¸Šä¸‹æ–‡çš„æƒ…å†µä¸‹ç”Ÿæˆé—®é¢˜çš„ç­”æ¡ˆã€‚

å¦‚æœä½ æœ‰è¶³å¤Ÿå¤§çš„ä¸¤ç§ï¼ˆæˆ–æ›´å¤šï¼‰è¯­è¨€çš„æ–‡æœ¬è¯­æ–™åº“ï¼Œä½ å¯ä»¥ä»å¤´å¼€å§‹è®­ç»ƒä¸€ä¸ªæ–°çš„ç¿»è¯‘æ¨¡å‹ï¼Œå°±åƒæˆ‘ä»¬åœ¨ [å› æœè¯­è¨€å»ºæ¨¡](/course/chapter7/6)(/course/chapter7/6) éƒ¨åˆ†ä¸­æ‰€åšçš„é‚£æ ·ã€‚ç„¶è€Œï¼Œå¾®è°ƒç°æœ‰çš„ç¿»è¯‘æ¨¡å‹ä¼šæ›´å¿«ï¼Œæ— è®ºæ˜¯ä»åƒ mT5 æˆ– mBART è¿™æ ·çš„å¤šè¯­è¨€æ¨¡å‹å¾®è°ƒåˆ°ç‰¹å®šçš„è¯­è¨€å¯¹ï¼Œè¿˜æ˜¯ä½ æƒ³å¾®è°ƒåˆ°ç‰¹å®šè¯­æ–™åº“çš„ä¸€ç§è¯­è¨€åˆ°å¦ä¸€ç§è¯­è¨€çš„ä¸“ç”¨ç¿»è¯‘æ¨¡å‹ã€‚

åœ¨è¿™ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†å¾®è°ƒä¸€ä¸ªé¢„è®­ç»ƒçš„ Marian æ¨¡å‹ï¼Œè¯¥æ¨¡å‹æ˜¯ç”¨æ¥ä»è‹±è¯­ç¿»è¯‘æˆæ³•è¯­çš„ï¼ˆå› ä¸ºå¾ˆå¤š Hugging Face çš„å‘˜å·¥éƒ½ä¼šè¯´è¿™ä¸¤ç§è¯­è¨€ï¼‰ï¼Œå¹¶åœ¨ [KDE4 æ•°æ®é›†](https://huggingface.co/datasets/kde4)(https://huggingface.co/datasets/kde4)(https://huggingface.co/datasets/kde4) ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äº [KDE åº”ç”¨](https://apps.kde.org/)(https://apps.kde.org/) çš„æœ¬åœ°åŒ–æ–‡ä»¶æ•°æ®é›†ã€‚æˆ‘ä»¬å°†ä½¿ç”¨çš„æ¨¡å‹å·²ç»åœ¨ä» [Opus æ•°æ®é›†](https://opus.nlpl.eu/)(https://opus.nlpl.eu/) ï¼ˆå®é™…ä¸ŠåŒ…å« KDE4 æ•°æ®é›†ï¼‰ä¸­æå–çš„æ³•è¯­å’Œè‹±è¯­æ–‡æœ¬çš„å¤§å‹è¯­æ–™åº“ä¸Šè¿›è¡Œäº†é¢„å…ˆè®­ç»ƒã€‚ä½†æ˜¯ï¼Œå³ä½¿æˆ‘ä»¬ä½¿ç”¨çš„é¢„è®­ç»ƒæ¨¡å‹åœ¨å…¶é¢„è®­ç»ƒæœŸé—´ä½¿ç”¨äº†è¿™éƒ¨åˆ†æ•°æ®é›†ï¼Œæˆ‘ä»¬ä¹Ÿä¼šçœ‹åˆ°ï¼Œç»è¿‡å¾®è°ƒåï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°ä¸€ä¸ªæ›´å¥½çš„ç‰ˆæœ¬ã€‚

å®Œæˆåï¼Œæˆ‘ä»¬å°†æ‹¥æœ‰ä¸€ä¸ªæ¨¡å‹ï¼Œå¯ä»¥è¿›è¡Œè¿™æ ·çš„ç¿»è¯‘ï¼š

![ç¿»è¯‘çš„ä¸€ä¸ªæ¼”ç¤ºä¾‹å­](./assets/translation-example.jpg)

![æ¨¡å‹çš„è¯„ä¼°ç»“æœ](./assets/modeleval-marian-finetuned-kde4-en-to-fr.png)

ä¸å‰é¢çš„éƒ¨åˆ†ä¸€æ ·ï¼Œä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹ä»£ç æ‰¾åˆ°æˆ‘ä»¬å°†è®­ç»ƒå¹¶ä¸Šä¼ åˆ° Hub çš„å®é™…æ¨¡å‹ï¼Œå¹¶ [åœ¨è¿™é‡Œ](https://huggingface.co/huggingface-course/marian-finetuned-kde4-en-to-fr?text=This+plugin+allows+you+to+automatically+translate+web+pages+between+several+languages.)(https://huggingface.co/huggingface-course/marian-finetuned-kde4-en-to-fr?text=This+plugin+allows+you+to+automatically+translate+web+pages+between+several+languages.) æŸ¥çœ‹æ¨¡å‹è¾“å‡ºçš„ç»“æœã€‚

### å‡†å¤‡æ•°æ® 

ä¸ºäº†ä»å¤´å¼€å§‹å¾®è°ƒæˆ–è®­ç»ƒç¿»è¯‘æ¨¡å‹ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªé€‚åˆè¯¥ä»»åŠ¡çš„æ•°æ®é›†ã€‚å¦‚å‰æ‰€è¿°ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ [KDE4 æ•°æ®é›†](https://huggingface.co/datasets/kde4)(https://huggingface.co/datasets/kde4)(https://huggingface.co/datasets/kde4) ã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œä½†ä½ å¯ä»¥å¾ˆå®¹æ˜“åœ°è°ƒæ•´ä»£ç ä»¥ä½¿ç”¨ä½ è‡ªå·±çš„æ•°æ®ï¼Œåªè¦ä½ æœ‰è¦äº’è¯‘çš„ä¸¤ç§è¯­è¨€çš„å¥å­å¯¹ã€‚å¦‚æœä½ éœ€è¦å¤ä¹ å¦‚ä½•å°†è‡ªå®šä¹‰æ•°æ®åŠ è½½åˆ° `Dataset` ï¼Œå¯ä»¥å¤ä¹ ä¸€ä¸‹ç¬¬å…­ç« ã€‚

#### KDE4 æ•°æ®é›† 

åƒå¾€å¸¸ä¸€æ ·ï¼Œæˆ‘ä»¬ä½¿ç”¨ `load_dataset()` å‡½æ•°ä¸‹è½½æˆ‘ä»¬çš„æ•°æ®é›†ï¼š

```python
from datasets import load_dataset

raw_datasets = load_dataset("kde4", lang1="en", lang2="fr")
```

å¦‚æœä½ æƒ³ä½¿ç”¨ä¸åŒçš„è¯­è¨€å¯¹ï¼Œä½ å¯ä»¥ä½¿ç”¨è¯­è¨€ä»£ç æ¥æŒ‡å®šå®ƒä»¬ã€‚è¯¥æ•°æ®é›†å…±æœ‰ 92 ç§è¯­è¨€å¯ç”¨ï¼›ä½ å¯ä»¥é€šè¿‡å±•å¼€ [æ•°æ®é›†å¡ç‰‡](https://huggingface.co/datasets/kde4)(https://huggingface.co/datasets/kde4) ä¸Šçš„è¯­è¨€æ ‡ç­¾æ¥æŸ¥çœ‹å®ƒä»¬ã€‚

![KDE4 æ•°æ®é›†å¯ä»¥ä½¿ç”¨çš„è¯­è¨€](./assets/language_tags.png)

æˆ‘ä»¬æ¥çœ‹çœ‹æ•°æ®é›†ï¼š

```python
raw_datasets
```

```python
DatasetDict({
    train: Dataset({
        features: ['id', 'translation'],
        num_rows: 210173
    })
})
```

æˆ‘ä»¬æœ‰ 210,173 å¯¹å¥å­ï¼Œä½†åœ¨ä¸€æ¬¡è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä¹Ÿéœ€è¦åˆ›å»ºè‡ªå·±çš„éªŒè¯é›†ã€‚æ­£å¦‚æˆ‘ä»¬åœ¨ç¬¬å…­ç« å­¦çš„çš„é‚£æ ·ï¼Œ `Dataset` æœ‰ä¸€ä¸ª `train_test_split()` æ–¹æ³•å¯ä»¥å¸®åŠ©æˆ‘ä»¬ã€‚æˆ‘ä»¬å°†è®¾ç½®ä¸€ä¸ªéšæœºæ•°ç§å­ä»¥ä¿è¯ç»“æœçš„å¯å¤ç°æ€§ï¼š

```python
split_datasets = raw_datasets["train"].train_test_split(train_size=0.9, seed=20)
split_datasets
```

```python
DatasetDict({
    train: Dataset({
        features: ['id', 'translation'],
        num_rows: 189155
    })
    test: Dataset({
        features: ['id', 'translation'],
        num_rows: 21018
    })
})
```

æˆ‘ä»¬å¯ä»¥åƒä¸‹é¢è¿™æ ·å°† â€œtestâ€ é”®é‡å‘½åä¸º â€œvalidationâ€ï¼š

```python
split_datasets["validation"] = split_datasets.pop("test")
```

ç°åœ¨è®©æˆ‘ä»¬çœ‹ä¸€ä¸‹æ•°æ®é›†çš„ä¸€ä¸ªå…ƒç´ ï¼š

```python
split_datasets["train"][1]["translation"]
```

```python
{'en': 'Default to expanded threads',
 'fr': 'Par dÃ©faut, dÃ©velopper les fils de discussion'}
```

æˆ‘ä»¬å¾—åˆ°ä¸€ä¸ªåŒ…å«æˆ‘ä»¬é€‰æ‹©çš„ä¸¤ç§è¯­è¨€çš„ä¸¤ä¸ªå¥å­çš„å­—å…¸ã€‚è¿™ä¸ªå……æ»¡æŠ€æœ¯è®¡ç®—æœºç§‘å­¦æœ¯è¯­çš„æ•°æ®é›†çš„ä¸€ä¸ªç‰¹æ®Šä¹‹å¤„åœ¨äºå®ƒä»¬éƒ½å®Œå…¨ç”¨æ³•è¯­ç¿»è¯‘ã€‚ç„¶è€Œï¼Œæ³•å›½å·¥ç¨‹å¸ˆé€šå¸¸å¾ˆæ‡’æƒ°ï¼Œåœ¨äº¤è°ˆæ—¶ï¼Œå¤§å¤šæ•°è®¡ç®—æœºç§‘å­¦ä¸“ç”¨è¯æ±‡éƒ½ç”¨è‹±è¯­è¡¨è¿°ã€‚ä¾‹å¦‚ï¼Œâ€œthreadsâ€è¿™ä¸ªè¯å¾ˆå¯èƒ½å‡ºç°åœ¨æ³•è¯­å¥å­ä¸­ï¼Œå°¤å…¶æ˜¯åœ¨æŠ€æœ¯å¯¹è¯ä¸­ï¼›ä½†åœ¨è¿™ä¸ªæ•°æ®é›†ä¸­ï¼Œå®ƒè¢«ç¿»è¯‘æˆæ›´å‡†ç¡®çš„â€œfils de Discussionâ€ã€‚æˆ‘ä»¬ä½¿ç”¨çš„é¢„è®­ç»ƒæ¨¡å‹å·²ç»åœ¨ä¸€ä¸ªæ›´å¤§çš„æ³•è¯­å’Œè‹±è¯­å¥å­è¯­æ–™åº“ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒï¼Œé€‰æ‹©äº†æ›´ä¸ºç®€å•çš„ä¿ç•™åŸè¯çš„æ–¹å¼ï¼š

```python
from transformers import pipeline

model_checkpoint = "Helsinki-NLP/opus-mt-en-fr"
translator = pipeline("translation", model=model_checkpoint)
translator("Default to expanded threads")
```

```python
[{'translation_text': 'Par dÃ©faut pour les threads Ã©largis'}]
```

è¿™ç§æƒ…å†µçš„å¦ä¸€ä¸ªä¾‹å­å¯ä»¥åœ¨â€œpluginâ€è¿™ä¸ªè¯ä¸Šçœ‹åˆ°ï¼Œå®ƒå¹¶éæ­£å¼çš„æ³•è¯­è¯æ±‡ï¼Œä½†å¤§å¤šæ•°æ¯è¯­æ˜¯æ³•è¯­çš„äººéƒ½ä¼šç†è§£å¹¶ä¸”ä¸ä¼šå»ç¿»è¯‘å®ƒã€‚åœ¨ KDE4 æ•°æ®é›†ä¸­ï¼Œè¿™ä¸ªè¯è¢«ç¿»è¯‘æˆäº†æ›´æ­£å¼çš„æ³•è¯­è¯æ±‡â€œmodule d'extensionâ€ï¼š

```python
split_datasets["train"][172]["translation"]
```

```python
{'en': 'Unable to import %1 using the OFX importer plugin. This file is not the correct format.',
 'fr': "Impossible d'importer %1 en utilisant le module d'extension d'importation OFX. Ce fichier n'a pas un format correct."}
```

ç„¶è€Œï¼Œæˆ‘ä»¬çš„é¢„è®­ç»ƒæ¨¡å‹åšæŒä½¿ç”¨ç®€ç»ƒè€Œç†Ÿæ‚‰çš„è‹±æ–‡å•è¯ï¼š

```python
translator(
    "Unable to import %1 using the OFX importer plugin. This file is not the correct format."
)
```

```python
[{'translation_text': "Impossible d'importer %1 en utilisant le plugin d'importateur OFX. Ce fichier n'est pas le bon format."}]
```

çœ‹çœ‹æˆ‘ä»¬çš„å¾®è°ƒæ¨¡å‹æ˜¯å¦èƒ½è¯†åˆ«æ•°æ®é›†çš„è¿™äº›ç‰¹æ®Šæ€§ã€‚ï¼ˆå‰§é€è­¦å‘Šï¼šå®ƒä¼šçš„ï¼‰ã€‚

<div custom-style="Tip-green">

âœï¸ **è½®åˆ°ä½ äº†ï¼** å¦ä¸€ä¸ªåœ¨æ³•è¯­ä¸­ç»å¸¸ä½¿ç”¨çš„è‹±è¯­å•è¯æ˜¯â€œemailâ€ã€‚åœ¨è®­ç»ƒæ•°æ®é›†ä¸­æ‰¾åˆ°ä½¿ç”¨è¿™ä¸ªè¯çš„ç¬¬ä¸€ä¸ªæ ·æœ¬ã€‚å®ƒæ˜¯å¦‚ä½•ç¿»è¯‘çš„ï¼Ÿé¢„è®­ç»ƒæ¨¡å‹å¦‚ä½•ç¿»è¯‘åŒä¸€ä¸ªè‹±æ–‡å¥å­ï¼Ÿ

</div>

#### å¤„ç†æ•°æ® 

ä½ ç°åœ¨åº”è¯¥çŸ¥é“æˆ‘ä»¬çš„ä¸‹ä¸€æ­¥è¯¥åšäº›ä»€ä¹ˆäº†ï¼šæ‰€æœ‰æ–‡æœ¬éƒ½éœ€è¦è½¬æ¢ä¸º token IDs çš„é›†åˆï¼Œä»¥ä¾¿æ¨¡å‹èƒ½å¤Ÿç†è§£å®ƒä»¬ã€‚å¯¹äºè¿™ä¸ªä»»åŠ¡ï¼Œæˆ‘ä»¬éœ€è¦åŒæ—¶å¯¹è¾“å…¥å’Œç›®æ ‡ tokenizeã€‚æˆ‘ä»¬çš„é¦–è¦ä»»åŠ¡æ˜¯åˆ›å»ºæˆ‘ä»¬çš„ `tokenizer` å¯¹è±¡ã€‚å¦‚å‰æ‰€è¿°ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ Marian è‹±è¯­åˆ°æ³•è¯­çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚å¦‚æœä½ ä½¿ç”¨å¦ä¸€å¯¹è¯­è¨€å°è¯•æ­¤ä»£ç ï¼Œè¯·ç¡®ä¿è°ƒæ•´æ¨¡å‹çš„ checkpointã€‚ [Helsinki-NLP](https://huggingface.co/Helsinki-NLP)(https://huggingface.co/Helsinki-NLP) ç»„ç»‡æä¾›äº†è¶…è¿‡ä¸€åƒä¸ªå¤šè¯­è¨€æ¨¡å‹ã€‚

```python
from transformers import AutoTokenizer

model_checkpoint = "Helsinki-NLP/opus-mt-en-fr"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors="pt")
```

ä½ ä¹Ÿå¯ä»¥å°† `model_checkpoint` æ›¿æ¢ä¸ºä½ ä» [Hub](https://huggingface.co/models)(https://huggingface.co/models) ä¸­é€‰æ‹©çš„å…¶ä»–æ¨¡å‹ï¼Œæˆ–è€…ä¸€ä¸ªä½ ä¿å­˜äº†é¢„è®­ç»ƒæ¨¡å‹å’Œ tokenizer çš„æœ¬åœ°æ–‡ä»¶å¤¹ã€‚

<div custom-style="Tip-green">

ğŸ’¡ å¦‚æœä½ åœ¨ä½¿ç”¨ä¸€ä¸ªå¤šè¯­è¨€çš„ tokenizerï¼Œæ¯”å¦‚ mBARTï¼ŒmBART-50ï¼Œæˆ–è€… M2M100ï¼Œä½ éœ€è¦é€šè¿‡è®¾ç½® `tokenizer.src_lang` å’Œ `tokenizer.tgt_lang` æ¥åœ¨ tokenizer ä¸­æŒ‡å®šä½ çš„è¾“å…¥å’Œç›®æ ‡çš„è¯­è¨€ä»£ç ã€‚

</div>

æˆ‘ä»¬çš„æ•°æ®å‡†å¤‡ç›¸å½“ç®€å•ã€‚åªæœ‰ä¸€ç‚¹è¦è®°ä½ï¼›ä½ éœ€è¦ç¡®ä¿ tokenizer å¤„ç†çš„ç›®æ ‡æ˜¯è¾“å‡ºè¯­è¨€ï¼ˆåœ¨è¿™é‡Œæ˜¯æ³•è¯­ï¼‰ã€‚ä½ å¯ä»¥é€šè¿‡å°†ç›®æ ‡è¯­è¨€ä¼ é€’ç»™ tokenizer çš„ `__call__` æ–¹æ³•çš„ `text_targets` å‚æ•°æ¥å®Œæˆæ­¤æ“ä½œã€‚

ä¸ºäº†çœ‹çœ‹è¿™æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Œè®©æˆ‘ä»¬å¤„ç†è®­ç»ƒé›†ä¸­æ¯ç§è¯­è¨€çš„ä¸€ä¸ªæ ·æœ¬ï¼š

```python
en_sentence = split_datasets["train"][1]["translation"]["en"]
fr_sentence = split_datasets["train"][1]["translation"]["fr"]

inputs = tokenizer(en_sentence, text_target=fr_sentence)
inputs
```

```python
{'input_ids': [47591, 12, 9842, 19634, 9, 0], 'attention_mask': [1, 1, 1, 1, 1, 1], 'labels': [577, 5891, 2, 3184, 16, 2542, 5, 1710, 0]}
```

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œè¾“å‡ºåŒ…å«äº†ä¸è‹±è¯­å¥å­ç›¸å…³è”çš„ inputs IDsï¼Œè€Œä¸æ³•è¯­å¥å­ç›¸å…³è”çš„ IDs å­˜å‚¨åœ¨ `labels` å­—æ®µä¸­ã€‚å¦‚æœä½ å¿˜è®°æŒ‡ç¤ºä½ æ­£åœ¨å¯¹ labels è¿›è¡Œ tokenizeï¼Œå®ƒä»¬å°†ç”±è¾“å…¥ tokenizerï¼ˆè¯­è¨€ç±»å‹ä¸ä¸€æ ·ï¼‰ è¿›è¡Œ tokenizeï¼Œè€Œå¯¹äº Marian æ¨¡å‹æ¥è¯´ï¼Œæ•ˆæœä¸ä¼šå¾ˆå¥½ã€‚

```python
wrong_targets = tokenizer(fr_sentence)
print(tokenizer.convert_ids_to_tokens(wrong_targets["input_ids"]))
print(tokenizer.convert_ids_to_tokens(inputs["labels"]))
```

```python
['â–Par', 'â–dÃ©', 'f', 'aut', ',', 'â–dÃ©', 've', 'lop', 'per', 'â–les', 'â–fil', 's', 'â–de', 'â–discussion', '</s>']
['â–Par', 'â–dÃ©faut', ',', 'â–dÃ©velopper', 'â–les', 'â–fils', 'â–de', 'â–discussion', '</s>']
```

å¦‚ä½ æ‰€è§ï¼Œå¦‚æœç”¨è‹±è¯­çš„ tokenizer æ¥é¢„å¤„ç†æ³•è¯­å¥å­ï¼Œä¼šäº§ç”Ÿæ›´å¤šçš„ tokensï¼Œå› ä¸ºè¿™ä¸ª tokenizer ä¸è®¤è¯†ä»»ä½•æ³•è¯­å•è¯ï¼ˆé™¤äº†é‚£äº›åœ¨è‹±è¯­é‡Œä¹Ÿå‡ºç°çš„ï¼Œæ¯”å¦‚â€œdiscussionâ€ï¼‰ã€‚

ç”±äºâ€œinputsâ€æ˜¯ä¸€ä¸ªåŒ…å«æˆ‘ä»¬å¸¸ç”¨é”®ï¼ˆinputs IDã€æ³¨æ„æ©ç ç­‰ï¼‰çš„å­—å…¸ï¼Œæœ€åä¸€æ­¥æ˜¯å®šä¹‰æˆ‘ä»¬æ•°æ®é›†çš„é¢„å¤„ç†å‡½æ•°ï¼š

```python
max_length = 128

def preprocess_function(examples):
    inputs = [ex["en"] for ex in examples["translation"]]
    targets = [ex["fr"] for ex in examples["translation"]]
    model_inputs = tokenizer(
        inputs, text_target=targets, max_length=max_length, truncation=True
    )
```

è¯·æ³¨æ„ï¼Œæˆ‘ä»¬ä¸ºè¾“å…¥å’Œè¾“å‡ºè®¾ç½®äº†ç›¸åŒçš„æœ€å¤§é•¿åº¦ã€‚ç”±äºæˆ‘ä»¬å¤„ç†çš„æ–‡æœ¬çœ‹èµ·æ¥å¾ˆçŸ­ï¼Œæˆ‘ä»¬ä½¿ç”¨ 128ã€‚

<div custom-style="Tip-green">

ğŸ’¡ å¦‚æœä½ æ­£åœ¨ä½¿ç”¨ T5 æ¨¡å‹ï¼ˆæ›´å…·ä½“åœ°è¯´ï¼Œä¸€ä¸ª `t5-xxx` checkpoint ï¼‰ï¼Œæ¨¡å‹ä¼šæœŸæœ›æ–‡æœ¬è¾“å…¥æœ‰ä¸€ä¸ªå‰ç¼€æŒ‡ç¤ºç€æ‰‹å¤´çš„ä»»åŠ¡ï¼Œæ¯”å¦‚ `translate: English to French:` ã€‚

</div>

<div custom-style="Tip-yellow">

âš ï¸ æˆ‘ä»¬ä¸å…³æ³¨ç›®æ ‡çš„æ³¨æ„åŠ›æ©ç ï¼Œå› ä¸ºæ¨¡å‹ä¸ä¼šéœ€è¦å®ƒã€‚ç›¸åï¼Œæˆ‘ä»¬åº”è¯¥å°†å¡«å……ï¼ˆpaddingï¼‰ token å¯¹åº”çš„æ ‡ç­¾è®¾ç½®ä¸º `-100` ï¼Œä»¥ä¾¿åœ¨ loss è®¡ç®—ä¸­å¿½ç•¥å®ƒä»¬ã€‚ç”±äºæˆ‘ä»¬æ­£åœ¨ä½¿ç”¨åŠ¨æ€å¡«å……ï¼Œè¿™å°†åœ¨ç¨åç”±æˆ‘ä»¬çš„æ•°æ®æ•´ç†å™¨å®Œæˆï¼Œä½†æ˜¯å¦‚æœä½ åœ¨æ­¤å¤„ä½¿ç”¨å¡«å……ï¼Œä½ åº”è¯¥è°ƒæ•´é¢„å¤„ç†å‡½æ•°ï¼Œå°†æ‰€æœ‰ä¸å¡«å……ï¼ˆpaddingï¼‰ token å¯¹åº”çš„æ ‡ç­¾è®¾ç½®ä¸º `-100` ã€‚

</div>

æˆ‘ä»¬ç°åœ¨å¯ä»¥å¯¹æ•°æ®é›†çš„æ‰€æœ‰æ•°æ®ä¸€æ¬¡æ€§è¿›è¡Œè¯¥é¢„å¤„ç†ï¼š

```python
tokenized_datasets = split_datasets.map(
    preprocess_function,
    batched=True,
    remove_columns=split_datasets["train"].column_names,
)
```

ç°åœ¨æ•°æ®å·²ç»è¿‡é¢„å¤„ç†ï¼Œæˆ‘ä»¬å‡†å¤‡å¥½å¾®è°ƒæˆ‘ä»¬çš„é¢„è®­ç»ƒæ¨¡å‹äº†ï¼

{#if fw === 'pt'}

### ä½¿ç”¨ `Trainer` API å¾®è°ƒæ¨¡å‹ 

ä½¿ç”¨ `Trainer` çš„ä»£ç å°†ä¸ä»¥å‰ç›¸åŒï¼Œåªæ˜¯ç¨ä½œæ”¹åŠ¨ï¼šæˆ‘ä»¬åœ¨è¿™é‡Œå°†ä½¿ç”¨ [`Seq2SeqTrainer`](https://huggingface.co/transformers/main_classes/trainer.html#seq2seqtrainer)(https://huggingface.co/transformers/main_classes/trainer.html#seq2seqtrainer) ï¼Œå®ƒæ˜¯ `Trainer` çš„å­ç±»ï¼Œå®ƒä½¿ç”¨ `generate()` æ–¹æ³•æ¥é¢„æµ‹è¾“å…¥çš„è¾“å‡ºï¼Œå¹¶ä¸”å¯ä»¥æ­£ç¡®å¤„ç†è¿™ç§åºåˆ—åˆ°åºåˆ—çš„è¯„ä¼°ã€‚å½“æˆ‘ä»¬è®¨è®ºè¯„ä¼°æŒ‡æ ‡æ—¶ï¼Œæˆ‘ä»¬å°†æ›´è¯¦ç»†åœ°æ¢è®¨è¿™ä¸€ç‚¹ã€‚

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ¨¡å‹æ¥è¿›è¡Œå¾®è°ƒã€‚æˆ‘ä»¬å°†ä½¿ç”¨å¸¸ç”¨çš„ `AutoModel` APIï¼š

```python
#####Pytorch}
from transformers import AutoModelForSeq2SeqLM

model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)####end
```

{:else}

### ä½¿ç”¨ Keras å¾®è°ƒæ¨¡å‹ 

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ¨¡å‹æ¥è¿›è¡Œå¾®è°ƒã€‚æˆ‘ä»¬å°†ä½¿ç”¨å¸¸ç”¨çš„ `AutoModel` APIï¼š

```python
#####TensorFlow}
from transformers import TFAutoModelForSeq2SeqLM

model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint, from_pt=True)####end
```

<div custom-style="Tip-green">

ğŸ’¡ `Helsinki-NLP/opus-mt-en-fr` checkpoint åªæœ‰ PyTorch çš„æƒé‡ï¼Œæ‰€ä»¥å¦‚æœä½ å°è¯•åŠ è½½æ¨¡å‹è€Œæ²¡æœ‰ä½¿ç”¨ `from_pt=True` å‚æ•°åœ¨ `from_pretrained()` æ–¹æ³•ä¸­ï¼Œä½ ä¼šå¾—åˆ°ä¸€ä¸ªé”™è¯¯ã€‚å½“ä½ æŒ‡å®š `from_pt=True` ï¼Œåº“ä¼šè‡ªåŠ¨ä¸‹è½½å¹¶ä¸ºä½ è½¬æ¢ PyTorch æƒé‡ã€‚å¦‚ä½ æ‰€è§ï¼Œä½¿ç”¨transormer åœ¨ä¸¤ç§æ¡†æ¶ä¹‹é—´åˆ‡æ¢éå¸¸ç®€å•ã€‚

</div>

{/if}

æ³¨æ„ï¼Œè¿™æ¬¡æˆ‘ä»¬ä½¿ç”¨çš„æ˜¯ä¸€ä¸ªå·²ç»åœ¨ç¿»è¯‘ä»»åŠ¡ä¸Šè¿›è¡Œè¿‡è®­ç»ƒçš„æ¨¡å‹ï¼Œå®é™…ä¸Šå·²ç»å¯ä»¥ä½¿ç”¨äº†ï¼Œæ‰€ä»¥æ²¡æœ‰å…³äºç¼ºå°‘æƒé‡æˆ–æ–°åˆå§‹åŒ–çš„æƒé‡çš„è­¦å‘Šã€‚

#### æ•°æ®æ•´ç† 

æˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ•°æ®æ•´ç†å™¨æ¥å¤„ç†åŠ¨æ€æ‰¹å¤„ç†çš„å¡«å……ã€‚åœ¨æœ¬ä¾‹ä¸­ï¼Œæˆ‘ä»¬ä¸èƒ½åƒç¬¬å››ç« é‚£æ ·åªä½¿ç”¨ `DataCollatorWithPadding` ï¼Œå› ä¸ºå®ƒåªå¡«å……è¾“å…¥ï¼ˆinputs IDã€æ³¨æ„æ©ç å’Œ token ç±»å‹ IDï¼‰ã€‚æˆ‘ä»¬çš„æ ‡ç­¾ä¹Ÿåº”è¯¥è¢«å¡«å……åˆ°æ‰€æœ‰æ ‡ç­¾ä¸­çš„æœ€å¤§é•¿åº¦ã€‚è€Œä¸”ï¼Œå¦‚å‰æ‰€è¿°ï¼Œç”¨äºå¡«å……æ ‡ç­¾çš„å¡«å……å€¼åº”ä¸º `-100` ï¼Œè€Œä¸æ˜¯ tokenizer çš„å¡«å…… tokenï¼Œä»¥ç¡®ä¿åœ¨æŸå¤±è®¡ç®—ä¸­å¿½ç•¥è¿™äº›å¡«å……å€¼ã€‚

è¿™ä¸€åˆ‡éƒ½å¯ä»¥ç”± [`DataCollatorForSeq2Seq`](https://huggingface.co/transformers/main_classes/data_collator.html#datacollatorforseq2seq)(https://huggingface.co/transformers/main_classes/data_collator.html#datacollatorforseq2seq) å®Œæˆã€‚ä¸ `DataCollatorWithPadding` ä¸€æ ·ï¼Œå®ƒæ¥æ”¶ç”¨äºé¢„å¤„ç†è¾“å…¥çš„ `tokenizer` ï¼Œä½†å®ƒä¹Ÿæ¥æ”¶ `model` ã€‚è¿™æ˜¯å› ä¸ºæ•°æ®æ•´ç†å™¨è¿˜å°†è´Ÿè´£å‡†å¤‡è§£ç å™¨ inputs IDï¼Œå®ƒä»¬æ˜¯æ ‡ç­¾åç§»ä¹‹åå½¢æˆçš„ï¼Œå¼€å¤´å¸¦æœ‰ç‰¹æ®Š token ç”±äºå¯¹äºä¸åŒçš„æ¶æ„æœ‰ç¨å¾®ä¸åŒçš„åç§»æ–¹å¼ï¼Œ `DataCollatorForSeq2Seq` éœ€è¦æ¥æ”¶ `model` å¯¹è±¡ï¼š

{#if fw === 'pt'}

```python
#####Pytorch}
from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)####end
```

{:else}

```python
#####TensorFlow}
from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors="tf")####end
```

{/if}

ä¸ºäº†åœ¨å‡ ä¸ªæ ·æœ¬ä¸Šè¿›è¡Œæµ‹è¯•ï¼Œæˆ‘ä»¬åœ¨å·²ç»å®Œæˆ tokenize çš„è®­ç»ƒé›†ä¸­çš„éƒ¨åˆ†æ•°æ®ä¸Šè°ƒç”¨å®ƒï¼š

```python
batch = data_collator([tokenized_datasets["train"][i] for i in range(1, 3)])
batch.keys()
```

```python
dict_keys(['attention_mask', 'input_ids', 'labels', 'decoder_input_ids'])
```

æˆ‘ä»¬å¯ä»¥æ£€æŸ¥æˆ‘ä»¬çš„æ ‡ç­¾æ˜¯å¦å·²ç»ç”¨ `-100` å¡«å……åˆ° batch çš„æœ€å¤§é•¿åº¦ï¼š

```python
batch["labels"]
```

```python
tensor([[  577,  5891,     2,  3184,    16,  2542,     5,  1710,     0,  -100,
          -100,  -100,  -100,  -100,  -100,  -100],
        [ 1211,     3,    49,  9409,  1211,     3, 29140,   817,  3124,   817,
           550,  7032,  5821,  7907, 12649,     0]])
```

æˆ‘ä»¬è¿˜å¯ä»¥æŸ¥çœ‹è§£ç å™¨çš„ inputs IDï¼Œå¯ä»¥çœ‹åˆ°å®ƒä»¬æ˜¯æ ‡ç­¾ç»è¿‡åç§»å½¢æˆçš„ç‰ˆæœ¬ï¼š

```python
batch["decoder_input_ids"]
```

```python
tensor([[59513,   577,  5891,     2,  3184,    16,  2542,     5,  1710,     0,
         59513, 59513, 59513, 59513, 59513, 59513],
        [59513,  1211,     3,    49,  9409,  1211,     3, 29140,   817,  3124,
           817,   550,  7032,  5821,  7907, 12649]])
```

ä»¥ä¸‹æ˜¯æˆ‘ä»¬æ•°æ®é›†ä¸­ç¬¬ä¸€ä¸ªå’Œç¬¬äºŒä¸ªå…ƒç´ çš„æ ‡ç­¾ï¼š

```python
for i in range(1, 3):
    print(tokenized_datasets["train"][i]["labels"])
```

```python
[577, 5891, 2, 3184, 16, 2542, 5, 1710, 0]
[1211, 3, 49, 9409, 1211, 3, 29140, 817, 3124, 817, 550, 7032, 5821, 7907, 12649, 0]
```

{#if fw === 'pt'}

æˆ‘ä»¬å°†æŠŠè¿™ä¸ª `data_collator` ä¼ é€’ç»™ `Seq2SeqTrainer` ã€‚æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬çœ‹ä¸€ä¸‹è¯„ä¼°æŒ‡æ ‡ã€‚

{:else}

æˆ‘ä»¬ç°åœ¨å¯ä»¥ä½¿ç”¨ `data_collator` å°†æˆ‘ä»¬çš„æ¯ä¸ªæ•°æ®é›†è½¬æ¢ä¸º `tf.data.Dataset` ï¼Œå‡†å¤‡å¥½è¿›è¡Œè®­ç»ƒï¼š

```python
#####TensorFlow}
tf_train_dataset = model.prepare_tf_dataset(
    tokenized_datasets["train"],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=32,
)
tf_eval_dataset = model.prepare_tf_dataset(
    tokenized_datasets["validation"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=16,
)####end
```

{/if}

#### è¯„ä¼°æŒ‡æ ‡ 

{#if fw === 'pt'}
`Seq2SeqTrainer` å¯¹å…¶è¶…ç±» `Trainer` çš„å¢å¼ºåŠŸèƒ½æ˜¯åœ¨è¯„ä¼°æˆ–é¢„æµ‹æ—¶ä½¿ç”¨ `generate()` æ–¹æ³•ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹ä¼šä½¿ç”¨ `decoder_input_ids` ï¼Œå¹¶é€šè¿‡ä¸€ä¸ªæ³¨æ„åŠ›æ©ç ç¡®ä¿å®ƒä¸ä½¿ç”¨åœ¨é¢„æµ‹çš„ token ä¹‹åçš„ tokenï¼Œä»¥åŠ é€Ÿè®­ç»ƒã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬æ— æ³•ä½¿ç”¨è¿™äº›æ ‡ç­¾ã€‚å› æ­¤ï¼Œä½¿ç”¨åŒæ ·çš„è®¾ç½®è¯„ä¼°æˆ‘ä»¬çš„æ¨¡å‹æ˜¯ä¸€ä¸ªå¥½ä¸»æ„ã€‚

æ­£å¦‚æˆ‘ä»¬åœ¨ç¬¬äºŒç« çœ‹åˆ°çš„ï¼Œè§£ç å™¨é€šè¿‡ä¸€ä¸ªä¸€ä¸ªåœ°é¢„æµ‹ token æ¥æ‰§è¡Œæ¨ç†â€”â€”è¿™æ˜¯Transformers åœ¨å¹•åé€šè¿‡ `generate()` æ–¹æ³•å®ç°çš„ã€‚ `Seq2SeqTrainer` å°†å…è®¸æˆ‘ä»¬åœ¨è®¾ç½® `predict_with_generate=True` æ—¶ï¼Œä½¿ç”¨è¯¥æ–¹æ³•è¿›è¡Œè¯„ä¼°ã€‚

{/if}

ç”¨äºç¿»è¯‘çš„ä¼ ç»ŸæŒ‡æ ‡æ˜¯ [BLEU åˆ†æ•°](https://en.wikipedia.org/wiki/BLEU)(https://en.wikipedia.org/wiki/BLEU) ï¼Œå®ƒæœ€åˆåœ¨ 2002 å¹´ç”± Kishore Papineni ç­‰äººçš„ä¸€ç¯‡æ–‡ç« ä¸­è¢«å¼•å…¥ã€‚BLEU åˆ†æ•°è¯„ä¼°ç¿»è¯‘ä¸å…¶æ ‡ç­¾çš„æ¥è¿‘ç¨‹åº¦ã€‚å®ƒä¸è¡¡é‡æ¨¡å‹ç”Ÿæˆè¾“å‡ºçš„å¯ç†è§£æ€§æˆ–è¯­æ³•æ­£ç¡®æ€§ï¼Œè€Œæ˜¯ä½¿ç”¨ç»Ÿè®¡è§„åˆ™æ¥ç¡®ä¿ç”Ÿæˆè¾“å‡ºä¸­çš„æ‰€æœ‰å•è¯ä¹Ÿå‡ºç°åœ¨è¾“å‡ºçš„ç›®æ ‡ä¸­ã€‚æ­¤å¤–ï¼Œè¿˜æœ‰ä¸€äº›è§„åˆ™å¯¹é‡å¤çš„è¯è¿›è¡Œæƒ©ç½šï¼Œå¦‚æœè¿™äº›è¯åœ¨è¾“å‡ºçš„ç›®æ ‡ä¸­æ²¡æœ‰é‡å¤ï¼ˆé¿å…æ¨¡å‹è¾“å‡ºåƒâ€œthe the the the theâ€è¿™æ ·çš„å¥å­ï¼‰ï¼›ä»¥åŠå¯¹è¾“å‡ºçš„å¥å­é•¿åº¦æ¯”ç›®æ ‡ä¸­çš„çŸ­ï¼ˆé¿å…æ¨¡å‹è¾“å‡ºåƒâ€œtheâ€è¿™æ ·çš„å¥å­ï¼‰è¿›è¡Œæƒ©ç½šã€‚

BLEU çš„ä¸€ä¸ªç¼ºç‚¹æ˜¯å®ƒéœ€è¦æ–‡æœ¬å·²ç»è¢«åˆ†è¯ï¼Œè¿™ä½¿å¾—æ¯”è¾ƒä½¿ç”¨ä¸åŒåˆ†è¯å™¨çš„æ¨¡å‹ä¹‹é—´çš„åˆ†æ•°å˜å¾—å›°éš¾ã€‚å› æ­¤ï¼Œå½“ä»Šç”¨äºåŸºå‡†ç¿»è¯‘æ¨¡å‹çš„æœ€å¸¸ç”¨æŒ‡æ ‡æ˜¯ [SacreBLEU](https://github.com/mjpost/sacrebleu)(https://github.com/mjpost/sacrebleu) ï¼Œå®ƒé€šè¿‡å¯¹åˆ†è¯æ­¥éª¤æ ‡å‡†åŒ–è§£å†³äº†è¿™ä¸ªç¼ºç‚¹ï¼ˆå’Œå…¶ä»–çš„ä¸€äº›ç¼ºç‚¹ï¼‰ã€‚è¦ä½¿ç”¨æ­¤æŒ‡æ ‡ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦å®‰è£… SacreBLEU åº“ï¼š

```python
!pip install sacrebleu
```

ç„¶åæˆ‘ä»¬å¯ä»¥å°±åƒæˆ‘ä»¬åœ¨ç¬¬å››ç« é‚£æ ·é€šè¿‡ `evaluate.load()` åŠ è½½å®ƒ 

```python
import evaluate

metric = evaluate.load("sacrebleu")
```

è¿™ä¸ªæŒ‡æ ‡å°†æ–‡æœ¬ä½œä¸ºè¾“å…¥å’Œç›®æ ‡ã€‚å®ƒçš„è®¾è®¡æ˜¯ä¸ºäº†æ¥å—å¤šä¸ªå¯æ¥å—çš„ç›®æ ‡ï¼Œå› ä¸ºåŒä¸€å¥è¯é€šå¸¸æœ‰å¤šç§å¯æ¥å—çš„ç¿»è¯‘â€”â€”æˆ‘ä»¬ä½¿ç”¨çš„æ•°æ®é›†åªæä¾›ä¸€ä¸ªï¼Œä½†åœ¨ NLP ä¸­æ‰¾åˆ°å°†å¤šä¸ªå¥å­ä½œä¸ºæ ‡ç­¾çš„æ•°æ®é›†æ˜¯å¾ˆå¸¸è§çš„ã€‚å› æ­¤ï¼Œé¢„æµ‹ç»“æœåº”è¯¥æ˜¯ä¸€ä¸ªå¥å­åˆ—è¡¨ï¼Œè€Œå‚è€ƒåº”è¯¥æ˜¯ä¸€ä¸ªå¥å­åˆ—è¡¨çš„åˆ—è¡¨ã€‚

è®©æˆ‘ä»¬å°è¯•ä¸€ä¸ªä¾‹å­ï¼š

```python
predictions = [
    "This plugin lets you translate web pages between several languages automatically."
]
references = [
    [
        "This plugin allows you to automatically translate web pages between several languages."
    ]
]
metric.compute(predictions=predictions, references=references)
```

```python
{'score': 46.750469682990165,
 'counts': [11, 6, 4, 3],
 'totals': [12, 11, 10, 9],
 'precisions': [91.67, 54.54, 40.0, 33.33],
 'bp': 0.9200444146293233,
 'sys_len': 12,
 'ref_len': 13}
```

è¿™å¾—åˆ°äº† 46.75 çš„ BLEU åˆ†æ•°ï¼Œè¿™æ˜¯ç›¸å½“ä¸é”™çš„â€”â€”ä½œä¸ºå‚è€ƒï¼ŒåŸå§‹ Transformer æ¨¡å‹åœ¨ [â€œAttention Is All You Needâ€ è®ºæ–‡](https://arxiv.org/pdf/1706.03762.pdf)(https://arxiv.org/pdf/1706.03762.pdf) ç±»ä¼¼çš„è‹±è¯­å’Œæ³•è¯­ç¿»è¯‘ä»»åŠ¡ä¸­è·å¾—äº† 41.8 çš„ BLEU åˆ†æ•°ï¼ï¼ˆå…³äºå…¶ä»–æŒ‡æ ‡ï¼Œå¦‚ `counts` å’Œ `bp` ï¼Œå¯ä»¥å‚è§ [SacreBLEUä»“åº“](https://github.com/mjpost/sacrebleu/blob/078c440168c6adc89ba75fe6d63f0d922d42bcfe/sacrebleu/metrics/bleu.py#L74)(https://github.com/mjpost/sacrebleu/blob/078c440168c6adc89ba75fe6d63f0d922d42bcfe/sacrebleu/metrics/bleu.py#L74) ï¼‰å¦ä¸€æ–¹é¢ï¼Œå¦‚æœæˆ‘ä»¬å°è¯•ä½¿ç”¨ç¿»è¯‘æ¨¡å‹ä¸­ç»å¸¸å‡ºç°çš„ä¸¤ç§ç³Ÿç³•çš„é¢„æµ‹ç±»å‹ï¼ˆå¤§é‡é‡å¤æˆ–å¤ªçŸ­ï¼‰ï¼Œæˆ‘ä»¬å°†å¾—åˆ°ç›¸å½“ç³Ÿç³•çš„ BLEU åˆ†æ•°ï¼š

```python
predictions = ["This This This This"]
references = [
    [
        "This plugin allows you to automatically translate web pages between several languages."
    ]
]
metric.compute(predictions=predictions, references=references)
```

```python
{'score': 1.683602693167689,
 'counts': [1, 0, 0, 0],
 'totals': [4, 3, 2, 1],
 'precisions': [25.0, 16.67, 12.5, 12.5],
 'bp': 0.10539922456186433,
 'sys_len': 4,
 'ref_len': 13}
```

```python
predictions = ["This plugin"]
references = [
    [
        "This plugin allows you to automatically translate web pages between several languages."
    ]
]
metric.compute(predictions=predictions, references=references)
```

```python
{'score': 0.0,
 'counts': [2, 1, 0, 0],
 'totals': [2, 1, 0, 0],
 'precisions': [100.0, 100.0, 0.0, 0.0],
 'bp': 0.004086771438464067,
 'sys_len': 2,
 'ref_len': 13}
```

åˆ†æ•°å¯ä»¥ä» 0 åˆ° 100ï¼Œè¶Šé«˜è¶Šå¥½ã€‚

{#if fw === 'tf'}

ä¸ºäº†å°†æ¨¡å‹çš„è¾“å‡ºè½¬åŒ–ä¸ºè¯„ä¼°æŒ‡æ ‡å¯ä»¥ä½¿ç”¨çš„æ–‡æœ¬ï¼Œæˆ‘ä»¬å°†åˆ©ç”¨ `tokenizer.batch_decode()` æ–¹æ³•ã€‚å› ä¸º tokenizer ä¼šè‡ªåŠ¨å¤„ç†å¡«å…… tokensï¼Œæ‰€ä»¥æˆ‘ä»¬åªéœ€è¦æ¸…ç†æ‰€æœ‰æ ‡ç­¾ä¸­çš„ `-100` ã€‚è®©æˆ‘ä»¬å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œè¿™ä¸ªå‡½æ•°ä¼šæ¥æ”¶æˆ‘ä»¬çš„æ¨¡å‹å’Œä¸€ä¸ªæ•°æ®é›†ï¼Œå¹¶åœ¨å…¶ä¸Šè®¡ç®—æŒ‡æ ‡ã€‚

æˆ‘ä»¬è¿˜å°†ä½¿ç”¨ä¸€ä¸ªæ˜¾è‘—æå‡æ€§èƒ½çš„æŠ€å·§ - ä½¿ç”¨ [XLA](https://www.tensorflow.org/xla)(https://www.tensorflow.org/xla) ï¼ŒTensorFlow çš„åŠ é€Ÿçº¿æ€§ä»£æ•°ç¼–è¯‘å™¨ï¼Œç¼–è¯‘æˆ‘ä»¬çš„ç”Ÿæˆä»£ç ã€‚XLA å¯¹æ¨¡å‹çš„è®¡ç®—å›¾è¿›è¡Œäº†å„ç§ä¼˜åŒ–ï¼Œä»è€Œæ˜¾è‘—æå‡äº†é€Ÿåº¦å’Œå†…å­˜ä½¿ç”¨ç‡ã€‚å¦‚ Hugging Face çš„ [åšå®¢](https://huggingface.co/blog/tf-xla-generate)(https://huggingface.co/blog/tf-xla-generate) æ‰€è¿°ï¼Œå½“æˆ‘ä»¬çš„è¾“å…¥å½¢çŠ¶ä¸ä¼šå˜åŒ–å¤ªå¤§æ—¶ï¼ŒXLA å·¥ä½œå¾—æœ€å¥½ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†æŠŠè¾“å…¥è¡¥é½åˆ° 128 çš„å€æ•°ï¼Œç„¶åç”¨å¡«å……æ•´ç†å™¨åˆ¶ä½œä¸€ä¸ªæ–°çš„æ•°æ®é›†ï¼Œç„¶åæˆ‘ä»¬ä½¿ç”¨ `@tf.function(jit_compile=True)` è£…é¥°å™¨è£…é¥°æˆ‘ä»¬çš„ç”Ÿæˆå‡½æ•°ï¼Œè¿™å°†æ ‡è®°æ•´ä¸ªå‡½æ•°ç”¨ XLA ç¼–è¯‘ã€‚

```python
#####TensorFlow}
import numpy as np
import tensorflow as tf
from tqdm import tqdm

generation_data_collator = DataCollatorForSeq2Seq(
    tokenizer, model=model, return_tensors="tf", pad_to_multiple_of=128
)

tf_generate_dataset = model.prepare_tf_dataset(
    tokenized_datasets["validation"],
    collate_fn=generation_data_collator,
    shuffle=False,
    batch_size=8,
)

@tf.function(jit_compile=True)
def generate_with_xla(batch):
    return model.generate(
        input_ids=batch["input_ids"],
        attention_mask=batch["attention_mask"],
        max_new_tokens=128,
    )

def compute_metrics():
    all_preds = []
    all_labels = []
    sampled_dataset = tokenized_datasets["validation"].shuffle().select(range(200))
    tf_generate_dataset = sampled_dataset.to_tf_dataset(
        columns=["input_ids", "attention_mask", "labels"],
        collate_fn=data_collator,
        shuffle=False,
        batch_size=4,
    )
    for batch, labels in tqdm(tf_generate_dataset):
        predictions = generate_with_xla(batch)
        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
        labels = labels.numpy()
        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
        decoded_preds = [pred.strip() for pred in decoded_preds]
        decoded_labels = [[label.strip()] for label in decoded_labels]
        all_preds.extend(decoded_preds)
        all_labels.extend(decoded_labels)

    result = metric.compute(predictions=all_preds, references=all_labels)
    return {"bleu": result["score"]}####end
```

{:else}

ä¸ºäº†å°†æ¨¡å‹çš„è¾“å‡ºè½¬åŒ–ä¸ºè¯„ä¼°æŒ‡æ ‡å¯ä»¥ä½¿ç”¨çš„æ–‡æœ¬ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ `tokenizer.batch_decode()` æ–¹æ³•ã€‚å› ä¸º tokenizer ä¼šè‡ªåŠ¨å¤„ç†å¡«å…… tokensï¼Œæ‰€ä»¥æˆ‘ä»¬åªéœ€è¦æ¸…ç†æ ‡ç­¾ä¸­çš„æ‰€æœ‰ `-100` ï¼š

```python
#####Pytorch}
import numpy as np

def compute_metrics(eval_preds):
    preds, labels = eval_preds
    # å¦‚æœæ¨¡å‹è¿”å›çš„å†…å®¹è¶…è¿‡äº†é¢„æµ‹çš„logits
    if isinstance(preds, tuple):
        preds = preds[0]

    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)

    # ç”±äºæˆ‘ä»¬æ— æ³•è§£ç  -100,å› æ­¤å°†æ ‡ç­¾ä¸­çš„ -100 æ›¿æ¢æ‰
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # ä¸€äº›ç®€å•çš„åå¤„ç†
    decoded_preds = [pred.strip() for pred in decoded_preds]
    decoded_labels = [[label.strip()] for label in decoded_labels]

    result = metric.compute(predictions=decoded_preds, references=decoded_labels)
    return {"bleu": result["score"]}####end
```

{/if}

ç°åœ¨è¿™å·²ç»å®Œæˆäº†ï¼Œæˆ‘ä»¬å·²ç»å‡†å¤‡å¥½å¾®è°ƒæˆ‘ä»¬çš„æ¨¡å‹äº†ï¼

#### å¾®è°ƒæ¨¡å‹ 

ç¬¬ä¸€æ­¥æ˜¯ç™»å½• Hugging Faceï¼Œè¿™æ ·ä½ å°±å¯ä»¥å°†ç»“æœä¸Šä¼ åˆ°æ¨¡å‹ä¸­å¿ƒã€‚æœ‰ä¸€ä¸ªæ–¹ä¾¿çš„åŠŸèƒ½å¯ä»¥å¸®åŠ©ä½ åœ¨ notebook ä¸­å®Œæˆæ­¤æ“ä½œï¼š

```python
from huggingface_hub import notebook_login

notebook_login()
```

è¿™å°†æ˜¾ç¤ºä¸€ä¸ªå°éƒ¨ä»¶ï¼Œä½ å¯ä»¥åœ¨å…¶ä¸­è¾“å…¥ä½ çš„ Hugging Face ç™»å½•å‡­æ®ã€‚

å¦‚æœä½ ä¸æ˜¯åœ¨ notebook ä¸Šè¿è¡Œä»£ç ï¼Œåªéœ€åœ¨ç»ˆç«¯ä¸­è¾“å…¥ä»¥ä¸‹è¡Œï¼š

```python
huggingface-cli login
```

{#if fw === 'tf'}

åœ¨æˆ‘ä»¬å¼€å§‹ä¹‹å‰ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬åœ¨æ²¡æœ‰ä»»ä½•è®­ç»ƒçš„æƒ…å†µä¸‹ä»æˆ‘ä»¬çš„æ¨¡å‹ä¸­å¾—åˆ°äº†ä»€ä¹ˆæ ·çš„ç»“æœï¼š

```python
#####TensorFlow}
print(compute_metrics())####end
```

```python
#####TensorFlow}
{'bleu': 33.26983701454733}####end
```

ä¸€æ—¦å®Œæˆï¼Œæˆ‘ä»¬å°±å¯ä»¥å‡†å¤‡ç¼–è¯‘å’Œè®­ç»ƒæ¨¡å‹æ‰€éœ€çš„ä¸€åˆ‡ã€‚è¯·æ³¨æ„å½“ä½¿ç”¨ `tf.keras.mixed_precision.set_global_policy("mixed_float16")` æ—¶â€”â€”è¿™å°†å‘Šè¯‰ Keras ä½¿ç”¨ float16 è¿›è¡Œè®­ç»ƒï¼Œè¿™å¯ä»¥æ˜¾ç€æé«˜æ”¯æŒå®ƒçš„ GPUï¼ˆNvidia 20xx/V100 æˆ–æ›´é«˜ç‰ˆæœ¬ï¼‰çš„é€Ÿåº¦ã€‚

```python
#####TensorFlow}
from transformers import create_optimizer
from transformers.keras_callbacks import PushToHubCallback
import tensorflow as tf

# è®­ç»ƒæ­¥æ•°æ˜¯æ•°æ®é›†ä¸­çš„æ ·æœ¬æ•°é‡,é™¤ä»¥ batch å¤§å°,ç„¶åä¹˜ä»¥æ€»çš„ epoch æ•°ã€‚
# æ³¨æ„è¿™é‡Œçš„ tf_train_dataset æ˜¯ batch å½¢å¼çš„ tf.data.Dataset,
# è€Œä¸æ˜¯åŸå§‹çš„ Hugging Face Dataset ,æ‰€ä»¥ä½¿ç”¨ len() è®¡ç®—å®ƒçš„é•¿åº¦å·²ç»æ˜¯ num_samples // batch_sizeã€‚

num_epochs = 3
num_train_steps = len(tf_train_dataset) * num_epochs

optimizer, schedule = create_optimizer(
    init_lr=5e-5,
    num_warmup_steps=0,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)
model.compile(optimizer=optimizer)

## ä½¿ç”¨ float16 æ··åˆç²¾åº¦è¿›è¡Œè®­ç»ƒ
tf.keras.mixed_precision.set_global_policy("mixed_float16")####end
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å®šä¹‰ä¸€ä¸ª `PushToHubCallback` ä»¥ä¾¿åœ¨è®­ç»ƒæœŸé—´å°†æˆ‘ä»¬çš„æ¨¡å‹ä¸Šä¼ åˆ° Hubï¼Œæ­£å¦‚æˆ‘ä»¬åœ¨ç¬¬ 2 èŠ‚ä¸­çœ‹åˆ°çš„ï¼Œç„¶åæˆ‘ä»¬åªéœ€ä½¿ç”¨è¯¥å›è°ƒæ¥æ‹Ÿåˆæ¨¡å‹ï¼š

```python
#####TensorFlow}
from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(
    output_dir="marian-finetuned-kde4-en-to-fr", tokenizer=tokenizer
)

model.fit(
    tf_train_dataset,
    validation_data=tf_eval_dataset,
    callbacks=[callback],
    epochs=num_epochs,
)####end
```

è¯·æ³¨æ„ï¼Œä½ å¯ä»¥ä½¿ç”¨ `hub_model_id` å‚æ•°æŒ‡å®šè¦æ¨é€åˆ°çš„å­˜å‚¨åº“çš„åç§°ï¼ˆå½“ä½ æƒ³æŠŠæ¨¡å‹æ¨é€åˆ°æŒ‡å®šçš„ç»„ç»‡çš„æ—¶å€™ï¼Œå°±å¿…é¡»ä½¿ç”¨æ­¤å‚æ•°ï¼‰ã€‚ä¾‹å¦‚ï¼Œå½“æˆ‘ä»¬å°†æ¨¡å‹æ¨é€åˆ° [`huggingface-course` ç»„ç»‡](https://huggingface.co/huggingface-course)(https://huggingface.co/huggingface-course)(https://huggingface.co/huggingface-course) æ—¶ï¼Œæˆ‘ä»¬æ·»åŠ äº† `hub_model_id="huggingface-course/marian-finetuned-kde4-en- to-fr"` åˆ° `Seq2SeqTrainingArguments` ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œè¯¥ä»“åº“å°†ä¿å­˜åœ¨ä½ çš„è´¦æˆ·é‡Œï¼Œå¹¶ä»¥ä½ è®¾ç½®çš„è¾“å‡ºç›®å½•å‘½åï¼Œå› æ­¤è¿™é‡Œå°†æ˜¯ `"sgugger/marian-finetuned-kde4-en-to-fr"` ã€‚

<div custom-style="Tip-green">

ğŸ’¡ å¦‚æœä½ æ­£åœ¨ä½¿ç”¨çš„è¾“å‡ºç›®å½•å·²ç»å­˜åœ¨ï¼Œå®ƒéœ€è¦æ˜¯ä½ æƒ³è¦æ¨é€åˆ°çš„ä»“åº“çš„æœ¬åœ°å…‹éš†ã€‚å¦‚æœä¸æ˜¯ï¼Œå½“è°ƒç”¨ `model.fit()` æ—¶ä¼šæ”¶åˆ°é”™è¯¯ï¼Œå¹¶éœ€è¦è®¾ç½®ä¸€ä¸ªæ–°åç§°ã€‚

</div>

æœ€åï¼Œè®©æˆ‘ä»¬çœ‹çœ‹è®­ç»ƒç»“æŸåæˆ‘ä»¬çš„æŒ‡æ ‡æ˜¯ä»€ä¹ˆæ ·çš„ï¼š

```python
#####TensorFlow}
print(compute_metrics())####end
```

```python
#####TensorFlow}
{'bleu': 57.334066271545865}####end
```

åœ¨è¿™ä¸ªé˜¶æ®µï¼Œä½ å¯ä»¥ä½¿ç”¨æ¨¡å‹ä¸­å¿ƒä¸Šçš„æ¨ç†å°éƒ¨ä»¶æ¥æµ‹è¯•ä½ çš„æ¨¡å‹å¹¶ä¸ä½ çš„æœ‹å‹åˆ†äº«ã€‚ä½ å·²ç»æˆåŠŸåœ°åœ¨ç¿»è¯‘ä»»åŠ¡ä¸Šå¾®è°ƒäº†ä¸€ä¸ªæ¨¡å‹â€”â€”æ­å–œï¼

{:else}

ä¸€æ—¦å®Œæˆè¿™äº›æ­¥éª¤ï¼Œæˆ‘ä»¬å°±å¯ä»¥å®šä¹‰æˆ‘ä»¬çš„ `Seq2SeqTrainingArguments` ã€‚ä¸ `Trainer` ä¸€æ ·ï¼Œæˆ‘ä»¬ä½¿ç”¨ `TrainingArguments` çš„å­ç±»ï¼Œå…¶ä¸­åŒ…å«æ›´å¤šå¯ä»¥è®¾ç½®çš„å­—æ®µï¼š

```python
#####Pytorch}
from transformers import Seq2SeqTrainingArguments

args = Seq2SeqTrainingArguments(
    f"marian-finetuned-kde4-en-to-fr",
    evaluation_strategy="no",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=64,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=3,
    predict_with_generate=True,
    fp16=True,
    push_to_hub=True,
)####end
```

é™¤äº†é€šå¸¸çš„è¶…å‚æ•°ï¼ˆå¦‚å­¦ä¹ ç‡ã€è®­ç»ƒè½®æ•°ã€æ‰¹æ¬¡å¤§å°å’Œä¸€äº›æƒé‡è¡°å‡ï¼‰ä¹‹å¤–ï¼Œè¿™é‡Œä¸æˆ‘ä»¬åœ¨å‰é¢ç« èŠ‚çœ‹åˆ°çš„æœ‰ä¸€äº›ä¸åŒï¼š

- æˆ‘ä»¬æ²¡æœ‰è®¾ç½®å®šæœŸè¿›è¡Œè¯„ä¼°ï¼Œå› ä¸ºè¯„ä¼°éœ€è¦è€—è´¹ä¸€å®šçš„æ—¶é—´ï¼›æˆ‘ä»¬åªä¼šåœ¨è®­ç»ƒå¼€å§‹ä¹‹å‰å’Œç»“æŸä¹‹åè¯„ä¼°æˆ‘ä»¬çš„æ¨¡å‹ä¸€æ¬¡ã€‚
- æˆ‘ä»¬è®¾ç½® `fp16=True` ï¼Œè¿™å¯ä»¥åŠ å¿«æ”¯æŒ fp16 çš„ GPU ä¸Šçš„è®­ç»ƒé€Ÿåº¦ã€‚
- å’Œä¸Šé¢æˆ‘ä»¬è®¨è®ºçš„é‚£æ ·ï¼Œæˆ‘ä»¬è®¾ç½® `predict_with_generate=True` ã€‚
- æˆ‘ä»¬ç”¨ `push_to_hub=True` åœ¨æ¯ä¸ª epoch ç»“æŸæ—¶å°†æ¨¡å‹ä¸Šä¼ åˆ° Hubã€‚

è¯·æ³¨æ„ï¼Œä½ å¯ä»¥ä½¿ç”¨ `hub_model_id` å‚æ•°æŒ‡å®šè¦æ¨é€åˆ°çš„å­˜å‚¨åº“çš„åç§°ï¼ˆå½“ä½ æƒ³æŠŠæ¨¡å‹æ¨é€åˆ°æŒ‡å®šçš„ç»„ç»‡çš„æ—¶å€™ï¼Œå°±å¿…é¡»ä½¿ç”¨æ­¤å‚æ•°ï¼‰ã€‚ä¾‹å¦‚ï¼Œå½“æˆ‘ä»¬å°†æ¨¡å‹æ¨é€åˆ° [`huggingface-course` ç»„ç»‡](https://huggingface.co/huggingface-course)(https://huggingface.co/huggingface-course)(https://huggingface.co/huggingface-course) æ—¶ï¼Œæˆ‘ä»¬æ·»åŠ äº† `hub_model_id="huggingface-course/marian-finetuned-kde4-en- to-fr"` åˆ° `Seq2SeqTrainingArguments` ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œè¯¥ä»“åº“å°†ä¿å­˜åœ¨ä½ çš„è´¦æˆ·ä¸­ï¼Œå¹¶ä»¥ä½ è®¾ç½®çš„è¾“å‡ºç›®å½•å‘½åï¼Œå› æ­¤è¿™é‡Œå°†æ˜¯ `"sgugger/marian-finetuned-kde4-en-to-fr"` ã€‚

<div custom-style="Tip-green">

ğŸ’¡å¦‚æœä½ ä½¿ç”¨çš„è¾“å‡ºç›®å½•å·²ç»å­˜åœ¨ï¼Œåˆ™å®ƒéœ€è¦æ˜¯ä½ è¦æ¨é€åˆ°çš„å­˜å‚¨åº“çš„æœ¬åœ°å…‹éš†ã€‚å¦‚æœä¸æ˜¯ï¼Œä½ å°†åœ¨å®šä¹‰ä½ çš„ `Seq2SeqTrainer` åç§°æ—¶ä¼šé‡åˆ°é”™è¯¯ï¼Œå¹¶ä¸”éœ€è¦è®¾ç½®ä¸€ä¸ªæ–°åç§°ã€‚

</div>

æœ€åï¼Œæˆ‘ä»¬å°†æ‰€æœ‰å†…å®¹ä¼ é€’ç»™ `Seq2SeqTrainer` ï¼š

```python
#####Pytorch}
from transformers import Seq2SeqTrainer

trainer = Seq2SeqTrainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)####end
```

åœ¨å¼€å§‹è®­ç»ƒä¹‹å‰ï¼Œæˆ‘ä»¬å…ˆçœ‹ä¸€ä¸‹æˆ‘ä»¬çš„æ¨¡å‹å¾—åˆ°çš„åˆ†æ•°ï¼Œä»¥ç¡®ä¿æˆ‘ä»¬çš„å¾®è°ƒå¹¶æœªä½¿æƒ…å†µå˜å¾—æ›´ç³Ÿã€‚è¿™ä¸ªå‘½ä»¤éœ€è¦ä¸€äº›æ—¶é—´ï¼Œæ‰€ä»¥ä½ å¯ä»¥åœ¨æ‰§è¡ŒæœŸé—´å»å–æ¯å’–å•¡ï¼š

```python
#####Pytorch}
trainer.evaluate(max_length=max_length)####end
```

```python
#####Pytorch}
{'eval_loss': 1.6964408159255981,
 'eval_bleu': 39.26865061007616,
 'eval_runtime': 965.8884,
 'eval_samples_per_second': 21.76,
 'eval_steps_per_second': 0.341}####end
```

BLEU å¾—åˆ†ä¸º 39 å¹¶ä¸ç®—å¤ªå·®ï¼Œè¿™åæ˜ äº†æˆ‘ä»¬çš„æ¨¡å‹å·²ç»æ“…é•¿å°†è‹±è¯­å¥å­ç¿»è¯‘æˆæ³•è¯­å¥å­ã€‚

æ¥ä¸‹æ¥æ˜¯è®­ç»ƒï¼Œè¿™ä¹Ÿéœ€è¦ä¸€äº›æ—¶é—´ï¼š

```python
#####Pytorch}
trainer.train()####end
```

è¯·æ³¨æ„ï¼Œå½“è®­ç»ƒå‘ç”Ÿæ—¶ï¼Œæ¯æ¬¡ä¿å­˜æ¨¡å‹æ—¶ï¼ˆè¿™é‡Œæ˜¯æ¯ä¸ª epochï¼‰ï¼Œå®ƒéƒ½ä¼šåœ¨åå°ä¸Šä¼ åˆ° Hubã€‚è¿™æ ·ï¼Œå¦‚æœ‰å¿…è¦ï¼Œä½ å°†èƒ½å¤Ÿåœ¨å¦ä¸€å°æœºå™¨ä¸Šç»§ç»­ä½ çš„è®­ç»ƒã€‚

è®­ç»ƒå®Œæˆåï¼Œæˆ‘ä»¬å†æ¬¡è¯„ä¼°æˆ‘ä»¬çš„æ¨¡å‹â€”â€”å¸Œæœ›æˆ‘ä»¬ä¼šçœ‹åˆ° BLEU åˆ†æ•°æœ‰æ‰€æé«˜ï¼

```python
#####Pytorch}
trainer.evaluate(max_length=max_length)####end
```

```python
#####Pytorch}
{'eval_loss': 0.8558505773544312,
 'eval_bleu': 52.94161337775576,
 'eval_runtime': 714.2576,
 'eval_samples_per_second': 29.426,
 'eval_steps_per_second': 0.461,
 'epoch': 3.0}####end
```

è¿™æ˜¯è¿‘ 14 ç‚¹çš„æ”¹è¿›ï¼Œè¿™å¾ˆæ£’ã€‚

æœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨ `push_to_hub()` æ–¹æ³•æ¥ç¡®ä¿æˆ‘ä»¬ä¸Šä¼ æ¨¡å‹çš„æœ€æ–°ç‰ˆæœ¬ã€‚ `Trainer` è¿˜åˆ›å»ºäº†ä¸€å¼ åŒ…å«æ‰€æœ‰è¯„ä¼°ç»“æœçš„æ¨¡å‹å¡å¹¶ä¸Šä¼ ã€‚æ­¤æ¨¡å‹å¡åŒ…å«å¯å¸®åŠ©æ¨¡å‹ä¸­å¿ƒä¸ºæ¨ç†æ¼”ç¤ºé€‰æ‹©å°éƒ¨ä»¶çš„å…ƒæ•°æ®ã€‚é€šå¸¸ä¸éœ€è¦åšé¢å¤–çš„æ›´æ”¹ï¼Œå› ä¸ºå®ƒå¯ä»¥ä»æ¨¡å‹ç±»ä¸­æ¨æ–­å‡ºæ­£ç¡®çš„å°éƒ¨ä»¶ï¼Œä½†åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œå®ƒä»…èƒ½é€šè¿‡æ¨¡å‹ç±»æ¨æ–­è¿™ä¸ªæ˜¯ä¸€ä¸ªåºåˆ—åˆ°åºåˆ—é—®é¢˜ï¼Œæ‰€ä»¥æˆ‘ä»¬æŒ‡å®šå®ƒæ˜¯ä¸€ä¸ªç¿»è¯‘æ¨¡å‹ï¼š

```python
#####Pytorch}
trainer.push_to_hub(tags="translation", commit_message="Training complete")####end
```

å¦‚æœä½ æƒ³æ£€æŸ¥å‘½ä»¤æ‰§è¡Œçš„ç»“æœï¼Œæ­¤å‘½ä»¤å°†è¿”å›å®ƒåˆšåˆšæ‰§è¡Œçš„æäº¤çš„ URLï¼Œä½ å¯ä»¥æ‰“å¼€ url è¿›è¡Œæ£€æŸ¥ï¼š

```python
#####Pytorch}
'https://huggingface.co/sgugger/marian-finetuned-kde4-en-to-fr/commit/3601d621e3baae2bc63d3311452535f8f58f6ef3'####end
```

åœ¨æ­¤é˜¶æ®µï¼Œä½ å¯ä»¥åœ¨ Model Hub ä¸Šä½¿ç”¨æ¨ç†å°éƒ¨ä»¶æ¥æµ‹è¯•ä½ çš„æ¨¡å‹ï¼Œå¹¶ä¸ä½ çš„æœ‹å‹åˆ†äº«ã€‚ä½ å·²ç»æˆåŠŸåœ°åœ¨ç¿»è¯‘ä»»åŠ¡ä¸Šè¿›è¡Œäº†æ¨¡å‹çš„å¾®è°ƒï¼Œæ­å–œä½ ï¼

å¦‚æœä½ æƒ³æ›´æ·±å…¥åœ°äº†è§£è®­ç»ƒå¾ªç¯ï¼Œæˆ‘ä»¬ç°åœ¨å°†å‘ä½ å±•ç¤ºå¦‚ä½•ä½¿ç”¨ Accelerate åšåŒæ ·çš„äº‹æƒ…ã€‚

{/if}

{#if fw === 'pt'}

### è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ 

æˆ‘ä»¬ç°åœ¨æ¥çœ‹ä¸€ä¸‹å®Œæ•´çš„è®­ç»ƒå¾ªç¯ï¼Œè¿™æ ·ä½ å°±å¯ä»¥è½»æ¾å®šåˆ¶ä½ éœ€è¦çš„éƒ¨åˆ†ã€‚å®ƒå°†ä¸æˆ‘ä»¬åœ¨ç¬¬ 2 èŠ‚å’Œç¬¬ 3 èŠ‚ä¸­åšçš„éå¸¸ç›¸ä¼¼ã€‚

#### å‡†å¤‡è®­ç»ƒæ‰€éœ€çš„ä¸€åˆ‡ 

ä½ å·²ç»å¤šæ¬¡çœ‹åˆ°æ‰€æœ‰è¿™äº›ï¼Œå› æ­¤è¿™ä¸€å—ä¼šç®€ç•¥è¿›è¡Œã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å°†æ•°æ®é›†è®¾ç½®ä¸ºâ€œtorchâ€æ ¼å¼ï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥å¾—åˆ° PyTorch å¼ é‡ï¼Œç„¶åæˆ‘ä»¬ç”¨æ•°æ®é›†æ„å»º `DataLoader` ï¼š

```python
#####Pytorch}
from torch.utils.data import DataLoader

tokenized_datasets.set_format("torch")
train_dataloader = DataLoader(
    tokenized_datasets["train"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], collate_fn=data_collator, batch_size=8
)####end
```

æ¥ä¸‹æ¥æˆ‘ä»¬é‡æ–°å®ä¾‹åŒ–æˆ‘ä»¬çš„æ¨¡å‹ï¼Œä»¥ç¡®ä¿æˆ‘ä»¬ä¸ä¼šç»§ç»­ä¸Šä¸€èŠ‚çš„å¾®è°ƒï¼Œè€Œæ˜¯å†æ¬¡ä»é¢„è®­ç»ƒæ¨¡å‹å¼€å§‹é‡æ–°è®­ç»ƒï¼š

```python
#####Pytorch}
model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)####end
```

ç„¶åæˆ‘ä»¬éœ€è¦ä¸€ä¸ªä¼˜åŒ–å™¨ï¼š

```python
#####Pytorch}
from transformers import AdamW

optimizer = AdamW(model.parameters(), lr=2e-5)####end
```

ä¸€æ—¦æˆ‘ä»¬æ‹¥æœ‰æ‰€æœ‰è¿™äº›å¯¹è±¡ï¼Œæˆ‘ä»¬å°±å¯ä»¥å°†å®ƒä»¬å‘é€åˆ° `accelerator.prepare()` æ–¹æ³•ä¸­ã€‚è¯·è®°ä½ï¼Œå¦‚æœä½ æƒ³åœ¨ Colab Notebook ä¸Šä½¿ç”¨ TPU è¿›è¡Œè®­ç»ƒï¼Œä½ éœ€è¦å°†æ‰€æœ‰è¿™äº›ä»£ç ç§»åŠ¨åˆ°ä¸€ä¸ªè®­ç»ƒå‡½æ•°ä¸­ï¼Œå¹¶ä¸”è¯¥å‡½æ•°ä¸åº”ä»»ä½•å®ä¾‹åŒ– `Accelerator` çš„å•å…ƒæ ¼ã€‚

```python
#####Pytorch}
from accelerate import Accelerator

accelerator = Accelerator()
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)####end
```

ç°åœ¨æˆ‘ä»¬å·²ç»å°†æˆ‘ä»¬çš„ `train_dataloader` å‘é€åˆ° `accelerator.prepare()` ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å®ƒçš„é•¿åº¦æ¥è®¡ç®—è®­ç»ƒæ­¥éª¤çš„æ•°é‡ã€‚è¯·è®°ä½ï¼Œæˆ‘ä»¬åº”è¯¥å§‹ç»ˆåœ¨å‡†å¤‡å¥½æ•°æ®åŠ è½½å™¨åæ‰§è¡Œæ­¤æ“ä½œï¼Œå› ä¸ºè¯¥æ–¹æ³•ä¼šæ”¹å˜ `DataLoader` çš„é•¿åº¦ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨å­¦ä¹ ç‡è¡°å‡åˆ° 0 çš„ç»å…¸çº¿æ€§å­¦ä¹ ç‡è°ƒåº¦ï¼š

```python
#####Pytorch}
from transformers import get_scheduler

num_train_epochs = 3
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)####end
```

æœ€åï¼Œè¦å°†æˆ‘ä»¬çš„æ¨¡å‹æ¨é€åˆ° Hubï¼Œæˆ‘ä»¬éœ€è¦åœ¨ä¸€ä¸ªå·¥ä½œæ–‡ä»¶å¤¹ä¸­åˆ›å»ºä¸€ä¸ª `Repository` å¯¹è±¡ã€‚å¦‚æœä½ å°šæœªç™»å½•ï¼Œè¯·å…ˆç™»å½• Hugging Faceã€‚æˆ‘ä»¬å°†ä»æˆ‘ä»¬æƒ³è¦ä¸ºæ¨¡å‹æä¾›çš„æ¨¡å‹ ID ä¸­ç¡®å®šä»“åº“åç§°ï¼ˆä½ å¯ä»¥ç”¨ä½ è‡ªå·±çš„é€‰æ‹©æ›¿æ¢ `repo_name` ï¼Œåªéœ€è¦åŒ…å«ä½ çš„ç”¨æˆ·åï¼Œç”¨å‡½æ•° `get_full_repo_name()` å‡½æ•°å¯ä»¥æŸ¥çœ‹å½“å‰çš„ç”¨æˆ·åï¼‰ï¼š

```python
#####Pytorch}
from huggingface_hub import Repository, get_full_repo_name

model_name = "marian-finetuned-kde4-en-to-fr-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name####end
```

```python
#####Pytorch}
'sgugger/marian-finetuned-kde4-en-to-fr-accelerate'####end
```

ç„¶åæˆ‘ä»¬å¯ä»¥åœ¨æœ¬åœ°æ–‡ä»¶å¤¹ä¸­å…‹éš†è¯¥å­˜å‚¨åº“ã€‚å¦‚æœå®ƒå·²ç»å­˜åœ¨ï¼Œè¿™ä¸ªæœ¬åœ°æ–‡ä»¶å¤¹åº”è¯¥æ˜¯æˆ‘ä»¬æ­£åœ¨ä½¿ç”¨çš„å­˜å‚¨åº“çš„å…‹éš†ï¼š

```python
#####Pytorch}
output_dir = "marian-finetuned-kde4-en-to-fr-accelerate"
repo = Repository(output_dir, clone_from=repo_name)####end
```

ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡è°ƒç”¨ `repo.push_to_hub()` æ–¹æ³•ä¸Šä¼ æˆ‘ä»¬åœ¨ `output_dir` ä¸­ä¿å­˜çš„æ‰€æœ‰æ–‡ä»¶ã€‚è¿™å°†å¸®åŠ©æˆ‘ä»¬åœ¨æ¯ä¸ª epoch ç»“æŸæ—¶ä¸Šä¼ ä¸­é—´æ¨¡å‹ã€‚

#### è®­ç»ƒå¾ªç¯ 

æˆ‘ä»¬ç°åœ¨å‡†å¤‡ç¼–å†™å®Œæ•´çš„è®­ç»ƒå¾ªç¯ã€‚ä¸ºäº†ç®€åŒ–å…¶è¯„ä¼°éƒ¨åˆ†ï¼Œæˆ‘ä»¬å®šä¹‰äº†è¿™ä¸ª `postprocess()` å‡½æ•°ï¼Œå®ƒæ¥å—é¢„æµ‹å’Œæ ‡ç­¾ï¼Œå¹¶å°†å®ƒä»¬è½¬æ¢ä¸ºæˆ‘ä»¬çš„ `metric` å¯¹è±¡æ‰€æœŸæœ›çš„å­—ç¬¦ä¸²åˆ—è¡¨ï¼š

```python
#####Pytorch}
def postprocess(predictions, labels):
    predictions = predictions.cpu().numpy()
    labels = labels.cpu().numpy()

    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)

    # æ›¿æ¢æ ‡ç­¾ä¸­çš„ -100,å› ä¸ºæˆ‘ä»¬æ— æ³•è§£ç å®ƒä»¬ã€‚
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # ä¸€äº›ç®€å•çš„åå¤„ç†
    decoded_preds = [pred.strip() for pred in decoded_preds]
    decoded_labels = [[label.strip()] for label in decoded_labels]
    return decoded_preds, decoded_labels####end
```

è®­ç»ƒå¾ªç¯çœ‹èµ·æ¥å’Œæœ¬ç« ç¬¬ 2 èŠ‚ä¸ç¬¬å››ç« å¾ˆåƒï¼Œåªæ˜¯åœ¨è¯„ä¼°éƒ¨åˆ†æœ‰ä¸€äº›ä¸åŒ â€”â€” æ‰€ä»¥è®©æˆ‘ä»¬é‡ç‚¹å…³æ³¨ä¸€ä¸‹è¿™ä¸€ç‚¹ï¼

é¦–å…ˆè¦æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬ä½¿ç”¨ `generate()` æ–¹æ³•æ¥è®¡ç®—é¢„æµ‹ï¼Œä½†è¿™æ˜¯æˆ‘ä»¬åŸºç¡€æ¨¡å‹ä¸Šçš„ä¸€ä¸ªæ–¹æ³•ï¼Œè€Œä¸æ˜¯Accelerate åœ¨ `prepare()` æ–¹æ³•ä¸­åˆ›å»ºçš„å°è£…æ¨¡å‹ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬é¦–å…ˆ `unwrap_model` ï¼Œç„¶åè°ƒç”¨æ­¤æ–¹æ³•ã€‚

ç¬¬äºŒä¸ªè¦æ³¨æ„çš„æ˜¯ï¼Œå°±åƒ [token åˆ†ç±»](https://chat.openai.com/course/chapter7/2)(https://chat.openai.com/course/chapter7/2) ä¸€æ ·ï¼Œä¸¤ä¸ªè¿‡ç¨‹å¯èƒ½ä»¥ä¸åŒçš„å½¢çŠ¶å¯¹è¾“å…¥å’Œæ ‡ç­¾è¿›è¡Œäº†å¡«å……ï¼Œæ‰€ä»¥æˆ‘ä»¬ä½¿ç”¨ `accelerator.pad_across_processes()` æ¥åœ¨è°ƒç”¨ `gather()` æ–¹æ³•ä¹‹å‰ä½¿é¢„æµ‹å’Œæ ‡ç­¾å…·æœ‰ç›¸åŒçš„å½¢çŠ¶ã€‚å¦‚æœæˆ‘ä»¬ä¸è¿™ä¹ˆåšï¼Œè¯„ä¼°å°†å‡ºé”™æˆ–æ°¸è¿œæŒ‚èµ·ã€‚

```python
#####Pytorch}
from tqdm.auto import tqdm
import torch

progress_bar = tqdm(range(num_training_steps))

for epoch in range(num_train_epochs):
    # è®­ç»ƒ
    model.train()
    for batch in train_dataloader:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

    # è¯„ä¼°
    model.eval()
    for batch in tqdm(eval_dataloader):
        with torch.no_grad():
            generated_tokens = accelerator.unwrap_model(model).generate(
                batch["input_ids"],
                attention_mask=batch["attention_mask"],
                max_length=128,
            )
        labels = batch["labels"]

        # éœ€è¦å¡«å……é¢„æµ‹å’Œæ ‡ç­¾æ‰èƒ½è°ƒç”¨gather()
        generated_tokens = accelerator.pad_across_processes(
            generated_tokens, dim=1, pad_index=tokenizer.pad_token_id
        )
        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)

        predictions_gathered = accelerator.gather(generated_tokens)
        labels_gathered = accelerator.gather(labels)

        decoded_preds, decoded_labels = postprocess(predictions_gathered, labels_gathered)
        metric.add_batch(predictions=decoded_preds, references=decoded_labels)

    results = metric.compute()
    print(f"epoch {epoch}, BLEU score: {results['score']:.2f}")

    # ä¿å­˜å’Œä¸Šä¼ 
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
    if accelerator.is_main_process:
        tokenizer.save_pretrained(output_dir)
        repo.push_to_hub(
            commit_message=f"Training in progress epoch {epoch}", blocking=False
        )####end
```

```python
#####Pytorch}
epoch 0, BLEU score: 53.47
epoch 1, BLEU score: 54.24
epoch 2, BLEU score: 54.44####end
```

ä¸€æ—¦å®Œæˆï¼Œä½ åº”è¯¥æœ‰ä¸€ä¸ªæ¨¡å‹ï¼Œå…¶ç»“æœä¸ `Seq2SeqTrainer` è®­ç»ƒçš„æ¨¡å‹éå¸¸ç›¸ä¼¼ã€‚ä½ å¯ä»¥åœ¨ [huggingface-course/marian-finetuned-kde4-en-to-fr-accelerate](https://huggingface.co/huggingface-course/marian-finetuned-kde4-en-to-fr-accelerate)(https://huggingface.co/huggingface-course/marian-finetuned-kde4-en-to-fr-accelerate) ä¸ŠæŸ¥çœ‹æˆ‘ä»¬ä½¿ç”¨æ­¤ä»£ç è®­ç»ƒçš„æ¨¡å‹ã€‚å¦‚æœä½ æƒ³æµ‹è¯•å¯¹è®­ç»ƒå¾ªç¯çš„ä»»ä½•è°ƒæ•´ï¼Œä½ å¯ä»¥ç›´æ¥é€šè¿‡ç¼–è¾‘ä¸Šé¢æ˜¾ç¤ºçš„ä»£ç æ¥å®ç°ï¼

{/if}

### ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹ 

æˆ‘ä»¬å·²ç»å‘ä½ å±•ç¤ºäº†å¦‚ä½•åœ¨æ¨¡å‹ Hub ä¸Šä½¿ç”¨æˆ‘ä»¬å¾®è°ƒçš„æ¨¡å‹ã€‚è¦åœ¨æœ¬åœ°çš„ `pipeline` ä¸­ä½¿ç”¨å®ƒï¼Œæˆ‘ä»¬åªéœ€è¦æŒ‡å®šæ­£ç¡®çš„æ¨¡å‹æ ‡è¯†ç¬¦ï¼š

```python
from transformers import pipeline

## å°†å…¶æ›¿æ¢æˆä½ è‡ªå·±çš„ checkpoint
model_checkpoint = "huggingface-course/marian-finetuned-kde4-en-to-fr"
translator = pipeline("translation", model=model_checkpoint)
translator("Default to expanded threads")
```

```python
[{'translation_text': 'Par dÃ©faut, dÃ©velopper les fils de discussion'}]
```

å¦‚é¢„æœŸï¼Œæˆ‘ä»¬çš„é¢„è®­ç»ƒæ¨¡å‹é€‚åº”äº†æˆ‘ä»¬å¾®è°ƒå®ƒçš„è¯­æ–™åº“ï¼Œè€Œä¸æ˜¯ä¿ç•™è‹±è¯­å•è¯â€œthreadsâ€ï¼Œè€Œæ˜¯å°†å®ƒç¿»è¯‘æˆæ³•è¯­çš„å®˜æ–¹ç‰ˆæœ¬ã€‚å¯¹äºâ€œpluginâ€ä¹Ÿæ˜¯å¦‚æ­¤ï¼š

```python
translator(
    "Unable to import %1 using the OFX importer plugin. This file is not the correct format."
)
```

```python
[{'translation_text': "Impossible d'importer %1 en utilisant le module externe d'importation OFX. Ce fichier n'est pas le bon format."}]
```

å¦ä¸€ä¸ªé¢†åŸŸé€‚åº”çš„å¥½ä¾‹å­ï¼

<div custom-style="Tip-green">

âœï¸ **è½®åˆ°ä½ äº†ï¼** æ¨¡å‹å¯¹ä½ ä¹‹å‰æ‰¾åˆ°çš„åŒ…å«å•è¯â€œemailâ€çš„æ ·æœ¬è¿”å›ä»€ä¹ˆç»“æœï¼Ÿ

</div>


## 8.4 æå–æ–‡æœ¬æ‘˜è¦ 

åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹çœ‹å¦‚ä½•ä½¿ç”¨ Transformer æ¨¡å‹å°†é•¿ç¯‡æ–‡æ¡£å‹ç¼©ä¸ºæ‘˜è¦ï¼Œè¿™é¡¹ä»»åŠ¡ç§°ä¸ºæ–‡æœ¬æ‘˜è¦ã€‚è¿™æ˜¯æœ€å…·æŒ‘æˆ˜æ€§çš„è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡ä¹‹ä¸€ï¼Œå› ä¸ºå®ƒéœ€è¦ä¸€ç³»åˆ—èƒ½åŠ›ï¼Œä¾‹å¦‚ç†è§£é•¿ç¯‡æ–‡ç« å’Œç”Ÿæˆèƒ½å¤Ÿæ•æ‰æ–‡æ¡£ä¸­ä¸»è¦ä¸»é¢˜çš„è¿è´¯æ–‡æœ¬ã€‚ä½†æ˜¯ï¼Œå¦‚æœåšå¾—å¥½ï¼Œæ–‡æœ¬æ‘˜è¦æ˜¯ä¸€ç§å¼ºå¤§çš„å·¥å…·ï¼Œå¯ä»¥å‡è½»é¢†åŸŸä¸“å®¶è¯¦ç»†é˜…è¯»é•¿æ–‡æ¡£çš„è´Ÿæ‹…ï¼Œä»è€ŒåŠ å¿«å„ç§ä¸šåŠ¡æµç¨‹ã€‚

å°½ç®¡åœ¨ [Hugging Face Hub](https://huggingface.co/models?pipeline_tag=summarization=downloads)(https://huggingface.co/models?pipeline_tag=summarization=downloads) ä¸Šå·²ç»å­˜åœ¨å„ç§å¾®è°ƒæ¨¡å‹ç”¨äºæ–‡æœ¬æ‘˜è¦ï¼Œå‡ ä¹æ‰€æœ‰è¿™äº›éƒ½åªé€‚ç”¨äºè‹±æ–‡æ–‡æ¡£ã€‚å› æ­¤ï¼Œä¸ºäº†åœ¨æœ¬èŠ‚ä¸­æ·»åŠ ä¸€äº›å˜åŒ–ï¼Œæˆ‘ä»¬å°†ä¸ºè‹±è¯­å’Œè¥¿ç­ç‰™è¯­è®­ç»ƒä¸€ä¸ªåŒè¯­æ¨¡å‹ã€‚åœ¨æœ¬èŠ‚ç»“æŸæ—¶ï¼Œä½ å°†æœ‰ä¸€ä¸ªå¯ä»¥æ€»ç»“å®¢æˆ·è¯„è®ºçš„ [æ¨¡å‹](https://huggingface.co/huggingface-course/mt5-small-finetuned-amazon-en-es)(https://huggingface.co/huggingface-course/mt5-small-finetuned-amazon-en-es) ã€‚

![æå–æ‘˜è¦çš„ä¸€ä¸ªæ¼”ç¤ºä¾‹å­](./assets/summary-example.jpg)

æ­£å¦‚æˆ‘ä»¬å°†çœ‹åˆ°çš„ï¼Œè¿™äº›æ‘˜è¦å¾ˆç®€æ´ï¼Œå› ä¸ºå®ƒä»¬æ˜¯ä»å®¢æˆ·åœ¨äº§å“è¯„è®ºä¸­æä¾›çš„æ ‡é¢˜ä¸­å­¦åˆ°çš„ã€‚è®©æˆ‘ä»¬é¦–å…ˆä¸ºè¿™é¡¹ä»»åŠ¡å‡†å¤‡ä¸€ä¸ªåˆé€‚çš„åŒè¯­è¯­æ–™åº“ã€‚

### å‡†å¤‡å¤šè¯­è¨€è¯­æ–™åº“ 

æˆ‘ä»¬å°†ä½¿ç”¨ [å¤šè¯­è¨€äºšé©¬é€Šè¯„è®ºè¯­æ–™åº“](https://huggingface.co/datasets/amazon_reviews_multi)(https://huggingface.co/datasets/amazon_reviews_multi) åˆ›å»ºæˆ‘ä»¬çš„åŒè¯­æ‘˜è¦å™¨ã€‚è¯¥è¯­æ–™åº“ç”±å…­ç§è¯­è¨€çš„äºšé©¬é€Šäº§å“è¯„è®ºç»„æˆï¼Œé€šå¸¸ç”¨äºå¤šè¯­è¨€åˆ†ç±»å™¨çš„åŸºå‡†æµ‹è¯•ã€‚ç„¶è€Œï¼Œç”±äºæ¯æ¡è¯„è®ºéƒ½é™„æœ‰ä¸€ä¸ªç®€çŸ­çš„æ ‡é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ ‡é¢˜ä½œä¸ºæˆ‘ä»¬æ¨¡å‹å­¦ä¹ çš„å‚è€ƒæ‘˜è¦ï¼é¦–å…ˆï¼Œè®©æˆ‘ä»¬ä» Hugging Face Hub ä¸‹è½½è‹±è¯­å’Œè¥¿ç­ç‰™è¯­å­é›†ï¼š

```python
from datasets import load_dataset

spanish_dataset = load_dataset("amazon_reviews_multi", "es")
english_dataset = load_dataset("amazon_reviews_multi", "en")
english_dataset
```

```python
DatasetDict({
    train: Dataset({
        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],
        num_rows: 200000
    })
    validation: Dataset({
        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],
        num_rows: 5000
    })
    test: Dataset({
        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],
        num_rows: 5000
    })
})
```

å¦‚ä½ æ‰€è§ï¼Œå¯¹äºæ¯ç§è¯­è¨€ï¼Œéƒ½æœ‰ 200,000 æ¡è¯„è®º `train` çš„æ•°æ®é›†ï¼Œä»¥åŠ 5,000 æ¡è¯„è®ºç”¨äº `validation` å’Œ `test` çš„æ•°æ®é›†ã€‚æˆ‘ä»¬æ„Ÿå…´è¶£çš„è¯„è®ºä¿¡æ¯åŒ…å«åœ¨ `review_body` å’Œ `review_title` åˆ—ä¸­ã€‚è®©æˆ‘ä»¬é€šè¿‡åˆ›å»ºä¸€ä¸ªç®€å•çš„å‡½æ•°æ¥ä»è®­ç»ƒé›†ä¸­éšæœºæŠ½å–ä¸€äº›æ ·æœ¬ï¼Œè¯¥å‡½æ•°ä½¿ç”¨æˆ‘ä»¬åœ¨ç¬¬å…­ç« å­¦åˆ°è¿‡ï¼š

```python
def show_samples(dataset, num_samples=3, seed=42):
    sample = dataset["train"].shuffle(seed=seed).select(range(num_samples))
    for example in sample:
        print(f"\n'>> Title: {example['review_title']}'")
        print(f"'>> Review: {example['review_body']}'")

show_samples(english_dataset)
```

```python
'>> Title: Worked in front position, not rear'
'>> Review: 3 stars because these are not rear brakes as stated in the item description. At least the mount adapter only worked on the front fork of the bike that I got it for.'

'>> Title: meh'
'>> Review: Does itâ€™s job and itâ€™s gorgeous but mine is falling apart, I had to basically put it together again with hot glue'

'>> Title: Can\'t beat these for the money'
'>> Review: Bought this for handling miscellaneous aircraft parts and hanger "stuff" that I needed to organize; it really fit the bill. The unit arrived quickly, was well packaged and arrived intact (always a good sign). There are five wall mounts-- three on the top and two on the bottom. I wanted to mount it on the wall, so all I had to do was to remove the top two layers of plastic drawers, as well as the bottom corner drawers, place it when I wanted and mark it; I then used some of the new plastic screw in wall anchors (the 50 pound variety) and it easily mounted to the wall. Some have remarked that they wanted dividers for the drawers, and that they made those. Good idea. My application was that I needed something that I can see the contents at about eye level, so I wanted the fuller-sized drawers. I also like that these are the new plastic that doesn\'t get brittle and split like my older plastic drawers did. I like the all-plastic construction. It\'s heavy duty enough to hold metal parts, but being made of plastic it\'s not as heavy as a metal frame, so you can easily mount it to the wall and still load it up with heavy stuff, or light stuff. No problem there. For the money, you can\'t beat it. Best one of these I\'ve bought to date-- and I\'ve been using some version of these for over forty years.'
```

<div custom-style="Tip-green">

âœï¸ **è¯•è¯•çœ‹ï¼** æ›´æ”¹ `Dataset.shuffle()` å‘½ä»¤ä¸­çš„éšæœºç§å­ä»¥æ¢ç´¢è¯­æ–™åº“ä¸­çš„å…¶ä»–è¯„è®ºã€‚å¦‚æœä½ æ˜¯è¯´è¥¿ç­ç‰™è¯­çš„äººï¼Œè¯·æŸ¥çœ‹ `spanish_dataset` ä¸­çš„ä¸€äº›è¯„è®ºï¼Œçœ‹çœ‹æ ‡é¢˜æ˜¯å¦ä¹Ÿåƒåˆç†çš„æ‘˜è¦ã€‚

</div>

æ­¤ç¤ºä¾‹æ˜¾ç¤ºäº†äººä»¬é€šå¸¸åœ¨ç½‘ä¸Šæ‰¾åˆ°çš„è¯„è®ºçš„å¤šæ ·æ€§ï¼Œä»ç§¯æçš„åˆ°æ¶ˆæçš„ï¼ˆä»¥åŠä»‹äºä¸¤è€…ä¹‹é—´çš„æ‰€æœ‰å†…å®¹ï¼ï¼‰ã€‚å°½ç®¡å¸¦æœ‰â€œmehâ€æ ‡é¢˜çš„ç¤ºä¾‹çš„ä¿¡æ¯é‡ä¸å¤§ï¼Œä½†å…¶ä»–æ ‡é¢˜çœ‹èµ·æ¥åƒæ˜¯å¯¹è¯„è®ºæœ¬èº«çš„ä¸é”™çš„æ€»ç»“ã€‚åœ¨å•ä¸ª GPU ä¸Šè®­ç»ƒæ‰€æœ‰ 400,000 æ¡è¯„è®ºçš„æ‘˜è¦æ¨¡å‹å°†èŠ±è´¹å¤ªé•¿æ—¶é—´ï¼Œå› æ­¤æˆ‘ä»¬å°†ä¸“æ³¨äºä¸ºå•ä¸ªäº§å“é¢†åŸŸç”Ÿæˆæ‘˜è¦ã€‚ä¸ºäº†äº†è§£æˆ‘ä»¬å¯ä»¥é€‰æ‹©å“ªäº›é¢†åŸŸï¼Œè®©æˆ‘ä»¬å°† `english_dataset` è½¬æ¢ä¸º `pandas.DataFrame` ï¼Œå¹¶è®¡ç®—æ¯ä¸ªäº§å“ç±»åˆ«çš„è¯„è®ºæ•°é‡ï¼š

```python
english_dataset.set_format("pandas")
english_df = english_dataset["train"][:]
## æ˜¾ç¤ºå‰ 20 ä¸ªäº§å“çš„æ•°é‡
english_df["product_category"].value_counts()[:20]
```

```python
home                      17679
apparel                   15951
wireless                  15717
other                     13418
beauty                    12091
drugstore                 11730
kitchen                   10382
toy                        8745
sports                     8277
automotive                 7506
lawn_and_garden            7327
home_improvement           7136
pet_products               7082
digital_ebook_purchase     6749
pc                         6401
electronics                6186
office_product             5521
shoes                      5197
grocery                    4730
book                       3756
Name: product_category, dtype: int64
```

åœ¨è‹±è¯­æ•°æ®é›†ä¸­ï¼Œæœ€å—æ¬¢è¿çš„äº§å“æ˜¯å®¶å±…ç”¨å“ã€æœè£…å’Œæ— çº¿ç”µå­äº§å“ã€‚ä¸è¿‡ï¼Œä¸ºäº†å¸¦æœ‰äºšé©¬é€Šçš„ç‰¹è‰²ï¼Œè®©æˆ‘ä»¬ä¸“æ³¨äºæ€»ç»“ä¹¦ç±çš„è¯„è®ºâ€”â€”æ¯•ç«Ÿï¼Œè¿™æ˜¯äºšé©¬é€Šè¿™å®¶å…¬å¸æˆç«‹çš„åŸºç¡€ï¼æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¸¤ä¸ªç¬¦åˆè¦æ±‚çš„äº§å“ç±»åˆ«ï¼ˆ `book` å’Œ `digital_ebook_purchase` ï¼‰ï¼Œæ‰€ä»¥è®©æˆ‘ä»¬ç”¨è¿™ä¸¤ä¸ªäº§å“ç±»åˆ«è¿‡æ»¤ä¸¤ç§è¯­è¨€çš„æ•°æ®é›†ã€‚æ­£å¦‚æˆ‘ä»¬åœ¨ç¬¬å…­ç« å­¦åˆ°çš„ï¼Œ `Dataset.filter()` å‡½æ•°å¯ä»¥è®©æˆ‘ä»¬éå¸¸æœ‰æ•ˆåœ°å¯¹æ•°æ®é›†è¿›è¡Œåˆ‡ç‰‡ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥å®šä¹‰ä¸€ä¸ªç®€å•çš„å‡½æ•°æ¥è¿›è¡Œæ­¤æ“ä½œï¼š

```python
def filter_books(example):
    return (
        example["product_category"] == "book"
        or example["product_category"] == "digital_ebook_purchase"
    )
```

å½“æˆ‘ä»¬ä½¿ç”¨è¿™ä¸ªå‡½æ•°å¯¹ `english_dataset` å’Œ `spanish_dataset` è¿‡æ»¤åï¼Œç»“æœå°†åªåŒ…å«æ¶‰åŠä¹¦ç±ç±»åˆ«çš„é‚£äº›è¡Œã€‚åœ¨ä½¿ç”¨è¿‡æ»¤å™¨ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å°† `english_dataset` çš„æ ¼å¼ä» `"pandas"` åˆ‡æ¢å› `"arrow"` ï¼š

```python
english_dataset.reset_format()
```

ç„¶åæˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¿‡æ»¤å™¨åŠŸèƒ½ï¼Œä½œä¸ºä¸€ä¸ªåŸºæœ¬çš„æ£€æŸ¥ï¼Œè®©æˆ‘ä»¬æ£€æŸ¥ä¸€äº›è¯„è®ºçš„æ ·æœ¬ï¼Œçœ‹çœ‹å®ƒä»¬æ˜¯å¦ç¡®å®ä¸ä¹¦ç±æœ‰å…³ï¼š

```python
spanish_books = spanish_dataset.filter(filter_books)
english_books = english_dataset.filter(filter_books)
show_samples(english_books)
```

```python
'>> Title: I\'m dissapointed.'
'>> Review: I guess I had higher expectations for this book from the reviews. I really thought I\'d at least like it. The plot idea was great. I loved Ash but, it just didnt go anywhere. Most of the book was about their radio show and talking to callers. I wanted the author to dig deeper so we could really get to know the characters. All we know about Grace is that she is attractive looking, Latino and is kind of a brat. I\'m dissapointed.'

'>> Title: Good art, good price, poor design'
'>> Review: I had gotten the DC Vintage calendar the past two years, but it was on backorder forever this year and I saw they had shrunk the dimensions for no good reason. This one has good art choices but the design has the fold going through the picture, so it\'s less aesthetically pleasing, especially if you want to keep a picture to hang. For the price, a good calendar'

'>> Title: Helpful'
'>> Review: Nearly all the tips useful and. I consider myself an intermediate to advanced user of OneNote. I would highly recommend.'
```

å¥½çš„ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¯„è®ºå¹¶ä¸æ˜¯ä¸¥æ ¼æ„ä¹‰ä¸Šçš„ä¹¦ç±ï¼Œå¯èƒ½æ˜¯æŒ‡æ—¥å†å’Œ OneNote ç­‰ç”µå­åº”ç”¨ç¨‹åºç­‰å†…å®¹ã€‚å°½ç®¡å¦‚æ­¤ï¼Œè¯¥é¢†åŸŸä¼¼ä¹é€‚åˆè®­ç»ƒæ‘˜è¦æ¨¡å‹ã€‚åœ¨æˆ‘ä»¬æŸ¥çœ‹é€‚åˆæ­¤ä»»åŠ¡çš„å„ç§æ¨¡å‹ä¹‹å‰ï¼Œæˆ‘ä»¬è¿˜æœ‰æœ€åä¸€ç‚¹æ•°æ®å‡†å¤‡è¦åšï¼šå°†è‹±æ–‡å’Œè¥¿ç­ç‰™æ–‡è¯„è®ºä½œä¸ºå•ä¸ª `DatasetDict` å¯¹è±¡ç»„åˆèµ·æ¥ã€‚Datasets æä¾›äº†ä¸€ä¸ªæ–¹ä¾¿çš„ `concatenate_datasets()` å‡½æ•°ï¼Œå®ƒï¼ˆåå¦‚å…¶å®ï¼‰å°†æŠŠä¸¤ä¸ª `Dataset` å¯¹è±¡å †å åœ¨ä¸€èµ·ã€‚å› æ­¤ï¼Œä¸ºäº†åˆ›å»ºæˆ‘ä»¬çš„åŒè¯­æ•°æ®é›†ï¼Œæˆ‘ä»¬å°†éå†æ•°æ®é›†çš„æ¯ä¸ªéƒ¨åˆ†ï¼Œå¹¶æ‰“ä¹±ç»“æœä»¥ç¡®ä¿æˆ‘ä»¬çš„æ¨¡å‹ä¸ä¼šè¿‡åº¦æ‹Ÿåˆå•ä¸€è¯­è¨€ï¼š

```python
from datasets import concatenate_datasets, DatasetDict

books_dataset = DatasetDict()

for split in english_books.keys():
    books_dataset[split] = concatenate_datasets(
        [english_books[split], spanish_books[split]]
    )
    books_dataset[split] = books_dataset[split].shuffle(seed=42)

## æŒ‘é€‰ä¸€äº›æ ·ä¾‹
show_samples(books_dataset)
```

```python
'>> Title: Easy to follow!!!!'
'>> Review: I loved The dash diet weight loss Solution. Never hungry. I would recommend this diet. Also the menus are well rounded. Try it. Has lots of the information need thanks.'

'>> Title: PARCIALMENTE DAÃ‘ADO'
'>> Review: Me llegÃ³ el dÃ­a que tocaba, junto a otros libros que pedÃ­, pero la caja llegÃ³ en mal estado lo cual daÃ±Ã³ las esquinas de los libros porque venÃ­an sin protecciÃ³n (forro).'

'>> Title: no lo he podido descargar'
'>> Review: igual que el anterior'
```

è¿™çš„ç¡®çœ‹èµ·æ¥åƒæ˜¯æ··åˆäº†è‹±è¯­å’Œè¥¿ç­ç‰™è¯­çš„è¯„è®ºï¼ç°åœ¨æˆ‘ä»¬æœ‰äº†ä¸€ä¸ªè®­ç»ƒè¯­æ–™åº“ï¼Œæœ€åè¦æ£€æŸ¥çš„ä¸€ä»¶äº‹æ˜¯è¯„è®ºåŠå…¶æ ‡é¢˜ä¸­å•è¯çš„åˆ†å¸ƒã€‚è¿™å¯¹äºæ‘˜è¦ä»»åŠ¡å°¤å…¶é‡è¦ï¼Œå…¶ä¸­æ•°æ®ä¸­çš„ç®€çŸ­å‚è€ƒæ‘˜è¦ä¼šä½¿æ¨¡å‹åå‘äºç”Ÿæˆçš„æ‘˜è¦ä¸­ä»…è¾“å‡ºä¸€ä¸¤ä¸ªå•è¯ã€‚ä¸‹é¢çš„å›¾æ˜¾ç¤ºäº†å•è¯åˆ†å¸ƒï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æœ‰äº›æ ‡é¢˜ä¸¥é‡åå‘äº 1-2 ä¸ªå•è¯ï¼š

![è¯„è®ºæ ‡é¢˜å’Œæ­£æ–‡çš„å­—æ•°åˆ†å¸ƒ](./assets/review-lengths.png "Word count distributions for the review titles and texts.")

ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å°†è¿‡æ»¤æ‰æ ‡é¢˜éå¸¸çŸ­çš„ç¤ºä¾‹ï¼Œä»¥ä¾¿æˆ‘ä»¬çš„æ¨¡å‹å¯ä»¥ç”Ÿæˆæ›´æœ‰æ•ˆçš„æ‘˜è¦ã€‚ç”±äºæˆ‘ä»¬æ­£åœ¨å¤„ç†è‹±æ–‡å’Œè¥¿ç­ç‰™æ–‡æ–‡æœ¬ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ç²—ç•¥çš„å¯å‘å¼æ–¹æ³•åœ¨ç©ºç™½å¤„æ‹†åˆ†æ ‡é¢˜ï¼Œç„¶åç”¨æˆ‘ä»¬å¼ºå¤§çš„ `Dataset.filter()` æ–¹æ³•å¦‚ä¸‹ï¼š

```python
books_dataset = books_dataset.filter(lambda x: len(x["review_title"].split()) > 2)
```

ç°åœ¨æˆ‘ä»¬å·²ç»å‡†å¤‡å¥½äº†æˆ‘ä»¬çš„è¯­æ–™åº“ï¼Œè®©æˆ‘ä»¬æ¥çœ‹çœ‹ä¸€äº›å¯ä»¥å¯¹å…¶è¿›è¡Œå¾®è°ƒçš„å¯é€‰çš„ Transformer æ¨¡å‹ï¼

### æ–‡æœ¬æ‘˜è¦æ¨¡å‹ 

å¦‚æœä½ ä»”ç»†æƒ³æƒ³ï¼Œæ–‡æœ¬æ‘˜è¦æ˜¯ä¸€ç§ç±»ä¼¼äºæœºå™¨ç¿»è¯‘çš„ä»»åŠ¡ï¼šæˆ‘ä»¬æœ‰ä¸€ä¸ªåƒè¯„è®ºè¿™æ ·çš„æ–‡æœ¬æ­£æ–‡ï¼Œæˆ‘ä»¬å¸Œæœ›å°†å…¶â€œç¿»è¯‘â€æˆä¸€ä¸ªè¾ƒçŸ­çš„ç‰ˆæœ¬ï¼ŒåŒæ—¶æ•æ‰åˆ°è¾“å…¥çš„ä¸»è¦ç‰¹å¾ã€‚å› æ­¤ï¼Œå¤§å¤šæ•°ç”¨äºæ–‡æœ¬æ‘˜è¦çš„ Transformer æ¨¡å‹é‡‡ç”¨äº†æˆ‘ä»¬åœ¨ç¬¬äºŒç« é‡åˆ°çš„ç¼–ç å™¨-è§£ç å™¨æ¶æ„ã€‚å°½ç®¡æœ‰ä¸€äº›ä¾‹å¤–ï¼Œä¾‹å¦‚ GPT ç³»åˆ—æ¨¡å‹ï¼Œå®ƒä»¬åœ¨ few-shotï¼ˆå°‘é‡å¾®è°ƒï¼‰ä¹‹åä¹Ÿå¯ä»¥æå–æ‘˜è¦ã€‚ä¸‹è¡¨åˆ—å‡ºäº†ä¸€äº›å¯ä»¥è¿›è¡Œæ‘˜è¦å¾®è°ƒçš„æµè¡Œé¢„è®­ç»ƒæ¨¡å‹ã€‚

| Transformer æ¨¡å‹ | æè¿°                                                                                                                                                                                                    | å¤šç§è¨€ï¼Ÿ|
| :---------: | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-----------: |
| [GPT-2](https://huggingface.co/gpt2-xl)(https://huggingface.co/gpt2-xl) | è™½ç„¶è®­ç»ƒä¸ºè‡ªå›å½’è¯­è¨€æ¨¡å‹ï¼Œä½†ä½ å¯ä»¥é€šè¿‡åœ¨è¾“å…¥æ–‡æœ¬æœ«å°¾é™„åŠ â€œTL;DRâ€æ¥ä½¿ GPT-2 ç”Ÿæˆæ‘˜è¦ã€‚|      âŒ       |
| [PEGASUS](https://huggingface.co/google/pegasus-large)(https://huggingface.co/google/pegasus-large) | åœ¨é¢„è®­ç»ƒæ—¶çš„ç›®æ ‡æ˜¯æ¥é¢„æµ‹å¤šå¥å­æ–‡æœ¬ä¸­çš„å±è”½å¥å­ã€‚è¿™ä¸ªé¢„è®­ç»ƒç›®æ ‡æ¯”æ™®é€šè¯­è¨€å»ºæ¨¡æ›´æ¥è¿‘æ–‡æœ¬æ‘˜è¦ï¼Œå¹¶ä¸”åœ¨æµè¡Œçš„åŸºå‡†æµ‹è¯•ä¸­å¾—åˆ†å¾ˆé«˜ã€‚|      âŒ       |
| [T5](https://huggingface.co/t5-base)(https://huggingface.co/t5-base) | é€šç”¨çš„ Transformer æ¶æ„ï¼Œæ‰€æœ‰ä»»åŠ¡éƒ½ä»¥æ–‡æœ¬åˆ°æ–‡æœ¬çš„æ¡†æ¶è¿›è¡Œæè¿°ï¼›ä¾‹å¦‚ï¼Œæ¨¡å‹æ–‡æœ¬æ‘˜è¦çš„è¾“å…¥æ ¼å¼æ˜¯ `summarize: ARTICLE` ã€‚|      âŒ       |
| [mT5](https://huggingface.co/google/mt5-base)(https://huggingface.co/google/mt5-base) | T5 çš„å¤šè¯­è¨€ç‰ˆæœ¬ï¼Œåœ¨å¤šè¯­è¨€ Common Crawl è¯­æ–™åº“ ï¼ˆmC4ï¼‰ ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œæ¶µç›–äº† 101 ç§è¯­è¨€ã€‚|      âœ…       |
| [BART](https://huggingface.co/facebook/bart-base)(https://huggingface.co/facebook/bart-base) | ä¸€ç§æ–°é¢–çš„ Transformer æ¶æ„ï¼Œå…¶ä¸­åŒ…å«ç»è¿‡è®­ç»ƒçš„ç¼–ç å™¨å’Œè§£ç å™¨å †æ ˆï¼Œä»¥é‡å»ºè¢«ç ´åçš„è¾“å…¥ï¼Œç»“åˆäº† BERT å’Œ GPT-2 çš„é¢„è®­ç»ƒæ–¹æ¡ˆã€‚|      âŒ       |
| [mBART-50](https://huggingface.co/facebook/mbart-large-50)(https://huggingface.co/facebook/mbart-large-50) | BART çš„å¤šè¯­è¨€ç‰ˆæœ¬ï¼Œé¢„è®­ç»ƒäº† 50 ç§è¯­è¨€ã€‚|      âœ…       |

ä»æ­¤è¡¨ä¸­å¯ä»¥çœ‹å‡ºï¼Œå¤§å¤šæ•°ç”¨äºæ‘˜è¦çš„ Transformer æ¨¡å‹ï¼ˆä»¥åŠå¤§å¤šæ•° NLP ä»»åŠ¡ï¼‰éƒ½æ˜¯å•è¯­çš„ã€‚å¦‚æœä½ çš„ä»»åŠ¡æ‰€ä½¿ç”¨çš„è¯­è¨€æ˜¯â€œæœ‰å¤§é‡è¯­æ–™åº“â€ï¼ˆå¦‚è‹±è¯­æˆ–å¾·è¯­ï¼‰çš„è¯­è¨€ï¼Œè¿™å¾ˆå¥½ã€‚ä½†å¯¹äºä¸–ç•Œå„åœ°æ­£åœ¨ä½¿ç”¨çš„æ•°åƒç§å…¶ä»–è¯­è¨€ï¼Œåˆ™ä¸ç„¶ã€‚å¹¸è¿çš„æ˜¯ï¼Œæœ‰ä¸€ç±»å¤šè¯­è¨€ Transformer æ¨¡å‹ï¼Œå¦‚ mT5 å’Œ mBARTï¼Œå¯ä»¥è§£å†³é—®é¢˜ã€‚è¿™äº›æ¨¡å‹æ˜¯ä½¿ç”¨è¯­è¨€å»ºæ¨¡è¿›è¡Œé¢„è®­ç»ƒçš„ï¼Œä½†æœ‰ä¸€ç‚¹ä¸åŒï¼šå®ƒä»¬ä¸æ˜¯åœ¨ä¸€ç§è¯­è¨€çš„è¯­æ–™åº“ä¸Šè®­ç»ƒï¼Œè€Œæ˜¯åŒæ—¶åœ¨ 50 å¤šç§è¯­è¨€çš„æ–‡æœ¬ä¸Šè¿›è¡Œè”åˆè®­ç»ƒï¼

æˆ‘ä»¬å°†ä½¿ç”¨ mT5ï¼Œè¿™æ˜¯ä¸€ç§åŸºäº T5 çš„æœ‰è¶£æ¶æ„ï¼Œåœ¨æ–‡æœ¬åˆ°æ–‡æœ¬æ¡†æ¶ä¸­è¿›è¡Œäº†é¢„è®­ç»ƒã€‚åœ¨ T5 ä¸­ï¼Œæ¯ä¸ª NLP ä»»åŠ¡éƒ½æ˜¯ä»¥æç¤ºå‰ç¼€ï¼ˆå¦‚ `summarize:` ï¼‰çš„å½¢å¼å®šä¹‰çš„ï¼Œè¿™ä½¿æ¨¡å‹ä½¿ç”Ÿæˆçš„æ–‡æœ¬é€‚åº”æç¤ºã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œè¿™è®© T5 å˜å¾—éå¸¸é€šç”¨ï¼Œå› ä¸ºä½ å¯ä»¥ç”¨ä¸€ä¸ªæ¨¡å‹è§£å†³å¾ˆå¤šä»»åŠ¡ï¼

![T5æ¶æ„æ‰§è¡Œçš„ä¸åŒä»»åŠ¡](./assets/t5.png "Different tasks performed by the T5 architecture.")

mT5 ä¸ä½¿ç”¨å‰ç¼€ï¼Œä½†å…·æœ‰ T5 çš„å¤§éƒ¨åˆ†åŠŸèƒ½ï¼Œå¹¶ä¸”å…·æœ‰å¤šè¯­è¨€çš„ä¼˜åŠ¿ã€‚ç°åœ¨æˆ‘ä»¬å·²ç»é€‰æ‹©äº†ä¸€ä¸ªæ¨¡å‹ï¼Œæ¥ä¸‹æ¥è®©æˆ‘ä»¬æ¥çœ‹çœ‹å¦‚ä½•å‡†å¤‡æˆ‘ä»¬çš„è®­ç»ƒæ•°æ®ã€‚

<div custom-style="Tip-green">

âœï¸ **è¯•è¯•çœ‹ï¼** å®Œæˆæœ¬èŠ‚åï¼Œå¯ä»¥å°è¯•æ¯”è¾ƒä¸€ä¸‹ mT5 å’Œç”¨ç›¸åŒæŠ€æœ¯å¾®è°ƒè¿‡çš„ mBART çš„æ€§èƒ½ã€‚é™„åŠ çš„æŒ‘æˆ˜ï¼šåªåœ¨è‹±æ–‡è¯„è®ºä¸Šå¾®è°ƒ T5ã€‚å› ä¸º T5 æœ‰ä¸€ä¸ªç‰¹æ®Šçš„å‰ç¼€æç¤ºï¼Œä½ éœ€è¦åœ¨ä¸‹é¢çš„é¢„å¤„ç†æ­¥éª¤ä¸­å°† `summarize:` æ·»åŠ åˆ°è¾“å…¥ä¾‹å­å‰ã€‚

</div>

### é¢„å¤„ç†æ•°æ® 

æˆ‘ä»¬æ¥ä¸‹æ¥çš„ä»»åŠ¡æ˜¯å¯¹æˆ‘ä»¬çš„è¯„è®ºåŠå…¶æ ‡é¢˜è¿›è¡Œ tokenize å’Œ encode é€šå¸¸ï¼Œæˆ‘ä»¬é¦–å…ˆåŠ è½½ä¸é¢„è®­ç»ƒæ¨¡å‹ checkpoint ç›¸å…³çš„ tokenizer æˆ‘ä»¬å°†ä½¿ç”¨ `mt5-small` ä½œä¸ºæˆ‘ä»¬çš„ checkpoint è¿™æ ·æˆ‘ä»¬å°±å¯ä»¥åœ¨åˆç†çš„æ—¶é—´æ¶ˆè€—å†…å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼š

```python
from transformers import AutoTokenizer

model_checkpoint = "google/mt5-small"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
```

<div custom-style="Tip-green">

ğŸ’¡åœ¨ NLP é¡¹ç›®çš„æ—©æœŸé˜¶æ®µï¼Œä¸€ä¸ªå¥½çš„åšæ³•æ˜¯åœ¨å°æ ·æœ¬æ•°æ®ä¸Šè®­ç»ƒä¸€ç±»â€œå°â€æ¨¡å‹ã€‚è¿™ä½¿ä½ å¯ä»¥æ›´å¿«åœ°è°ƒè¯•å’Œè¿­ä»£ç«¯åˆ°ç«¯å·¥ä½œæµã€‚ä¸€æ—¦ä½ å¯¹ç»“æœå……æ»¡ä¿¡å¿ƒï¼Œä½ å§‹ç»ˆå¯ä»¥é€šè¿‡ç®€å•åœ°æ›´æ”¹æ¨¡å‹ checkpoint æ¥åœ¨å¤§è§„æ¨¡æ•°æ®ä¸Šè®­ç»ƒæ¨¡å‹ï¼

</div>

è®©æˆ‘ä»¬åœ¨ä¸€ä¸ªå°ä¾‹å­ä¸Šæµ‹è¯• mT5  tokenizer 

```python
inputs = tokenizer("I loved reading the Hunger Games!")
inputs
```

```python
{'input_ids': [336, 259, 28387, 11807, 287, 62893, 295, 12507, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

åœ¨è¿™é‡Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ç†Ÿæ‚‰çš„ `input_ids` å’Œ `attention_mask` ï¼Œæˆ‘ä»¬åœ¨ç¬¬å››ç« çš„ç¬¬ä¸€æ¬¡å¾®è°ƒå®éªŒä¸­é‡åˆ°è¿‡ã€‚è®©æˆ‘ä»¬ä½¿ç”¨ tokenizer çš„ `convert_ids_to_tokens()` å‡½æ•°è§£ç è¿™äº›è¾“å…¥ IDï¼Œçœ‹çœ‹æˆ‘ä»¬æ­£åœ¨å¤„ç†çš„æ˜¯ä»€ä¹ˆç±»å‹çš„ tokenizer 

```python
tokenizer.convert_ids_to_tokens(inputs.input_ids)
```

```python
['â–I', 'â–', 'loved', 'â–reading', 'â–the', 'â–Hung', 'er', 'â–Games', '</s>']
```

ç‰¹æ®Šçš„ Unicode å­—ç¬¦ `â–` å’Œåºåˆ—ç»“æŸ token `</s>` è¡¨æ˜æˆ‘ä»¬æ­£åœ¨ä½¿ç”¨åŸºäºç¬¬ä¸ƒç« ä¸­è®¨è®ºçš„ Unigram å­è¯åˆ†è¯ç®—æ³•çš„ SentencePiece tokenizer Unigram å¯¹äºå¤šè¯­è¨€è¯­æ–™åº“ç‰¹åˆ«æœ‰ç”¨ï¼Œå› ä¸ºå®ƒè®© SentencePiece ä¸å¿…å…³æ³¨å£éŸ³ã€æ ‡ç‚¹ç¬¦å·ä»¥åŠå¾ˆå¤šè¯­è¨€ï¼ˆå¦‚æ—¥è¯­ï¼‰æ²¡æœ‰ç©ºç™½å­—ç¬¦çš„äº‹å®ï¼Œåªä¸“æ³¨äºæ‰¾å‡ºæœ€ä¼˜çš„åˆ†è¯æ–¹å¼ã€‚

ä¸ºäº†å¯¹æˆ‘ä»¬çš„è¯­æ–™åº“ tokenize æˆ‘ä»¬éœ€è¦å¤„ç†ä¸æ‘˜è¦ç›¸å…³çš„ä¸€ä¸ªç»†å¾®é—®é¢˜ï¼šå› ä¸ºæˆ‘ä»¬çš„ç›®æ ‡æ–‡æœ¬ä¹Ÿæ˜¯æ–‡æœ¬ï¼Œæ‰€ä»¥å®ƒä»¬å¯èƒ½è¶…è¿‡æ¨¡å‹çš„æœ€å¤§ä¸Šä¸‹æ–‡å¤§å°ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬éœ€è¦å¯¹è¯„è®ºåŠå…¶æ ‡é¢˜è¿›è¡Œæˆªæ–­ï¼Œä»¥ç¡®ä¿æˆ‘ä»¬ä¸ä¼šå°†è¿‡é•¿çš„è¾“å…¥ä¼ é€’ç»™æˆ‘ä»¬çš„æ¨¡å‹ã€‚Transformers ä¸­çš„ tokenizer æä¾›äº†ä¸€ä¸ªç»å¦™çš„ `text_target` å‚æ•°ï¼Œå…è®¸ä½ å°†ç›®æ ‡æ–‡æœ¬ä¸è¾“å…¥å¹¶è¡Œ tokenize ä»¥ä¸‹æ˜¯å¦‚ä½•ä¸º mT5 å¤„ç†è¾“å…¥å’Œç›®æ ‡æ–‡æœ¬çš„ç¤ºä¾‹ï¼š

```python
max_input_length = 512
max_target_length = 30

def preprocess_function(examples):
    model_inputs = tokenizer(
        examples["review_body"],
        max_length=max_input_length,
        truncation=True,
    )
    labels = tokenizer(
        examples["review_title"], max_length=max_target_length, truncation=True
    )
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs
```

è®©æˆ‘ä»¬é€æ­¥è§£æè¿™æ®µä»£ç ï¼Œç†è§£å‘ç”Ÿäº†ä»€ä¹ˆã€‚æˆ‘ä»¬é¦–å…ˆå®šä¹‰äº† `max_input_length` å’Œ `max_target_length` çš„å€¼ï¼Œè¿™äº›å€¼è®¾å®šäº†æˆ‘ä»¬çš„è¯„è®ºå’Œæ ‡é¢˜çš„æœ€å¤§é•¿åº¦ã€‚ç”±äºè¯„è®ºä¸»ä½“é€šå¸¸æ¯”æ ‡é¢˜å¤§å¾—å¤šï¼Œæˆ‘ä»¬ç›¸åº”åœ°è°ƒæ•´äº†è¿™äº›å€¼ã€‚

ä½¿ç”¨ `preprocess_function()` ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æˆ‘ä»¬åœ¨è¿™é—¨è¯¾ç¨‹ä¸­å¹¿æ³›ä½¿ç”¨çš„æ–¹ä¾¿çš„ `Dataset.map()` å‡½æ•°ï¼Œè½»æ¾åœ°å¯¹æ•´ä¸ªè¯­æ–™åº“ tokenize 

```python
tokenized_datasets = books_dataset.map(preprocess_function, batched=True)
```

æ—¢ç„¶è¯­æ–™åº“å·²ç»é¢„å¤„ç†å®Œæ¯•ï¼Œæˆ‘ä»¬æ¥çœ‹çœ‹ä¸€äº›å¸¸ç”¨çš„æ‘˜è¦æŒ‡æ ‡ã€‚æ­£å¦‚æˆ‘ä»¬åœ¨ä¸‹é¢å³å°†çœ‹åˆ°çš„ï¼Œåœ¨è¡¡é‡æœºå™¨ç”Ÿæˆçš„æ–‡æœ¬çš„è´¨é‡æ–¹é¢æ²¡æœ‰çµä¸¹å¦™è¯ã€‚

<div custom-style="Tip-green">

ğŸ’¡ ä½ å¯èƒ½å·²ç»æ³¨æ„åˆ°æˆ‘ä»¬åœ¨ä¸Šé¢çš„ `Dataset.map()` å‡½æ•°ä¸­ä½¿ç”¨äº† `batched=True` ã€‚è¿™å°†ä»¥ 1000ï¼ˆé»˜è®¤å€¼ï¼‰çš„æ‰¹æ¬¡ç¼–ç ä¾‹å­ï¼Œå¹¶è®©ä½ å¯ä»¥åˆ©ç”¨ Transformers ä¸­å¿«é€Ÿ tokenizer çš„å¤šçº¿ç¨‹åŠŸèƒ½ã€‚åœ¨å¯èƒ½çš„æƒ…å†µä¸‹ï¼Œå°è¯•ä½¿ç”¨ `batched=True` æ¥åŠ é€Ÿä½ çš„é¢„å¤„ç†ï¼

</div>

### æ–‡æœ¬æ‘˜è¦çš„è¯„ä¼°æŒ‡æ ‡ 

ä¸æˆ‘ä»¬åœ¨æœ¬è¯¾ç¨‹ä¸­æ¶µç›–çš„å¤§å¤šæ•°å…¶ä»–ä»»åŠ¡ç›¸æ¯”ï¼Œè¡¡é‡æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ï¼ˆå¦‚æ‘˜è¦æˆ–ç¿»è¯‘ï¼‰çš„æ€§èƒ½å¹¶ä¸é‚£ä¹ˆç®€å•ã€‚ä¾‹å¦‚ï¼Œå¯¹äºâ€œæˆ‘å–œæ¬¢é˜…è¯»é¥¥é¥¿æ¸¸æˆâ€è¿™æ ·çš„è¯„è®ºï¼Œæœ‰å¤šä¸ªæœ‰æ•ˆæ‘˜è¦ï¼Œä¾‹å¦‚â€œæˆ‘å–œæ¬¢é¥¥é¥¿æ¸¸æˆâ€æˆ–â€œé¥¥é¥¿æ¸¸æˆæ˜¯ä¸€æœ¬å¥½ä¹¦â€ã€‚æ˜¾ç„¶ï¼Œåœ¨ç”Ÿæˆçš„æ‘˜è¦å’Œæ ‡ç­¾ä¹‹é—´è¿›è¡ŒæŸç§ç²¾ç¡®åŒ¹é…å¹¶ä¸æ˜¯ä¸€ä¸ªå¥½çš„è§£å†³æ–¹æ¡ˆâ€”â€”å³ä½¿æ˜¯äººç±»åœ¨è¿™æ ·çš„è¯„ä¼°æŒ‡æ ‡ä¸‹ä¹Ÿä¼šè¡¨ç°ä¸ä½³ï¼Œå› ä¸ºæˆ‘ä»¬éƒ½æœ‰è‡ªå·±çš„å†™ä½œé£æ ¼ã€‚

æ€»è€Œè¨€ä¹‹ï¼Œæœ€å¸¸ç”¨çš„æŒ‡æ ‡ä¹‹ä¸€æ˜¯[ROUGE åˆ†æ•°](https://en.wikipedia.org/wiki/ROUGE_(metric)(https://en.wikipedia.org/wiki/ROUGE_(metric))ï¼ˆRecall-Oriented Understudy for Gisting Evaluation çš„ç¼©å†™ï¼‰ã€‚è¯¥æŒ‡æ ‡èƒŒåçš„åŸºæœ¬æ€æƒ³æ˜¯å°†ç”Ÿæˆçš„æ‘˜è¦ä¸ä¸€ç»„é€šå¸¸ç”±äººç±»åˆ›å»ºçš„å‚è€ƒæ‘˜è¦è¿›è¡Œæ¯”è¾ƒã€‚æ›´å…·ä½“åœ°è¯´ï¼Œå‡è®¾æˆ‘ä»¬è¦æ¯”è¾ƒä»¥ä¸‹ä¸¤ä¸ªæ‘˜è¦ï¼š

```python
generated_summary = "I absolutely loved reading the Hunger Games"
reference_summary = "I loved reading the Hunger Games"
```
æ¯”è¾ƒå®ƒä»¬çš„ä¸€ç§æ–¹æ³•æ˜¯è®¡ç®—é‡å å•è¯çš„æ•°é‡ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ä¸º 6ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•æœ‰äº›ç²—ç³™ï¼Œå› æ­¤ ROUGE æ˜¯åŸºäºè®¡ç®—è®¡ç®—é‡å éƒ¨åˆ†çš„ `ç²¾ç¡®åº¦(Precision)` å’Œ `å¬å›ç‡(Recall)` åˆ†æ•°ã€‚

<div custom-style="Tip-green">

ğŸ™‹ å¦‚æœè¿™æ˜¯ä½ ç¬¬ä¸€æ¬¡å¬è¯´ç²¾ç¡®åº¦ï¼ˆPrecisionï¼‰å’Œå¬å›ç‡ï¼ˆRecallï¼‰ï¼Œè¯·ä¸è¦æ‹…å¿ƒâ€”â€”æˆ‘ä»¬å°†ä¸€èµ·é€šè¿‡ä¸€äº›æ¸…æ™°çš„ç¤ºä¾‹æ¥ç†è§£å®ƒä»¬ã€‚è¿™äº›æŒ‡æ ‡é€šå¸¸åœ¨åˆ†ç±»ä»»åŠ¡ä¸­é‡åˆ°ï¼Œæ‰€ä»¥å¦‚æœä½ æƒ³äº†è§£åœ¨åˆ†ç±»ä»»åŠ¡ä¸­ç²¾ç¡®åº¦ï¼ˆPrecisionï¼‰å’Œå¬å›ç‡ï¼ˆRecallï¼‰æ˜¯å¦‚ä½•å®šä¹‰çš„ï¼Œæˆ‘ä»¬å»ºè®®ä½ æŸ¥çœ‹ `scikit-learn` çš„ [æŒ‡å—](https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html)(https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html) ã€‚

</div>

å¯¹äº ROUGEï¼Œå¬å›ç‡è¡¡é‡çš„æ˜¯å‚è€ƒæ‘˜è¦ä¸­è¢«ç”Ÿæˆæ‘˜è¦æ•è·çš„å†…å®¹é‡ã€‚å¦‚æœæˆ‘ä»¬åªæ˜¯æ¯”è¾ƒå•è¯ï¼Œå¬å›ç‡å¯ä»¥æŒ‰ç…§ä»¥ä¸‹å…¬å¼è®¡ç®—ï¼š

$$ \mathrm{å¬å›ç‡} = \frac{\mathrm{é‡å è¯çš„æ•°é‡}}{\mathrm{å‚è€ƒæ‘˜è¦ä¸­çš„æ€»è¯æ•°}} $$

å¯¹äºæˆ‘ä»¬ä¸Šé¢çš„ç®€å•ä¾‹å­ï¼Œè¿™ä¸ªå…¬å¼ç»™å‡ºäº† 6/6 = 1 çš„å®Œç¾å¬å›ç‡ï¼›å³ï¼Œå‚è€ƒæ‘˜è¦ä¸­çš„æ‰€æœ‰å•è¯éƒ½å·²ç”±æ¨¡å‹ç”Ÿæˆã€‚è¿™å¬èµ·æ¥å¯èƒ½å¾ˆæ£’ï¼Œä½†æƒ³è±¡ä¸€ä¸‹ï¼Œå¦‚æœæˆ‘ä»¬ç”Ÿæˆçš„æ‘˜è¦æ˜¯â€œæˆ‘çœŸçš„å¾ˆå–œæ¬¢æ•´æ™šé˜…è¯»é¥¥é¥¿æ¸¸æˆâ€ã€‚è¿™ä¹Ÿå°†æœ‰å®Œç¾çš„ recallï¼Œä½†å¯ä»¥è¯´æ˜¯ä¸€ä¸ªæ›´ç³Ÿç³•çš„æ€»ç»“ï¼Œå› ä¸ºå®ƒå¾ˆå†—é•¿ã€‚ä¸ºäº†é€‚åº”äºè¿™äº›åœºæ™¯ï¼Œæˆ‘ä»¬è¿˜è®¡ç®—äº†ç²¾ç¡®åº¦ï¼Œå®ƒåœ¨ ROUGE ä¸Šä¸‹æ–‡ä¸­è¡¡é‡ç”Ÿæˆçš„æ‘˜è¦ä¸­æœ‰å¤šå°‘æ˜¯ç›¸å…³çš„ï¼š

$$ \mathrm{ç²¾ç¡®åº¦} = \frac{\mathrm{é‡å è¯çš„æ•°é‡}}{\mathrm{ç”Ÿæˆæ‘˜è¦ä¸­çš„æ€»è¯æ•°}} $$

æˆ‘ä»¬çš„è¯¦ç»†æ‘˜è¦ä½¿ç”¨è¿™ç§è®¡ç®—æ–¹æ³•ä¼šå¾—åˆ° 6/10 = 0.6 çš„ç²¾ç¡®åº¦ï¼Œè¿™æ¯”æˆ‘ä»¬è¾ƒçŸ­çš„æ‘˜è¦è·å¾—çš„ 6/7 = 0.86 çš„ç²¾ç¡®åº¦è¦å·®å¾—å¤šã€‚åœ¨å®è·µä¸­ï¼Œé€šå¸¸è®¡ç®—ç²¾åº¦å’Œå¬å›ç‡ï¼Œç„¶åå¾—åˆ° F1 å¾—åˆ†ï¼ˆç²¾ç¡®åº¦å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡æ•°ï¼‰ã€‚æˆ‘ä»¬å¯ä»¥å¾ˆå®¹æ˜“åœ°åœ¨Datasets ä¸­é€šè¿‡å®‰è£… `rouge_score` åŒ…æ¥å®ç°è¿™ä¸€ç‚¹ï¼š

```python
!pip install rouge_score
```

ç„¶åæŒ‰å¦‚ä¸‹æ–¹å¼åŠ è½½ ROUGE æŒ‡æ ‡ï¼š

```python
import evaluate

rouge_score = evaluate.load("rouge")
```

æ¥ç€æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `rouge_score.compute()` å‡½æ•°æ¥ä¸€æ¬¡æ€§è®¡ç®—æ‰€æœ‰çš„æŒ‡æ ‡ï¼š

```python
scores = rouge_score.compute(
    predictions=[generated_summary], references=[reference_summary]
)
scores
```

```python
{'rouge1': AggregateScore(low=Score(precision=0.86, recall=1.0, fmeasure=0.92), mid=Score(precision=0.86, recall=1.0, fmeasure=0.92), high=Score(precision=0.86, recall=1.0, fmeasure=0.92)),
 'rouge2': AggregateScore(low=Score(precision=0.67, recall=0.8, fmeasure=0.73), mid=Score(precision=0.67, recall=0.8, fmeasure=0.73), high=Score(precision=0.67, recall=0.8, fmeasure=0.73)),
 'rougeL': AggregateScore(low=Score(precision=0.86, recall=1.0, fmeasure=0.92), mid=Score(precision=0.86, recall=1.0, fmeasure=0.92), high=Score(precision=0.86, recall=1.0, fmeasure=0.92)),
 'rougeLsum': AggregateScore(low=Score(precision=0.86, recall=1.0, fmeasure=0.92), mid=Score(precision=0.86, recall=1.0, fmeasure=0.92), high=Score(precision=0.86, recall=1.0, fmeasure=0.92))}
```

å“‡ï¼Œè¿™ä¸ªè¾“å‡ºä¸­åŒ…å«äº†å¾ˆå¤šä¿¡æ¯â€”â€”å®ƒä»¬éƒ½ä»£è¡¨ä»€ä¹ˆæ„æ€å‘¢ï¼Ÿé¦–å…ˆï¼ŒDatasets å®é™…ä¸Šè®¡ç®—äº†ç²¾åº¦ã€å¬å›ç‡å’Œ F1 åˆ†æ•°çš„ç½®ä¿¡åŒºé—´ï¼›ä¹Ÿäº›å°±æ˜¯ä½ åœ¨è¿™é‡Œçœ‹åˆ°çš„ `low` ã€ `mid` å’Œ `high` å±æ€§ã€‚æ­¤å¤–ï¼ŒDatasets è¿˜è®¡ç®—äº†åŸºäºåœ¨æ¯”è¾ƒç”Ÿæˆæ‘˜è¦å’Œå‚è€ƒæ‘˜è¦æ—¶çš„é‡‡ç”¨ä¸åŒæ–‡æœ¬ç²’åº¦çš„å„ç§ ROUGE å¾—åˆ†ã€‚ `rouge1` æµ‹é‡çš„æ˜¯ç”Ÿæˆæ‘˜è¦å’Œå‚è€ƒæ‘˜è¦ä¸­å•ä¸ªå•è¯çš„é‡å ç¨‹åº¦ã€‚
ä¸ºäº†éªŒè¯è¿™ä¸€ç‚¹ï¼Œè®©æˆ‘ä»¬æå–å‡ºæˆ‘ä»¬å¾—åˆ†çš„ `mid` å€¼ï¼š

```python
scores["rouge1"].mid
```

```python
Score(precision=0.86, recall=1.0, fmeasure=0.92)
```
å¤ªå¥½äº†ï¼Œç²¾ç¡®åº¦å’Œå¬å›ç‡çš„æ•°å­—éƒ½å¯¹ä¸Šäº†ï¼é‚£ä¹ˆå…¶ä»–çš„ ROUGE å¾—åˆ†æ˜¯æ€ä¹ˆå›äº‹å‘¢ï¼Ÿ `rouge2` åº¦é‡äº†äºŒå…ƒè¯ç»„ï¼ˆè€ƒè™‘å•è¯å¯¹çš„é‡å ï¼‰ä¹‹é—´çš„é‡å ï¼Œè€Œ `rougeL` å’Œ `rougeLsum` é€šè¿‡å¯»æ‰¾ç”Ÿæˆçš„æ‘˜è¦å’Œå‚è€ƒæ‘˜è¦ä¸­æœ€é•¿çš„å…¬å…±å­ä¸²æ¥åº¦é‡å•è¯çš„æœ€é•¿åŒ¹é…åºåˆ—ã€‚ `rougeLsum` ä¸­çš„â€œsumâ€æŒ‡çš„æ˜¯è¯¥æŒ‡æ ‡æ˜¯åœ¨æ•´ä¸ªæ‘˜è¦ä¸Šè®¡ç®—çš„ï¼Œè€Œ `rougeL` æ˜¯æŒ‡åœ¨å„ä¸ªå¥å­ä¸Šè®¡ç®—çš„å¹³å‡å€¼ã€‚

<div custom-style="Tip-green">

âœï¸ **è¯•è¯•çœ‹ï¼** åˆ›å»ºä½ è‡ªå·±çš„ç”Ÿæˆæ‘˜è¦å’Œå‚è€ƒæ‘˜è¦çš„ä¾‹å­ï¼Œçœ‹çœ‹å¾—å‡ºçš„ ROUGE åˆ†æ•°æ˜¯å¦ä¸åŸºäºç²¾ç¡®åº¦å’Œå¬å›ç‡å…¬å¼çš„æ‰‹åŠ¨è®¡ç®—ä¸€è‡´ã€‚é™„åŠ çš„æŒ‘æˆ˜ï¼šå°†æ–‡æœ¬åˆ‡åˆ†ä¸ºäºŒå…ƒè¯ç»„ï¼Œå¹¶ä¸ `rouge2` æŒ‡æ ‡çš„ç²¾ç¡®åº¦å’Œå¬å›ç‡è¿›è¡Œå¯¹æ¯”ã€‚

</div>

æˆ‘ä»¬å°†ä½¿ç”¨è¿™äº› ROUGE åˆ†æ•°æ¥è·Ÿè¸ªæˆ‘ä»¬æ¨¡å‹çš„æ€§èƒ½ï¼Œä½†åœ¨æ­¤ä¹‹å‰ï¼Œè®©æˆ‘ä»¬åšæ¯ä¸ªä¼˜ç§€çš„ NLP ä»ä¸šè€…éƒ½åº”è¯¥åšçš„äº‹æƒ…ï¼šåˆ›å»ºä¸€ä¸ªå¼ºå¤§è€Œç®€å•çš„ baselineï¼

#### åˆ›å»ºå¼ºå¤§çš„ baseline 

å¯¹äºæ–‡æœ¬æ‘˜è¦ï¼Œä¸€ä¸ªå¸¸è§çš„åŸºçº¿æ˜¯ç®€å•åœ°å–æ–‡ç« çš„å‰ä¸‰å¥è¯ï¼Œé€šå¸¸ç§°ä¸º `lead-3` åŸºçº¿ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨å¥å·ï¼ˆè‹±æ–‡ä½¿ç”¨ï¼ï¼‰æ¥è·Ÿè¸ªå¥å­è¾¹ç•Œï¼Œä½†è¿™åœ¨â€œU.S.â€ or â€œU.N.â€ä¹‹ç±»çš„é¦–å­—æ¯ç¼©ç•¥è¯ä¸Šä¼šå¤±è´¥ã€‚æ‰€ä»¥æˆ‘ä»¬å°†ä½¿ç”¨ `nltk` åº“ï¼Œå®ƒåŒ…å«ä¸€ä¸ªæ›´å¥½çš„ç®—æ³•æ¥å¤„ç†è¿™äº›æƒ…å†µã€‚ä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹æ–¹å¼å®‰è£…è¯¥åŒ…ï¼š

```python
!pip install nltk
```

ç„¶åä¸‹è½½æ ‡ç‚¹è§„åˆ™ï¼š

```python
import nltk

nltk.download("punkt")
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä» `nltk` å¯¼å…¥å¥å­çš„ tokenizer å¹¶åˆ›å»ºä¸€ä¸ªæå–è¯„è®ºä¸­çš„å‰ä¸‰ä¸ªå¥å­ç®€å•çš„å‡½æ•°ã€‚æ–‡æœ¬æ‘˜è¦çš„çº¦å®šæ˜¯ç”¨æ¢è¡Œç¬¦åˆ†éš”æ¯ä¸ªæ‘˜è¦ï¼Œå› æ­¤æˆ‘ä»¬ä¹Ÿè¿™æ ·å¤„ç†ï¼Œå¹¶åœ¨è®­ç»ƒé›†çš„ç¤ºä¾‹ä¸Šå¯¹å…¶è¿›è¡Œæµ‹è¯•ï¼š

```python
from nltk.tokenize import sent_tokenize

def three_sentence_summary(text):
    return "\n".join(sent_tokenize(text)[:3])

print(three_sentence_summary(books_dataset["train"][1]["review_body"]))
```

```python
'I grew up reading Koontz, and years ago, I stopped,convinced i had "outgrown" him.'
'Still,when a friend was looking for something suspenseful too read, I suggested Koontz.'
'She found Strangers.'
```

è¿™ä¼¼ä¹æœ‰æ•ˆï¼Œæ‰€ä»¥è®©æˆ‘ä»¬ç°åœ¨å®ç°ä¸€ä¸ªå‡½æ•°ï¼Œä»æ•°æ®é›†ä¸­æå–è¿™äº›â€œæ‘˜è¦â€å¹¶è®¡ç®— baseline çš„ ROUGE åˆ†æ•°ï¼š

```python
def evaluate_baseline(dataset, metric):
    summaries = [three_sentence_summary(text) for text in dataset["review_body"]]
    return metric.compute(predictions=summaries, references=dataset["review_title"])
```

ç„¶åæˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¿™ä¸ªå‡½æ•°æ¥è®¡ç®—éªŒè¯é›†ä¸Šçš„ ROUGE åˆ†æ•°ï¼Œå¹¶ä½¿ç”¨ Pandas å¯¹å®ƒä»¬è¿›è¡Œä¸€äº›ç¾åŒ–ï¼š

```python
import pandas as pd

score = evaluate_baseline(books_dataset["validation"], rouge_score)
rouge_names = ["rouge1", "rouge2", "rougeL", "rougeLsum"]
rouge_dict = dict((rn, round(score[rn].mid.fmeasure * 100, 2)) for rn in rouge_names)
rouge_dict
```

```python
{'rouge1': 16.74, 'rouge2': 8.83, 'rougeL': 15.6, 'rougeLsum': 15.96}
```

æˆ‘ä»¬å¯ä»¥çœ‹åˆ° `rouge2` çš„åˆ†æ•°æ˜æ˜¾ä½äºå…¶ä»–ï¼›è¿™å¯èƒ½åæ˜ äº†è¿™æ ·ä¸€ä¸ªäº‹å®ï¼Œå³è¯„è®ºæ ‡é¢˜é€šå¸¸å¾ˆç®€æ´ï¼Œå› æ­¤ `lead-3` baseline è¿‡äºå†—é•¿ã€‚ç°åœ¨æˆ‘ä»¬æœ‰äº†ä¸€ä¸ªå¾ˆå¥½çš„åŸºå‡†ï¼Œè®©æˆ‘ä»¬å°†æ³¨æ„åŠ›è½¬å‘å¾®è°ƒ mT5ï¼

{#if fw === 'pt'}

### ä½¿ç”¨ `Trainer` API å¾®è°ƒ mT5  

å¾®è°ƒæ¨¡å‹ä»¥è¿›è¡Œæå–æ‘˜è¦ä¸æˆ‘ä»¬åœ¨æœ¬ç« ä¸­ä»‹ç»çš„å…¶ä»–ä»»åŠ¡éå¸¸ç›¸ä¼¼ã€‚æˆ‘ä»¬éœ€è¦åšçš„ç¬¬ä¸€ä»¶äº‹æ˜¯ä» `mt5-small` checkpoint åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ã€‚ç”±äºæ‘˜è¦æå–æ˜¯ä¸€ä¸ªåºåˆ—åˆ°åºåˆ—çš„ä»»åŠ¡ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ AutoModelForSeq2SeqLM ç±»åŠ è½½æ¨¡å‹ï¼Œè¯¥ç±»ä¼šè‡ªåŠ¨ä¸‹è½½å¹¶ç¼“å­˜æ¨¡å‹æƒé‡ï¼š

```python
#####Pytorch}
from transformers import AutoModelForSeq2SeqLM

model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)####end
```

{:else}

### ä½¿ç”¨ `Keras` API å¾®è°ƒ mT5  

å¾®è°ƒæ¨¡å‹ä»¥è¿›è¡Œæå–æ‘˜è¦ä¸æˆ‘ä»¬åœ¨æœ¬ç« ä¸­ä»‹ç»çš„å…¶ä»–ä»»åŠ¡éå¸¸ç›¸ä¼¼ã€‚æˆ‘ä»¬éœ€è¦åšçš„ç¬¬ä¸€ä»¶äº‹æ˜¯ä» `mt5-small` checkpoint åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ã€‚ç”±äºæ‘˜è¦æå–æ˜¯ä¸€ä¸ªåºåˆ—åˆ°åºåˆ—çš„ä»»åŠ¡ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `TFAutoModelForSeq2SeqLM` ç±»åŠ è½½æ¨¡å‹ï¼Œè¯¥ç±»ä¼šè‡ªåŠ¨ä¸‹è½½å¹¶ç¼“å­˜æ¨¡å‹æƒé‡ï¼š

```python
#####TensorFlow}
from transformers import TFAutoModelForSeq2SeqLM

model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)####end
```

{/if}

<div custom-style="Tip-green">

ğŸ’¡ å¦‚æœä½ æƒ³çŸ¥é“ä¸ºä»€ä¹ˆåœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­æ²¡æœ‰çœ‹åˆ°ä»»ä½•å…³äºå¾®è°ƒæ¨¡å‹çš„è­¦å‘Šï¼Œé‚£æ˜¯å› ä¸ºå¯¹äºåºåˆ—åˆ°åºåˆ—çš„ä»»åŠ¡ï¼Œæˆ‘ä»¬ä¿ç•™äº†ç½‘ç»œçš„æ‰€æœ‰æƒé‡ã€‚ä¸æ­¤ç›¸æ¯”ï¼Œåœ¨ç¬¬å››ç« ä¸­çš„æ–‡æœ¬åˆ†ç±»æ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬ç”¨ä¸€ä¸ªéšæœºåˆå§‹åŒ–çš„ç½‘ç»œæ›¿æ¢äº†é¢„è®­ç»ƒæ¨¡å‹çš„å¤´éƒ¨ã€‚

</div>

æˆ‘ä»¬éœ€è¦åšçš„ä¸‹ä¸€ä»¶äº‹æ˜¯ç™»å½• Hugging Face Hubã€‚å¦‚æœä½ åœ¨ notebook ä¸­è¿è¡Œæ­¤ä»£ç ï¼Œåˆ™å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å®ç”¨ç¨‹åºå‡½æ•°è¿›è¡Œæ­¤æ“ä½œï¼š

```python
from huggingface_hub import notebook_login

notebook_login()
```

è¿™å°†æ˜¾ç¤ºä¸€ä¸ªå°å·¥å…·ï¼Œä½ å¯ä»¥åœ¨å…¶ä¸­è¾“å…¥ä½ çš„å‡­æ®ã€‚æˆ–è€…ï¼Œä½ å¯ä»¥åœ¨ä½ çš„ç»ˆç«¯è¿è¡Œè¿™æ¡å‘½ä»¤æ¥ç™»é™†ï¼š

```python
huggingface-cli login
```

{#if fw === 'pt'}

æˆ‘ä»¬éœ€è¦ç”Ÿæˆæ‘˜è¦ä»¥ä¾¿åœ¨è®­ç»ƒæœŸé—´è®¡ç®— ROUGE åˆ†æ•°ã€‚å¹¸è¿çš„æ˜¯ï¼ŒTransformers æä¾›äº†ä¸“ç”¨çš„ `Seq2SeqTrainingArguments` å’Œ `Seq2SeqTrainer` ç±»ï¼Œå¯ä»¥è‡ªåŠ¨ä¸ºæˆ‘ä»¬å®Œæˆè¿™é¡¹å·¥ä½œï¼ä¸ºäº†äº†è§£å®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Œè®©æˆ‘ä»¬é¦–å…ˆä¸ºæˆ‘ä»¬çš„å®éªŒå®šä¹‰è¶…å‚æ•°å’Œå…¶ä»–å‚æ•°ï¼š

```python
#####Pytorch}
from transformers import Seq2SeqTrainingArguments

batch_size = 8
num_train_epochs = 8
## æ¯ä¸ªè®­ç»ƒå‘¨æœŸéƒ½è¾“å‡ºè®­ç»ƒæŸå¤±
logging_steps = len(tokenized_datasets["train"]) // batch_size
model_name = model_checkpoint.split("/")[-1]

args = Seq2SeqTrainingArguments(
    output_dir=f"{model_name}-finetuned-amazon-en-es",
    evaluation_strategy="epoch",
    learning_rate=5.6e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=num_train_epochs,
    predict_with_generate=True,
    logging_steps=logging_steps,
    push_to_hub=True,
)####end
```

åœ¨è¿™é‡Œï¼Œ `predict_with_generate` å‚æ•°å·²ç»è¢«è®¾ç½®ä¸º True è¡¨ç¤ºæˆ‘ä»¬åº”è¯¥åœ¨è¯„ä¼°æœŸé—´ç”Ÿæˆæ‘˜è¦ï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥è®¡ç®—æ¯ä¸ªæ—¶æœŸçš„ ROUGE åˆ†æ•°ã€‚æ­£å¦‚åœ¨ç¬¬äºŒç« ç§æ‰€è®¨è®ºçš„é‚£æ ·ï¼Œè§£ç å™¨é€šè¿‡é€ä¸ªé¢„æµ‹å•è¯æ¥æ‰§è¡Œæ¨ç†ï¼Œè¿™æ˜¯ç”±æ¨¡å‹çš„ `generate()` æ–¹æ³•å®ç°çš„ã€‚è®¾ç½® `predict_with_generate=True` å‘Šè¯‰ `Seq2SeqTrainer` åœ¨è¯„ä¼°æ—¶éœ€è¦ä½¿ç”¨è¯¥æ–¹æ³•ã€‚æˆ‘ä»¬è¿˜è°ƒæ•´äº†ä¸€äº›é»˜è®¤çš„è¶…å‚æ•°ï¼Œä¾‹å¦‚å­¦ä¹ ç‡ã€epochs æ•°å’Œæƒé‡è¡°å‡ï¼Œå¹¶ä¸”æˆ‘ä»¬è®¾ç½®äº† `save_total_limit` é€‰é¡¹ï¼Œè®­ç»ƒæœŸé—´æœ€å¤šåªä¿å­˜ 3 ä¸ª checkpoint çš„é€‰é¡¹â€”â€”è¿™æ˜¯å› ä¸ºå³ä½¿æ˜¯ mT5 çš„â€œsmallâ€ç‰ˆæœ¬ä¹Ÿä½¿ç”¨å¤§çº¦ 1 GB çš„ç¡¬ç›˜ç©ºé—´ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡é™åˆ¶æˆ‘ä»¬ä¿å­˜çš„å‰¯æœ¬æ•°é‡æ¥èŠ‚çœä¸€ç‚¹ç©ºé—´ã€‚
`push_to_hub=True` å‚æ•°å°†å…è®¸æˆ‘ä»¬åœ¨è®­ç»ƒåå°†æ¨¡å‹æ¨é€åˆ° Hubï¼›ä½ å¯ä»¥åœ¨ç”± `output_dir` å®šä¹‰çš„ä½ç½®ä¸‹çš„ç”¨æˆ·é…ç½®æ–‡ä»¶ä¸­æ‰¾åˆ°å¯¹åº”çš„ä»“åº“ã€‚è¯·æ³¨æ„ï¼Œä½ å¯ä»¥ä½¿ç”¨ `hub_model_id` å‚æ•°æŒ‡å®šè¦æ¨é€åˆ°çš„ä»“åº“çš„åç§°ï¼ˆç‰¹åˆ«æ˜¯å½“ä½ æƒ³è¦æ¨é€åˆ°ç»„ç»‡æ—¶ï¼Œå°±å¿…é¡»ä½¿ç”¨æ­¤å‚æ•°ï¼‰ã€‚ä¾‹å¦‚ï¼Œå½“æˆ‘ä»¬å°†æ¨¡å‹æ¨é€åˆ° [`huggingface-course` ç»„ç»‡](https://huggingface.co/huggingface-course)(https://huggingface.co/huggingface-course) æ—¶ï¼Œæˆ‘ä»¬åœ¨ `Seq2SeqTrainingArguments` ä¸­æ·»åŠ äº† `hub_model_id="huggingface-course/mt5-finetuned-amazon-en-es"` ã€‚

æˆ‘ä»¬éœ€è¦åšçš„ä¸‹ä¸€ä»¶äº‹æ˜¯ä¸º Trainer æä¾›ä¸€ä¸ª `compute_metrics()` å‡½æ•°ï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥åœ¨è®­ç»ƒæœŸé—´è¯„ä¼°æˆ‘ä»¬çš„æ¨¡å‹ã€‚å¯¹äºæ‘˜è¦æ¨¡å‹æ¥è¯´ï¼Œè¿™æ¯”ç®€å•åœ°åœ¨æ¨¡å‹çš„é¢„æµ‹ä¸Šè°ƒç”¨ `rouge_score.compute()` æ›´å¤æ‚ä¸€äº›ï¼Œå› ä¸ºæˆ‘ä»¬éœ€è¦å°†è¾“å‡ºå’Œæ ‡ç­¾è§£ç ä¸ºæ–‡æœ¬ï¼Œç„¶åæ‰èƒ½è®¡ç®— ROUGE åˆ†æ•°ã€‚ä¸‹é¢çš„å‡½æ•°æ­£æ˜¯è¿™æ ·åšçš„ï¼Œé™¤æ­¤ä¹‹å¤–è¿˜ä½¿ç”¨äº† `nltk` ä¸­çš„ `sent_tokenize()` å‡½æ•°å°†æ‘˜è¦å¥å­ç”¨æ¢è¡Œç¬¦åˆ†éš”å¼€ï¼š

```python
#####Pytorch}
import numpy as np

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    # å°†ç”Ÿæˆçš„æ‘˜è¦è§£ç ä¸ºæ–‡æœ¬
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    # æ›¿æ¢æ ‡ç­¾ä¸­çš„-100,å› ä¸ºæˆ‘ä»¬æ— æ³•è§£ç å®ƒä»¬
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    # å°†å‚è€ƒæ‘˜è¦è§£ç ä¸ºæ–‡æœ¬
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    # ROUGEæœŸæœ›æ¯ä¸ªå¥å­åéƒ½æœ‰ä¸€ä¸ªæ¢è¡Œç¬¦
    decoded_preds = ["\n".join(sent_tokenize(pred.strip())) for pred in decoded_preds]
    decoded_labels = ["\n".join(sent_tokenize(label.strip())) for label in decoded_labels]
    # è®¡ç®—ROUGEåˆ†æ•°
    result = rouge_score.compute(
        predictions=decoded_preds, references=decoded_labels, use_stemmer=True
    )
    # è®¡ç®—ROUGEåˆ†æ•°
    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}
    return {k: round(v, 4) for k, v in result.items()}####end
```

{/if}

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬éœ€è¦ä¸ºæˆ‘ä»¬çš„åºåˆ—åˆ°åºåˆ—ä»»åŠ¡å®šä¹‰ä¸€ä¸ªæ•°æ®æ•´ç†å™¨ï¼ˆdata collatorï¼‰ã€‚ç”±äº mT5 æ˜¯ä¸€ä¸ªç¼–ç å™¨-è§£ç å™¨çš„ Transformer æ¨¡å‹ï¼Œå‡†å¤‡æˆ‘ä»¬æˆæ‰¹æ¬¡çš„æ•°æ®æ—¶æœ‰ä¸€ç‚¹éœ€è¦æ³¨æ„ï¼Œé‚£å°±æ˜¯åœ¨è§£ç æœŸé—´ï¼Œæˆ‘ä»¬éœ€è¦å°†æ ‡ç­¾å‘å³ç§»åŠ¨ä¸€ä¸ªå•ä½ã€‚è¿™æ˜¯ä¸ºäº†ç¡®ä¿è§£ç å™¨åªçœ‹åˆ°ä¹‹å‰çš„çœŸå®çš„æ ‡ç­¾ï¼Œè€Œä¸æ˜¯å½“å‰æˆ–æœªæ¥çš„æ ‡ç­¾ï¼Œè¿™æ ·æ¨¡å‹å°±èƒ½é¿å…å®¹æ˜“è®°ä½æ ‡ç­¾ã€‚è¿™ä¸åœ¨åƒ [å› æœè¯­è¨€æ¨¡å‹](https://chat.openai.com/course/chapter7/6)(https://chat.openai.com/course/chapter7/6) è¿™æ ·çš„ä»»åŠ¡ä¸­å¦‚ä½•ä½¿ç”¨æ©ç è‡ªæ³¨æ„åŠ›æœºåˆ¶ç±»ä¼¼ã€‚

å¹¸è¿çš„æ˜¯ï¼ŒTransformers æä¾›äº†ä¸€ä¸ª `DataCollatorForSeq2Seq` æ•´ç†å™¨ï¼Œå®ƒä¼šåŠ¨æ€åœ°å¡«å……æˆ‘ä»¬çš„è¾“å…¥å’Œæ ‡ç­¾ã€‚è¦å®ä¾‹åŒ–è¿™ä¸ªæ•´ç†å™¨ï¼Œæˆ‘ä»¬åªéœ€è¦æä¾› `tokenizer` å’Œ `model` å³å¯ï¼š

{#if fw === 'pt'}

```python
#####Pytorch}
from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)####end
```

{:else}

```python
#####TensorFlow}
from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors="tf")####end
```

{/if}

è®©æˆ‘ä»¬çœ‹çœ‹å½“ç»™è¿™ä¸ªæ•´ç†å™¨æä¾›ä¸€ä¸ªå°æ‰¹æ¬¡çš„æ ·æœ¬æ—¶ï¼Œå®ƒä¼šäº§ç”Ÿä»€ä¹ˆã€‚é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦åˆ é™¤å¸¦æœ‰å­—ç¬¦ä¸²çš„åˆ—ï¼Œå› ä¸ºæ•´ç†å™¨ä¸çŸ¥é“å¦‚ä½•å¯¹è¿™äº›å…ƒç´ è¿›è¡Œå¡«å……ï¼ˆpaddingï¼‰ï¼š

```python
tokenized_datasets = tokenized_datasets.remove_columns(
    books_dataset["train"].column_names
)
```

ç”±äº collator éœ€è¦ä¸€ä¸ª `dict` çš„åˆ—è¡¨ï¼Œå…¶ä¸­æ¯ä¸ª `dict` ä»£è¡¨æ•°æ®é›†ä¸­çš„ä¸€ä¸ªæ ·æœ¬ï¼Œæ‰€ä»¥æˆ‘ä»¬ä¹Ÿéœ€è¦åœ¨å°†æ•°æ®ä¼ ç»™æ•°æ®æ•´ç†å™¨ä¹‹å‰ï¼Œå°†æ•°æ®æ•´ç†æˆé¢„æœŸçš„æ ¼å¼ï¼š

```python
features = [tokenized_datasets["train"][i] for i in range(2)]
data_collator(features)
```

```python
{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'input_ids': tensor([[  1494,    259,   8622,    390,    259,    262,   2316,   3435,    955,
            772,    281,    772,   1617,    263,    305,  14701,    260,   1385,
           3031,    259,  24146,    332,   1037,    259,  43906,    305,    336,
            260,      1,      0,      0,      0,      0,      0,      0],
        [   259,  27531,  13483,    259,   7505,    260, 112240,  15192,    305,
          53198,    276,    259,  74060,    263,    260,    459,  25640,    776,
           2119,    336,    259,   2220,    259,  18896,    288,   4906,    288,
           1037,   3931,    260,   7083, 101476,   1143,    260,      1]]), 'labels': tensor([[ 7483,   259,  2364, 15695,     1,  -100],
        [  259, 27531, 13483,   259,  7505,     1]]), 'decoder_input_ids': tensor([[    0,  7483,   259,  2364, 15695,     1],
        [    0,   259, 27531, 13483,   259,  7505]])}
```

é¦–å…ˆè¦æ³¨æ„çš„æ˜¯ï¼Œç¬¬äºŒä¸ªä¾‹å­æ¯”ç¬¬ä¸€ä¸ªä¾‹å­è¦é•¿ï¼Œæ‰€ä»¥ç¬¬ä¸ªä¸€ä¾‹å­çš„ `input_ids` å’Œ `attention_mask` åœ¨å³è¾¹ç”¨ `[PAD]` token ï¼ˆID ä¸º `0` ï¼‰è¿›è¡Œäº†å¡«å……ã€‚åŒæ ·ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ ‡ç­¾è¢«å¡«å……äº† `-100` ï¼Œä»¥ç¡®ä¿å¡«å…… tokens è¢«æŸå¤±å‡½æ•°å¿½ç•¥ã€‚æœ€åï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¸€ä¸ªæ–°çš„ `decoder_input_ids` ï¼Œå®ƒé€šè¿‡åœ¨ç¬¬ä¸€ä¸ªæ¡ç›®ä¸­æ’å…¥ `[PAD]` tokens æ¥å°†æ ‡ç­¾å‘å³ç§»åŠ¨ã€‚

{#if fw === 'pt'}

æˆ‘ä»¬ç»ˆäºæ‹¥æœ‰äº†è®­ç»ƒæ‰€éœ€çš„æ‰€æœ‰çš„å‰æœŸå‡†å¤‡ï¼æˆ‘ä»¬ç°åœ¨åªéœ€è¦ä½¿ç”¨æ ‡å‡†å‚æ•°å®ä¾‹åŒ– Trainer 

```python
#####Pytorch}
from transformers import Seq2SeqTrainer

trainer = Seq2SeqTrainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)####end
```

ç„¶åå¯åŠ¨æˆ‘ä»¬çš„è®­ç»ƒï¼š

```python
#####Pytorch}
trainer.train()####end
```

åœ¨è®­ç»ƒæœŸé—´ï¼Œä½ åº”è¯¥çœ‹åˆ°è®­ç»ƒæŸå¤±é€æ¸å‡å°ï¼Œè€Œ ROUGE åˆ†æ•°éšç€æ¯ä¸ªé˜¶æ®µçš„å¢åŠ è€Œå¢åŠ ã€‚ä¸€æ—¦è®­ç»ƒå®Œæˆï¼Œä½ å¯ä»¥é€šè¿‡è¿è¡Œ `Trainer.evaluate()` æ¥æŸ¥çœ‹æœ€åçš„ ROUGE åˆ†æ•°ï¼š

```python
#####Pytorch}
trainer.evaluate()####end
```

```python
#####Pytorch}
{'eval_loss': 3.028524398803711,
 'eval_rouge1': 16.9728,
 'eval_rouge2': 8.2969,
 'eval_rougeL': 16.8366,
 'eval_rougeLsum': 16.851,
 'eval_gen_len': 10.1597,
 'eval_runtime': 6.1054,
 'eval_samples_per_second': 38.982,
 'eval_steps_per_second': 4.914}####end
```

ä»åˆ†æ•°ä¸­æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œæˆ‘ä»¬çš„æ¨¡å‹è½»æ¾è¶…è¿‡äº†æˆ‘ä»¬çš„ `lead-3` baselineâ€”â€”å¾ˆå¥½ï¼æœ€åè¦åšçš„æ˜¯å°†æ¨¡å‹æƒé‡æ¨é€åˆ° Hubï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```python
#####Pytorch}
trainer.push_to_hub(commit_message="Training complete", tags="summarization")####end
```

```python
#####Pytorch}
'https://huggingface.co/huggingface-course/mt5-finetuned-amazon-en-es/commit/aa0536b829b28e73e1e4b94b8a5aacec420d40e0'####end
```

è¿™å°†æŠŠ checkpoint å’Œé…ç½®æ–‡ä»¶ä¿å­˜åˆ° `output_dir` ï¼Œç„¶åå°†æ‰€æœ‰æ–‡ä»¶ä¸Šä¼ åˆ° Hubã€‚é€šè¿‡æŒ‡å®š `tags` å‚æ•°ï¼Œæˆ‘ä»¬è¿˜ç¡®ä¿åœ¨ Hub ä¸Šçš„å°å·¥å…·ä¼šæ˜¯ä¸€ä¸ªæ‘˜è¦ç”Ÿæˆçš„å°å·¥å…·ï¼Œè€Œä¸æ˜¯ä¸ mT5 æ¶æ„ç›¸å…³è”çš„é»˜è®¤æ–‡æœ¬ç”Ÿæˆå°å·¥å…·ï¼ˆå…³äºæ¨¡å‹æ ‡ç­¾çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è§ [Hubæ–‡æ¡£](https://huggingface.co/docs/hub/main#how-is-a-models-type-of-inference-api-and-widget-determined)(https://huggingface.co/docs/hub/main#how-is-a-models-type-of-inference-api-and-widget-determined) ï¼‰ã€‚ `trainer.push_to_hub()` çš„è¾“å‡ºæ˜¯ Git æäº¤å“ˆå¸Œçš„ URLï¼Œæ‰€ä»¥ä½ å¯ä»¥è½»æ¾æŸ¥çœ‹å¯¹æ¨¡å‹åº“è¿›è¡Œçš„ä¿®æ”¹ï¼

åœ¨ç»“æŸæœ¬èŠ‚ä¹‹å‰ï¼Œè®©æˆ‘ä»¬çœ‹ä¸€ä¸‹å¦‚ä½•ä½¿ç”¨ Accelerate æä¾›çš„åº•å±‚ API å¯¹ mT5 è¿›è¡Œå¾®è°ƒã€‚

{:else}

æˆ‘ä»¬å‡ ä¹å‡†å¤‡å¥½è®­ç»ƒäº†ï¼æˆ‘ä»¬åªéœ€è¦ä½¿ç”¨æˆ‘ä»¬ä¸Šé¢å®šä¹‰çš„æ•°æ®æ•´ç†å™¨å°†æˆ‘ä»¬çš„æ•°æ®é›†è½¬æ¢ä¸º `tf.data.Dataset` ï¼Œç„¶å `compile()` å’Œ `fit()` æ¨¡å‹ã€‚é¦–å…ˆï¼Œè½¬æ¢æ•°æ®é›†ï¼š
```python
#####TensorFlow}
tf_train_dataset = model.prepare_tf_dataset(
    tokenized_datasets["train"],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=8,
)
tf_eval_dataset = model.prepare_tf_dataset(
    tokenized_datasets["validation"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=8,
)####end
```

ç°åœ¨ï¼Œæˆ‘ä»¬å®šä¹‰è®­ç»ƒè¶…å‚æ•°å¹¶ç¼–è¯‘ï¼š

```python
#####TensorFlow}
from transformers import create_optimizer
import tensorflow as tf

# è®­ç»ƒæ­¥æ•°æ˜¯æ•°æ®é›†ä¸­çš„æ ·æœ¬æ•°é‡,é™¤ä»¥ batch å¤§å°,ç„¶åä¹˜ä»¥æ€»çš„ epoch æ•°ã€‚
# æ³¨æ„è¿™é‡Œçš„ tf_train_dataset æ˜¯ batch å½¢å¼çš„ tf.data.Dataset,
# è€Œä¸æ˜¯åŸå§‹çš„ Hugging Face Dataset ,æ‰€ä»¥ä½¿ç”¨ len() è®¡ç®—å®ƒçš„é•¿åº¦å·²ç»æ˜¯ num_samples // batch_sizeã€‚

num_train_epochs = 8
num_train_steps = len(tf_train_dataset) * num_train_epochs
model_name = model_checkpoint.split("/")[-1]

optimizer, schedule = create_optimizer(
    init_lr=5.6e-5,
    num_warmup_steps=0,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)

model.compile(optimizer=optimizer)

## ä½¿ç”¨ float16 æ··åˆç²¾åº¦è¿›è¡Œè®­ç»ƒ
tf.keras.mixed_precision.set_global_policy("mixed_float16")####end
```

æœ€åï¼Œæˆ‘ä»¬è®­ç»ƒæ¨¡å‹ã€‚æˆ‘ä»¬ä½¿ç”¨ `PushToHubCallback` åœ¨æ¯ä¸ª epoch åå°†æ¨¡å‹ä¿å­˜åˆ° Hubï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥åœ¨åé¢ç”¨å®ƒæ¥è¿›è¡Œæ¨ç†ï¼š

```python
#####TensorFlow}
from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(
    output_dir=f"{model_name}-finetuned-amazon-en-es", tokenizer=tokenizer
)

model.fit(
    tf_train_dataset, validation_data=tf_eval_dataset, callbacks=[callback], epochs=8
)####end
```

æˆ‘ä»¬åœ¨è®­ç»ƒæœŸé—´å¾—åˆ°äº†ä¸€äº› lossï¼Œä½†å®é™…ä¸Šæˆ‘ä»¬æƒ³çœ‹åˆ°æˆ‘ä»¬ä¹‹å‰è®¡ç®—çš„ ROUGE æŒ‡æ ‡ã€‚è¦è·å¾—è¿™äº›æŒ‡æ ‡ï¼Œæˆ‘ä»¬éœ€è¦è·å–æ¨¡å‹ç”Ÿæˆè¾“å‡ºå¹¶å°†å®ƒä»¬è½¬æ¢ä¸ºå­—ç¬¦ä¸²ã€‚è®©æˆ‘ä»¬æ„å»ºä¸€äº›å‚è€ƒæ‘˜è¦å’Œé¢„æµ‹çš„åˆ—è¡¨ï¼Œä»¥ä¾¿ ROUGE æŒ‡æ ‡è¿›è¡Œæ¯”è¾ƒï¼ˆè¯·æ³¨æ„ï¼Œå¦‚æœä½ åœ¨è¿™ä¸€éƒ¨åˆ†é‡åˆ°å¯¼å…¥é”™è¯¯ï¼Œä½ å¯èƒ½éœ€è¦æ‰§è¡Œ `!pip install tqdm` ï¼‰ã€‚

æˆ‘ä»¬è¿˜å°†ä½¿ç”¨ä¸€ç§å¯ä»¥æ˜¾ç€æé«˜æ€§èƒ½çš„æŠ€å·§ â€”â€” ä½¿ç”¨ TensorFlow çš„åŠ é€Ÿçº¿æ€§ä»£æ•°ç¼–è¯‘å™¨ [XLA](https://www.tensorflow.org/xla)(https://www.tensorflow.org/xla) ç¼–è¯‘æˆ‘ä»¬çš„ç”Ÿæˆä»£ç ã€‚XLA å¯¹æ¨¡å‹çš„è®¡ç®—å›¾åº”ç”¨äº†å„ç§ä¼˜åŒ–ï¼Œå¹¶æ˜¾ç€æé«˜äº†é€Ÿåº¦å’Œå†…å­˜ä½¿ç”¨ç‡ã€‚æ­£å¦‚ Hugging Face [åšå®¢](https://huggingface.co/blog/tf-xla-generate)(https://huggingface.co/blog/tf-xla-generate) ä¸­æ‰€è¿°ï¼Œå½“æˆ‘ä»¬çš„è¾“å…¥å½¢çŠ¶å˜åŒ–ä¸å¤§æ—¶ï¼ŒXLA æ•ˆæœæœ€ä½³ã€‚ä¸ºäº†å¤„ç†è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å°†è¾“å…¥å¡«å……åˆ° 128 çš„å€æ•°ï¼Œå¹¶ä½¿ç”¨å¡«å……æ•´ç†å™¨åˆ›å»ºä¸€ä¸ªæ–°æ•°æ®é›†ï¼Œç„¶åæˆ‘ä»¬ä½¿ç”¨ `@tf.function(jit_compile=True)` è£…é¥°å™¨è£…é¥°æˆ‘ä»¬çš„ç”Ÿæˆå‡½æ•°ï¼Œè¿™å°†æŠŠæ•´ä¸ªå‡½æ•°æ ‡è®°ä¸ºç”¨ XLA ç¼–è¯‘ã€‚

```python
#####TensorFlow}
from tqdm import tqdm
import numpy as np

generation_data_collator = DataCollatorForSeq2Seq(
    tokenizer, model=model, return_tensors="tf", pad_to_multiple_of=320
)

tf_generate_dataset = model.prepare_tf_dataset(
    tokenized_datasets["validation"],
    collate_fn=generation_data_collator,
    shuffle=False,
    batch_size=8,
    drop_remainder=True,
)

@tf.function(jit_compile=True)
def generate_with_xla(batch):
    return model.generate(
        input_ids=batch["input_ids"],
        attention_mask=batch["attention_mask"],
        max_new_tokens=32,
    )

all_preds = []
all_labels = []
for batch, labels in tqdm(tf_generate_dataset):
    predictions = generate_with_xla(batch)
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    labels = labels.numpy()
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    decoded_preds = ["\n".join(sent_tokenize(pred.strip())) for pred in decoded_preds]
    decoded_labels = ["\n".join(sent_tokenize(label.strip())) for label in decoded_labels]
    all_preds.extend(decoded_preds)
    all_labels.extend(decoded_labels)####end
```

ä¸€æ—¦æˆ‘ä»¬æœ‰äº†å‚è€ƒæ‘˜è¦å’Œæ¨¡å‹è¾“å‡ºé¢„æµ‹çš„å­—ç¬¦ä¸²çš„åˆ—è¡¨ï¼Œè®¡ç®— ROUGE åˆ†æ•°å°±å¾ˆå®¹æ˜“äº†ï¼š

```python
#####TensorFlow}
result = rouge_score.compute(
    predictions=decoded_preds, references=decoded_labels, use_stemmer=True
)
result = {key: value.mid.fmeasure * 100 for key, value in result.items()}
{k: round(v, 4) for k, v in result.items()}####end
```

```python
#####TensorFlow}
{'rouge1': 31.4815, 'rouge2': 25.4386, 'rougeL': 31.4815, 'rougeLsum': 31.4815}####end
```

{/if}

{#if fw === 'pt'}

### ä½¿ç”¨ Accelerate å¾®è°ƒ mT5 

ä½¿ç”¨ Accelerate å¾®è°ƒæˆ‘ä»¬çš„æ¨¡å‹ä¸æˆ‘ä»¬åœ¨ç¬¬å››ç« ä¸­é‡åˆ°çš„æ–‡æœ¬åˆ†ç±»ç¤ºä¾‹éå¸¸ç›¸ä¼¼ã€‚ä¸»è¦åŒºåˆ«åœ¨äºéœ€è¦åœ¨è®­ç»ƒæœŸé—´æ˜¾å¼ç”Ÿæˆæ‘˜è¦å¹¶å®šä¹‰æˆ‘ä»¬å¦‚ä½•è®¡ç®— ROUGE åˆ†æ•°ï¼ˆè¯·è®°ä½ï¼Œ `Seq2SeqTrainer` å·²ç»ä¸ºæˆ‘ä»¬å¤„ç†äº†ç”Ÿæˆæ‘˜è¦çš„éƒ¨åˆ†ï¼‰ã€‚è®©æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬å¦‚ä½•åœ¨ Accelerate ä¸­å®ç°è¿™ä¸¤ä¸ªè¦æ±‚ï¼

#### ä¸ºè®­ç»ƒåšå¥½ä¸€åˆ‡å‡†å¤‡ 

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦ä¸ºæ¯ä¸ªæ•°æ®åˆ†ç»„åˆ›å»ºä¸€ä¸ª `DataLoader` ã€‚ç”±äº PyTorch çš„ dataloaders æœŸæœ›å¾—åˆ°çš„æ˜¯å¼ é‡ç»„æˆçš„ batchï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦å°†æ•°æ®é›†çš„æ ¼å¼è®¾å®šä¸º `"torch"` ï¼š

```python
#####Pytorch}
tokenized_datasets.set_format("torch")####end
```

ç°åœ¨æˆ‘ä»¬å·²ç»æœ‰äº†ä»…ç”±å¼ é‡ç»„æˆçš„æ•°æ®é›†ï¼Œæ¥ä¸‹æ¥è¦åšçš„æ˜¯å†æ¬¡å®ä¾‹åŒ– `DataCollatorForSeq2Seq` ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬éœ€è¦æä¾›æ¨¡å‹å¾®è°ƒå‰çš„ç‰ˆæœ¬ï¼Œæ‰€ä»¥è®©æˆ‘ä»¬ä»ç¼“å­˜ä¸­å†æ¬¡åŠ è½½å®ƒï¼š

```python
#####Pytorch}
model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)####end
```

ç„¶åæˆ‘ä»¬å¯ä»¥å®ä¾‹åŒ–æ•°æ®æ•´ç†å™¨ï¼Œå¹¶ä½¿ç”¨å®ƒæ¥å®šä¹‰æˆ‘ä»¬çš„ DataLoaderï¼š

```python
#####Pytorch}
from torch.utils.data import DataLoader

batch_size = 8
train_dataloader = DataLoader(
    tokenized_datasets["train"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=batch_size,
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], collate_fn=data_collator, batch_size=batch_size
)####end
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬éœ€è¦å®šä¹‰æˆ‘ä»¬è¦ä½¿ç”¨çš„ä¼˜åŒ–å™¨ã€‚ä¸æˆ‘ä»¬çš„å…¶ä»–ä¾‹å­ä¸€æ ·ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ `AdamW` ï¼Œè¿™ä¸ªä¼˜åŒ–å™¨å¯¹å¤§å¤šæ•°é—®é¢˜éƒ½å¾ˆæœ‰æ•ˆï¼š

```python
#####Pytorch}
from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr=2e-5)####end
```

æœ€åï¼Œæˆ‘ä»¬å°†æ¨¡å‹ã€ä¼˜åŒ–å™¨å’Œ dataloaders è¾“å…¥åˆ° `accelerator.prepare()` æ–¹æ³•ä¸­ï¼š

```python
#####Pytorch}
from accelerate import Accelerator

accelerator = Accelerator()
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)####end
```

<div custom-style="Tip-red">

ğŸš¨å¦‚æœä½ åœ¨ TPU ä¸Šè¿›è¡Œè®­ç»ƒï¼Œåˆ™éœ€è¦å°†ä¸Šè¿°æ‰€æœ‰ä»£ç ç§»åŠ¨åˆ°ä¸“é—¨çš„è®­ç»ƒå‡½æ•°ä¸­ã€‚æœ‰å…³è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…ç¬¬å››ç« ã€‚

</div>

ç°åœ¨æˆ‘ä»¬å·²ç»å‡†å¤‡å¥½äº†æˆ‘ä»¬çš„å¯¹è±¡ï¼Œè¿˜æœ‰ä¸‰ä¸ªäº‹æƒ…éœ€è¦åš

* å®šä¹‰å­¦ä¹ ç‡è°ƒåº¦è®¡åˆ’ã€‚
* å®ç°ä¸€ä¸ªåŠŸèƒ½æ¥å¯¹æ¨¡å‹è¾“å‡ºçš„æ‘˜è¦è¿›è¡Œåç»­å¤„ç†ä»¥è¿›è¡Œè¯„ä¼°ã€‚
* åœ¨ Hub ä¸Šåˆ›å»ºä¸€ä¸ªæ¨¡å‹ä»“åº“ï¼Œæˆ‘ä»¬å¯ä»¥å°†æ¨¡å‹æ¨é€åˆ°è¯¥ä»“åº“ã€‚

å¯¹äºå­¦ä¹ ç‡è°ƒåº¦ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å‰å‡ èŠ‚ä¸­çš„æ ‡å‡†çº¿æ€§è¡°å‡ï¼š

```python
#####Pytorch}
from transformers import get_scheduler

num_train_epochs = 10
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)####end
```

å¯¹äºåç»­å¤„ç†ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªå‡½æ•°ï¼Œå°†ç”Ÿæˆçš„æ‘˜è¦æ‹†åˆ†ä¸ºç”±æ¢è¡Œç¬¦åˆ†éš”çš„å¥å­ã€‚è¿™æ˜¯ ROUGE æŒ‡æ ‡æ‰€æœŸæœ›çš„æ ¼å¼ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä»¥ä¸‹ä»£ç ç‰‡æ®µæ¥å®ç°ï¼š

```python
#####Pytorch}
def postprocess_text(preds, labels):
    preds = [pred.strip() for pred in preds]
    labels = [label.strip() for label in labels]

    # ROUGE éœ€è¦æ¯ä¸ªå¥å­åæœ‰ä¸€ä¸ªæ¢è¡Œç¬¦
    preds = ["\n".join(nltk.sent_tokenize(pred)) for pred in preds]
    labels = ["\n".join(nltk.sent_tokenize(label)) for label in labels]

    return preds, labels####end
```

å¦‚æœä½ è¿˜è®°å¾—æˆ‘ä»¬æ˜¯å¦‚ä½•å®šä¹‰ `Seq2SeqTrainer` çš„ `compute_metrics()` å‡½æ•°ï¼Œè¿™å¯¹ä½ æ¥è¯´åº”è¯¥å¾ˆç†Ÿæ‚‰ã€‚

æœ€åï¼Œæˆ‘ä»¬éœ€è¦åœ¨ Hugging Face Hub ä¸Šåˆ›å»ºä¸€ä¸ªæ¨¡å‹ä»“åº“ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨åä¸ºHub çš„åº“ã€‚æˆ‘ä»¬åªéœ€è¦ä¸ºæˆ‘ä»¬çš„ä»“åº“å–ä¸€ä¸ª IDï¼Œåº“ä¸­æœ‰ä¸€ä¸ªå®ç”¨çš„å‡½æ•°å¯ä»¥å°†ä»“åº“ ID ä¸ç”¨æˆ· ID ç»„åˆèµ·æ¥ï¼š

```python
#####Pytorch}
from huggingface_hub import get_full_repo_name

model_name = "test-bert-finetuned-squad-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name####end
```

```python
#####Pytorch}
'lewtun/mt5-finetuned-amazon-en-es-accelerate'####end
```

ç°åœ¨æˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¿™ä¸ªä»“åº“åç§°å°†æœ¬åœ°ç‰ˆæœ¬å…‹éš†åˆ°æˆ‘ä»¬çš„ç»“æœç›®å½•ä¸­ï¼Œè¯¥ç›®å½•å°†å­˜å‚¨è®­ç»ƒç”Ÿæˆçš„æ–‡ä»¶ï¼š

```python
#####Pytorch}
from huggingface_hub import Repository

output_dir = "results-mt5-finetuned-squad-accelerate"
repo = Repository(output_dir, clone_from=repo_name)####end
```

è¿™å°†å…è®¸æˆ‘ä»¬åœ¨è®­ç»ƒæœŸé—´é€šè¿‡è°ƒç”¨ `repo.push_to_hub()` æ–¹æ³•å°†æ¨¡å‹æ¨é€åˆ° Hubï¼ç°åœ¨è®©æˆ‘ä»¬é€šè¿‡å†™å‡ºå®Œæ•´çš„è®­ç»ƒå¾ªç¯æ¥ç»“æŸæˆ‘ä»¬çš„åˆ†æã€‚

#### è®­ç»ƒå¾ªç¯ 

æ–‡æœ¬æ‘˜è¦çš„è®­ç»ƒå¾ªç¯ä¸æˆ‘ä»¬é‡åˆ°çš„å…¶ä»– Accelerate ç¤ºä¾‹éå¸¸ç›¸ä¼¼ï¼Œå¤§è‡´åˆ†ä¸ºå››ä¸ªä¸»è¦æ­¥éª¤ï¼š

1. é€šè¿‡åœ¨æ¯ä¸ª epoch è¿­ä»£ `train_dataloader` ä¸­çš„æ‰€æœ‰ç¤ºä¾‹æ¥è®­ç»ƒæ¨¡å‹ã€‚
2. åœ¨æ¯ä¸ª epoch ç»“æŸæ—¶ç”Ÿæˆæ‘˜è¦ï¼Œé¦–å…ˆç”Ÿæˆ tokens ç„¶åå°†å®ƒä»¬ï¼ˆå’Œå‚è€ƒæ‘˜è¦ï¼‰è§£ç ä¸ºæ–‡æœ¬ã€‚
3. ä½¿ç”¨æˆ‘ä»¬ä¹‹å‰çœ‹åˆ°çš„æŠ€æœ¯è®¡ç®— ROUGE åˆ†æ•°ã€‚
4. ä¿å­˜ checkpoint å¹¶å°†æ‰€æœ‰å†…å®¹æ¨é€åˆ° Hubã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä¾èµ– `Repository` å¯¹è±¡çš„å·§å¦™çš„ `blocking=False` å‚æ•°ï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥åœ¨æ¯ä¸ª epoch å¼‚æ­¥åœ°ä¸Šä¼  checkpoint è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿç»§ç»­è®­ç»ƒï¼Œè€Œä¸å¿…ç­‰å¾…ä¸ GB å¤§å°çš„æ¨¡å‹æ…¢å‘¼å‘¼çš„ä¸Šä¼ ï¼

è¿™äº›æ­¥éª¤å¯ä»¥åœ¨ä»¥ä¸‹ä»£ç å—ä¸­çœ‹åˆ°ï¼š

```python
#####Pytorch}
from tqdm.auto import tqdm
import torch
import numpy as np

progress_bar = tqdm(range(num_training_steps))

for epoch in range(num_train_epochs):
    # è®­ç»ƒ
    model.train()
    for step, batch in enumerate(train_dataloader):
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

    # è¯„ä¼°
    model.eval()
    for step, batch in enumerate(eval_dataloader):
        with torch.no_grad():
            generated_tokens = accelerator.unwrap_model(model).generate(
                batch["input_ids"],
                attention_mask=batch["attention_mask"],
            )

            generated_tokens = accelerator.pad_across_processes(
                generated_tokens, dim=1, pad_index=tokenizer.pad_token_id
            )
            labels = batch["labels"]

            # å¦‚æœæˆ‘ä»¬æ²¡æœ‰å¡«å……åˆ°æœ€å¤§é•¿åº¦,æˆ‘ä»¬éœ€è¦å¡«å……æ ‡ç­¾
            labels = accelerator.pad_across_processes(
                batch["labels"], dim=1, pad_index=tokenizer.pad_token_id
            )

            generated_tokens = accelerator.gather(generated_tokens).cpu().numpy()
            labels = accelerator.gather(labels).cpu().numpy()

            # æ›¿æ¢æ ‡ç­¾ä¸­çš„ -100,å› ä¸ºæˆ‘ä»¬æ— æ³•è§£ç å®ƒä»¬
            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
            if isinstance(generated_tokens, tuple):
                generated_tokens = generated_tokens[0]
            decoded_preds = tokenizer.batch_decode(
                generated_tokens, skip_special_tokens=True
            )
            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

            decoded_preds, decoded_labels = postprocess_text(
                decoded_preds, decoded_labels
            )

            rouge_score.add_batch(predictions=decoded_preds, references=decoded_labels)

    # è®¡ç®—è¯„ä¼°çš„ loss
    result = rouge_score.compute()
    # æå–ä¸­ä½ ROUGE åˆ†æ•°
    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}
    result = {k: round(v, 4) for k, v in result.items()}
    print(f"Epoch {epoch}:", result)

    # ä¿å­˜å’Œä¸Šä¼ 
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
    if accelerator.is_main_process:
        tokenizer.save_pretrained(output_dir)
        repo.push_to_hub(
            commit_message=f"Training in progress epoch {epoch}", blocking=False
        )####end
```

```python
#####Pytorch}
Epoch 0: {'rouge1': 5.6351, 'rouge2': 1.1625, 'rougeL': 5.4866, 'rougeLsum': 5.5005}
Epoch 1: {'rouge1': 9.8646, 'rouge2': 3.4106, 'rougeL': 9.9439, 'rougeLsum': 9.9306}
Epoch 2: {'rouge1': 11.0872, 'rouge2': 3.3273, 'rougeL': 11.0508, 'rougeLsum': 10.9468}
Epoch 3: {'rouge1': 11.8587, 'rouge2': 4.8167, 'rougeL': 11.7986, 'rougeLsum': 11.7518}
Epoch 4: {'rouge1': 12.9842, 'rouge2': 5.5887, 'rougeL': 12.7546, 'rougeLsum': 12.7029}
Epoch 5: {'rouge1': 13.4628, 'rouge2': 6.4598, 'rougeL': 13.312, 'rougeLsum': 13.2913}
Epoch 6: {'rouge1': 12.9131, 'rouge2': 5.8914, 'rougeL': 12.6896, 'rougeLsum': 12.5701}
Epoch 7: {'rouge1': 13.3079, 'rouge2': 6.2994, 'rougeL': 13.1536, 'rougeLsum': 13.1194}
Epoch 8: {'rouge1': 13.96, 'rouge2': 6.5998, 'rougeL': 13.9123, 'rougeLsum': 13.7744}
Epoch 9: {'rouge1': 14.1192, 'rouge2': 7.0059, 'rougeL': 14.1172, 'rougeLsum': 13.9509}####end
```

å°±æ˜¯è¿™æ ·ï¼è¿è¡Œæ­¤ç¨‹åºåï¼Œä½ å°†è·å¾—ä¸æˆ‘ä»¬ä½¿ç”¨â€œTrainerâ€è·å¾—çš„æ¨¡å‹å’Œç»“æœéå¸¸ç›¸ä¼¼çš„æ¨¡å‹å’Œç»“æœã€‚

{/if}

### ä½¿ç”¨ä½ å¾®è°ƒçš„æ¨¡å‹ 

å°†æ¨¡å‹æ¨é€åˆ° Hub åï¼Œä½ å¯ä»¥é€šè¿‡æ¨ç†å°éƒ¨ä»¶æˆ– `pipeline` å¯¹è±¡æ¥ä½¿ç”¨å®ƒï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```python
from transformers import pipeline

hub_model_id = "huggingface-course/mt5-small-finetuned-amazon-en-es"
summarizer = pipeline("summarization", model=hub_model_id)
```

æˆ‘ä»¬å¯ä»¥å°†æµ‹è¯•é›†ï¼ˆæ¨¡å‹è¿˜æ²¡æœ‰çœ‹åˆ°ï¼‰ä¸­å–ä¸€äº›æ ·æœ¬æä¾›ç»™æˆ‘ä»¬çš„ç®¡é“ï¼Œæ¥æ„Ÿå—ä¸€ä¸‹ç”Ÿæˆçš„æ‘˜è¦çš„è´¨é‡ã€‚é¦–å…ˆè®©æˆ‘ä»¬å®ç°ä¸€ä¸ªç®€å•çš„å‡½æ•°æ¥ä¸€èµ·æ˜¾ç¤ºè¯„è®ºã€æ ‡é¢˜å’Œç”Ÿæˆçš„æ‘˜è¦ï¼š

```python
def print_summary(idx):
    review = books_dataset["test"][idx]["review_body"]
    title = books_dataset["test"][idx]["review_title"]
    summary = summarizer(books_dataset["test"][idx]["review_body"])[0]["summary_text"]
    print(f"'>>> Review: {review}'")
    print(f"\n'>>> Title: {title}'")
    print(f"\n'>>> Summary: {summary}'")
```

è®©æˆ‘ä»¬çœ‹ä¸€ä¸‹æˆ‘ä»¬å¾—åˆ°çš„å…¶ä¸­ä¸€ä¸ªè‹±æ–‡ä¾‹å­ï¼š

```python
print_summary(100)
```

```python
'>>> Review: Nothing special at all about this product... the book is too small and stiff and hard to write in. The huge sticker on the back doesnâ€™t come off and looks super tacky. I would not purchase this again. I could have just bought a journal from the dollar store and it would be basically the same thing. Itâ€™s also really expensive for what it is.'

'>>> Title: Not impressed at all... buy something else'

'>>> Summary: Nothing special at all about this product'
```

è¿™è¿˜ä¸é”™ï¼æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å®é™…ä¸Šå·²ç»èƒ½å¤Ÿé€šè¿‡å¢åŠ éƒ¨åˆ†æ–°è¯æ¥æŠ½è±¡æ‘˜è¦æ€»ç»“ã€‚æˆ‘ä»¬æ¨¡å‹æœ€é…·çš„æ–¹é¢æ˜¯å®ƒæ˜¯åŒè¯­çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬è¿˜å¯ä»¥ç”Ÿæˆè¥¿ç­ç‰™è¯­è¯„è®ºçš„æ‘˜è¦ï¼š

```python
print_summary(0)
```

```python
'>>> Review: Es una trilogia que se hace muy facil de leer. Me ha gustado, no me esperaba el final para nada'

'>>> Title: Buena literatura para adolescentes'

'>>> Summary: Muy facil de leer'
```

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œç”Ÿæˆçš„æ‘˜è¦ç¿»è¯‘æˆè‹±æ–‡æ˜¯â€œéå¸¸å®¹æ˜“é˜…è¯»â€ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å®ƒæ˜¯ç›´æ¥ä»è¯„è®ºä¸­æå–çš„ã€‚å°½ç®¡å¦‚æ­¤ï¼Œè¿™æ˜¾ç¤ºäº† mT5 æ¨¡å‹çš„å¤šåŠŸèƒ½æ€§ï¼Œå¹¶è®©ä½ ä½“éªŒäº†å¤„ç†å¤šè¯­è¨€è¯­æ–™åº“çš„æ„Ÿè§‰ï¼

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†è½¬å‘ä¸€ä¸ªç¨å¾®å¤æ‚ä¸€ç‚¹çš„ä»»åŠ¡ï¼šä»å¤´å¼€å§‹è®­ç»ƒä¸€ä¸ªè¯­è¨€æ¨¡å‹ã€‚


## 8.5 ä»å¤´å¼€å§‹è®­ç»ƒå› æœè¯­è¨€æ¨¡å‹ 

åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬ä¸»è¦ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¹¶é€šè¿‡é‡ç”¨é¢„è®­ç»ƒçš„æƒé‡æ¥é’ˆå¯¹æ–°ç”¨ä¾‹å¯¹å®ƒä»¬è¿›è¡Œå¾®è°ƒï¼Œä»¥é€‚åº”æ–°çš„åº”ç”¨åœºæ™¯ã€‚æ­£å¦‚æˆ‘ä»¬åœ¨ç¬¬äºŒç« ä¸­çœ‹åˆ°çš„ï¼Œè¿™é€šå¸¸ç§°ä¸º `è¿ç§»å­¦ä¹ ï¼ˆtransfer learningï¼‰` ï¼Œå¯¹äºå¤§å¤šæ•°æ ‡æ³¨æ•°æ®ç¨€ç¼ºçš„çœŸå®ä¸–ç•Œï¼Œå®ƒæ˜¯ä¸€ç§å°† Transformer æ¨¡å‹åº”ç”¨åˆ°å¤§éƒ¨åˆ†çœŸå®çš„åº”ç”¨åœºæ™¯ä¸­çš„ä¸€ä¸ªéå¸¸æˆåŠŸçš„ç­–ç•¥ã€‚åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†é‡‡ç”¨ä¸åŒçš„æ–¹æ³•å¹¶ä»å¤´å¼€å§‹è®­ç»ƒä¸€ä¸ªå…¨æ–°çš„æ¨¡å‹ã€‚å¦‚æœä½ æœ‰å¤§é‡æ•°æ®è€Œä¸”è¿™äº›æ•°æ®ä¸å¯ç”¨æ¨¡å‹çš„é¢„è®­ç»ƒæ•°æ®å·®å¼‚å¾ˆå¤§ï¼Œé‚£ä¹ˆè¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œç›¸æ¯”ä»…å¾®è°ƒç°æœ‰æ¨¡å‹ï¼Œé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹éœ€è¦æ›´å¤šçš„è®¡ç®—èµ„æºã€‚è®­ç»ƒä¸€ä¸ªæ–°æ¨¡å‹å¯èƒ½æ˜¯æœ‰æ„ä¹‰çš„ç¤ºä¾‹åŒ…æ‹¬ç”±éŸ³ä¹ç¬¦å·ã€DNA ç­‰åˆ†å­åºåˆ—æˆ–ç¼–ç¨‹è¯­è¨€ç»„æˆçš„æ•°æ®é›†ã€‚ç¼–ç¨‹è¯­è¨€ç»„æˆçš„æ•°æ®é›†æœ€è¿‘å¹¿æ³›åœ°å—åˆ°å…³æ³¨ï¼Œè¿™è¦å½’åŠŸäº TabNine å’Œ GitHub çš„ Copilot ç­‰å·¥å…·ï¼Œå®ƒä»¬ç”± OpenAI çš„ Codex æ¨¡å‹æä¾›æ”¯æŒï¼Œå¯ä»¥ç”Ÿæˆé•¿ä»£ç åºåˆ—ã€‚è¿™ç§æ–‡æœ¬ç”Ÿæˆä»»åŠ¡æœ€é€‚åˆä½¿ç”¨è‡ªå›å½’æˆ–å› æœè¯­è¨€æ¨¡å‹ï¼ˆä¾‹å¦‚ GPT-2ï¼‰ã€‚

åœ¨è¿™ä¸€èŠ‚ï¼Œæˆ‘ä»¬å°†æ„å»ºä¸€ä¸ªç²¾ç®€ç‰ˆçš„ä»£ç ç”Ÿæˆæ¨¡å‹ï¼šæˆ‘ä»¬å°†åªå…³æ³¨ä¸€è¡Œä»£ç çš„è¡¥å…¨ï¼Œè€Œä¸æ˜¯å®Œæ•´çš„å‡½æ•°æˆ–ç±»ï¼Œä½¿ç”¨çš„æ˜¯ Python ä»£ç çš„ä¸€ä¸ªå­é›†ã€‚å½“ä½ ä½¿ç”¨ Python å¤„ç†æ•°æ®æ—¶ï¼Œä½ ç»å¸¸ä¼šæ¥è§¦åˆ° Python æ•°æ®ç§‘å­¦æ ˆï¼ŒåŒ…æ‹¬ `matplotlib` ï¼Œ `seaborn` ï¼Œ `pandas` ï¼Œå’Œ `scikit-learn` è¿™äº›åº“ã€‚å½“ä½¿ç”¨è¿™äº›æ¡†æ¶æ—¶ï¼Œç»å¸¸éœ€è¦æŸ¥æ‰¾ç‰¹å®šçš„å‘½ä»¤ï¼Œå¦‚æœæˆ‘ä»¬èƒ½ç”¨æ¨¡å‹æ¥å¸®åŠ©æˆ‘ä»¬å®Œæˆè¿™äº›è°ƒç”¨å°±å¤ªå¥½äº†ã€‚

åœ¨ç¬¬ä¸ƒç« ä¸­ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªé«˜æ•ˆçš„ tokenizer æ¥å¤„ç† Python æºä»£ç ï¼Œä½†æˆ‘ä»¬è¿˜éœ€è¦ä¸€ä¸ªå¤§è§„æ¨¡çš„æ•°æ®é›†æ¥é¢„è®­ç»ƒæ¨¡å‹ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨ tokenizer å¤„ç†ä¸€ä¸ªæ¥è‡ª GitHub ä»“åº“çš„ Python ä»£ç è¯­æ–™åº“ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ Trainer API å’Œ Accelerate æ¥è®­ç»ƒæ¨¡å‹ã€‚è®©æˆ‘ä»¬å¼€å§‹å§ï¼

![å› æœè¯­è¨€æ¨¡å‹çš„ä¸€ä¸ªæ¼”ç¤ºä¾‹å­](./assets/causal-language-model-example.jpg)

ä¸Šå›¾å±•ç¤ºçš„æ˜¯ä¸€ä¸ªå·²ç»è®­ç»ƒå¹¶ä¸Šä¼ åˆ° Hub çš„æ¨¡å‹ï¼Œå®ƒä½¿ç”¨çš„æ˜¯æœ¬èŠ‚ä¸­æ˜¾ç¤ºçš„ä»£ç ã€‚ä½ å¯ä»¥åœ¨ [è¿™é‡Œ](https://huggingface.co/huggingface-course/codeparrot-ds?text=plt.imshow%28)(https://huggingface.co/huggingface-course/codeparrot-ds?text=plt.imshow%28) æ‰¾åˆ°å®ƒã€‚æ³¨æ„ï¼Œç”±äºæ–‡æœ¬ç”Ÿæˆè¿‡ç¨‹ä¸­æœ‰ä¸€äº›éšæœºæ€§ï¼Œä½ å¯èƒ½ä¼šå¾—åˆ°ç¨å¾®ä¸åŒçš„ç»“æœã€‚

### æ”¶é›†æ•°æ® 

æˆ‘ä»¬å¯ä»¥ä»è¯¸å¦‚ GitHub è¿™æ ·çš„ä»£ç ä»“åº“ä¸­è·å–ä¸°å¯Œçš„ Python ä»£ç ï¼Œé€šè¿‡å¯¹æ¯ä¸ª Python ä»“åº“è¿›è¡ŒæŠ“å–ï¼Œæˆ‘ä»¬å°±å¯ä»¥åˆ›å»ºä¸€ä¸ªæ•°æ®é›†ã€‚è¿™æ˜¯åœ¨ [Transformers textbook](https://learning.oreilly.com/library/view/natural-language-processing/9781098136789/)(https://learning.oreilly.com/library/view/natural-language-processing/9781098136789/) ä¸­é¢„è®­ç»ƒä¸€ä¸ªå¤§å‹ GPT-2 æ¨¡å‹çš„æ–¹æ³•ã€‚ä½œè€…ä»¬ä½¿ç”¨äº†ä¸€ä¸ªå¤§çº¦ 180GB çš„ GitHub æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«å¤§çº¦ 2000 ä¸‡ä¸ªåä¸º `codeparrot` çš„ Python æ–‡ä»¶ï¼Œä»–ä»¬æ„å»ºäº†ä¸€ä¸ªæ•°æ®é›†ï¼Œå¹¶åœ¨ [Hugging Face Hub](https://huggingface.co/datasets/transformersbook/codeparrot)(https://huggingface.co/datasets/transformersbook/codeparrot) ä¸Šåˆ†äº«äº†è¿™ä¸ªæ•°æ®é›†ã€‚

ç„¶è€Œï¼Œä½¿ç”¨å®Œæ•´è¯­æ–™åº“çš„è®­ç»ƒæ—¢è€—æ—¶åˆè´¹åŠ›ï¼Œæˆ‘ä»¬åªéœ€è¦æ‰¾åˆ° Python æ•°æ®ç§‘å­¦å †æ ˆç›¸å…³çš„æ•°æ®é›†å­é›†ã€‚æ‰€ä»¥ï¼Œè®©æˆ‘ä»¬ä» `codeparrot` æ•°æ®é›†ä¸­ç­›é€‰å‡ºåŒ…å«è¿™ä¸ªæ ˆä¸­æ‰€æœ‰ç›¸å…³åº“çš„æ‰€æœ‰æ–‡ä»¶ã€‚ç”±äºæ•°æ®é›†çš„å¤ªå¤§ï¼Œæˆ‘ä»¬å¸Œæœ›é¿å…ä¸‹è½½å®ƒï¼›å› æ­¤ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨æµåŠŸèƒ½æ¥åŠ¨æ€è¿‡æ»¤å®ƒã€‚ä¸ºäº†ä½¿ç”¨ä¹‹å‰æåˆ°çš„åº“æ¥ç­›é€‰ä»£ç æ ·æœ¬ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä»¥ä¸‹å‡½æ•°ï¼š

```python
def any_keyword_in_string(string, keywords):
    for keyword in keywords:
        if keyword in string:
            return True
    return False
```

è®©æˆ‘ä»¬ç”¨ä¸¤ä¸ªä¾‹å­æ¥æµ‹è¯•ä¸€ä¸‹ï¼š

```python
filters = ["pandas", "sklearn", "matplotlib", "seaborn"]
example_1 = "import numpy as np"
example_2 = "import pandas as pd"

print(
    any_keyword_in_string(example_1, filters), any_keyword_in_string(example_2, filters)
)
```

```python
False True
```

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨å®ƒæ¥åˆ›å»ºä¸€ä¸ªå‡½æ•°ï¼Œè¯¥å‡½æ•°å°†æµå¼ä¼ è¾“æ•°æ®é›†å¹¶è¿‡æ»¤æˆ‘ä»¬æƒ³è¦çš„å…ƒç´ ï¼š

```python
from collections import defaultdict
from tqdm import tqdm
from datasets import Dataset

def filter_streaming_dataset(dataset, filters):
    filtered_dict = defaultdict(list)
    total = 0
    for sample in tqdm(iter(dataset)):
        total += 1
        if any_keyword_in_string(sample["content"], filters):
            for k, v in sample.items():
                filtered_dict[k].append(v)
    print(f"{len(filtered_dict['content'])/total:.2%} of data after filtering.")
    return Dataset.from_dict(filtered_dict)
```

ç„¶åæˆ‘ä»¬å¯ä»¥ç›´æ¥ä½¿ç”¨æ­¤å‡½æ•°å¤„ç†æµæ•°æ®é›†ï¼š

```python
## æ‰§è¡Œè¿™ä¸ªä»£ç å—éœ€è¦éå¸¸é•¿çš„æ—¶é—´,å› æ­¤ä½ å¯ä»¥è·³è¿‡å®ƒ,ç»§ç»­æ‰§è¡Œä¸‹ä¸€ä¸ª!
from datasets import load_dataset

split = "train"  # "valid"
filters = ["pandas", "sklearn", "matplotlib", "seaborn"]

data = load_dataset(f"transformersbook/codeparrot-{split}", split=split, streaming=True)
filtered_data = filter_streaming_dataset(data, filters)
```

```python
3.26% of data after filtering.
```

è¿™å°†æˆ‘ä»¬çš„æ•°æ®é›†å‹ç¼©åˆ°äº†åŸå§‹æ•°æ®é›†çš„å¤§çº¦ 3ï¼…ï¼Œä½†è¿™ä»ç„¶æ˜¯ç›¸å½“å¯è§‚çš„å¤§å°â€”â€”æœ€ç»ˆçš„æ•°æ®é›†æ˜¯ 6GBï¼Œç”± 600,000 ä¸ª Python è„šæœ¬ç»„æˆï¼

è¿‡æ»¤å®Œæ•´çš„æ•°æ®é›†å¯èƒ½éœ€è¦ 2-3 å°æ—¶ï¼Œè¿™å–å†³äºä½ çš„æœºå™¨æ€§èƒ½å’Œå¸¦å®½ã€‚å¦‚æœä½ ä¸æƒ³äº²è‡ªç»å†è¿™ä¸ªæ¼«é•¿çš„è¿‡ç¨‹ï¼Œæˆ‘ä»¬åœ¨ Hub ä¸Šæä¾›äº†è¿‡æ»¤åçš„æ•°æ®é›†ä¾›ä½ ä¸‹è½½ï¼š

```python
from datasets import load_dataset, DatasetDict

ds_train = load_dataset("huggingface-course/codeparrot-ds-train", split="train")
ds_valid = load_dataset("huggingface-course/codeparrot-ds-valid", split="validation")

raw_datasets = DatasetDict(
    {
        "train": ds_train,  # .shuffle().select(range(50000)),
        "valid": ds_valid,  # .shuffle().select(range(500))
    }
)

raw_datasets
```

```python
DatasetDict({
    train: Dataset({
        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],
        num_rows: 606720
    })
    valid: Dataset({
        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],
        num_rows: 3322
    })
})
```

è®©æˆ‘ä»¬çœ‹ä¸€ä¸ªæ¥è‡ªæ•°æ®é›†çš„ä¾‹å­ã€‚æˆ‘ä»¬å°†åªæ˜¾ç¤ºæ¯ä¸ªå­—æ®µçš„å‰ 200 ä¸ªå­—ç¬¦ï¼š

```python
for key in raw_datasets["train"][0]:
    print(f"{key.upper()}: {raw_datasets['train'][0][key][:200]}")
```

```python
'REPO_NAME: kmike/scikit-learn'
'PATH: sklearn/utils/__init__.py'
'COPIES: 3'
'SIZE: 10094'
'''CONTENT: """
The :mod:`sklearn.utils` module includes various utilites.
"""

from collections import Sequence

import numpy as np
from scipy.sparse import issparse
import warnings

from .murmurhash import murm
LICENSE: bsd-3-clause'''
```

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œ `content` å­—æ®µåŒ…å«äº†æˆ‘ä»¬å¸Œæœ›æ¨¡å‹è®­ç»ƒçš„ä»£ç ã€‚ç°åœ¨æˆ‘ä»¬æœ‰äº†æ•°æ®é›†ï¼Œæˆ‘ä»¬éœ€è¦å¯¹æ–‡æœ¬è¿›è¡Œä¸€äº›å¤„ç†ï¼Œä»¥ä¾¿å®ƒä»¬é€‚åˆäºé¢„è®­ç»ƒã€‚

### å‡†å¤‡æ•°æ®é›† 

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦å°†æ•°æ®è¿›è¡Œåˆ†è¯å¤„ç†ï¼Œè¿™æ ·æ‰èƒ½è¿›è¡Œè®­ç»ƒã€‚ç”±äºæˆ‘ä»¬çš„ä¸»è¦ç›®æ ‡æ˜¯è‡ªåŠ¨è¡¥å…¨çŸ­çš„å‡½æ•°è°ƒç”¨ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥å°†ä¸Šä¸‹æ–‡å¤§å°è®¾ç½®å¾—ç›¸å¯¹è¾ƒå°ã€‚è¿™æ ·åšçš„å¥½å¤„æ˜¯æˆ‘ä»¬å¯ä»¥æ›´å¿«åœ°è®­ç»ƒæ¨¡å‹ï¼Œè€Œä¸”éœ€è¦çš„å†…å­˜ä¹Ÿå¤§å¤§å‡å°‘ã€‚å¦‚æœä½ çš„åº”ç”¨éœ€è¦æ›´å¤šçš„ä¸Šä¸‹æ–‡ï¼ˆæ¯”å¦‚ï¼Œä½ å¸Œæœ›æ¨¡å‹æ ¹æ®åŒ…å«å‡½æ•°å®šä¹‰çš„æ–‡ä»¶ç¼–å†™å•å…ƒæµ‹è¯•ï¼‰ï¼Œé‚£ä¹ˆåº”è¯¥å¢å¤§è¯¥æ•°å­—ï¼Œä½†æ˜¯ä¹Ÿè¦è®°ä½è¿™ä¼šå¢åŠ  GPU å†…å­˜çš„å ç”¨ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬å°†ä¸Šä¸‹æ–‡å¤§å°å›ºå®šä¸º 128 ä¸ª tokens è€Œä¸æ˜¯åœ¨ GPT-2 æˆ– GPT-3 ä¸­ä½¿ç”¨çš„ 1,024 æˆ– 2,048 ä¸ª tokens 

å¤§å¤šæ•°æ–‡æ¡£éƒ½åŒ…å«è¶…è¿‡ 128 ä¸ª tokens å› æ­¤ç®€å•åœ°å°†è¾“å…¥æˆªæ–­åˆ°æœ€å¤§é•¿åº¦ä¼šåˆ é™¤æˆ‘ä»¬æ•°æ®é›†çš„å¾ˆä¸€å¤§éƒ¨åˆ†ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ `return_overflowing_tokens` é€‰é¡¹å°†æ•´ä¸ªè¾“å…¥è¿›è¡Œåˆ†è¯å¤„ç†ï¼Œå¹¶å°†å…¶åˆ†å‰²ä¸ºå‡ ä¸ªå—ï¼Œæ­£å¦‚æˆ‘ä»¬åœ¨ç¬¬ä¸ƒç« ä¸­æ‰€åšçš„é‚£æ ·ã€‚æˆ‘ä»¬è¿˜å°†ä½¿ç”¨ `return_length` é€‰é¡¹è‡ªåŠ¨è¿”å›æ¯ä¸ªåˆ›å»ºçš„å—çš„é•¿åº¦ã€‚é€šå¸¸ï¼Œæœ€åä¸€ä¸ªå—çš„å¤§å°ä¼šå°äºä¸Šä¸‹æ–‡å¤§å°ï¼Œæˆ‘ä»¬å°†å»æ‰æœ€åä¸€å—ä»¥é¿å…å¡«å……é—®é¢˜ï¼›å› ä¸ºæˆ‘ä»¬å·²ç»æœ‰è¶³å¤Ÿçš„æ•°æ®ï¼Œæ‰€ä»¥ä¸éœ€è¦å®ƒä»¬ã€‚

![å°†å¤§æ–‡æœ¬åˆ†æˆå‡ éƒ¨åˆ†](./assets/chunking_texts.png "Chunking a large texts in several pieces.")

è®©æˆ‘ä»¬é€šè¿‡æŸ¥çœ‹å‰ä¸¤ä¸ªç¤ºä¾‹æ¥å…·ä½“äº†è§£è¿™æ˜¯å¦‚ä½•å®ç°çš„ï¼š

```python
from transformers import AutoTokenizer

context_length = 128
tokenizer = AutoTokenizer.from_pretrained("huggingface-course/code-search-net-tokenizer")

outputs = tokenizer(
    raw_datasets["train"][:2]["content"],
    truncation=True,
    max_length=context_length,
    return_overflowing_tokens=True,
    return_length=True,
)

print(f"Input IDs length: {len(outputs['input_ids'])}")
print(f"Input chunk lengths: {(outputs['length'])}")
print(f"Chunk mapping: {outputs['overflow_to_sample_mapping']}")
```

```python
Input IDs length: 34
Input chunk lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 117, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 41]
Chunk mapping: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
```

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œè¿™ä¸¤ä¸ªä¾‹å­æ€»å…±å¾—åˆ°äº† 34 ä¸ªå—ã€‚æŸ¥çœ‹å—é•¿åº¦ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¸¤ä¸ªæ–‡æ¡£æœ«ç«¯çš„å—å°‘äº 128 ä¸ª tokens ï¼ˆåˆ†åˆ«ä¸º 117 å’Œ 41ï¼‰ã€‚è¿™äº›åªå æˆ‘ä»¬æ‰€æ‹¥æœ‰çš„æ€»å—æ•°çš„ä¸€å°éƒ¨åˆ†ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥æ”¾å¿ƒåœ°ä¸¢æ‰å®ƒä»¬ã€‚é€šè¿‡ `overflow_to_sample_mapping` å­—æ®µï¼Œæˆ‘ä»¬è¿˜å¯ä»¥é‡æ–°æ„å»ºå“ªäº›å—å±äºå“ªä¸ªè¾“å…¥æ ·æœ¬ã€‚

åœ¨è¿™ä¸ªæ“ä½œä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†Datasets ä¸­çš„ `Dataset.map()` å‡½æ•°çš„ä¸€ä¸ªä¾¿æ·çš„ç‰¹æ€§ï¼Œå³å®ƒå¹¶ä¸éœ€è¦ä¸€å¯¹ä¸€çš„æ˜ å°„ï¼›æ­£å¦‚æˆ‘ä»¬åœ¨ç¬¬ä¸‰èŠ‚ä¸­çœ‹åˆ°çš„ï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºæ¯”è¾“å…¥ `batch_size` æ›´å¤šæˆ–æ›´å°‘å…ƒç´ çš„ batchã€‚å½“è¿›è¡Œåƒæ•°æ®å¢å¼ºæˆ–æ•°æ®è¿‡æ»¤è¿™æ ·æ”¹å˜å…ƒç´ æ•°é‡çš„æ“ä½œæ—¶éå¸¸æœ‰ç”¨ã€‚åœ¨æˆ‘ä»¬çš„æƒ…å†µä¸‹ï¼Œå½“å°†æ¯ä¸ªå…ƒç´ åˆ†è¯æˆæŒ‡å®šä¸Šä¸‹æ–‡å¤§å°çš„å—æ—¶ï¼Œæˆ‘ä»¬ä»æ¯ä¸ªæ–‡æ¡£ä¸­åˆ›å»ºäº†è®¸å¤šæ ·æœ¬ã€‚æˆ‘ä»¬éœ€ä¿åˆ é™¤åŸæœ‰çš„åˆ—ï¼Œå› ä¸ºå®ƒä»¬çš„å¤§å°æœ‰å†²çªã€‚å¦‚æœæˆ‘ä»¬æƒ³ä¿ç•™å®ƒä»¬ï¼Œæˆ‘ä»¬å¯ä»¥å¤åˆ¶å¹¶å¡«å……å®ƒä»¬ï¼Œå¹¶åœ¨ `Dataset.map()` è°ƒç”¨ä¸­è¿”å›å®ƒä»¬ã€‚

```python
def tokenize(element):
    outputs = tokenizer(
        element["content"],
        truncation=True,
        max_length=context_length,
        return_overflowing_tokens=True,
        return_length=True,
    )
    input_batch = []
    for length, input_ids in zip(outputs["length"], outputs["input_ids"]):
        if length == context_length:
            input_batch.append(input_ids)
    return {"input_ids": input_batch}

tokenized_datasets = raw_datasets.map(
    tokenize, batched=True, remove_columns=raw_datasets["train"].column_names
)
tokenized_datasets
```

```python
DatasetDict({
    train: Dataset({
        features: ['input_ids'],
        num_rows: 16702061
    })
    valid: Dataset({
        features: ['input_ids'],
        num_rows: 93164
    })
})
```

æˆ‘ä»¬ç°åœ¨æœ‰ 1670 ä¸‡ä¸ªç¤ºä¾‹ï¼Œæ¯ä¸ªç¤ºä¾‹æœ‰ 128 ä¸ª tokens æ€»å…±ç›¸å½“äºå¤§çº¦ 21 äº¿ä¸ª tokensã€‚ä½œä¸ºå‚è€ƒï¼ŒOpenAI çš„ GPT-3 å’Œ Codex æ¨¡å‹åˆ†åˆ«åœ¨ 300 å’Œ 1000 äº¿ä¸ª tokens ä¸Šè®­ç»ƒï¼Œå…¶ä¸­ Codex æ¨¡å‹ä» GPT-3 checkpoint åˆå§‹åŒ–ã€‚æœ¬èŠ‚çš„ç›®æ ‡ä¸æ˜¯ä¸è¿™äº›èƒ½ç”Ÿæˆé•¿ä¸”è¿è´¯æ–‡æœ¬çš„æ¨¡å‹ç«äº‰ï¼Œè€Œæ˜¯åˆ›å»ºä¸€ä¸ªèƒ½ä¸ºæ•°æ®ç§‘å­¦å®¶æä¾›å¿«é€Ÿè‡ªåŠ¨ä»£ç å®ŒæˆåŠŸèƒ½çš„ç²¾ç®€ç‰ˆæœ¬ã€‚

æ—¢ç„¶æˆ‘ä»¬å·²ç»å‡†å¤‡å¥½äº†æ•°æ®é›†ï¼Œé‚£å°±æ¥è®¾ç½®æ¨¡å‹å§ï¼

<div custom-style="Tip-green">

âœï¸ **è¯•ä¸€è¯•ï¼**è¿™é‡Œæˆ‘ä»¬åˆ é™¤äº†æ‰€æœ‰å°äºä¸Šä¸‹æ–‡å¤§å°çš„å—ï¼Œå¹¶ä¸ä¼šé€ æˆå¤§é—®é¢˜ï¼Œå› ä¸ºæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯å°çš„ä¸Šä¸‹æ–‡çª—å£ã€‚éšç€å¢å¤§ä¸Šä¸‹æ–‡å¤§å°ï¼ˆæˆ–è€…è¯­æ–™åº“ä¸­çš„æ–‡æ¡£é•¿åº¦éƒ½å¾ˆçŸ­ï¼‰ï¼Œè¢«æŠ›å¼ƒçš„å—çš„æ¯”ä¾‹ä¹Ÿä¼šå¢åŠ ã€‚å‡†å¤‡æ•°æ®çš„æ›´æœ‰æ•ˆæ–¹æ³•æ˜¯å°†æ‰€æœ‰ tokenize åçš„æ ·æœ¬åŠ å…¥ä¸€ä¸ª batch ä¸­ï¼Œæ¯ä¸ªè¯­æ–™ä¹‹é—´æœ‰ä¸€ä¸ª `eos_token_id` token ç„¶åå¯¹è¿æ¥åçš„åºåˆ—è¿›è¡Œåˆ‡å—å¤„ç†ã€‚ä½œä¸ºç»ƒä¹ ï¼Œä¿®æ”¹ `tokenize()` å‡½æ•°ä»¥åˆ©ç”¨è¿™ç§æ–¹æ³•ã€‚è¯·æ³¨æ„ï¼Œä¸ºäº†è·å–å®Œæ•´çš„ token  ID åºåˆ—ä½ éœ€è¦è®¾ç½® `truncation=False` ï¼Œå¹¶ä» tokenizer ä¸­åˆ é™¤å…¶ä»–å‚æ•°ã€‚

</div>

### åˆå§‹åŒ–ä¸€ä¸ªæ–°æ¨¡å‹ 

æˆ‘ä»¬çš„ç¬¬ä¸€æ­¥æ˜¯åˆå§‹åŒ–ä¸€ä¸ªå…¨æ–°åœ° GPT-2 æ¨¡å‹ã€‚æˆ‘ä»¬å°†é€šè¿‡åŠ è½½é¢„è®­ç»ƒé…ç½®æ¥ä½¿ç”¨ GPT-2 small æ¨¡å‹ç›¸åŒçš„é…ç½®ï¼Œå¹¶ç¡®ä¿ tokenizer å¤§å°ä¸æ¨¡å‹è¯æ±‡è¡¨å¤§å°åŒ¹é…ï¼Œä»¥åŠè®¾ç½® `bos` å’Œ `eos` ï¼ˆåºåˆ—çš„å¼€å§‹å’Œç»“æŸï¼‰ token IDsï¼š

{#if fw === 'pt'}

```python
#####Pytorch}
from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig

config = AutoConfig.from_pretrained(
    "gpt2",
    vocab_size=len(tokenizer),
    n_ctx=context_length,
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
)####end
```

æœ‰äº†è¿™ä¸ªé…ç½®ï¼Œæˆ‘ä»¬å¯ä»¥åŠ è½½ä¸€ä¸ªæ–°æ¨¡å‹ã€‚æ³¨æ„ï¼Œè¿™æ˜¯æˆ‘ä»¬ç¬¬ä¸€æ¬¡ä¸ä½¿ç”¨ `from_pretrained()` å‡½æ•°ï¼Œå› ä¸ºæˆ‘ä»¬å®é™…ä¸Šæ˜¯è‡ªå·±åˆå§‹åŒ–ä¸€ä¸ªå…¨æ–°çš„æ¨¡å‹ï¼š

```python
#####Pytorch}
model = GPT2LMHeadModel(config)
model_size = sum(t.numel() for t in model.parameters())
print(f"GPT-2 size: {model_size/1000**2:.1f}M parameters")####end
```

```python
#####Pytorch}
GPT-2 size: 124.2M parameters####end
```

{:else}

```python
#####TensorFlow}
from transformers import AutoTokenizer, TFGPT2LMHeadModel, AutoConfig

config = AutoConfig.from_pretrained(
    "gpt2",
    vocab_size=len(tokenizer),
    n_ctx=context_length,
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
)####end
```

æœ‰äº†è¿™ä¸ªé…ç½®ï¼Œæˆ‘ä»¬å¯ä»¥åŠ è½½ä¸€ä¸ªæ–°æ¨¡å‹ã€‚æ³¨æ„ï¼Œè¿™æ˜¯æˆ‘ä»¬ç¬¬ä¸€æ¬¡ä¸ä½¿ç”¨ `from_pretrained()` å‡½æ•°ï¼Œå› ä¸ºæˆ‘ä»¬å®é™…ä¸Šæ˜¯è‡ªå·±åˆå§‹åŒ–ä¸€ä¸ªå…¨æ–°çš„æ¨¡å‹ï¼š

```python
#####TensorFlow}
model = TFGPT2LMHeadModel(config)
model(model.dummy_inputs)  # æ„å»ºæ¨¡å‹
model.summary()####end
```

```python
#####TensorFlow}
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
transformer (TFGPT2MainLayer multiple)                  124242432 
=================================================================
Total params: 124,242,432
Trainable params: 124,242,432
Non-trainable params: 0
_________________________________________________________________####end
```

{/if}

æˆ‘ä»¬çš„æ¨¡å‹æœ‰ 124M ä¸ªå‚æ•°éœ€è¦è®­ç»ƒã€‚åœ¨å¼€å§‹è®­ç»ƒä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦è®¾ç½®ä¸€ä¸ªæ•°æ®æ•´ç†å™¨ï¼ˆDataCollatorï¼‰ï¼Œå®ƒå°†è´Ÿè´£åˆ›å»º Batchã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `DataCollatorForLanguageModeling` ï¼Œå®ƒä¸“é—¨ç”¨äºè¯­è¨€å»ºæ¨¡ï¼ˆæ­£å¦‚å…¶åå­—æ‰€æš—ç¤ºï¼‰ã€‚é™¤äº†å †å å’Œå¡«å……åˆ›å»º Batch ä¹‹å¤–ï¼Œå®ƒè¿˜è´Ÿè´£åˆ›å»ºè¯­è¨€æ¨¡å‹çš„æ ‡ç­¾ â€”â€” åœ¨å› æœè¯­è¨€å»ºæ¨¡ä¸­ï¼Œè¾“å…¥å°±æ˜¯æ ‡ç­¾ï¼ˆåªæ˜¯åç§»ä¸€ä¸ªå…ƒç´ ï¼‰ï¼Œè€Œè¿™ä¸ªæ•°æ®æ•´ç†å™¨ï¼ˆDataCollatorï¼‰åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å®æ—¶åˆ›å»ºå®ƒä»¬ï¼Œå› æ­¤æˆ‘ä»¬ä¸éœ€è¦å¤åˆ¶ `input_ids` ã€‚

æ³¨æ„ï¼Œ `DataCollatorForLanguageModeling` åŒæ—¶æ”¯æŒæ©ç è¯­è¨€å»ºæ¨¡ ï¼ˆMLMï¼‰ å’Œå› æœè¯­è¨€å»ºæ¨¡ ï¼ˆCLMï¼‰ã€‚é»˜è®¤æƒ…å†µä¸‹å®ƒä¼šä¸º MLM å‡†å¤‡æ•°æ®ï¼Œä½†æˆ‘ä»¬å¯ä»¥é€šè¿‡è®¾ç½® `mlm=False` å‚æ•°åˆ‡æ¢åˆ° CLMã€‚

{#if fw === 'pt'}

```python
#####Pytorch}
from transformers import DataCollatorForLanguageModeling

tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)####end
```

{:else}

```python
#####TensorFlow}
from transformers import DataCollatorForLanguageModeling

tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False, return_tensors="tf")####end
```

{/if}

è®©æˆ‘ä»¬çœ‹ä¸€ä¸ªä¾‹å­ï¼š

```python
out = data_collator([tokenized_dataset["train"][i] for i in range(5)])
for key in out:
    print(f"{key} shape: {out[key].shape}")
```

{#if fw === 'pt'}

```python
#####Pytorch}
input_ids shape: torch.Size([5, 128])
attention_mask shape: torch.Size([5, 128])
labels shape: torch.Size([5, 128])####end
```

{:else}

```python
#####TensorFlow}
input_ids shape: (5, 128)
attention_mask shape: (5, 128)
labels shape: (5, 128)####end
```

{/if}

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ç¤ºä¾‹å·²ç»å †å åœ¨ä¸€èµ·ï¼Œå¹¶ä¸”æ‰€æœ‰ tensor éƒ½å…·æœ‰ç›¸åŒçš„å½¢çŠ¶ã€‚

{#if fw === 'tf'}

ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `prepare_tf_dataset()` æ–¹æ³•ï¼Œå°†ä¸Šé¢åˆ›å»ºçš„æ•°æ®æ•´ç†å™¨ï¼ˆDataCollatorï¼‰å°†æ•°æ®é›†è½¬æ¢ä¸º TensorFlow æ•°æ®é›†ï¼š

```python
#####TensorFlow}
tf_train_dataset = model.prepare_tf_dataset(
    tokenized_dataset["train"],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=32,
)
tf_eval_dataset = model.prepare_tf_dataset(
    tokenized_dataset["valid"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=32,
)####end
```

{/if}

<div custom-style="Tip-yellow">

âš ï¸ è°ƒæ•´è¾“å…¥å’Œæ ‡ç­¾ä»¥å¯¹é½å®ƒä»¬çš„æ“ä½œå°†åœ¨æ¨¡å‹å†…éƒ¨è¿›è¡Œï¼Œæ‰€ä»¥æ•°æ®æ•´ç†å™¨ï¼ˆDataCollatorï¼‰åªéœ€å¤åˆ¶è¾“å…¥æ¥åˆ›å»ºæ ‡ç­¾ã€‚

</div>

ç°åœ¨æˆ‘ä»¬å·²ç»å‡†å¤‡å¥½äº†æ‰€æœ‰ä¸œè¥¿ï¼Œå¯ä»¥å¼€å§‹è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹äº†â€”â€”å¥½åƒä¹Ÿä¸æ˜¯é‚£ä¹ˆå›°éš¾ï¼åœ¨æˆ‘ä»¬å¼€å§‹è®­ç»ƒä¹‹å‰ï¼Œæˆ‘ä»¬åº”è¯¥ç™»å½•åˆ° Hugging Faceã€‚å¦‚æœä½ æ­£åœ¨ä½¿ç”¨ Notebook è¿›è¡Œå·¥ä½œï¼Œä½ å¯ä»¥ä½¿ç”¨ä¸‹é¢çš„å®ç”¨å‡½æ•°è¿›è¡Œç™»å½•ï¼š

```python
from huggingface_hub import notebook_login

notebook_login()
```

è¿™å°†æ˜¾ç¤ºä¸€ä¸ªå°éƒ¨ä»¶ï¼Œä½ å¯ä»¥åœ¨å…¶ä¸­è¾“å…¥ä½ çš„ Hugging Face ç™»å½•å‡­æ®ã€‚

å¦‚æœä½ ä¸æ˜¯åœ¨ Notebook ä¸Šå·¥ä½œï¼Œåªéœ€åœ¨ç»ˆç«¯ä¸­è¾“å…¥ä»¥ä¸‹è¡Œï¼š

```python
huggingface-cli login
```

{#if fw === 'pt'}

å‰©ä¸‹è¦åšçš„å°±æ˜¯é…ç½®è®­ç»ƒå‚æ•°å¹¶å¯åŠ¨ `Trainer` ï¼æˆ‘ä»¬å°†ä½¿ç”¨ä½™å¼¦å­¦ä¹ ç‡è°ƒåº¦ï¼Œå¹¶è¿›è¡Œä¸€äº› Warmup æœ‰æ•ˆ batch size æ˜¯ 256 ï¼ˆ `per_device_train_batch_size` * `gradient_accumulation_steps` ï¼‰ã€‚å½“å½“å•ä¸ª batch æ— æ³•æ”¾å…¥å†…å­˜æ—¶ï¼Œä¼šä½¿ç”¨æ¢¯åº¦ç´¯ç§¯ï¼Œå¹¶é€šè¿‡å¤šæ¬¡å‘å‰/å‘åä¼ é€’é€æ­¥å»ºç«‹æ¢¯åº¦ã€‚å½“æˆ‘ä»¬ä½¿ç”¨ Accelerate åˆ›å»ºè®­ç»ƒå¾ªç¯æ—¶ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°è¿™ä¸€ç‚¹ã€‚

```python
#####Pytorch}
from transformers import Trainer, TrainingArguments

args = TrainingArguments(
    output_dir="codeparrot-ds",
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    evaluation_strategy="steps",
    eval_steps=5_000,
    logging_steps=5_000,
    gradient_accumulation_steps=8,
    num_train_epochs=1,
    weight_decay=0.1,
    warmup_steps=1_000,
    lr_scheduler_type="cosine",
    learning_rate=5e-4,
    save_steps=5_000,
    fp16=True,
    push_to_hub=True,
)

trainer = Trainer(
    model=model,
    tokenizer=tokenizer,
    args=args,
    data_collator=data_collator,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["valid"],
)####end
```

ç°åœ¨æˆ‘ä»¬åªéœ€å¯åŠ¨ `Trainer` å¹¶ç­‰å¾…è®­ç»ƒå®Œæˆã€‚æ ¹æ®ä½ æ˜¯åœ¨æ•´ä¸ªè®­ç»ƒé›†è¿˜æ˜¯åœ¨è®­ç»ƒé›†çš„ä¸€ä¸ªå­é›†ä¸Šè¿è¡Œå®ƒï¼Œè¿™å°†åˆ†åˆ«éœ€è¦ 20 æˆ– 2 ä¸ªå°æ—¶ï¼Œå› æ­¤è¯·å–æ¯å’–å•¡æˆ–è€…æ‰¾ä¸€æœ¬å¥½ä¹¦æ¥é˜…è¯»ï¼

```python
#####Pytorch}
trainer.train()####end
```

è®­ç»ƒå®Œæˆåï¼Œæˆ‘ä»¬å¯ä»¥å°†æ¨¡å‹å’Œ tokenizer æ¨é€åˆ° Hubï¼š

```python
#####Pytorch}
trainer.push_to_hub()####end
```

{:else}

æˆ‘ä»¬éœ€è¦åšçš„å°±æ˜¯é…ç½®è®­ç»ƒè¶…å‚æ•°å¹¶è°ƒç”¨ `compile()` å’Œ `fit()` ã€‚æˆ‘ä»¬å°†ä½¿ç”¨å¸¦æœ‰ä¸€äº›é¢„çƒ­çš„å­¦ä¹ ç‡è°ƒæ•´ç­–ç•¥æ¥æé«˜è®­ç»ƒçš„ç¨³å®šæ€§ï¼š

```python
#####TensorFlow}
from transformers import create_optimizer
import tensorflow as tf

num_train_steps = len(tf_train_dataset)
optimizer, schedule = create_optimizer(
    init_lr=5e-5,
    num_warmup_steps=1_000,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)
model.compile(optimizer=optimizer)

## ä½¿ç”¨ float16 æ··åˆç²¾åº¦è¿›è¡Œè®­ç»ƒ
tf.keras.mixed_precision.set_global_policy("mixed_float16")####end
```

ç°åœ¨æˆ‘ä»¬åªéœ€è°ƒç”¨ `model.fit()` ï¼Œå¹¶ç­‰å¾…è®­ç»ƒå®Œæˆã€‚æ ¹æ®ä½ æ˜¯å¦åœ¨å®Œæ•´çš„è®­ç»ƒé›†æˆ–è€…è®­ç»ƒé›†çš„å­é›†ä¸Šè¿è¡Œï¼Œè¿™å°†åˆ†åˆ«éœ€è¦ 20 å°æ—¶æˆ–è€… 2 å°æ—¶ï¼Œæ‰€ä»¥æ‹¿ä¸€äº›å’–å•¡å’Œä¸€æœ¬å¥½ä¹¦æ¥é˜…è¯»ï¼è®­ç»ƒå®Œæˆåï¼Œæˆ‘ä»¬å¯ä»¥å°†æ¨¡å‹å’Œ tokenizer æ¨é€åˆ° Hubï¼š

```python
#####TensorFlow}
from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(output_dir="codeparrot-ds", tokenizer=tokenizer)

model.fit(tf_train_dataset, validation_data=tf_eval_dataset, callbacks=[callback])####end
```

{/if}

<div custom-style="Tip-green">

âœï¸ **è¯•è¯•çœ‹ï¼** é™¤äº† `TrainingArguments` ä¹‹å¤–ï¼Œæˆ‘ä»¬åªéœ€è¦å¤§çº¦ 30 è¡Œä»£ç å°±å¯ä»¥ä»åŸå§‹æ–‡æœ¬åˆ°è®­ç»ƒ GPT-2ã€‚ç”¨ä½ è‡ªå·±çš„æ•°æ®é›†è¯•è¯•çœ‹ï¼Œçœ‹çœ‹ä½ èƒ½ä¸èƒ½å¾—åˆ°å¥½çš„ç»“æœï¼

</div>

<div custom-style="Tip-green">

{#if fw === 'pt'}

ğŸ’¡ å¦‚æœä½ èƒ½ä½¿ç”¨å¤š GPU çš„æœºå™¨ï¼Œå°è¯•åœ¨é‚£é‡Œè¿è¡Œä»£ç ã€‚ `Trainer` è‡ªåŠ¨ç®¡ç†å¤šå°æœºå™¨ï¼Œè¿™èƒ½æå¤§åœ°åŠ å¿«è®­ç»ƒé€Ÿåº¦ã€‚

{:else}

ğŸ’¡ å¦‚æœä½ æ­£åœ¨ä½¿ç”¨å…·æœ‰å¤šä¸ª GPU çš„è®¡ç®—æœºï¼Œåˆ™å¯ä»¥å°è¯•ä½¿ç”¨ `MirroredStrategy` ä¸Šä¸‹æ–‡æ¥å¤§å¹…åŠ å¿«è®­ç»ƒé€Ÿåº¦ã€‚ä½ éœ€è¦åˆ›å»ºä¸€ä¸ª `tf.distribute.MirroredStrategy` å¯¹è±¡ï¼Œå¹¶ç¡®ä¿æ‰€æœ‰çš„ `to_tf_dataset` æˆ– `prepare_tf_dataset()` æ–¹æ³•ä»¥åŠæ¨¡å‹åˆ›å»ºå’Œå¯¹ `fit()` çš„è°ƒç”¨éƒ½åœ¨å…¶ `scope()` ä¸Šä¸‹æ–‡ä¸­è¿è¡Œã€‚ä½ å¯ä»¥åœ¨ [è¿™é‡Œ](https://www.tensorflow.org/guide/distributed_training#use_tfdistributestrategy_with_keras_modelfit)(https://www.tensorflow.org/guide/distributed_training#use_tfdistributestrategy_with_keras_modelfit) æŸ¥çœ‹æœ‰å…³æ­¤å†…å®¹çš„æ–‡æ¡£ã€‚

{/if}

</div>

### ä½¿ç”¨ pipeline è¿›è¡Œä»£ç ç”Ÿæˆ 

ç°åœ¨æ˜¯è§è¯å¥‡è¿¹çš„æ—¶åˆ»ï¼šæˆ‘ä»¬æ¥çœ‹çœ‹è®­ç»ƒå¥½çš„æ¨¡å‹åˆ°åº•è¡¨ç°å¦‚ä½•ï¼æˆ‘ä»¬å¯ä»¥åœ¨æ—¥å¿—ä¸­çœ‹åˆ°æŸå¤±æŒç»­ä¸‹é™ï¼Œä½†è¦æµ‹è¯•æ¨¡å‹çš„æ•ˆæœï¼Œæˆ‘ä»¬å°±çœ‹çœ‹å®ƒå¯¹ä¸€äº›æç¤ºçš„ååº”å¦‚ä½•ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†æ¨¡å‹åŒ…è£…åœ¨ä¸€ä¸ªæ–‡æœ¬ç”Ÿæˆçš„ `pipeline` ä¸­ï¼Œå¹¶å¦‚æœæœ‰ GPU å¯ç”¨ï¼Œæˆ‘ä»¬å°†æŠŠå®ƒæ”¾åœ¨ GPU ä¸Šè¿›è¡Œå¿«é€Ÿç”Ÿæˆï¼š

{#if fw === 'pt'}

```python
#####Pytorch}
import torch
from transformers import pipeline

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
pipe = pipeline(
    "text-generation", model="huggingface-course/codeparrot-ds", device=device
)####end
```

{:else}

```python
#####TensorFlow}
from transformers import pipeline

course_model = TFGPT2LMHeadModel.from_pretrained("huggingface-course/codeparrot-ds")
course_tokenizer = AutoTokenizer.from_pretrained("huggingface-course/codeparrot-ds")
pipe = pipeline(
    "text-generation", model=course_model, tokenizer=course_tokenizer, device=0
)####end
```

{/if}

è®©æˆ‘ä»¬ä»åˆ›å»ºæ•£ç‚¹å›¾çš„ç®€å•ä»»åŠ¡å¼€å§‹ï¼š

```python
txt = """\
## åˆ›å»ºä¸€äº›æ•°æ®
x = np.random.randn(100)
y = np.random.randn(100)

## ä½¿ç”¨ x,y åˆ›å»ºæ•£ç‚¹å›¾
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python
## åˆ›å»ºä¸€äº›æ•°æ®
x = np.random.randn(100)
y = np.random.randn(100)

## ä½¿ç”¨ x,y åˆ›å»ºæ•£ç‚¹å›¾
plt.scatter(x, y)

## åˆ›å»ºæ•£ç‚¹
```

ç»“æœçœ‹èµ·æ¥æ˜¯æ­£ç¡®çš„ã€‚é‚£ä¹ˆå¯¹äº `pandas` æ“ä½œä¹Ÿå¯ä»¥å—ï¼Ÿè®©æˆ‘ä»¬çœ‹çœ‹æ˜¯å¦èƒ½ä»ä¸¤ä¸ªæ•°ç»„åˆ›å»ºä¸€ä¸ª `DataFrame` ï¼š

```python
txt = """\
## åˆ›å»ºä¸€äº›æ•°æ®
x = np.random.randn(100)
y = np.random.randn(100)

## ä» x å’Œ y åˆ›å»º dataframe
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python
## åˆ›å»ºä¸€äº›æ•°æ®
x = np.random.randn(100)
y = np.random.randn(100)

## ä» x å’Œ y åˆ›å»º dataframe
df = pd.DataFrame({'x': x, 'y': y})
df.insert(0,'x', x)
for
```

å¾ˆå¥½ï¼Œè¿™æ˜¯æ­£ç¡®çš„ç­”æ¡ˆâ€”â€”å°½ç®¡å®ƒåˆæŠŠ `x` æ’å…¥äº†ä¸€æ¬¡ã€‚ç”±äºç”Ÿæˆçš„ token æ•°é‡æœ‰é™ï¼Œæ‰€ä»¥ä¸‹é¢çš„ `for` å¾ªç¯è¢«åˆ‡æ–­äº†ã€‚è®©æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬æ˜¯å¦èƒ½åšäº›æ›´å¤æ‚çš„äº‹æƒ…ï¼Œè®©æ¨¡å‹å¸®åŠ©æˆ‘ä»¬ä½¿ç”¨ `groupby` æ“ä½œï¼š

```python
txt = """\
## æœ‰èŒä¸š,æ”¶å…¥å’Œåå­—çš„ dataframe
df = pd.DataFrame({'profession': x, 'income':y, 'name': z})

## è®¡ç®—æ¯ä¸ªèŒä¸šçš„å¹³å‡æ”¶å…¥
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python
## æœ‰èŒä¸š,æ”¶å…¥å’Œåå­—çš„ dataframe
df = pd.DataFrame({'profession': x, 'income':y, 'name': z})

## è®¡ç®—æ¯ä¸ªèŒä¸šçš„å¹³å‡æ”¶å…¥
profession = df.groupby(['profession']).mean()

## è®¡ç®—
```

ä¸é”™ï¼›è¿™æ˜¯æ­£ç¡®çš„åšæ³•ã€‚æœ€åï¼Œè®©æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬æ˜¯å¦ä¹Ÿå¯ä»¥å°†å…¶ç”¨äº `scikit-learn` å¹¶å»ºç«‹ä¸€ä¸ªéšæœºæ£®æ—æ¨¡å‹ï¼š

```python
txt = """
## ä» scikit-learn å¯¼å…¥éšæœºæ£®æ—å›å½’å™¨
from sklearn.ensemble import RandomForestRegressor

## ç”¨ X, y æ‹Ÿåˆå¸¦æœ‰ 300 ä¸ªä¼°ç®—å™¨çš„éšæœºæ£®æ—æ¨¡å‹:
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python
## ä» scikit-learn å¯¼å…¥éšæœºæ£®æ—å›å½’å™¨
from sklearn.ensemble import RandomForestRegressor

## ç”¨ X, y æ‹Ÿåˆå¸¦æœ‰ 300 ä¸ªä¼°ç®—å™¨çš„éšæœºæ£®æ—æ¨¡å‹:
rf = RandomForestRegressor(n_estimators=300, random_state=random_state, max_depth=3)
rf.fit(X, y)
rf
```

{#if fw === 'tf'}

çœ‹çœ‹è¿™å‡ ä¸ªä¾‹å­ï¼Œä¼¼ä¹æ¨¡å‹å·²ç»å­¦ä¹ äº† Python æ•°æ®ç§‘å­¦å †æ ˆçš„ä¸€äº›è¯­æ³•ã€‚å½“ç„¶ï¼Œåœ¨å°†æ¨¡å‹éƒ¨ç½²åˆ°ç°å®ä¸–ç•Œä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦æ›´å½»åº•åœ°è¯„ä¼°æ¨¡å‹ï¼Œä½†è¿™ä»ç„¶æ˜¯ä¸€ä¸ªä»¤äººå°è±¡æ·±åˆ»çš„åŸå‹ã€‚

{:else}

ä»è¿™å‡ ä¸ªä¾‹å­æ¥çœ‹ï¼Œæ¨¡å‹ä¼¼ä¹å·²ç»å­¦ä¹ äº† Python æ•°æ®ç§‘å­¦å †æ ˆçš„ä¸€äº›è¯­æ³•ï¼ˆå½“ç„¶ï¼Œåœ¨å°†æ¨¡å‹éƒ¨ç½²åˆ°ç°å®ä¸–ç•Œä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å¯¹å…¶è¿›è¡Œæ›´å…¨é¢çš„è¯„ä¼°ï¼‰ã€‚ç„¶è€Œï¼Œæœ‰æ—¶å€™å®ƒéœ€è¦æ›´å¤šçš„æ¨¡å‹è®­ç»ƒå®šåˆ¶æ¥è¾¾åˆ°ç‰¹å®šæƒ…å¢ƒæ‰€éœ€çš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬æƒ³åŠ¨æ€æ›´æ–° `batch_size` æˆ–æ·»åŠ ä¸€ä¸ªæ¡ä»¶è®­ç»ƒå¾ªç¯æ¥è·³è¿‡åç¤ºä¾‹æ€ä¹ˆåŠï¼Ÿä¸€ç§é€‰æ‹©æ˜¯ä¿®æ”¹ `Trainer` æ·»åŠ æ–°çš„åŠŸèƒ½ï¼Œä½†æœ‰æ—¶ä»å¤´å¼€å§‹ç¼–å†™è®­ç»ƒå¾ªç¯ä¼šæ›´ç®€å•ã€‚è¿™å°±æ˜¯Accelerate çš„ç”¨æ­¦ä¹‹åœ°ã€‚

{/if}

{#if fw === 'pt'}

### ä½¿ç”¨Accelerate è¿›è¡Œè®­ç»ƒ 

æˆ‘ä»¬å·²ç»çœ‹åˆ°äº†å¦‚ä½•ä½¿ç”¨ `Trainer` è®­ç»ƒæ¨¡å‹ï¼Œè¿™å¯ä»¥å¯¹è®­ç»ƒè¿‡ç¨‹è¿›è¡Œä¸€äº›å®šåˆ¶ã€‚ç„¶è€Œï¼Œæœ‰æ—¶æˆ‘ä»¬æƒ³è¦å®Œå…¨æ§åˆ¶è®­ç»ƒå¾ªç¯ï¼Œæˆ–è€…æˆ‘ä»¬æƒ³è¦è¿›è¡Œä¸€äº›å¥‡ç‰¹çš„æ›´æ”¹ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ Accelerate æ˜¯ä¸€ä¸ªä¸é”™çš„é€‰æ‹©ï¼Œæœ¬èŠ‚æˆ‘ä»¬å°†ä»‹ç»å¦‚ä½•ä½¿ç”¨å®ƒæ¥è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ã€‚ä¸ºäº†è®©äº‹æƒ…å˜å¾—æ›´æœ‰è¶£ï¼Œæˆ‘ä»¬è¿˜å°†åœ¨è®­ç»ƒå¾ªç¯ä¸­æ·»åŠ ä¸€äº›ä¿®æ”¹ã€‚

ç”±äºæˆ‘ä»¬ä¸»è¦å…³æ³¨çš„æ˜¯ä¸ºæ•°æ®ç§‘å­¦åº“æä¾›åˆç†çš„ä»£ç è‡ªåŠ¨è¡¥å……åŠŸèƒ½ï¼Œå› æ­¤å¯¹äºæ›´å¤šä½¿ç”¨è¿™äº›åº“çš„è®­ç»ƒæ ·æœ¬èµ‹äºˆæ›´é«˜çš„æƒé‡æ˜¯æœ‰æ„ä¹‰çš„ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿ç”¨ `plt` ã€ `pd` ã€ `sk` ã€ `fit` å’Œ `predict` ç­‰å…³é”®è¯æ¥è½»æ¾åœ°è¯†åˆ«å‡ºè¿™äº›ä¾‹å­ï¼Œè¿™äº›å…³é”®è¯æ˜¯ `matplotlib.pyplot` ã€ `pandas` å’Œ `sklearn` çš„æœ€å¸¸ç”¨é‡å‘½ååçš„å¯¼å…¥åç§°ï¼Œä»¥åŠ sklearn çš„ fit/predict æ–¹æ³•ã€‚å¦‚æœè¿™äº›éƒ½è¡¨ç¤ºä¸ºä¸€ä¸ªå•ä¸€çš„ tokenï¼Œæˆ‘ä»¬å¯ä»¥è½»æ¾åœ°æ£€æŸ¥å®ƒä»¬æ˜¯å¦å‡ºç°åœ¨è¾“å…¥åºåˆ—ä¸­ã€‚Tokens æœ‰å¯èƒ½æœ‰ç©ºæ ¼å‰ç¼€ï¼Œæ‰€ä»¥æˆ‘ä»¬ä¹Ÿä¼šåœ¨ tokenizer è¯æ±‡è¡¨ä¸­æ£€æŸ¥è¿™äº›ç‰ˆæœ¬ã€‚ä¸ºäº†éªŒè¯å…¶æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬ä¼šæ·»åŠ ä¸€ä¸ªåº”è¯¥è¢«åˆ†å‰²ä¸ºå¤šä¸ª tokens çš„æµ‹è¯• tokenï¼š

```python
#####Pytorch}
keytoken_ids = []
for keyword in [
    "plt",
    "pd",
    "sk",
    "fit",
    "predict",
    " plt",
    " pd",
    " sk",
    " fit",
    " predict",
    "testtest",
]:
    ids = tokenizer([keyword]).input_ids[0]
    if len(ids) == 1:
        keytoken_ids.append(ids[0])
    else:
        print(f"Keyword has not single token: {keyword}")####end
```

```python
#####Pytorch}
'Keyword has not single token: testtest'####end
```

å¤ªå¥½äº†ï¼Œè¿™ä¸ªæ–¹æ³•ä¼¼ä¹å¾ˆæœ‰æ•ˆï¼æˆ‘ä»¬ç°åœ¨å¯ä»¥ç¼–å†™ä¸€ä¸ªè‡ªå®šä¹‰çš„æŸå¤±å‡½æ•°ï¼Œå®ƒå°†è¾“å…¥åºåˆ—ã€logits å’Œæˆ‘ä»¬åˆšåˆšé€‰æ‹©çš„å…³é”®ä½œä¸ºè¾“å…¥ã€‚é¦–å…ˆæˆ‘ä»¬éœ€è¦å¯¹é½ logits å’Œ inputsï¼šå³ç§»ä¸€ä¸ªå•ä½çš„è¾“å…¥åºåˆ—å½¢æˆäº†æ ‡ç­¾ï¼Œå› ä¸ºä¸‹ä¸€ä¸ª token å°±æ˜¯å½“å‰ token çš„æ ‡ç­¾ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡ä»è¾“å…¥åºåˆ—çš„ç¬¬äºŒä¸ª token å¼€å§‹è®¾ç½®æ ‡ç­¾ï¼Œå› ä¸ºæ¨¡å‹ä¸ä¼šé¢„æµ‹ç¬¬ä¸€ä¸ª tokenã€‚ç„¶åæˆ‘ä»¬æˆªæ–­æœ€åä¸€ä¸ª logitï¼Œå› ä¸ºæˆ‘ä»¬æ²¡æœ‰å®Œæ•´è¾“å…¥åºåˆ—åé¢çš„æ ‡ç­¾ã€‚æœ‰äº†è¿™äº›ï¼Œæˆ‘ä»¬å°±å¯ä»¥è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„æŸå¤±ï¼Œå¹¶è®¡ç®—æ¯ä¸ªæ ·æœ¬ä¸­æ‰€æœ‰å…³é”®è¯çš„å‡ºç°æ¬¡æ•°ã€‚æœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨å‡ºç°æ¬¡æ•°ä½œä¸ºæƒé‡ï¼Œè®¡ç®—æ‰€æœ‰æ ·æœ¬çš„åŠ æƒå¹³å‡å€¼ã€‚ç”±äºæˆ‘ä»¬ä¸æƒ³æŠ›å¼ƒæ‰€æœ‰æ²¡æœ‰å…³é”®è¯çš„æ ·æœ¬ï¼Œæˆ‘ä»¬å°†æ‰€æœ‰çš„æƒé‡éƒ½åŠ  1ï¼š

```python
#####Pytorch}
from torch.nn import CrossEntropyLoss
import torch

def keytoken_weighted_loss(inputs, logits, keytoken_ids, alpha=1.0):
    # å·¦ç§» tokens < n é¢„æµ‹ n
    shift_labels = inputs[..., 1:].contiguous()
    shift_logits = logits[..., :-1, :].contiguous()
    # è®¡ç®—æ¯ä¸€ä¸ªtokençš„loss
    loss_fct = CrossEntropyLoss(reduce=False)
    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
    # å¯¹äºæ¯ä¸ªæ ·æœ¬é‡æ–°è°ƒæ•´å¤§å°å¹¶å¹³å‡
    loss_per_sample = loss.view(shift_logits.size(0), shift_logits.size(1)).mean(axis=1)
    # è®¡ç®—å¹¶ç¼©æ”¾æƒé‡
    weights = torch.stack([(inputs == kt).float() for kt in keytoken_ids]).sum(
        axis=[0, 2]
    )
    weights = alpha * (1.0 + weights)
    # è®¡ç®—è¯„ä»·æƒé‡
    weighted_loss = (loss_per_sample * weights).mean()
    return weighted_loss####end
```

åœ¨æˆ‘ä»¬å¼€å§‹ä½¿ç”¨è¿™ä¸ªç²¾å¦™çš„æ–°æŸå¤±å‡½æ•°è¿›è¡Œè®­ç»ƒä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å‡†å¤‡ä¸€äº›äº‹æƒ…ï¼š

- æˆ‘ä»¬éœ€è¦æ•°æ®åŠ è½½å™¨æ¥æ‰¹é‡åŠ è½½æ•°æ®ã€‚
- æˆ‘ä»¬éœ€è¦è®¾ç½®æƒé‡è¡°å‡å‚æ•°ã€‚
- æœ‰æ—¶æˆ‘ä»¬å¸Œæœ›è¿›è¡Œè¯„ä¼°ï¼Œæ‰€ä»¥å°†è¯„ä¼°ä»£ç åŒ…è£…åœ¨ä¸€ä¸ªå‡½æ•°ä¸­æ˜¯æœ‰æ„ä¹‰çš„ã€‚

è®©æˆ‘ä»¬ä»æ•°æ®åŠ è½½å™¨å¼€å§‹ã€‚æˆ‘ä»¬åªéœ€è¦å°†æ•°æ®é›†çš„æ ¼å¼è®¾ç½®ä¸º `"torch"` ï¼Œç„¶åæˆ‘ä»¬å°±å¯ä»¥å°†å®ƒä¼ é€’ç»™ä¸€ä¸ªå…·æœ‰é€‚å½“ batch size çš„ PyTorch çš„ `DataLoader` ï¼š

```python
#####Pytorch}
from torch.utils.data.dataloader import DataLoader

tokenized_dataset.set_format("torch")
train_dataloader = DataLoader(tokenized_dataset["train"], batch_size=32, shuffle=True)
eval_dataloader = DataLoader(tokenized_dataset["valid"], batch_size=32)####end
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†å‚æ•°åˆ†ç»„ï¼Œä»¥ä¾¿ä¼˜åŒ–å™¨çŸ¥é“å“ªäº›å‚æ•°éœ€è¦è¿›è¡Œé¢å¤–çš„æƒé‡è¡°å‡ã€‚é€šå¸¸ï¼Œæ‰€æœ‰çš„åç½®å’Œ LayerNorm æƒé‡é¡¹éƒ½ä¸éœ€è¦ï¼›å› æ­¤æˆ‘ä»¬å¯ä»¥è¿™æ ·åšï¼š

```python
#####Pytorch}
weight_decay = 0.1

def get_grouped_params(model, no_decay=["bias", "LayerNorm.weight"]):
    params_with_wd, params_without_wd = [], []
    for n, p in model.named_parameters():
        if any(nd in n for nd in no_decay):
            params_without_wd.append(p)
        else:
            params_with_wd.append(p)
    return [
        {"params": params_with_wd, "weight_decay": weight_decay},
        {"params": params_without_wd, "weight_decay": 0.0},
    ]####end
```

æˆ‘ä»¬å¸Œæœ›åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å®šæœŸåœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹ï¼Œè®©æˆ‘ä»¬ä¸ºæ­¤ç¼–å†™ä¸€ä¸ªå‡½æ•°ã€‚å®ƒåªéœ€éå†è¯„ä¼°æ•°æ®åŠ è½½å™¨ï¼Œå¹¶æ”¶é›†æ‰€æœ‰è¿›ç¨‹ä¸­çš„æŸå¤±å€¼ï¼š

```python
#####Pytorch}
def evaluate():
    model.eval()
    losses = []
    for step, batch in enumerate(eval_dataloader):
        with torch.no_grad():
            outputs = model(batch["input_ids"], labels=batch["input_ids"])

        losses.append(accelerator.gather(outputs.loss))
    loss = torch.mean(torch.cat(losses))
    try:
        perplexity = torch.exp(loss)
    except OverflowError:
        perplexity = float("inf")
    return loss.item(), perplexity.item()####end
```

é€šè¿‡ `evaluate()` å‡½æ•°æˆ‘ä»¬å®šæœŸå¯ä»¥è·å–æŸå¤±å€¼å’Œ [å›°æƒ‘åº¦ï¼ˆperplexityï¼‰](/course/chapter7/3)(/course/chapter7/3) ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬é‡æ–°åŠ è½½æˆ‘ä»¬çš„æ¨¡å‹ä»¥ç¡®ä¿æˆ‘ä»¬å†æ¬¡ä»å¤´å¼€å§‹è®­ç»ƒï¼š

```python
#####Pytorch}
model = GPT2LMHeadModel(config)####end
```

ç„¶åæˆ‘ä»¬å¯ä»¥å®šä¹‰æˆ‘ä»¬çš„ä¼˜åŒ–å™¨ï¼Œä½¿ç”¨ä¹‹å‰çš„å‡½æ•°æ¥åˆ†å‰²æƒé‡è¡°å‡çš„å‚æ•°ï¼š

```python
#####Pytorch}
from torch.optim import AdamW

optimizer = AdamW(get_grouped_params(model), lr=5e-4)####end
```

ç°åœ¨è®©æˆ‘ä»¬å‡†å¤‡æ¨¡å‹ã€ä¼˜åŒ–å™¨å’Œæ•°æ®åŠ è½½å™¨ï¼Œç„¶åæˆ‘ä»¬å¯ä»¥å¼€å§‹è®­ç»ƒï¼š

```python
#####Pytorch}
from accelerate import Accelerator

accelerator = Accelerator(fp16=True)

model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)####end
```

<div custom-style="Tip-red">

ğŸš¨ å¦‚æœä½ åœ¨ TPU ä¸Šè®­ç»ƒï¼Œä½ éœ€è¦å°†ä¸Šè¿°å•å…ƒæ ¼å¼€å§‹çš„æ‰€æœ‰ä»£ç ç§»åˆ°ä¸€ä¸ªä¸“é—¨çš„è®­ç»ƒå‡½æ•°ä¸­ã€‚æ›´å¤šè¯¦æƒ…è¯·å‚é˜…ç¬¬å››ç« ã€‚

</div>

ç°åœ¨æˆ‘ä»¬å·²ç»å°†æˆ‘ä»¬çš„ `train_dataloader` ä¼ é€’ç»™äº† `accelerator.prepare()` ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `len()` æ¥è®¡ç®—è®­ç»ƒæ­¥éª¤çš„æ•°é‡ã€‚è¯·è®°ä½ï¼Œæˆ‘ä»¬åº”è¯¥åœ¨å‡†å¤‡å¥½ `dataloader` åå†ä½¿ç”¨ `len()` ï¼Œå› ä¸ºæ”¹åŠ¨ `dataloader` ä¼šæ”¹å˜å…¶é•¿åº¦ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªä»å­¦ä¹ ç‡åˆ° 0 çš„ç»å…¸çº¿æ€§å­¦ä¹ ç‡è°ƒåº¦ï¼š

```python
#####Pytorch}
num_train_epochs = 1
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    name="linear",
    optimizer=optimizer,
    num_warmup_steps=1_000,
    num_training_steps=num_training_steps,
)####end
```

æœ€åï¼Œä¸ºäº†å°†æˆ‘ä»¬çš„æ¨¡å‹æ¨é€åˆ° Hubï¼Œæˆ‘ä»¬éœ€è¦åœ¨ä¸€ä¸ªå·¥ä½œæ–‡ä»¶å¤¹ä¸­åˆ›å»ºä¸€ä¸ª `Repository` å¯¹è±¡ã€‚å¦‚æœä½ è¿˜æ²¡æœ‰ç™»å½•çš„è¯ï¼Œé¦–å…ˆéœ€è¦ç™»å½•åˆ° Hugging Faceï¼Œæˆ‘ä»¬å°†æ ¹æ®æ¨¡å‹ ID æ¥ç¡®å®šä»“åº“åç§°ï¼ˆä½ å¯ä»¥ä½¿ç”¨ä½ å–œæ¬¢çš„åå­—æ›¿æ¢ `repo_name` ï¼›å®ƒåªéœ€è¦åŒ…å«ä½ çš„ç”¨æˆ·åï¼Œå¯ä»¥ä½¿ç”¨ `get_full_repo_name()` å‡½æ•°çš„æŸ¥çœ‹ç›®å‰çš„ `repo_name`ï¼‰ï¼š

```python
#####Pytorch}
from huggingface_hub import Repository, get_full_repo_name

model_name = "codeparrot-ds-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name####end
```

```python
#####Pytorch}
'sgugger/codeparrot-ds-accelerate'####end
```

ç„¶åæˆ‘ä»¬å¯ä»¥å°†è¯¥ä»“åº“å…‹éš†åˆ°æœ¬åœ°æ–‡ä»¶å¤¹ä¸­ã€‚å¦‚æœå®ƒå·²ç»å­˜åœ¨ï¼Œè¿™ä¸ªæœ¬åœ°æ–‡ä»¶å¤¹åº”è¯¥æ˜¯æˆ‘ä»¬æ­£åœ¨ä½¿ç”¨çš„ä»“åº“çš„ç°æœ‰å…‹éš†ï¼š
```python
#####Pytorch}
output_dir = "codeparrot-ds-accelerate"
repo = Repository(output_dir, clone_from=repo_name)####end
```

æˆ‘ä»¬ç°åœ¨å¯ä»¥é€šè¿‡è°ƒç”¨ `repo.push_to_hub()` æ–¹æ³•ä¸Šä¼ ä¿å­˜åœ¨ `output_dir` ä¸­çš„æ‰€æœ‰å†…å®¹ã€‚è¿™å°†å¸®åŠ©æˆ‘ä»¬åœ¨æ¯ä¸ªè®­ç»ƒå‘¨æœŸç»“æŸæ—¶ä¸Šä¼ ä¸­é—´æ¨¡å‹ã€‚

```python
#####Pytorch}
evaluate()####end
```

```python
#####Pytorch}
(10.934126853942871, 56057.14453125)####end
```

ç›®å‰çš„æŸå¤±å’Œå›°æƒ‘åº¦éƒ½æ˜¯éå¸¸é«˜çš„å€¼ï¼Œä½†è¿™å¹¶ä¸å¥‡æ€ªï¼Œå› ä¸ºæˆ‘ä»¬è¿˜æ²¡æœ‰è®­ç»ƒæ¨¡å‹ã€‚æœ‰äº†è¿™äº›ï¼Œæˆ‘ä»¬å·²ç»ä¸ºç¼–å†™è®­ç»ƒè„šæœ¬çš„æ ¸å¿ƒéƒ¨åˆ†ï¼šè®­ç»ƒå¾ªç¯åšå¥½äº†å‡†å¤‡ã€‚åœ¨è®­ç»ƒå¾ªç¯ä¸­ï¼Œæˆ‘ä»¬è¿­ä»£éå†æ•°æ®åŠ è½½å™¨å¹¶å°†æˆæ‰¹é‡çš„æ•°æ®ä¼ é€’ç»™æ¨¡å‹ã€‚æœ‰äº† logitsï¼Œæˆ‘ä»¬å°±å¯ä»¥è¯„ä¼°æˆ‘ä»¬çš„è‡ªå®šä¹‰æŸå¤±å‡½æ•°ã€‚æˆ‘ä»¬é€šè¿‡æ¢¯åº¦ç´¯ç§¯æ­¥éª¤çš„æ•°é‡æ¥ç¼©æ”¾æŸå¤±ï¼Œä»¥é¿å…åœ¨èšåˆæ›´å¤šæ­¥éª¤æ—¶äº§ç”Ÿæ›´å¤§çš„æŸå¤±ã€‚åœ¨æˆ‘ä»¬ä¼˜åŒ–ä¹‹å‰ï¼Œæˆ‘ä»¬ä¹Ÿä¼šå‰ªè£æ¢¯åº¦æ¥æ›´å¥½çš„æ”¶æ•›ã€‚æœ€åï¼Œæ¯éš”ä¸€æ®µæ­¥æ•°ï¼Œæˆ‘ä»¬ç”¨æˆ‘ä»¬æ–°çš„ `evaluate()` å‡½æ•°åœ¨è¯„ä¼°é›†ä¸Šè¯„ä¼°æ¨¡å‹ï¼š

```python
#####Pytorch}
from tqdm.notebook import tqdm

gradient_accumulation_steps = 8
eval_steps = 5_000

model.train()
completed_steps = 0
for epoch in range(num_train_epochs):
    for step, batch in tqdm(
        enumerate(train_dataloader, start=1), total=num_training_steps
    ):
        logits = model(batch["input_ids"]).logits
        loss = keytoken_weighted_loss(batch["input_ids"], logits, keytoken_ids)
        if step % 100 == 0:
            accelerator.print(
                {
                    "lr": get_lr(),
                    "samples": step * samples_per_step,
                    "steps": completed_steps,
                    "loss/train": loss.item() * gradient_accumulation_steps,
                }
            )
        loss = loss / gradient_accumulation_steps
        accelerator.backward(loss)
        if step % gradient_accumulation_steps == 0:
            accelerator.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            lr_scheduler.step()
            optimizer.zero_grad()
            completed_steps += 1
        if (step % (eval_steps * gradient_accumulation_steps)) == 0:
            eval_loss, perplexity = evaluate()
            accelerator.print({"loss/eval": eval_loss, "perplexity": perplexity})
            model.train()
            accelerator.wait_for_everyone()
            unwrapped_model = accelerator.unwrap_model(model)
            unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
            if accelerator.is_main_process:
                tokenizer.save_pretrained(output_dir)
                repo.push_to_hub(
                    commit_message=f"Training in progress step {step}", blocking=False
                )####end
```

å°±æ˜¯è¿™æ · - ä½ ç°åœ¨æ‹¥æœ‰è‡ªå·±çš„å› æœè¯­è¨€æ¨¡å‹ï¼ˆä¾‹å¦‚ GPT-2ï¼‰çš„è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ï¼Œä½ å¯ä»¥æ ¹æ®è‡ªå·±çš„éœ€è¦è¿›ä¸€æ­¥å®šåˆ¶ã€‚

<div custom-style="Tip-green">

âœï¸ **è¯•è¯•çœ‹ï¼** åˆ›å»ºé€‚åˆä½ çš„ç”¨ä¾‹çš„è‡ªå®šä¹‰æŸå¤±å‡½æ•°ï¼Œæˆ–åœ¨è®­ç»ƒå¾ªç¯ä¸­æ·»åŠ å¦ä¸€ä¸ªè‡ªå®šä¹‰æ­¥éª¤ã€‚

</div>

<div custom-style="Tip-green">

âœï¸ **è¯•è¯•çœ‹ï¼**  å½“è¿è¡Œé•¿æ—¶é—´çš„è®­ç»ƒå®éªŒæ—¶ï¼Œä½¿ç”¨ TensorBoard æˆ– Weights & Biases ç­‰å·¥å…·è®°å½•é‡è¦æŒ‡æ ‡æ˜¯ä¸ªå¥½ä¸»æ„ã€‚å‘è®­ç»ƒå¾ªç¯ä¸­æ·»åŠ é€‚å½“çš„æ—¥å¿—è®°å½•ï¼Œè¿™æ ·ä½ å¯ä»¥éšæ—¶æ£€æŸ¥è®­ç»ƒè¿›åº¦ã€‚

</div>

{/if}


## 8.6 æŠ½å–å¼é—®ç­” 

ç°åœ¨æˆ‘ä»¬æ¥çœ‹çœ‹é—®ç­”è¿™ä¸ªä»»åŠ¡ï¼è¿™ä¸ªä»»åŠ¡æœ‰å¾ˆå¤šç§ç±»å‹ï¼Œä½†æˆ‘ä»¬åœ¨æœ¬èŠ‚å°†è¦å…³æ³¨çš„æ˜¯ç§°ä¸º `æŠ½å–å¼ï¼ˆextractiveï¼‰` é—®é¢˜å›ç­”çš„å½¢å¼ã€‚é’ˆå¯¹ä¸€äº›æ–‡æ¡£ä¼šæœ‰ä¸€äº›é—®é¢˜ï¼Œå…¶ä¸­ç­”æ¡ˆå°±åœ¨æ–‡æ¡£æ®µè½ä¹‹å†…ã€‚

æˆ‘ä»¬å°†ä½¿ç”¨ [SQuAD æ•°æ®é›†](https://rajpurkar.github.io/SQuAD-explorer/)(https://rajpurkar.github.io/SQuAD-explorer/) å¾®è°ƒä¸€ä¸ª BERT æ¨¡å‹ï¼Œå…¶ä¸­åŒ…æ‹¬ç¾¤ä¼—å·¥ä½œè€…å¯¹ä¸€ç»„ç»´åŸºç™¾ç§‘æ–‡ç« æå‡ºçš„é—®é¢˜ã€‚ä»¥ä¸‹æ˜¯ä¸€ä¸ªå°çš„æµ‹è¯•æ ·ä¾‹ï¼š

![æŠ½å–å¼é—®ç­”çš„ä¸€ä¸ªæ¼”ç¤ºä¾‹å­](./assets/qa-example.jpg)

æœ¬èŠ‚ä½¿ç”¨çš„ä»£ç å·²ç»ä¸Šä¼ åˆ°äº† Hubã€‚ä½ å¯ä»¥åœ¨ [è¿™é‡Œ](https://huggingface.co/huggingface-course/bert-finetuned-squad?context=%F0%9F%A4%97+Transformers+is+backed+by+the+three+most+popular+deep+learning+libraries+%E2%80%94+Jax%2C+PyTorch+and+TensorFlow+%E2%80%94+with+a+seamless+integration+between+them.+It%27s+straightforward+to+train+your+models+with+one+before+loading+them+for+inference+with+the+other.&question=Which+deep+learning+libraries+back+%F0%9F%A4%97+Transformers%3F)(https://huggingface.co/huggingface-course/bert-finetuned-squad?context=%F0%9F%A4%97+Transformers+is+backed+by+the+three+most+popular+deep+learning+libraries+%E2%80%94+Jax%2C+PyTorch+and+TensorFlow+%E2%80%94+with+a+seamless+integration+between+them.+It%27s+straightforward+to+train+your+models+with+one+before+loading+them+for+inference+with+the+other.&question=Which+deep+learning+libraries+back+%F0%9F%A4%97+Transformers%3F) æ‰¾åˆ°å®ƒå¹¶å°è¯•ç”¨å®ƒè¿›è¡Œé¢„æµ‹ã€‚

<div custom-style="Tip-green">

ğŸ’¡ åƒ BERT è¿™æ ·çš„çº¯ç¼–ç å™¨æ¨¡å‹å¾€å¾€å¾ˆæ“…é•¿æå–è¯¸å¦‚ â€œè°å‘æ˜äº† Transformer æ¶æ„ï¼Ÿâ€ä¹‹ç±»çš„äº‹å®æ€§é—®é¢˜çš„ç­”æ¡ˆã€‚ä½†åœ¨ç»™å‡ºè¯¸å¦‚ â€œä¸ºä»€ä¹ˆå¤©ç©ºæ˜¯è“è‰²çš„ï¼Ÿâ€ ä¹‹ç±»çš„å¼€æ”¾å¼é—®é¢˜æ—¶è¡¨ç°ä¸ä½³ã€‚åœ¨è¿™äº›æ›´å…·æŒ‘æˆ˜æ€§çš„æƒ…å†µä¸‹ï¼Œé€šå¸¸ä½¿ç”¨ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹å¦‚ T5 å’Œ BART æ¥ä»¥ç±»ä¼¼äº [æ–‡æœ¬æ‘˜è¦](https://chat.openai.com/course/chapter7/5)(https://chat.openai.com/course/chapter7/5) çš„æ–¹å¼æ•´åˆä¿¡æ¯ã€‚å¦‚æœä½ å¯¹è¿™ç§ `ç”Ÿæˆå¼ï¼ˆgenerativeï¼‰` é—®é¢˜å›ç­”æ„Ÿå…´è¶£ï¼Œæˆ‘ä»¬æ¨èä½ æŸ¥çœ‹æˆ‘ä»¬åŸºäº [ELI5 æ•°æ®é›†](https://huggingface.co/datasets/eli5)(https://huggingface.co/datasets/eli5) çš„ [æ¼”ç¤ºdemo](https://yjernite.github.io/lfqa.html)(https://yjernite.github.io/lfqa.html) ã€‚

### å‡†å¤‡æ•°æ® 

ä½œä¸ºæŠ½å–å¼é—®é¢˜å›ç­”çš„å­¦æœ¯åŸºå‡†æœ€å¸¸ç”¨çš„æ•°æ®é›†æ˜¯ [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/)(https://rajpurkar.github.io/SQuAD-explorer/) ï¼Œæ‰€ä»¥æˆ‘ä»¬åœ¨è¿™é‡Œå°†ä½¿ç”¨å®ƒã€‚è¿˜æœ‰ä¸€ä¸ªæ›´éš¾çš„ [SQuAD v2](https://huggingface.co/datasets/squad_v2)(https://huggingface.co/datasets/squad_v2) åŸºå‡†ï¼Œå…¶ä¸­åŒ…å«ä¸€äº›æ²¡æœ‰ç­”æ¡ˆçš„é—®é¢˜ã€‚åªè¦ä½ è‡ªå·±çš„æ•°æ®é›†åŒ…å«äº† Context åˆ—ã€é—®é¢˜åˆ—å’Œç­”æ¡ˆåˆ—ï¼Œä½ åº”è¯¥èƒ½å¤Ÿé€‚ç”¨ä¸‹é¢çš„æ­¥éª¤ã€‚

#### SQuAD æ•°æ®é›† 

åƒå¾€å¸¸ä¸€æ ·ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `load_dataset()` åœ¨ä¸€æ­¥ä¸­ä¸‹è½½å’Œç¼“å­˜æ•°æ®é›†ï¼š

```python
from datasets import load_dataset

raw_datasets = load_dataset("squad")
```

æˆ‘ä»¬å¯ä»¥æŸ¥çœ‹è¿™ä¸ª `raw_datasets` å¯¹è±¡æ¥äº†è§£æ›´å¤šå…³äº SQuAD æ•°æ®é›†çš„ä¿¡æ¯ï¼š

```python
raw_datasets
```

```python
DatasetDict({
    train: Dataset({
        features: ['id', 'title', 'context', 'question', 'answers'],
        num_rows: 87599
    })
    validation: Dataset({
        features: ['id', 'title', 'context', 'question', 'answers'],
        num_rows: 10570
    })
})
```

çœ‹èµ·æ¥æˆ‘ä»¬æ‹¥æœ‰æ‰€éœ€çš„ `context` ã€ `question` å’Œ `answers` å­—æ®µï¼Œæ‰€ä»¥è®©æˆ‘ä»¬æ‰“å°è®­ç»ƒé›†çš„ç¬¬ä¸€ä¸ªå…ƒç´ ï¼š

```python
print("Context: ", raw_datasets["train"][0]["context"])
print("Question: ", raw_datasets["train"][0]["question"])
print("Answer: ", raw_datasets["train"][0]["answers"])
```

```python
Context: 'Architecturally, the school has a Catholic character. Atop the Main Building\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend "Venite Ad Me Omnes". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.'
Question: 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?'
Answer: {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}
```
`context` å’Œ `question` å­—æ®µçš„ä½¿ç”¨éå¸¸ç›´è§‚ã€‚ `answers` å­—æ®µç›¸å¯¹å¤æ‚ä¸€äº›ï¼Œå› ä¸ºå®ƒåŒ…å«ä¸€ä¸ªå­—å…¸ï¼Œå…¶ä¸­æœ‰ä¸¤ä¸ªéƒ½æ˜¯åˆ—è¡¨çš„å­—æ®µã€‚è¿™æ˜¯åœ¨è¯„ä¼°æ—¶ `squad` æŒ‡æ ‡æ‰€æœŸæœ›çš„æ ¼å¼ï¼›å¦‚æœä½ ä½¿ç”¨çš„æ˜¯ä½ è‡ªå·±çš„æ•°æ®ï¼Œä½ ä¸å¿…ä¸€å®šå°†ç­”æ¡ˆæ”¾åœ¨åŒæ ·çš„æ ¼å¼ä¸­ã€‚ `text` å­—æ®µæ˜¯éå¸¸æ˜æ˜¾çš„ç­”æ¡ˆæ–‡æœ¬ï¼Œè€Œ `answer_start` å­—æ®µåŒ…å«äº† Context ä¸­æ¯ä¸ªç­”æ¡ˆçš„èµ·å§‹ç´¢å¼•ã€‚

åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œåªæœ‰ä¸€ä¸ªå¯èƒ½çš„ç­”æ¡ˆã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `Dataset.filter()` æ–¹æ³•æ¥è¿›è¡Œæ£€æŸ¥ï¼š

```python
raw_datasets["train"].filter(lambda x: len(x["answers"]["text"]) != 1)
```

```python
Dataset({
    features: ['id', 'title', 'context', 'question', 'answers'],
    num_rows: 0
})
```

ç„¶è€Œï¼Œåœ¨è¯„ä¼°è¿‡ç¨‹ä¸­ï¼Œæ¯ä¸ªæ ·æœ¬å¯èƒ½æœ‰å¤šä¸ªç­”æ¡ˆï¼Œè¿™äº›ç­”æ¡ˆå¯èƒ½ç›¸åŒæˆ–ä¸åŒï¼š

```python
print(raw_datasets["validation"][0]["answers"])
print(raw_datasets["validation"][2]["answers"])
```

```python
{'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'], 'answer_start': [177, 177, 177]}
{'text': ['Santa Clara, California', "Levi's Stadium", "Levi's Stadium in the San Francisco Bay Area at Santa Clara, California."], 'answer_start': [403, 355, 355]}
```

æˆ‘ä»¬ä¸ä¼šæ·±å…¥æ¢ç©¶è¯„ä¼°çš„ä»£ç ï¼Œå› ä¸ºæ‰€æœ‰çš„ä¸œè¥¿éƒ½å°†ç”±Datasets metric å¸®æˆ‘ä»¬å®Œæˆï¼Œä½†ç®€å•æ¥è¯´ï¼Œä¸€äº›é—®é¢˜å¯èƒ½æœ‰å¤šä¸ªå¯èƒ½çš„ç­”æ¡ˆï¼Œè€Œè¯¥è¯„ä¼°ä»£ç å°†æŠŠé¢„æµ‹çš„ç­”æ¡ˆä¸æ‰€æœ‰å¯æ¥å—çš„ç­”æ¡ˆè¿›è¡Œæ¯”è¾ƒï¼Œå¹¶é€‰æ‹©æœ€ä½³åˆ†æ•°ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬çœ‹ä¸€ä¸‹ç´¢å¼•ä¸º 2 çš„æ ·æœ¬ï¼š

```python
print(raw_datasets["validation"][2]["context"])
print(raw_datasets["validation"][2]["question"])
```

```python
'Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24â€“10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi\'s Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the "golden anniversary" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as "Super Bowl L"), so that the logo could prominently feature the Arabic numerals 50.'
'Where did Super Bowl 50 take place?'
```

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œç­”æ¡ˆçš„ç¡®å¯èƒ½æ˜¯æˆ‘ä»¬ä¹‹å‰çœ‹åˆ°çš„ä¸‰ä¸ªå¯èƒ½é€‰æ‹© `['Denver Broncos', 'Denver Broncos', 'Denver Broncos']` çš„ä¹‹ä¸€ã€‚

#### å¤„ç†è®­ç»ƒæ•°æ® 

æˆ‘ä»¬ä»é¢„å¤„ç†è®­ç»ƒæ•°æ®å¼€å§‹ã€‚æœ€å›°éš¾çš„éƒ¨åˆ†å°†æ˜¯ç”Ÿæˆé—®é¢˜ç­”æ¡ˆçš„æ ‡ç­¾ï¼Œå³æ‰¾åˆ° Context ä¸­å¯¹åº”ç­”æ¡ˆ token çš„èµ·å§‹å’Œç»“æŸä½ç½®ã€‚

ä½†æˆ‘ä»¬ä¸è¦æ€¥äºæ±‚æˆã€‚é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨ tokenizer å°†è¾“å…¥ä¸­çš„æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯ä»¥ç†è§£çš„ IDï¼š

```python
from transformers import AutoTokenizer

model_checkpoint = "bert-base-cased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
```

å¦‚å‰æ‰€è¿°ï¼Œæˆ‘ä»¬å°†å¯¹ BERT æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä½†ä½ å¯ä»¥ä½¿ç”¨ä»»ä½•å…¶ä»–æ¨¡å‹ç±»å‹ï¼Œåªè¦å®ƒå®ç°äº†å¿«é€Ÿ tokenizer å³å¯ã€‚ä½ å¯ä»¥åœ¨ [æ”¯æŒå¿«é€Ÿ tokenizer çš„æ¡†æ¶](https://huggingface.co/transformers/#supported-frameworks)(https://huggingface.co/transformers/#supported-frameworks) è¡¨ä¸­çœ‹åˆ°æ‰€æœ‰å¸¦æœ‰å¿«é€Ÿç‰ˆæœ¬çš„æ¶æ„ï¼Œè¦æ£€æŸ¥ä½ æ­£åœ¨ä½¿ç”¨çš„ `tokenizer` å¯¹è±¡æ˜¯å¦çœŸçš„æ˜¯ç”±Tokenizers æ”¯æŒçš„ï¼Œä½ å¯ä»¥æŸ¥çœ‹å®ƒçš„ `is_fast` å±æ€§ï¼š

```python
tokenizer.is_fast
```

```python
True
```

æˆ‘ä»¬å¯ä»¥å°† question å’Œ context ä¸€èµ·ä¼ é€’ç»™æˆ‘ä»¬çš„ tokenizer å®ƒä¼šæ­£ç¡®æ’å…¥ç‰¹æ®Š tokens å½¢æˆå¦‚ä¸‹å¥å­ï¼š

```python
[CLS] question [SEP] context [SEP]
```

è®©æˆ‘ä»¬ä»”ç»†æ£€æŸ¥ä¸€ä¸‹ï¼š

```python
context = raw_datasets["train"][0]["context"]
question = raw_datasets["train"][0]["question"]

inputs = tokenizer(question, context)
tokenizer.decode(inputs["input_ids"])
```

```python
'[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] Architecturally, '
'the school has a Catholic character. Atop the Main Building\'s gold dome is a golden statue of the Virgin '
'Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms '
'upraised with the legend " Venite Ad Me Omnes ". Next to the Main Building is the Basilica of the Sacred '
'Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a '
'replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette '
'Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 statues '
'and the Gold Dome ), is a simple, modern stone statue of Mary. [SEP]'
```

æ ‡ç­¾å°†æ˜¯æŒ‡ç¤ºç­”æ¡ˆèµ·å§‹å’Œç»“æŸ token çš„ç´¢å¼•ï¼Œæ¨¡å‹çš„ä»»åŠ¡æ˜¯ä¸ºè¾“å…¥ä¸­çš„æ¯ä¸ªæ ‡è®°é¢„æµ‹ä¸€ä¸ªèµ·å§‹å’Œç»“æŸçš„ logit å€¼ï¼Œç†è®ºä¸Šçš„æ ‡ç­¾å¦‚ä¸‹æ‰€ç¤ºï¼š

![åˆå§‹æ–‡æœ¬ä¸­çš„å•è¯èµ·å§‹å’Œç»“æŸçš„ logit å€¼](./assets/qa_labels.png "One-hot encoded labels for question answering.")

åœ¨åšä¸ªä¾‹å­ä¸­ï¼ŒContext æ²¡æœ‰å¾ˆé•¿ï¼Œä½†æ˜¯æ•°æ®é›†ä¸­çš„ä¸€äº›ç¤ºä¾‹çš„ Context ä¼šå¾ˆé•¿ï¼Œä¼šè¶…è¿‡æˆ‘ä»¬è®¾ç½®çš„æœ€å¤§é•¿åº¦ï¼ˆæœ¬ä¾‹ä¸­ä¸º 384ï¼‰ã€‚æ­£å¦‚æˆ‘ä»¬åœ¨ç¬¬ä¸ƒç« ä¸­æ‰€çœ‹åˆ°çš„ï¼Œå½“æˆ‘ä»¬æ¢ç´¢ `question-answering` ç®¡é“çš„å†…éƒ¨ç»“æ„æ—¶ï¼Œæˆ‘ä»¬ä¼šé€šè¿‡å°†ä¸€ä¸ªæ ·æœ¬çš„è¾ƒé•¿çš„ Context åˆ’åˆ†æˆå¤šä¸ªè®­ç»ƒç‰¹å¾ï¼Œå¹¶åœ¨è¿™äº›ç‰¹å¾ä¹‹é—´ä½¿ç”¨æ»‘åŠ¨çª—å£ï¼Œæ¥å¤„ç†è¾ƒé•¿çš„ Contextã€‚

è¦äº†è§£åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­å¯¹å½“å‰ç¤ºä¾‹è¿›è¡Œäº†å“ªäº›å¤„ç†ï¼Œæˆ‘ä»¬å¯ä»¥å°†é•¿åº¦é™åˆ¶ä¸º 100ï¼Œå¹¶ä½¿ç”¨é•¿åº¦ä¸º 50 çš„ token çª—å£ã€‚æˆ‘ä»¬è®¾ç½®ï¼š

- `max_length` æ¥è®¾ç½®æœ€å¤§é•¿åº¦ ï¼ˆè¿™é‡Œä¸º 100ï¼‰
- `truncation="only_second"` åœ¨é—®é¢˜å’Œ Context è¿‡é•¿æ—¶æˆªæ–­ Contextï¼ˆContext ä½äºç¬¬äºŒä¸ªä½ç½®ï¼‰
- `stride` è®¾ç½®ä¸¤ä¸ªè¿ç»­å—ä¹‹é—´çš„é‡å  tokens æ•° ï¼ˆè¿™é‡Œä¸º 50ï¼‰
- `return_overflowing_tokens=True` å‘Šè¯‰ tokenizer æˆ‘ä»¬æƒ³è¦ä¿ç•™è¶…è¿‡é•¿åº¦çš„ tokens 

```python
inputs = tokenizer(
    question,
    context,
    max_length=100,
    truncation="only_second",
    stride=50,
    return_overflowing_tokens=True,
)

for ids in inputs["input_ids"]:
    print(tokenizer.decode(ids))
```

```python
'[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] Architecturally, the school has a Catholic character. Atop the Main Building\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend " Venite Ad Me Omnes ". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basi [SEP]'
'[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend " Venite Ad Me Omnes ". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin [SEP]'
'[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP] Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 [SEP]'
'[CLS] To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? [SEP]. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive ( and in a direct line that connects through 3 statues and the Gold Dome ), is a simple, modern stone statue of Mary. [SEP]'
```

å¦‚æˆ‘ä»¬æ‰€è§ï¼Œæˆ‘ä»¬çš„ç¤ºä¾‹è¢«åˆ†æˆå››ä¸ªè¾“å…¥ï¼Œæ¯ä¸ªè¾“å…¥éƒ½åŒ…å«é—®é¢˜å’Œ Context çš„ä¸€éƒ¨åˆ†ã€‚è¯·æ³¨æ„ï¼Œé—®é¢˜çš„ç­”æ¡ˆ ï¼ˆâ€œBernadette Soubirousâ€ï¼‰ ä»…å‡ºç°åœ¨ç¬¬ä¸‰ä¸ªä¹Ÿæ˜¯æœ€åä¸€ä¸ªè¾“å…¥ä¸­ï¼Œå› æ­¤é€šè¿‡ä»¥è¿™ç§æ–¹å¼å¤„ç†è¾ƒé•¿çš„ Context æ—¶ï¼Œæˆ‘ä»¬å¯èƒ½åˆ›å»ºä¸€äº› Context ä¸­ä¸åŒ…å«ç­”æ¡ˆçš„è®­ç»ƒç¤ºä¾‹ã€‚æˆ‘ä»¬æŠŠè¿™äº›ç¤ºä¾‹çš„æ ‡ç­¾è®¾ç½®ä¸º `start_position = end_position = 0` ï¼ˆäº‹å®ä¸Šæˆ‘ä»¬åœ¨é¢„æµ‹ `[CLS]` tokensï¼‰ã€‚å¦‚æœä¸å¹¸ç­”æ¡ˆè¢«æˆªæ–­ï¼Œæˆ‘ä»¬åªæœ‰ç­”æ¡ˆçš„å¼€å¤´ï¼ˆæˆ–ç»“æŸï¼‰ï¼Œæˆ‘ä»¬ä¹Ÿè¿™æ ·è®¾ç½®è¿™äº›æ ‡ç­¾ã€‚å¯¹äºç­”æ¡ˆå®Œå…¨åœ¨ Context ä¸­çš„ç¤ºä¾‹ï¼Œæ ‡ç­¾å°†æ˜¯ç­”æ¡ˆèµ·å§‹çš„ token çš„ç´¢å¼•å’Œç­”æ¡ˆç»“æŸçš„ token çš„ç´¢å¼•ã€‚

æ•°æ®é›†ä¸ºæˆ‘ä»¬æä¾›äº† Context ä¸­ç­”æ¡ˆçš„èµ·å§‹å­—ç¬¦ï¼ŒåŠ ä¸Šç­”æ¡ˆçš„é•¿åº¦ï¼Œæˆ‘ä»¬å¯ä»¥æ‰¾åˆ° Context ä¸­çš„ç»“æŸå­—ç¬¦ã€‚è¦å°†å®ƒä»¬æ˜ å°„åˆ° tokens ç´¢å¼•ï¼Œæˆ‘ä»¬å°†éœ€è¦ä½¿ç”¨æˆ‘ä»¬åœ¨ç¬¬ä¸ƒç« ä¸­ç ”ç©¶çš„åç§»æ˜ å°„ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿ç”¨ `return_offsets_mapping=True` è®©æˆ‘ä»¬çš„ tokenizer è¿”å›è¿™äº›å†…å®¹ï¼š

```python
inputs = tokenizer(
    question,
    context,
    max_length=100,
    truncation="only_second",
    stride=50,
    return_overflowing_tokens=True,
    return_offsets_mapping=True,
)
inputs.keys()
```

```python
dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping'])
```

å¦‚æˆ‘ä»¬æ‰€è§ï¼Œæˆ‘ä»¬å¾—åˆ°äº† inputs IDã€tokens ç±»å‹ ID å’Œæ³¨æ„åŠ›æ©ç ï¼Œä»¥åŠæˆ‘ä»¬æ‰€éœ€çš„åç§»æ˜ å°„å’Œä¸€ä¸ªé¢å¤–çš„é”® `overflow_to_sample_mapping` ã€‚å½“æˆ‘ä»¬åŒæ—¶å¯¹å¤šä¸ªæ–‡æœ¬è¿›è¡Œ tokenize æ—¶ï¼Œä¸ºäº†ä»æ”¯æŒ Rust ä¸­å—ç›Šï¼Œè¿™ä¸ªé”®çš„å€¼å¯¹æˆ‘ä»¬å¾ˆæœ‰ç”¨ã€‚ç”±äºä¸€ä¸ªæ ·æœ¬å¯ä»¥äº§ç”Ÿå¤šä¸ªç‰¹å¾ï¼Œå®ƒå°†æ¯ä¸ªç‰¹å¾æ˜ å°„åˆ°å®ƒæ¥æºçš„ç¤ºä¾‹ã€‚å› ä¸ºè¿™é‡Œæˆ‘ä»¬åªå¯¹ä¸€ä¸ªç¤ºä¾‹è¿›è¡Œäº† tokenizeï¼Œæ‰€ä»¥æˆ‘ä»¬å¾—åˆ°ä¸€ä¸ªç”± `0` ç»„æˆçš„åˆ—è¡¨ï¼š

```python
inputs["overflow_to_sample_mapping"]
```

```python
[0, 0, 0, 0]
```

ä½†æ˜¯ï¼Œå¦‚æœæˆ‘ä»¬å¯¹æ›´å¤šçš„ç¤ºä¾‹è¿›è¡Œ tokenize è¿™å°†å˜å¾—æ›´åŠ æœ‰ç”¨ï¼š

```python
inputs = tokenizer(
    raw_datasets["train"][2:6]["question"],
    raw_datasets["train"][2:6]["context"],
    max_length=100,
    truncation="only_second",
    stride=50,
    return_overflowing_tokens=True,
    return_offsets_mapping=True,
)

print(f"The 4 examples gave {len(inputs['input_ids'])} features.")
print(f"Here is where each comes from: {inputs['overflow_to_sample_mapping']}.")
```

```python
'The 4 examples gave 19 features.'
'Here is where each comes from: [0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3].'
```

åœ¨æˆ‘ä»¬çš„è¿™ä¸ªä¾‹å­ä¸­ï¼Œå‰ä¸‰ä¸ªç¤ºä¾‹ ï¼ˆåœ¨è®­ç»ƒé›†ä¸­çš„ç´¢å¼• 2ã€3 å’Œ 4 å¤„ï¼‰ æ¯ä¸ªç¤ºä¾‹éƒ½ç»™å‡ºäº†å››ä¸ªç‰¹å¾ï¼Œæœ€åä¸€ä¸ªç¤ºä¾‹ï¼ˆåœ¨è®­ç»ƒé›†ä¸­çš„ç´¢å¼• 5 å¤„ï¼‰ ç»™å‡ºäº† 7 ä¸ªç‰¹å¾ã€‚

è¿™äº›ä¿¡æ¯å°†æœ‰åŠ©äºå°†æˆ‘ä»¬è·å¾—çš„æ¯ä¸ªç‰¹å¾æ˜ å°„åˆ°å…¶ç›¸åº”çš„æ ‡ç­¾ã€‚å¦‚å‰æ‰€è¿°ï¼Œè¿™äº›æ ‡ç­¾çš„è§„åˆ™æ˜¯ï¼š

- å¦‚æœç­”æ¡ˆä¸åœ¨ç›¸åº”ä¸Šä¸‹æ–‡çš„èŒƒå›´å†…ï¼Œåˆ™ä¸º `(0, 0)` 
- å¦‚æœç­”æ¡ˆåœ¨ç›¸åº”ä¸Šä¸‹æ–‡çš„èŒƒå›´å†…ï¼Œåˆ™ä¸º `(start_position, end_position)` ï¼Œå…¶ä¸­ `start_position` æ˜¯ç­”æ¡ˆèµ·å§‹å¤„çš„ token ç´¢å¼•ï¼ˆåœ¨ inputs ID ä¸­ï¼‰ï¼Œ `end_position` æ˜¯ç­”æ¡ˆç»“æŸå¤„çš„ token ç´¢å¼•ï¼ˆåœ¨ inputs ID ä¸­ï¼‰

ä¸ºäº†ç¡®å®šè¿™ä¸¤ç§æƒ…å†µä¸­çš„å“ªä¸€ç§ï¼Œå¹¶ä¸”å¦‚æœæ˜¯ç¬¬äºŒç§ï¼Œåˆ™éœ€è¦ç¡®å®š token çš„ä½ç½®ï¼Œæˆ‘ä»¬é¦–å…ˆæ‰¾åˆ°åœ¨è¾“å…¥ ID ä¸­èµ·å§‹å’Œç»“æŸä¸Šä¸‹æ–‡çš„ç´¢å¼•ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ token ç±»å‹ ID æ¥å®Œæˆæ­¤æ“ä½œï¼Œä½†ç”±äºå¹¶éæ‰€æœ‰æ¨¡å‹éƒ½éœ€è¦å®ƒä»¬ï¼ˆä¾‹å¦‚ DistilBERT ä¸éœ€è¦å®ƒä»¬ï¼‰ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ tokenizer çš„ `sequence_ids()` æ–¹æ³•è¿”å›çš„ `BatchEncoding` ã€‚

ä¸€æ—¦æˆ‘ä»¬æœ‰äº†è¿™äº› tokens çš„ç´¢å¼•ï¼Œæˆ‘ä»¬å°±ä¼šæŸ¥çœ‹ç›¸åº”çš„åç§»é‡ï¼Œå®ƒä»¬æ˜¯ä¸¤ä¸ªæ•´æ•°çš„å…ƒç»„ï¼Œè¡¨ç¤ºåŸå§‹ Context ä¸­çš„å­—ç¬¦èŒƒå›´ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥æ£€æµ‹æ­¤ç‰¹å¾ä¸­çš„ Context å—æ˜¯åœ¨ç­”æ¡ˆä¹‹åèµ·å§‹è¿˜æ˜¯åœ¨ç­”æ¡ˆèµ·å§‹ä¹‹å‰ç»“æŸï¼ˆåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ ‡ç­¾æ˜¯ `(0, 0)` ï¼‰ã€‚å¦‚æœä¸æ˜¯è¿™æ ·ï¼Œæˆ‘ä»¬å¾ªç¯æŸ¥æ‰¾ç­”æ¡ˆçš„ç¬¬ä¸€ä¸ªå’Œæœ€åä¸€ä¸ª tokenï¼š

```python
answers = raw_datasets["train"][2:6]["answers"]
start_positions = []
end_positions = []

for i, offset in enumerate(inputs["offset_mapping"]):
    sample_idx = inputs["overflow_to_sample_mapping"][i]
    answer = answers[sample_idx]
    start_char = answer["answer_start"][0]
    end_char = answer["answer_start"][0] + len(answer["text"][0])
    sequence_ids = inputs.sequence_ids(i)

    # æ‰¾åˆ°ä¸Šä¸‹æ–‡çš„èµ·å§‹å’Œç»“æŸ
    idx = 0
    while sequence_ids[idx] != 1:
        idx += 1
    context_start = idx
    while sequence_ids[idx] == 1:
        idx += 1
    context_end = idx - 1

    # å¦‚æœç­”æ¡ˆä¸å®Œå…¨åœ¨ä¸Šä¸‹æ–‡å†…,æ ‡ç­¾ä¸º(0, 0)
    if offset[context_start][0] > start_char or offset[context_end][1] < end_char:
        start_positions.append(0)
        end_positions.append(0)
    else:
        # å¦åˆ™,å®ƒå°±æ˜¯èµ·å§‹å’Œç»“æŸ token çš„ä½ç½®
        idx = context_start
        while idx <= context_end and offset[idx][0] <= start_char:
            idx += 1
        start_positions.append(idx - 1)

        idx = context_end
        while idx >= context_start and offset[idx][1] >= end_char:
            idx -= 1
        end_positions.append(idx + 1)

start_positions, end_positions
```

```python
([83, 51, 19, 0, 0, 64, 27, 0, 34, 0, 0, 0, 67, 34, 0, 0, 0, 0, 0],
 [85, 53, 21, 0, 0, 70, 33, 0, 40, 0, 0, 0, 68, 35, 0, 0, 0, 0, 0])
```

è®©æˆ‘ä»¬æŸ¥çœ‹ä¸€äº›ç»“æœä»¥éªŒè¯æˆ‘ä»¬çš„æ–¹æ³•æ˜¯å¦æ­£ç¡®ã€‚å¯¹äºç¬¬ä¸€ä¸ªç‰¹å¾ï¼Œæˆ‘ä»¬æ‰¾åˆ°äº† `(83, 85)` ä½œä¸ºæ ‡ç­¾ï¼Œå› æ­¤è®©æˆ‘ä»¬å°†ç†è®ºç­”æ¡ˆä¸ä» 83 åˆ° 85ï¼ˆåŒ…æ‹¬ 85ï¼‰çš„ tokens è§£ç çš„ç»“æœè¿›è¡Œæ¯”è¾ƒï¼š

```python
idx = 0
sample_idx = inputs["overflow_to_sample_mapping"][idx]
answer = answers[sample_idx]["text"][0]

start = start_positions[idx]
end = end_positions[idx]
labeled_answer = tokenizer.decode(inputs["input_ids"][idx][start : end + 1])

print(f"Theoretical answer: {answer}, labels give: {labeled_answer}")
```

```python
'Theoretical answer: the Main Building, labels give: the Main Building'
```

å¾ˆå¥½ï¼å¯»æ‰¾çš„ç­”æ¡ˆæ˜¯æ­£ç¡®çš„ï¼ç°åœ¨è®©æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹ç´¢å¼•ä¸º 4 çš„ä½ç½®ï¼Œæˆ‘ä»¬æˆ‘ä»¬å¾—åˆ°çš„æ ‡ç­¾æ˜¯ `(0, 0)` ï¼Œè¿™æ„å‘³ç€ç­”æ¡ˆä¸åœ¨è¯¥ç‰¹å¾çš„ä¸Šä¸‹æ–‡å—ä¸­ï¼š

```python
idx = 4
sample_idx = inputs["overflow_to_sample_mapping"][idx]
answer = answers[sample_idx]["text"][0]

decoded_example = tokenizer.decode(inputs["input_ids"][idx])
print(f"Theoretical answer: {answer}, decoded example: {decoded_example}")
```

```python
'Theoretical answer: a Marian place of prayer and reflection, decoded example: [CLS] What is the Grotto at Notre Dame? [SEP] Architecturally, the school has a Catholic character. Atop the Main Building\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend " Venite Ad Me Omnes ". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grot [SEP]'
```

ç¡®å®ï¼Œæˆ‘ä»¬åœ¨ Context ä¸­æ²¡æœ‰çœ‹åˆ°ç­”æ¡ˆã€‚

<div custom-style="Tip-green">

âœï¸ **è½®ä½ æ¥äº†ï¼** åœ¨ä½¿ç”¨ XLNet æ¶æ„æ—¶ï¼Œéœ€è¦åœ¨å·¦ä¾§è¿›è¡Œå¡«å……ï¼Œå¹¶ä¸”é—®é¢˜å’Œ Context åº”è¯¥äº’æ¢ã€‚å°†æˆ‘ä»¬åˆšåˆšçœ‹åˆ°çš„æ‰€æœ‰ä»£ç è°ƒæ•´ä¸º XLNet æ¶æ„ï¼ˆå¹¶æ·»åŠ  `padding=True` ï¼‰ã€‚è¯·æ³¨æ„ï¼Œå¡«å……åçš„ `[CLS]` tokens å¯èƒ½ä¸åœ¨ç´¢å¼•ä¸º 0 çš„ä½ç½®ã€‚

</div>

ç°åœ¨ï¼Œæˆ‘ä»¬å·²ç»é€æ­¥äº†è§£äº†å¦‚ä½•é¢„å¤„ç†æˆ‘ä»¬çš„è®­ç»ƒæ•°æ®ï¼Œæˆ‘ä»¬å¯ä»¥å°†å…¶ç»„åˆåˆ°ä¸€ä¸ªå‡½æ•°ä¸­ï¼Œå¹¶ä½¿ç”¨è¯¥å‡½æ•°å¤„ç†æ•´ä¸ªè®­ç»ƒæ•°æ®é›†ã€‚æˆ‘ä»¬å°†æ¯ä¸ªç‰¹å¾éƒ½å¡«å……åˆ°æˆ‘ä»¬è®¾ç½®çš„æœ€å¤§é•¿åº¦ï¼Œå› ä¸ºå¤§å¤šæ•°ä¸Šä¸‹æ–‡éƒ½å¾ˆé•¿ï¼ˆç›¸åº”çš„æ ·æœ¬å°†è¢«åˆ†æˆå‡ ä¸ªç‰¹å¾ï¼‰ï¼Œæ‰€ä»¥åœ¨è¿™é‡Œè¿›è¡ŒåŠ¨æ€å¡«å……æ²¡æœ‰å¸¦æ¥çœŸæ­£æœ‰æ•ˆçš„æˆæ•ˆï¼š

```python
max_length = 384
stride = 128

def preprocess_training_examples(examples):
    questions = [q.strip() for q in examples["question"]]
    inputs = tokenizer(
        questions,
        examples["context"],
        max_length=max_length,
        truncation="only_second",
        stride=stride,
        return_overflowing_tokens=True,
        return_offsets_mapping=True,
        padding="max_length",
    )

    offset_mapping = inputs.pop("offset_mapping")
    sample_map = inputs.pop("overflow_to_sample_mapping")
    answers = examples["answers"]
    start_positions = []
    end_positions = []

    for i, offset in enumerate(offset_mapping):
        sample_idx = sample_map[i]
        answer = answers[sample_idx]
        start_char = answer["answer_start"][0]
        end_char = answer["answer_start"][0] + len(answer["text"][0])
        sequence_ids = inputs.sequence_ids(i)

        # æ‰¾åˆ°ä¸Šä¸‹æ–‡çš„èµ·å§‹å’Œç»“æŸ
        idx = 0
        while sequence_ids[idx] != 1:
            idx += 1
        context_start = idx
        while sequence_ids[idx] == 1:
            idx += 1
        context_end = idx - 1

        # å¦‚æœç­”æ¡ˆä¸å®Œå…¨åœ¨ä¸Šä¸‹æ–‡å†…,æ ‡ç­¾ä¸º(0, 0)
        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:
            start_positions.append(0)
            end_positions.append(0)
        else:
            # å¦åˆ™,å®ƒå°±æ˜¯èµ·å§‹å’Œç»“æŸ tokens çš„ä½ç½®
            idx = context_start
            while idx <= context_end and offset[idx][0] <= start_char:
                idx += 1
            start_positions.append(idx - 1)

            idx = context_end
            while idx >= context_start and offset[idx][1] >= end_char:
                idx -= 1
            end_positions.append(idx + 1)

    inputs["start_positions"] = start_positions
    inputs["end_positions"] = end_positions
    return inputs
```

è¯·æ³¨æ„ï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸¤ä¸ªå¸¸é‡æ¥ç¡®å®šæ‰€ä½¿ç”¨çš„æœ€å¤§é•¿åº¦ä»¥åŠæ»‘åŠ¨çª—å£çš„é•¿åº¦ï¼Œå¹¶ä¸”åœ¨ä¹‹å‰ tokenize ä¹‹å‰å¯¹æ•°æ®è¿›è¡Œäº†ä¸€äº›æ¸…æ´—ï¼šSQuAD æ•°æ®é›†ä¸­çš„ä¸€äº›é—®é¢˜åœ¨å¼€å¤´å’Œç»“å°¾æœ‰é¢å¤–çš„ç©ºæ ¼ï¼Œè¿™äº›ç©ºæ ¼æ²¡æœ‰ä»»ä½•æ„ä¹‰ï¼ˆå¦‚æœä½ ä½¿ç”¨åƒ RoBERTa è¿™æ ·çš„æ¨¡å‹ï¼Œå®ƒä»¬ä¼šå ç”¨ tokenize æ—¶çš„ç©ºé—´ï¼‰ï¼Œå› æ­¤æˆ‘ä»¬å»æ‰äº†è¿™äº›é¢å¤–çš„ç©ºæ ¼ã€‚

è¦ä½¿ç”¨è¯¥å‡½æ•°å¤„ç†æ•´ä¸ªè®­ç»ƒé›†ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `Dataset.map()` æ–¹æ³•å¹¶ä¼ é€’ `batched=True` å‚æ•°ã€‚è¿™æ˜¯å¿…è¦çš„ï¼Œå› ä¸ºæˆ‘ä»¬æ­£åœ¨æ›´æ”¹æ•°æ®é›†çš„é•¿åº¦ï¼ˆå› ä¸ºä¸€ä¸ªç¤ºä¾‹å¯èƒ½ä¼šäº§ç”Ÿå¤šä¸ªè®­ç»ƒç‰¹å¾ï¼‰ï¼š

```python
train_dataset = raw_datasets["train"].map(
    preprocess_training_examples,
    batched=True,
    remove_columns=raw_datasets["train"].column_names,
)
len(raw_datasets["train"]), len(train_dataset)
```

```python
(87599, 88729)
```

å¦‚æˆ‘ä»¬æ‰€è§ï¼Œé¢„å¤„ç†æ·»åŠ äº†å¤§çº¦ 1000 ä¸ªç‰¹å¾ã€‚æˆ‘ä»¬çš„è®­ç»ƒé›†ç°åœ¨å·²ç»å‡†å¤‡å¥½ä½¿ç”¨äº†â€”â€”è®©æˆ‘ä»¬æ·±å…¥ç ”ç©¶ä¸€ä¸‹éªŒè¯é›†çš„é¢„å¤„ç†ï¼

#### å¤„ç†éªŒè¯æ•°æ® 

å¤„ç†éªŒè¯æ•°æ®çš„é¢„å¤„ç†ä¼šæ›´åŠ å®¹æ˜“ï¼Œå› ä¸ºæˆ‘ä»¬ä¸éœ€è¦ç”Ÿæˆæ ‡ç­¾ï¼ˆé™¤éæˆ‘ä»¬æƒ³è®¡ç®—éªŒè¯æŸå¤±ï¼Œä½†é‚£ä¸ªæ•°å­—å¹¶ä¸èƒ½çœŸæ­£å¸®åŠ©æˆ‘ä»¬äº†è§£æ¨¡å‹çš„å¥½åï¼‰ã€‚çœŸæ­£çš„æŒ‘æˆ˜åœ¨äºå°†æ¨¡å‹çš„é¢„æµ‹è½¬åŒ–ä¸ºä¸ºåŸå§‹ Context çš„ç‰‡æ®µã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬åªéœ€è¦å­˜å‚¨åç§»æ˜ å°„å¹¶ä¸”æ‰¾åˆ°ä¸€ç§æ–¹æ³•æ¥å°†æ¯ä¸ªåˆ›å»ºçš„ç‰¹å¾ä¸å…¶æ¥è‡ªçš„åŸå§‹ç¤ºä¾‹åŒ¹é…èµ·æ¥ã€‚ç”±äºåŸå§‹æ•°æ®é›†ä¸­æœ‰ä¸€ä¸ª ID åˆ—ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨è¯¥åˆ—ã€‚

æˆ‘ä»¬å”¯ä¸€è¦æ·»åŠ çš„æ˜¯å¯¹åç§»æ˜ å°„çš„å¾®å°ä¿®æ”¹ã€‚åç§»æ˜ å°„åŒ…å«é—®é¢˜å’Œ Context çš„åç§»é‡ï¼Œä½†ä¸€æ—¦æˆ‘ä»¬è¿›å…¥åå¤„ç†é˜¶æ®µï¼Œæˆ‘ä»¬å°†æ— æ³•çŸ¥é“ inputs ID çš„å“ªä¸ªéƒ¨åˆ†å¯¹åº”äº Contextï¼Œå“ªä¸ªéƒ¨åˆ†æ˜¯é—®é¢˜ï¼ˆæˆ‘ä»¬ä½¿ç”¨çš„ `sequence_ids()` æ–¹æ³•ä»…å¯ç”¨äº tokenizer çš„è¾“å‡ºï¼‰ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†å°†ä¸é—®é¢˜å¯¹åº”çš„åç§»è®¾ç½®ä¸º `None` Context å¯¹åº”çš„åç§»é‡ä¿æŒä¸å˜ï¼š

```python
def preprocess_validation_examples(examples):
    questions = [q.strip() for q in examples["question"]]
    inputs = tokenizer(
        questions,
        examples["context"],
        max_length=max_length,
        truncation="only_second",
        stride=stride,
        return_overflowing_tokens=True,
        return_offsets_mapping=True,
        padding="max_length",
    )

    sample_map = inputs.pop("overflow_to_sample_mapping")
    example_ids = []

    for i in range(len(inputs["input_ids"])):
        sample_idx = sample_map[i]
        example_ids.append(examples["id"][sample_idx])

        sequence_ids = inputs.sequence_ids(i)
        offset = inputs["offset_mapping"][i]
        inputs["offset_mapping"][i] = [
            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)
        ]

    inputs["example_id"] = example_ids
    return inputs
```

æˆ‘ä»¬å¯ä»¥åƒä»¥å‰ä¸€æ ·ä½¿ç”¨æ­¤å‡½æ•°å¤„ç†æ•´ä¸ªéªŒè¯æ•°æ®é›†ï¼š

```python
validation_dataset = raw_datasets["validation"].map(
    preprocess_validation_examples,
    batched=True,
    remove_columns=raw_datasets["validation"].column_names,
)
len(raw_datasets["validation"]), len(validation_dataset)
```

```python
(10570, 10822)
```

åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬åªæ·»åŠ äº†å‡ ç™¾ä¸ªæ ·æœ¬ï¼Œå› æ­¤éªŒè¯æ•°æ®é›†ä¸­çš„ Context ä¼¼ä¹è¦çŸ­ä¸€äº›ã€‚

ç°åœ¨æˆ‘ä»¬å·²ç»å¯¹æ‰€æœ‰æ•°æ®è¿›è¡Œäº†é¢„å¤„ç†ï¼Œæˆ‘ä»¬å¯ä»¥å¼€å§‹è®­ç»ƒäº†ã€‚

{#if fw === 'pt'}

### ä½¿ç”¨ `Trainer` API å¾®è°ƒæ¨¡å‹ 

è¿™ä¸ªä¾‹å­çš„è®­ç»ƒä»£ç ä¸å‰é¢çš„éƒ¨åˆ†éå¸¸ç›¸ä¼¼ï¼Œæœ€å›°éš¾çš„éƒ¨åˆ†æ˜¯ç¼–å†™ `compute_metrics()` å‡½æ•°ã€‚ç”±äºæˆ‘ä»¬å°†æ‰€æœ‰æ ·æœ¬å¡«å……åˆ°äº†æˆ‘ä»¬è®¾ç½®çš„æœ€å¤§é•¿åº¦ï¼Œæ‰€ä»¥æ²¡æœ‰éœ€è¦å®šä¹‰çš„æ•°æ®æ•´ç†å™¨ï¼Œå› æ­¤æˆ‘ä»¬å”¯ä¸€éœ€è¦æ‹…å¿ƒçš„äº‹æƒ…æ˜¯è®¡ç®—è¯„ä¼°æŒ‡æ ‡ã€‚æ¯”è¾ƒå›°éš¾çš„éƒ¨åˆ†å°†æ˜¯å°†æ¨¡å‹é¢„æµ‹çš„ç»“æœè¿˜åŸåˆ°åŸå§‹ç¤ºä¾‹ä¸­çš„æ–‡æœ¬ç‰‡æ®µï¼›ä¸€æ—¦æˆ‘ä»¬å®Œæˆäº†è¿™ä¸€æ­¥éª¤ï¼ŒDatasets åº“ä¸­çš„ metric å°±å¯ä»¥ä¸ºæˆ‘ä»¬åšå¤§éƒ¨åˆ†å·¥ä½œã€‚

{:else}

### ä½¿ç”¨ Keras å¾®è°ƒæ¨¡å‹ 

è¿™ä¸ªä¾‹å­çš„è®­ç»ƒä»£ç ä¸å‰é¢çš„éƒ¨åˆ†éå¸¸ç›¸ä¼¼ï¼Œæœ€å›°éš¾çš„éƒ¨åˆ†æ˜¯è®¡ç®—è¯„ä¼°æŒ‡æ ‡ã€‚ç”±äºæˆ‘ä»¬å°†æ‰€æœ‰æ ·æœ¬å¡«å……åˆ°äº†æˆ‘ä»¬è®¾ç½®çš„æœ€å¤§é•¿åº¦ï¼Œæ‰€ä»¥æ²¡æœ‰éœ€è¦å®šä¹‰çš„æ•°æ®æ•´ç†å™¨ï¼Œå› æ­¤æˆ‘ä»¬å”¯ä¸€éœ€è¦æ‹…å¿ƒçš„äº‹æƒ…æ˜¯è®¡ç®—è¯„ä¼°æŒ‡æ ‡ã€‚æ¯”è¾ƒå›°éš¾çš„éƒ¨åˆ†å°†æ˜¯å°†æ¨¡å‹é¢„æµ‹çš„ç»“æœè¿˜åŸåˆ°åŸå§‹ç¤ºä¾‹ä¸­çš„æ–‡æœ¬ç‰‡æ®µï¼›ä¸€æ—¦æˆ‘ä»¬å®Œæˆäº†è¿™ä¸€æ­¥éª¤ï¼ŒDatasets åº“ä¸­çš„ metric å°±å¯ä»¥ä¸ºæˆ‘ä»¬åšå¤§éƒ¨åˆ†å·¥ä½œã€‚

{/if}

#### åå¤„ç† 

æ¨¡å‹å°†è¾“å‡ºç­”æ¡ˆåœ¨ inputs ID ä¸­èµ·å§‹å’Œç»“æŸä½ç½®çš„ logitï¼Œæ­£å¦‚æˆ‘ä»¬åœ¨æ¢ç´¢ [`question-answering` pipeline](/course/chapter6/3b)(/course/chapter6/3b) æ—¶çœ‹åˆ°çš„é‚£æ ·ã€‚åå¤„ç†æ­¥éª¤å°†ç±»ä¼¼äºæˆ‘ä»¬åœ¨é‚£é‡Œæ‰€åšçš„ï¼Œæ‰€ä»¥è¿™é‡Œç®€å•å›é¡¾ä¸€ä¸‹æˆ‘ä»¬æ‰€é‡‡å–çš„æ“ä½œï¼š

- æˆ‘ä»¬å±è”½äº†é™¤äº† Context ä¹‹å¤–çš„ tokens å¯¹åº”çš„èµ·å§‹å’Œç»“æŸ logitã€‚
- ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨ softmax å°†èµ·å§‹å’Œç»“æŸ logits è½¬æ¢ä¸ºæ¦‚ç‡ã€‚
- æˆ‘ä»¬é€šè¿‡å°†ä¸¤ä¸ªæ¦‚ç‡å¯¹åº”çš„ä¹˜ç§¯æ¥ä¸ºæ¯ä¸ª `(start_token, end_token)` å¯¹åˆ†é…ä¸€ä¸ªåˆ†æ•°ã€‚
- æˆ‘ä»¬å¯»æ‰¾å…·æœ‰æœ€å¤§åˆ†æ•°ä¸”äº§ç”Ÿæœ‰æ•ˆç­”æ¡ˆï¼ˆä¾‹å¦‚ï¼Œ `start_token` å°äº `end_token` ï¼‰çš„å¯¹ã€‚

è¿™æ¬¡æˆ‘ä»¬å°†ç¨å¾®æ”¹å˜è¿™ä¸ªæµç¨‹ï¼Œå› ä¸ºæˆ‘ä»¬ä¸éœ€è¦è®¡ç®—å®é™…åˆ†æ•°ï¼ˆåªéœ€è¦é¢„æµ‹çš„ç­”æ¡ˆï¼‰ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬å¯ä»¥è·³è¿‡ softmax æ­¥éª¤ï¼ˆå› ä¸º softmax å¹¶ä¸ä¼šæ”¹å˜åˆ†æ•°çš„é¡ºåºï¼‰ã€‚ä¸ºäº†åŠ å¿«è®¡ç®—é€Ÿåº¦ï¼Œæˆ‘ä»¬ä¹Ÿä¸ä¼šä¸ºæ‰€æœ‰å¯èƒ½çš„ `(start_token, end_token)` å¯¹è®¡ç®—åˆ†æ•°ï¼Œè€Œåªä¼šè®¡ç®—ä¸æœ€é«˜çš„ `n_best` å¯¹åº”çš„ logit åˆ†æ•°ï¼ˆå…¶ä¸­ `n_best=20` ï¼‰ã€‚ç”±äºæˆ‘ä»¬å°†è·³è¿‡ softmaxï¼Œè¿™äº›åˆ†æ•°å°†æ˜¯ logit åˆ†æ•°ï¼Œè€Œä¸”æ˜¯èµ·å§‹å’Œç»“æŸå¯¹æ•°å‡ ç‡çš„å’Œï¼ˆè€Œä¸æ˜¯ä¹˜ç§¯ï¼Œå› ä¸ºè§„åˆ™ \($\log(ab) = \log(a) + \log(b))$ã€‚

ä¸ºäº†è¯æ˜è¿™ä¸€åˆ‡ï¼Œæˆ‘ä»¬éœ€è¦ä¸€äº›é¢„æµ‹ã€‚ç”±äºæˆ‘ä»¬è¿˜æ²¡æœ‰è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ QA ç®¡é“çš„é»˜è®¤æ¨¡å‹å¯¹ä¸€å°éƒ¨åˆ†éªŒè¯é›†ç”Ÿæˆä¸€äº›é¢„æµ‹ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨å’Œä¹‹å‰ä¸€æ ·çš„å¤„ç†å‡½æ•°ï¼›å› ä¸ºå®ƒä¾èµ–äºå…¨å±€å¸¸é‡ `tokenizer` ï¼Œæˆ‘ä»¬åªéœ€å°†è¯¥å¯¹è±¡æ›´æ”¹ä¸ºæˆ‘ä»¬è¦ä¸´æ—¶ä½¿ç”¨çš„æ¨¡å‹çš„ tokenizer 

ä¸ºäº†æµ‹è¯•è¿™äº›ä»£ç ï¼Œæˆ‘ä»¬éœ€è¦ä¸€äº›é¢„æµ‹ç»“æœã€‚ç”±äºæˆ‘ä»¬è¿˜æ²¡æœ‰è®­ç»ƒæ¨¡å‹ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ QA pipeline çš„é»˜è®¤æ¨¡å‹åœ¨éªŒè¯é›†çš„ä¸€å°éƒ¨åˆ†ä¸Šç”Ÿæˆä¸€äº›é¢„æµ‹ç»“æœã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸ä¹‹å‰ç›¸åŒçš„å¤„ç†å‡½æ•°ï¼›å› ä¸ºå®ƒä¾èµ–äºå…¨å±€å¸¸é‡ `tokenizer` ï¼Œæ‰€ä»¥æˆ‘ä»¬åªéœ€è¦æš‚æ—¶å°†è¯¥å¯¹è±¡æ›´æ”¹ä¸ºæˆ‘ä»¬æƒ³è¦ä½¿ç”¨çš„æ¨¡å‹çš„ tokenizerï¼š

```python
small_eval_set = raw_datasets["validation"].select(range(100))
trained_checkpoint = "distilbert-base-cased-distilled-squad"

tokenizer = AutoTokenizer.from_pretrained(trained_checkpoint)
eval_set = small_eval_set.map(
    preprocess_validation_examples,
    batched=True,
    remove_columns=raw_datasets["validation"].column_names,
)
```

ç°åœ¨é¢„å¤„ç†å·²ç»å®Œæˆï¼Œæˆ‘ä»¬å°† tokenizer æ”¹å›æˆ‘ä»¬æœ€åˆé€‰æ‹©çš„é‚£ä¸ªï¼š

```python
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
```

ç„¶åæˆ‘ä»¬ç§»é™¤ `eval_set` ä¸­æ¨¡å‹ä¸éœ€è¦çš„åˆ—ï¼Œæ„å»ºä¸€ä¸ªåŒ…å«æ‰€æœ‰å°å‹éªŒè¯é›†æ•°æ®çš„ batchï¼Œå¹¶å°†å…¶ä¼ é€’ç»™æ¨¡å‹ã€‚å¦‚æœæœ‰å¯ç”¨çš„ GPUï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å®ƒä»¥åŠ å¿«é€Ÿåº¦ï¼š

{#if fw === 'pt'}

```python
#####Pytorch}
import torch
from transformers import AutoModelForQuestionAnswering

eval_set_for_model = eval_set.remove_columns(["example_id", "offset_mapping"])
eval_set_for_model.set_format("torch")

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
batch = {k: eval_set_for_model[k].to(device) for k in eval_set_for_model.column_names}
trained_model = AutoModelForQuestionAnswering.from_pretrained(trained_checkpoint).to(
    device
)

with torch.no_grad():
    outputs = trained_model(**batch)####end
```

ä¸ºäº†ä¾¿äºå®éªŒï¼Œè®©æˆ‘ä»¬å°†è¿™äº›è¾“å‡ºè½¬æ¢ä¸º NumPy æ•°ç»„ï¼š

```python
#####Pytorch}
start_logits = outputs.start_logits.cpu().numpy()
end_logits = outputs.end_logits.cpu().numpy()####end
```

{:else}

```python
#####TensorFlow}
import tensorflow as tf
from transformers import TFAutoModelForQuestionAnswering

eval_set_for_model = eval_set.remove_columns(["example_id", "offset_mapping"])
eval_set_for_model.set_format("numpy")

batch = {k: eval_set_for_model[k] for k in eval_set_for_model.column_names}
trained_model = TFAutoModelForQuestionAnswering.from_pretrained(trained_checkpoint)

outputs = trained_model(**batch)####end
```

ä¸ºäº†ä¾¿äºå®éªŒï¼Œè®©æˆ‘ä»¬å°†è¿™äº›è¾“å‡ºè½¬æ¢ä¸º NumPy æ•°ç»„ï¼š

```python
#####TensorFlow}
start_logits = outputs.start_logits.numpy()
end_logits = outputs.end_logits.numpy()####end
```

{/if}

ç°åœ¨ï¼Œæˆ‘ä»¬éœ€è¦æ‰¾åˆ° `small_eval_set` ä¸­æ¯ä¸ªç¤ºä¾‹çš„é¢„æµ‹ç­”æ¡ˆã€‚ä¸€ä¸ªç¤ºä¾‹å¯èƒ½ä¼šè¢«æ‹†åˆ†æˆ `eval_set` ä¸­çš„å¤šä¸ªç‰¹å¾ï¼Œæ‰€ä»¥ç¬¬ä¸€æ­¥æ˜¯å°† `small_eval_set` ä¸­çš„æ¯ä¸ªç¤ºä¾‹æ˜ å°„åˆ° `eval_set` ä¸­å¯¹åº”çš„ç‰¹å¾ï¼š

```python
import collections

example_to_features = collections.defaultdict(list)
for idx, feature in enumerate(eval_set):
    example_to_features[feature["example_id"]].append(idx)
```

æœ‰äº†è¿™ä¸ªæ˜ å°„ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡å¾ªç¯éå†æ‰€æœ‰ç¤ºä¾‹ï¼Œå¹¶é’ˆå¯¹æ¯ä¸ªç¤ºä¾‹éå†æ‰€æœ‰ç›¸å…³çš„ç‰¹å¾ã€‚æ­£å¦‚ä¹‹å‰æ‰€è¯´ï¼Œæˆ‘ä»¬å°†æŸ¥çœ‹ `n_best` ä¸ªèµ·å§‹ logit å’Œç»“æŸ logit çš„å¾—åˆ†ï¼Œæ’é™¤ä»¥ä¸‹æƒ…å†µï¼š

- ç­”æ¡ˆä¸åœ¨ä¸Šä¸‹æ–‡ä¸­
- ç­”æ¡ˆé•¿åº¦ä¸ºè´Ÿæ•°
- ç­”æ¡ˆè¿‡é•¿ï¼ˆæˆ‘ä»¬å°†å¯èƒ½æ€§é™åˆ¶åœ¨ `max_answer_length=30` ï¼‰

å½“æˆ‘ä»¬å¾—åˆ°ä¸€ä¸ªç¤ºä¾‹çš„æ‰€æœ‰å¾—åˆ†å¯èƒ½ç­”æ¡ˆï¼Œæˆ‘ä»¬åªéœ€é€‰æ‹©å…·æœ‰æœ€ä½³ logit å¾—åˆ†çš„ç­”æ¡ˆï¼š

```python
import numpy as np

n_best = 20
max_answer_length = 30
predicted_answers = []

for example in small_eval_set:
    example_id = example["id"]
    context = example["context"]
    answers = []

    for feature_index in example_to_features[example_id]:
        start_logit = start_logits[feature_index]
        end_logit = end_logits[feature_index]
        offsets = eval_set["offset_mapping"][feature_index]

        start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()
        end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()
        for start_index in start_indexes:
            for end_index in end_indexes:
                # è·³è¿‡ä¸å®Œå…¨åœ¨ä¸Šä¸‹æ–‡ä¸­çš„ç­”æ¡ˆ
                if offsets[start_index] is None or offsets[end_index] is None:
                    continue
                # è·³è¿‡é•¿åº¦ä¸ºè´Ÿæ•°æˆ–å¤§äº max_answer_length çš„ç­”æ¡ˆã€‚
                if (
                    end_index < start_index
                    or end_index - start_index + 1 > max_answer_length
                ):
                    continue

                answers.append(
                    {
                        "text": context[offsets[start_index][0] : offsets[end_index][1]],
                        "logit_score": start_logit[start_index] + end_logit[end_index],
                    }
                )

    best_answer = max(answers, key=lambda x: x["logit_score"])
    predicted_answers.append({"id": example_id, "prediction_text": best_answer["text"]})
```

é¢„æµ‹ç­”æ¡ˆçš„æœ€ç»ˆæ ¼å¼å·²ç»æ˜¯æˆ‘ä»¬å°†ä½¿ç”¨çš„è¯„ä¼°æŒ‡æ ‡æ‰€æœŸæœ›çš„æ ¼å¼ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥å€ŸåŠ©Evaluate åº“æ¥åŠ è½½å®ƒï¼š

```python
import evaluate

metric = evaluate.load("squad")
```

è¿™ä¸ªè¯„ä¼°æŒ‡æ ‡ä¸€ä¸ªå¦‚ä¸Šæ‰€ç¤ºæ ¼å¼ï¼ˆä¸€ä¸ªåŒ…å«ç¤ºä¾‹ ID å’Œé¢„æµ‹æ–‡æœ¬çš„å­—å…¸åˆ—è¡¨ï¼‰çš„é¢„æµ‹ç­”æ¡ˆï¼ŒåŒæ—¶ä¹Ÿéœ€è¦ä¸€ä¸ªå¦‚ä¸‹æ ¼å¼ï¼ˆä¸€ä¸ªåŒ…å«ç¤ºä¾‹ ID å’Œå¯èƒ½ç­”æ¡ˆçš„å­—å…¸åˆ—è¡¨ï¼‰çš„å‚è€ƒç­”æ¡ˆï¼š

```python
theoretical_answers = [
    {"id": ex["id"], "answers": ex["answers"]} for ex in small_eval_set
]
```

ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æŸ¥çœ‹ä¸¤ä¸ªåˆ—è¡¨ä¸­çš„ç¬¬ä¸€ä¸ªå…ƒç´ æ¥æ£€æŸ¥æ˜¯å¦è·å¾—äº†åˆç†çš„ç»“æœï¼š

```python
print(predicted_answers[0])
print(theoretical_answers[0])
```

```python
{'id': '56be4db0acb8001400a502ec', 'prediction_text': 'Denver Broncos'}
{'id': '56be4db0acb8001400a502ec', 'answers': {'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'], 'answer_start': [177, 177, 177]}}
```

è¿˜ä¸é”™ï¼ç°åœ¨è®©æˆ‘ä»¬çœ‹ä¸€ä¸‹è¯„ä¼°æŒ‡æ ‡ç»™å‡ºçš„åˆ†æ•°ï¼š

```python
metric.compute(predictions=predicted_answers, references=theoretical_answers)
```

```python
{'exact_match': 83.0, 'f1': 88.25}
```

æ ¹æ® [DistilBERT çš„è®ºæ–‡](https://arxiv.org/abs/1910.01108v2)(https://arxiv.org/abs/1910.01108v2) æ‰€è¿°ï¼ŒDistilBERT åœ¨ SQuAD ä¸Šå¾®è°ƒåæ•´ä½“æ•°æ®é›†çš„å¾—åˆ†ä¸º 79.1 å’Œ 86.9ï¼Œè¿™ä¸ªç»“æœç›¸å½“ä¸é”™ã€‚

{#if fw === 'pt'}

ç°åœ¨ï¼Œè®©æˆ‘ä»¬å°†åˆšæ‰æ‰€åšçš„æ”¾å…¥ `compute_metrics()` å‡½æ•°ä¸­ï¼Œæˆ‘ä»¬å°†åœ¨ `Trainer` ä¸­ä½¿ç”¨å®ƒã€‚é€šå¸¸ï¼Œ `compute_metrics()` å‡½æ•°åªæ¥æ”¶ä¸€ä¸ªåŒ…å« logits å’Œæ ‡ç­¾çš„ `eval_preds` å…ƒç»„ã€‚ä½†æ˜¯åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬éœ€è¦æ›´å¤šçš„ä¿¡æ¯ï¼Œå› ä¸ºæˆ‘ä»¬éœ€è¦åœ¨ç‰¹å¾æ•°æ®é›†ä¸­æŸ¥æ‰¾åç§»é‡ï¼Œå¹¶åœ¨ç¤ºä¾‹æ•°æ®é›†ä¸­æŸ¥æ‰¾åŸå§‹ Contextï¼Œå› æ­¤æˆ‘ä»¬æ— æ³•åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨æ­¤å‡½æ•°æ¥è·å–å¸¸è§„çš„è¯„ä¼°ç»“æœã€‚æˆ‘ä»¬åªä¼šåœ¨è®­ç»ƒç»“æŸæ—¶ä½¿ç”¨å®ƒæ¥æ£€æŸ¥ç»“æœã€‚
`compute_metrics()` å‡½æ•°ä¸ä¹‹å‰çš„æ­¥éª¤ç›¸åŒï¼›æˆ‘ä»¬åªæ˜¯æ·»åŠ äº†ä¸€ä¸ªå°çš„æ£€æŸ¥ï¼Œä»¥é˜²æˆ‘ä»¬æ‰¾ä¸åˆ°ä»»ä½•æœ‰æ•ˆçš„ç­”æ¡ˆï¼ˆåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çš„é¢„æµ‹ä¼šè¾“å‡ºä¸€ä¸ªç©ºå­—ç¬¦ä¸²ï¼‰ã€‚

{:else}

ç°åœ¨ï¼Œè®©æˆ‘ä»¬å°†åˆšæ‰æ‰€åšçš„æ”¾å…¥ `compute_metrics()` å‡½æ•°ä¸­ï¼Œæˆ‘ä»¬å°†åœ¨è®­ç»ƒæ¨¡å‹æ—¶ä½¿ç”¨è¯¥å‡½æ•°ã€‚æˆ‘ä»¬éœ€è¦ä¼ é€’çš„ä¸ä»…ä»…æ˜¯è¾“å‡ºçš„ logitsï¼Œå› ä¸ºæˆ‘ä»¬å¿…é¡»åœ¨ç‰¹å¾æ•°æ®é›†ä¸­æŸ¥æ‰¾åç§»é‡ï¼Œå¹¶åœ¨ç¤ºä¾‹æ•°æ®é›†ä¸­æŸ¥æ‰¾åŸå§‹ä¸Šä¸‹æ–‡ï¼š

{/if}

```python
from tqdm.auto import tqdm

def compute_metrics(start_logits, end_logits, features, examples):
    example_to_features = collections.defaultdict(list)
    for idx, feature in enumerate(features):
        example_to_features[feature["example_id"]].append(idx)

    predicted_answers = []
    for example in tqdm(examples):
        example_id = example["id"]
        context = example["context"]
        answers = []

        # å¾ªç¯éå†ä¸è¯¥ç¤ºä¾‹ç›¸å…³è”çš„æ‰€æœ‰ç‰¹å¾
        for feature_index in example_to_features[example_id]:
            start_logit = start_logits[feature_index]
            end_logit = end_logits[feature_index]
            offsets = features[feature_index]["offset_mapping"]

            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()
            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()
            for start_index in start_indexes:
                for end_index in end_indexes:
                    # è·³è¿‡ä¸å®Œå…¨ä½äºä¸Šä¸‹æ–‡ä¸­çš„ç­”æ¡ˆ
                    if offsets[start_index] is None or offsets[end_index] is None:
                        continue
                    # è·³è¿‡é•¿åº¦å°äº 0 æˆ–å¤§äº max_answer_length çš„ç­”æ¡ˆ
                    if (
                        end_index < start_index
                        or end_index - start_index + 1 > max_answer_length
                    ):
                        continue

                    answer = {
                        "text": context[offsets[start_index][0] : offsets[end_index][1]],
                        "logit_score": start_logit[start_index] + end_logit[end_index],
                    }
                    answers.append(answer)

        # é€‰æ‹©å¾—åˆ†æœ€é«˜çš„ç­”æ¡ˆ
        if len(answers) > 0:
            best_answer = max(answers, key=lambda x: x["logit_score"])
            predicted_answers.append(
                {"id": example_id, "prediction_text": best_answer["text"]}
            )
        else:
            predicted_answers.append({"id": example_id, "prediction_text": ""})

    theoretical_answers = [{"id": ex["id"], "answers": ex["answers"]} for ex in examples]
    return metric.compute(predictions=predicted_answers, references=theoretical_answers)
```

æˆ‘ä»¬å¯ä»¥æ£€æŸ¥å®ƒåœ¨æˆ‘ä»¬çš„é¢„æµ‹ç»“æœä¸Šçš„è®¡ç®—çš„ç»“æœï¼š

```python
compute_metrics(start_logits, end_logits, eval_set, small_eval_set)
```

```python
{'exact_match': 83.0, 'f1': 88.25}
```

çœ‹èµ·æ¥ä¸é”™ï¼ç°åœ¨è®©æˆ‘ä»¬ä½¿ç”¨å®ƒæ¥å¾®è°ƒæˆ‘ä»¬çš„æ¨¡å‹ã€‚

#### å¾®è°ƒæ¨¡å‹ 

{#if fw === 'pt'}

ç°åœ¨æˆ‘ä»¬å‡†å¤‡å¥½è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹äº†ã€‚é¦–å…ˆï¼Œè®©æˆ‘ä»¬åƒä¹‹å‰ä¸€æ ·ä½¿ç”¨ `AutoModelForQuestionAnswering` ç±»åˆ›å»ºæ¨¡å‹ï¼š

```python
#####Pytorch}
model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)####end
```

{:else}

ç°åœ¨æˆ‘ä»¬å‡†å¤‡å¥½è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹äº†ã€‚é¦–å…ˆï¼Œè®©æˆ‘ä»¬åƒä¹‹å‰ä¸€æ ·ä½¿ç”¨ `TFAutoModelForQuestionAnswering` ç±»åˆ›å»ºæ¨¡å‹ï¼š

```python
#####TensorFlow}
model = TFAutoModelForQuestionAnswering.from_pretrained(model_checkpoint)####end
```

{/if}

å’Œå¾€å¸¸ä¸€æ ·ï¼Œæˆ‘ä»¬ä¼šæ”¶åˆ°ä¸€ä¸ªè­¦å‘Šï¼Œæç¤ºæœ‰äº›æƒé‡æ²¡æœ‰è¢«ä½¿ç”¨ï¼ˆæ¥è‡ªé¢„è®­ç»ƒå¤´éƒ¨çš„æƒé‡ï¼‰ï¼Œè€Œå…¶ä»–ä¸€äº›æƒé‡æ˜¯éšæœºåˆå§‹åŒ–çš„ï¼ˆç”¨äºé—®ç­”å¤´éƒ¨çš„æƒé‡ï¼‰ã€‚ä½ ç°åœ¨åº”è¯¥å·²ç»ä¹ æƒ¯äº†è¿™ç§æƒ…å†µï¼Œä½†è¿™æ„å‘³ç€è¿™ä¸ªæ¨¡å‹è¿˜æ²¡æœ‰å‡†å¤‡å¥½ä½¿ç”¨ï¼Œéœ€è¦è¿›è¡Œå¾®è°ƒâ€”â€”å¥½åœ¨æˆ‘ä»¬æ­£åœ¨åšè¿™ä¸€ç‚¹ï¼

ä¸ºäº†èƒ½å¤Ÿå°†æˆ‘ä»¬çš„æ¨¡å‹æ¨é€åˆ° Hubï¼Œæˆ‘ä»¬éœ€è¦ç™»å½• Hugging Faceã€‚å¦‚æœä½ åœ¨ Notebook ä¸­è¿è¡Œæ­¤ä»£ç ï¼Œåˆ™å¯ä»¥ä½¿ç”¨ä»¥ä¸‹çš„å‡½æ•°æ‰§è¡Œæ­¤æ“ä½œï¼Œè¯¥å‡½æ•°ä¼šæ˜¾ç¤ºä¸€ä¸ªå°éƒ¨ä»¶ï¼Œä½ å¯ä»¥åœ¨å…¶ä¸­è¾“å…¥ç™»å½•å‡­æ®ï¼š

```python
from huggingface_hub import notebook_login

notebook_login()
```

å¦‚æœä½ ä¸åœ¨ Notebook ä¸­å·¥ä½œï¼Œåªéœ€åœ¨ç»ˆç«¯ä¸­è¾“å…¥ä»¥ä¸‹è¡Œï¼š

```python
huggingface-cli login
```

{#if fw === 'pt'}

å®Œæˆåï¼Œæˆ‘ä»¬å°±å¯ä»¥å®šä¹‰æˆ‘ä»¬çš„ `TrainingArguments` ã€‚æ­£å¦‚æˆ‘ä»¬åœ¨å®šä¹‰è®¡ç®—è¯„ä¼°å‡½æ•°æ—¶æ‰€è¯´çš„ï¼Œç”±äº `compute_metrics()` å‡½æ•°çš„è¾“å…¥å‚æ•°é™åˆ¶ï¼Œæˆ‘ä»¬æ— æ³•ä½¿ç”¨å¸¸è§„çš„æ–¹æ³•æ¥ç¼–å†™è¯„ä¼°å¾ªç¯ã€‚ä¸è¿‡ï¼Œæˆ‘ä»¬å¯ä»¥ç¼–å†™è‡ªå·±çš„ `Trainer` å­ç±»æ¥å®ç°è¿™ä¸€ç‚¹ï¼ˆä½ å¯ä»¥åœ¨ [é—®ç­”ç¤ºä¾‹è„šæœ¬](https://github.com/huggingface/transformers/blob/master/examples/pytorch/question-answering/trainer_qa.py)(https://github.com/huggingface/transformers/blob/master/examples/pytorch/question-answering/trainer_qa.py) ä¸­æ‰¾åˆ°è¯¥æ–¹æ³•ï¼‰ï¼Œä½†æ”¾åœ¨æœ¬èŠ‚ä¸­ä¼šæœ‰äº›å†—é•¿ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œå°†ä»…åœ¨è®­ç»ƒç»“æŸæ—¶è¯„ä¼°æ¨¡å‹ï¼Œå¹¶åœ¨ä¸‹é¢çš„â€œè‡ªå®šä¹‰è®­ç»ƒå¾ªç¯â€ä¸­å‘ä½ å±•ç¤ºå¦‚ä½•ä½¿ç”¨å¸¸è§„çš„æ–¹æ³•è¿›è¡Œè¯„ä¼°ã€‚

è¿™ç¡®å®æ˜¯ `Trainer` API å±€é™æ€§çš„åœ°æ–¹ï¼Œè€ŒAccelerate åº“åˆ™è¡¨ç°å‡ºè‰²ï¼šå®šåˆ¶åŒ–ç‰¹å®šç”¨ä¾‹çš„ç±»å¯èƒ½ä¼šå¾ˆç¹çï¼Œä½†å®šåˆ¶åŒ–è°ƒæ•´è®­ç»ƒå¾ªç¯å´å¾ˆç®€å•ã€‚

è®©æˆ‘ä»¬æ¥çœ‹çœ‹æˆ‘ä»¬çš„ `TrainingArguments` ï¼š

```python
#####Pytorch}
from transformers import TrainingArguments

args = TrainingArguments(
    "bert-finetuned-squad",
    evaluation_strategy="no",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
    fp16=True,
    push_to_hub=True,
)####end
```

æˆ‘ä»¬ä¹‹å‰å·²ç»è§è¿‡å…¶ä¸­å¤§éƒ¨åˆ†å†…å®¹ï¼šæˆ‘ä»¬è®¾ç½®äº†ä¸€äº›è¶…å‚æ•°ï¼ˆå¦‚å­¦ä¹ ç‡ã€è®­ç»ƒçš„å‘¨æœŸæ•°å’Œä¸€äº›æƒé‡è¡°å‡ï¼‰ï¼Œå¹¶è®¾å®šæˆ‘ä»¬æƒ³åœ¨æ¯ä¸ªå‘¨æœŸç»“æŸæ—¶ä¿å­˜æ¨¡å‹ã€è·³è¿‡è¯„ä¼°ï¼Œå¹¶å°†ç»“æœä¸Šä¼ åˆ°æ¨¡å‹ä¸­å¿ƒã€‚æˆ‘ä»¬è¿˜å¯ç”¨äº† `fp16=True` çš„æ··åˆç²¾åº¦è®­ç»ƒï¼Œå› ä¸ºå®ƒå¯ä»¥åœ¨æœ€æ–°çš„ GPU ä¸ŠåŠ å¿«è®­ç»ƒé€Ÿåº¦ã€‚

{:else}

ç°åœ¨å®Œæˆäº†è¿™ä¸€æ­¥éª¤ï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºæˆ‘ä»¬çš„ TF æ•°æ®é›†ã€‚è¿™æ¬¡æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ç®€å•çš„é»˜è®¤æ•°æ®æ•´ç†å™¨ï¼š

```python
#####TensorFlow}
from transformers import DefaultDataCollator

data_collator = DefaultDataCollator(return_tensors="tf")####end
```

ç„¶ååƒå¾€å¸¸ä¸€æ ·åˆ›å»ºæ•°æ®é›†ã€‚

```python
#####TensorFlow}
tf_train_dataset = model.prepare_tf_dataset(
    train_dataset,
    collate_fn=data_collator,
    shuffle=True,
    batch_size=16,
)
tf_eval_dataset = model.prepare_tf_dataset(
    validation_dataset,
    collate_fn=data_collator,
    shuffle=False,
    batch_size=16,
)####end
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬è®¾ç½®è®­ç»ƒè¶…å‚æ•°å¹¶ç¼–è¯‘æˆ‘ä»¬çš„æ¨¡å‹

```python
#####TensorFlow}
from transformers import create_optimizer
from transformers.keras_callbacks import PushToHubCallback
import tensorflow as tf

## è®­ç»ƒæ­¥éª¤çš„æ•°é‡æ˜¯æ•°æ®é›†ä¸­çš„æ ·æœ¬æ•°é‡é™¤ä»¥ batch å¤§å°,ç„¶åä¹˜ä»¥æ€»çš„è®­ç»ƒå‘¨æœŸæ•°ã€‚
## æ³¨æ„,è¿™é‡Œçš„ tf_train_dataset æ˜¯ä¸€ä¸ªbatchçš„ tf.data.Dataset,
## è€Œä¸æ˜¯åŸå§‹çš„ Hugging Face æ•°æ®é›†,æ‰€ä»¥ä½¿ç”¨ len() è®¡ç®—å…¶é•¿åº¦å·²ç»æ˜¯ num_samples // batch_sizeã€‚
num_train_epochs = 3
num_train_steps = len(tf_train_dataset) * num_train_epochs
optimizer, schedule = create_optimizer(
    init_lr=2e-5,
    num_warmup_steps=0,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)
model.compile(optimizer=optimizer)

## ä½¿ç”¨ float16 æ··åˆç²¾åº¦è¿›è¡Œè®­ç»ƒ
tf.keras.mixed_precision.set_global_policy("mixed_float16")####end
```

æœ€åï¼Œæˆ‘ä»¬å‡†å¤‡ä½¿ç”¨ `model.fit()` è¿›è¡Œè®­ç»ƒï¼Œä½¿ç”¨ `PushToHubCallback` åœ¨æ¯ä¸ªå‘¨æœŸç»“æŸåå°†æ¨¡å‹ä¸Šä¼ åˆ°æ¨¡å‹ä¸­å¿ƒã€‚

{/if}

é»˜è®¤æƒ…å†µä¸‹ï¼Œä½¿ç”¨çš„ä»“åº“å°†ä¿å­˜åœ¨ä½ çš„è´¦æˆ·ä¸­ï¼Œå¹¶ä»¥ä½ è®¾ç½®çš„è¾“å‡ºç›®å½•å‘½åï¼Œæ‰€ä»¥åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­å®ƒå°†ä½äº `"sgugger/bert-finetuned-squad"` ä¸­ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡ä¼ é€’ä¸€ä¸ª `hub_model_id` æ¥è¦†ç›–è¿™ä¸ªè®¾ç½®ï¼›ä¾‹å¦‚ï¼Œè¦å°†æ¨¡å‹æ¨é€åˆ°æˆ‘ä»¬ä½¿ç”¨çš„ `huggingface_course` ç»„ç»‡ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº† `hub_model_id="huggingface_course/bert-finetuned-squad"` ï¼ˆè¿™æ˜¯æˆ‘ä»¬åœ¨æœ¬èŠ‚å¼€å§‹æ—¶æ¼”ç¤ºçš„æ¨¡å‹ï¼‰ã€‚

{#if fw === 'pt'}

<div custom-style="Tip-green">

ğŸ’¡ å¦‚æœä½ æ­£åœ¨ä½¿ç”¨çš„è¾“å‡ºç›®å½•å·²ç»å­˜åœ¨ï¼Œåˆ™å®ƒéœ€è¦æ˜¯ä½ è¦æ¨é€åˆ°çš„å­˜å‚¨åº“çš„æœ¬åœ°å…‹éš†ï¼ˆå› æ­¤ï¼Œå¦‚æœåœ¨å®šä¹‰ä½ çš„ `Trainer` æ—¶å‡ºç°é”™è¯¯ï¼Œè¯·è®¾ç½®ä¸€ä¸ªæ–°çš„åç§°ï¼‰ã€‚

</div>

æœ€åï¼Œæˆ‘ä»¬åªéœ€å°†æ‰€æœ‰å†…å®¹ä¼ é€’ç»™ `Trainer` ç±»å¹¶å¯åŠ¨è®­ç»ƒï¼š

```python
#####Pytorch}
from transformers import Trainer

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=train_dataset,
    eval_dataset=validation_dataset,
    tokenizer=tokenizer,
)
trainer.train()####end
```

{:else}

```python
#####TensorFlow}
from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(output_dir="bert-finetuned-squad", tokenizer=tokenizer)

## æˆ‘ä»¬å°†åœ¨ä¹‹åè¿›è¡ŒéªŒè¯,å› æ­¤è®­ç»ƒè¿‡ç¨‹ä¸­ä¸ä¼šè¿›è¡ŒéªŒè¯
model.fit(tf_train_dataset, callbacks=[callback], epochs=num_train_epochs)####end
```

{/if}

è¯·æ³¨æ„ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¯æ¬¡æ¨¡å‹ä¿å­˜ï¼ˆä¾‹å¦‚ï¼Œæ¯ä¸ª epoch ç»“æŸæ—¶ï¼‰ï¼Œæ¨¡å‹éƒ½ä¼šåœ¨åå°ä¸Šä¼ åˆ° Hubã€‚è¿™æ ·ï¼Œå¦‚æœéœ€è¦çš„è¯ï¼Œä½ å°±å¯ä»¥åœ¨å¦ä¸€å°æœºå™¨ä¸Šæ¢å¤è®­ç»ƒã€‚æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹éœ€è¦ä¸€äº›æ—¶é—´ï¼ˆåœ¨ Titan RTX ä¸Šç•¥è¶…è¿‡ä¸€ä¸ªå°æ—¶ï¼‰ï¼Œæ‰€ä»¥ä½ å¯ä»¥å–æ¯å’–å•¡æˆ–è€…é‡æ–°é˜…è¯»ä¸€äº›ä½ è§‰å¾—æ›´å…·æŒ‘æˆ˜æ€§çš„è¯¾ç¨‹éƒ¨åˆ†æ¥æ¶ˆç£¨æ—¶é—´ã€‚è¿˜è¦æ³¨æ„ï¼Œä¸€æ—¦ç¬¬ä¸€ä¸ª epoch å®Œæˆï¼Œä½ å°†çœ‹åˆ°ä¸€äº›æƒé‡ä¸Šä¼ åˆ° Hubï¼Œå¹¶ä¸”ä½ å¯ä»¥åœ¨å…¶é¡µé¢ä¸Šå¼€å§‹ä½¿ç”¨ä½ çš„æ¨¡å‹è¿›è¡Œæµ‹è¯•ã€‚

{#if fw === 'pt'}

è®­ç»ƒå®Œæˆåï¼Œæˆ‘ä»¬å¯ä»¥æœ€ç»ˆè¯„ä¼°æˆ‘ä»¬çš„æ¨¡å‹ï¼ˆå¹¶ç¥ˆç¥·æˆ‘ä»¬å¯ä»¥ä¸€æ¬¡æˆåŠŸï¼‰ã€‚ `Trainer` çš„ `predict()` æ–¹æ³•å°†è¿”å›ä¸€ä¸ªå…ƒç»„ï¼Œå…¶ä¸­ç¬¬ä¸€ä¸ªå…ƒç´ å°†æ˜¯æ¨¡å‹çš„é¢„æµ‹ç»“æœï¼ˆåœ¨è¿™é‡Œæ˜¯ä¸€ä¸ªåŒ…å«èµ·å§‹å’Œç»“æŸ logits çš„å¯¹ï¼‰ã€‚æˆ‘ä»¬å°†è¿™ä¸ªç»“æœä¼ é€’ç»™æˆ‘ä»¬çš„ `compute_metrics()` å‡½æ•°ï¼š

```python
#####Pytorch}
predictions, _, _ = trainer.predict(validation_dataset)
start_logits, end_logits = predictions
compute_metrics(start_logits, end_logits, validation_dataset, raw_datasets["validation"])####end
```

{:else}

è®­ç»ƒå®Œæˆåï¼Œæˆ‘ä»¬å¯ä»¥æœ€ç»ˆè¯„ä¼°æˆ‘ä»¬çš„æ¨¡å‹ï¼ˆå¹¶ç¥ˆç¥·æˆ‘ä»¬å¯ä»¥ä¸€æ¬¡æˆåŠŸï¼‰ã€‚æˆ‘ä»¬çš„ `model` çš„ `predict()` æ–¹æ³•å°†è´Ÿè´£è·å–é¢„æµ‹ç»“æœï¼Œå¹¶ä¸”ç”±äºæˆ‘ä»¬ä¹‹å‰å·²ç»å®šä¹‰äº†ä¸€ä¸ª `compute_metrics()` å‡½æ•°ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥ç”¨ä¸€è¡Œä»£ç å¾—åˆ°æˆ‘ä»¬çš„ç»“æœï¼š

```python
#####TensorFlow}
predictions = model.predict(tf_eval_dataset)
compute_metrics(
    predictions["start_logits"],
    predictions["end_logits"],
    validation_dataset,
    raw_datasets["validation"],
)####end
```

{/if}

```python
{'exact_match': 81.18259224219489, 'f1': 88.67381321905516}
```

å¾ˆæ£’ï¼ä½œä¸ºå¯¹æ¯”ï¼ŒBERT æ–‡ç« ä¸­æŠ¥å‘Šçš„è¯¥æ¨¡å‹çš„åŸºå‡†åˆ†æ•°åˆ†åˆ«ä¸º 80.8 å’Œ 88.5ï¼Œæ‰€ä»¥æˆ‘ä»¬çš„ç»“æœæ­£å¥½è¾¾åˆ°äº†é¢„æœŸåˆ†æ•°ã€‚

{#if fw === 'pt'}

æœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨ `push_to_hub()` æ–¹æ³•ç¡®ä¿ä¸Šä¼ æ¨¡å‹çš„æœ€æ–°ç‰ˆæœ¬ï¼š

```python
#####Pytorch}
trainer.push_to_hub(commit_message="Training complete")####end
```

å¦‚æœä½ æƒ³æ£€æŸ¥å®ƒï¼Œä¸Šé¢çš„ä»£ç è¿”å›å®ƒåˆšåˆšæ‰§è¡Œçš„æäº¤çš„ URLï¼š

```python
#####Pytorch}
'https://huggingface.co/sgugger/bert-finetuned-squad/commit/9dcee1fbc25946a6ed4bb32efb1bd71d5fa90b68'####end
```
`Trainer` è¿˜ä¼šåˆ›å»ºä¸€ä¸ªåŒ…å«æ‰€æœ‰è¯„ä¼°ç»“æœçš„æ¨¡å‹å¡ç‰‡ï¼Œå¹¶å°†å…¶ä¸Šä¼ ã€‚

{/if}

åœ¨è¿™ä¸ªé˜¶æ®µï¼Œä½ å¯ä»¥ä½¿ç”¨æ¨¡å‹åº“ä¸­çš„æ¨ç†å°éƒ¨ä»¶æ¥æµ‹è¯•æ¨¡å‹ï¼Œå¹¶ä¸ä½ çš„æœ‹å‹ã€å®¶äººå’ŒåŒä¼´åˆ†äº«ã€‚æ­å–œä½ æˆåŠŸåœ°åœ¨é—®ç­”ä»»åŠ¡ä¸Šå¯¹æ¨¡å‹è¿›è¡Œäº†å¾®è°ƒï¼

<div custom-style="Tip-green">

âœï¸ **è½®åˆ°ä½ äº†ï¼** å°è¯•ä½¿ç”¨å¦ä¸€ä¸ªæ¨¡å‹æ¶æ„ï¼Œçœ‹çœ‹å®ƒåœ¨è¿™ä¸ªä»»åŠ¡ä¸Šè¡¨ç°å¾—æ˜¯å¦æ›´å¥½ï¼

</div>

{#if fw === 'pt'}

å¦‚æœä½ æƒ³æ›´æ·±å…¥åœ°äº†è§£è®­ç»ƒå¾ªç¯ï¼Œæˆ‘ä»¬ç°åœ¨å°†å‘ä½ å±•ç¤ºå¦‚ä½•ä½¿ç”¨ Accelerate æ¥åšåŒæ ·çš„äº‹æƒ…ã€‚

### è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ 

ç°åœ¨ï¼Œè®©æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹å®Œæ•´çš„è®­ç»ƒå¾ªç¯ï¼Œè¿™æ ·ä½ å°±å¯ä»¥è½»æ¾åœ°è‡ªå®šä¹‰æ‰€éœ€çš„éƒ¨åˆ†ã€‚å®ƒçœ‹èµ·æ¥å¾ˆåƒç¬¬å››ç« ä¸­çš„è®­ç»ƒå¾ªç¯ï¼Œåªæ˜¯è¯„ä¼°å¾ªç¯æœ‰æ‰€ä¸åŒã€‚ç”±äºæˆ‘ä»¬ä¸å†å— `Trainer` ç±»çš„é™åˆ¶ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥åœ¨æ¨¡å‹è®­ç»ƒçš„è¿‡ç¨‹ä¸­å®šæœŸè¯„ä¼°æ¨¡å‹ã€‚

#### ä¸ºè®­ç»ƒåšå‡†å¤‡ 

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨æ•°æ®é›†æ„å»º `DataLoader` ã€‚æˆ‘ä»¬å°†è¿™äº›æ•°æ®é›†çš„æ ¼å¼è®¾ç½®ä¸º `"torch"` ï¼Œå¹¶åˆ é™¤æ¨¡å‹ä¸ä½¿ç”¨çš„éªŒè¯é›†åˆ—ã€‚ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ Transformers æä¾›çš„ `default_data_collator` ä½œä¸º `collate_fn` ï¼Œå¹¶æ‰“ä¹±è®­ç»ƒé›†ï¼Œä½†ä¸æ‰“ä¹±éªŒè¯é›†ï¼š

```python
#####Pytorch}
from torch.utils.data import DataLoader
from transformers import default_data_collator

train_dataset.set_format("torch")
validation_set = validation_dataset.remove_columns(["example_id", "offset_mapping"])
validation_set.set_format("torch")

train_dataloader = DataLoader(
    train_dataset,
    shuffle=True,
    collate_fn=default_data_collator,
    batch_size=8,
)
eval_dataloader = DataLoader(
    validation_set, collate_fn=default_data_collator, batch_size=8
)####end
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬é‡æ–°å®ä¾‹åŒ–æˆ‘ä»¬çš„æ¨¡å‹ï¼Œä»¥ç¡®ä¿æˆ‘ä»¬ä¸æ˜¯ä»ä¹‹å‰çš„å¾®è°ƒç»§ç»­è®­ç»ƒï¼Œè€Œæ˜¯ä» BERT é¢„è®­ç»ƒæ¨¡å‹é‡æ–°å¼€å§‹ï¼š

```python
#####Pytorch}
model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)####end
```

ç„¶åï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªä¼˜åŒ–å™¨ã€‚é€šå¸¸æˆ‘ä»¬ä½¿ç”¨ç»å…¸çš„ `AdamW` ä¼˜åŒ–å™¨ï¼Œå®ƒä¸ Adam ç±»ä¼¼ï¼Œä¸è¿‡åœ¨æƒé‡è¡°å‡çš„æ–¹å¼ä¸Šæœ‰äº›ä¸åŒï¼š

```python
#####Pytorch}
from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr=2e-5)####end
```

å½“æˆ‘ä»¬æ‹¥æœ‰äº†æ‰€æœ‰è¿™äº›å¯¹è±¡ï¼Œæˆ‘ä»¬å¯ä»¥å°†å®ƒä»¬å‘é€åˆ° `accelerator.prepare()` æ–¹æ³•ã€‚è¯·è®°ä½ï¼Œå¦‚æœä½ æƒ³åœ¨ Colab Notebook ä¸Šä½¿ç”¨ TPU è¿›è¡Œè®­ç»ƒï¼Œä½ éœ€è¦å°†æ‰€æœ‰è¿™äº›ä»£ç ç§»åˆ°ä¸€ä¸ªè®­ç»ƒå‡½æ•°ä¸­ï¼Œå¹¶ä¸”ä¸åº”è¯¥å¯¹ `Accelerator` çš„å•å…ƒæ ¼æ‰§è¡Œä»»ä½•å®ä¾‹åŒ–ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡å‘ `Accelerator` ä¼ é€’ `fp16=True` æ¥å¼ºåˆ¶ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆæˆ–è€…ï¼Œå¦‚æœä½ æƒ³è¦å°†ä»£ç ä½œä¸ºè„šæœ¬æ‰§è¡Œï¼Œåªéœ€ç¡®ä¿å¡«å†™é€‚å½“çš„Accelerate `config` ï¼‰ã€‚

```python
#####Pytorch}
from accelerate import Accelerator

accelerator = Accelerator(fp16=True)
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)####end
```

ä»å‰é¢å‡ èŠ‚ä¸­ä½ åº”è¯¥çŸ¥é“ï¼Œæˆ‘ä»¬åªæœ‰åœ¨ `train_dataloader` é€šè¿‡ `accelerator.prepare()` æ–¹æ³•åæ‰èƒ½ä½¿ç”¨å…¶é•¿åº¦æ¥è®¡ç®—è®­ç»ƒæ­¥éª¤çš„æ•°é‡ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸ä¹‹å‰ç« èŠ‚ç›¸åŒçš„çº¿æ€§å­¦ä¹ ç‡è°ƒåº¦ï¼š

```python
#####Pytorch}
from transformers import get_scheduler

num_train_epochs = 3
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)####end
```

è¦å°†æ¨¡å‹æ¨é€åˆ° Hubï¼Œæˆ‘ä»¬éœ€è¦åœ¨å·¥ä½œæ–‡ä»¶å¤¹ä¸­åˆ›å»ºä¸€ä¸ª `Repository` å¯¹è±¡ã€‚å¦‚æœä½ å°šæœªç™»å½• Hugging Face Hubï¼Œè¯·å…ˆç™»å½•ã€‚æˆ‘ä»¬å°†æ ¹æ®æˆ‘ä»¬ç»™æ¨¡å‹æŒ‡å®šçš„æ¨¡å‹ ID ç¡®å®šä»“åº“åç§°ï¼ˆå¯ä»¥æ ¹æ®è‡ªå·±çš„é€‰æ‹©æ›¿æ¢ `repo_name` ï¼›å®ƒåªéœ€è¦åŒ…å«ä½ çš„ç”¨æˆ·åï¼Œä½¿ç”¨ `get_full_repo_name()` å‡½æ•°å¯ä»¥è·å–ï¼‰ï¼š

```python
#####Pytorch}
from huggingface_hub import Repository, get_full_repo_name

model_name = "bert-finetuned-squad-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name####end
```

```python
#####Pytorch}
'sgugger/bert-finetuned-squad-accelerate'####end
```

ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥å°†è¯¥å­˜å‚¨åº“å…‹éš†åˆ°æœ¬åœ°æ–‡ä»¶å¤¹ä¸­ã€‚å¦‚æœå®ƒå·²ç»å­˜åœ¨ï¼Œé‚£ä¹ˆè¿™ä¸ªæœ¬åœ°æ–‡ä»¶å¤¹åº”è¯¥æ˜¯æˆ‘ä»¬æ­£åœ¨ä½¿ç”¨çš„å­˜å‚¨åº“çš„å…‹éš†ç‰ˆæœ¬ï¼š

```python
#####Pytorch}
output_dir = "bert-finetuned-squad-accelerate"
repo = Repository(output_dir, clone_from=repo_name)####end
```

ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡è°ƒç”¨ `repo.push_to_hub()` æ–¹æ³•ä¸Šä¼ ä¿å­˜åœ¨ `output_dir` ä¸­çš„æ‰€æœ‰å†…å®¹ã€‚è¿™å°†å¸®åŠ©æˆ‘ä»¬åœ¨æ¯ä¸ªæ—¶æœŸç»“æŸæ—¶ä¸Šä¼ ä¸­é—´æ¨¡å‹ã€‚

### è®­ç»ƒå¾ªç¯ 

ç°åœ¨ï¼Œæˆ‘ä»¬å‡†å¤‡ç¼–å†™å®Œæ•´çš„è®­ç»ƒå¾ªç¯ã€‚åœ¨å®šä¹‰ä¸€ä¸ªè¿›åº¦æ¡ä»¥è·Ÿè¸ªè®­ç»ƒè¿›åº¦ä¹‹åï¼Œå¾ªç¯åˆ†ä¸ºä¸‰ä¸ªéƒ¨åˆ†ï¼š

- è®­ç»ƒæœ¬èº«ï¼Œå³å¯¹ `train_dataloader` è¿›è¡Œè¿­ä»£ï¼Œæ¨¡å‹å‰å‘ä¼ æ’­ã€åå‘ä¼ æ’­å’Œä¼˜åŒ–å™¨æ›´æ–°ã€‚
- è¯„ä¼°ï¼Œå…¶ä¸­æˆ‘ä»¬åœ¨å°† `start_logits` å’Œ `end_logits` çš„æ‰€æœ‰å€¼æ”¶é›†èµ·æ¥ä¹‹å‰ï¼Œéå†æ•´ä¸ªè¯„ä¼°æ•°æ®é›†ã€‚å®Œæˆè¯„ä¼°å¾ªç¯åï¼Œæˆ‘ä»¬å°†æ‰€æœ‰ç»“æœæ”¶é›†èµ·æ¥ã€‚è¯·æ³¨æ„ï¼Œç”±äº `Accelerator` å¯èƒ½åœ¨æœ€åæ·»åŠ äº†ä¸€äº›æ ·æœ¬ä»¥ç¡®ä¿æ¯ä¸ªè¿›ç¨‹ä¸­çš„ç¤ºä¾‹æ•°ç›¸åŒï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦è¿›è¡Œæˆªæ–­ã€‚
- ä¿å­˜å’Œä¸Šä¼ ï¼Œé¦–å…ˆä¿å­˜æ¨¡å‹å’Œåˆ†è¯å™¨ï¼Œç„¶åè°ƒç”¨ `repo.push_to_hub()` ã€‚ä¸ä¹‹å‰ä¸€æ ·ï¼Œæˆ‘ä»¬ä½¿ç”¨ `blocking=False` å‚æ•°å‘Šè¯‰Hub åº“åœ¨å¼‚æ­¥è¿‡ç¨‹ä¸­æ¨é€ã€‚è¿™æ ·ï¼Œè®­ç»ƒå°†ç»§ç»­è¿›è¡Œï¼Œè€Œè¿™ä¸ªï¼ˆéœ€è¦å¾ˆé•¿æ—¶é—´çš„ï¼‰æŒ‡ä»¤å°†åœ¨åå°å¼‚æ­¥æ‰§è¡Œã€‚

è¿™æ˜¯è®­ç»ƒå¾ªç¯çš„å®Œæ•´ä»£ç ï¼š

```python
#####Pytorch}
from tqdm.auto import tqdm
import torch

progress_bar = tqdm(range(num_training_steps))

for epoch in range(num_train_epochs):
    # è®­ç»ƒ
    model.train()
    for step, batch in enumerate(train_dataloader):
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

    # è¯„ä¼°
    model.eval()
    start_logits = []
    end_logits = []
    accelerator.print("Evaluation!")
    for batch in tqdm(eval_dataloader):
        with torch.no_grad():
            outputs = model(**batch)

        start_logits.append(accelerator.gather(outputs.start_logits).cpu().numpy())
        end_logits.append(accelerator.gather(outputs.end_logits).cpu().numpy())

    start_logits = np.concatenate(start_logits)
    end_logits = np.concatenate(end_logits)
    start_logits = start_logits[: len(validation_dataset)]
    end_logits = end_logits[: len(validation_dataset)]

    metrics = compute_metrics(
        start_logits, end_logits, validation_dataset, raw_datasets["validation"]
    )
    print(f"epoch {epoch}:", metrics)

    # ä¿å­˜å’Œä¸Šä¼ 
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
    if accelerator.is_main_process:
        tokenizer.save_pretrained(output_dir)
        repo.push_to_hub(
            commit_message=f"Training in progress epoch {epoch}", blocking=False
        )####end
```

å¦‚æœè¿™æ˜¯ä½ ç¬¬ä¸€æ¬¡çœ‹åˆ°ä½¿ç”¨Accelerate ä¿å­˜çš„æ¨¡å‹ï¼Œè¯·èŠ±ç‚¹æ—¶é—´äº†è§£ä¸€ä¸‹ä¸ä¹‹ç›¸å…³çš„ä¸‰è¡Œä»£ç 

```python
#####Pytorch}
accelerator.wait_for_everyone()
unwrapped_model = accelerator.unwrap_model(model)
unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)####end
```

ç¬¬ä¸€è¡Œå¾ˆå¥½ç†è§£ï¼šå®ƒå‘Šè¯‰æ‰€æœ‰è¿›ç¨‹åœ¨ç»§ç»­ä¹‹å‰ç­‰å¾…æ‰€æœ‰è¿›ç¨‹éƒ½åˆ°è¾¾è¯¥é˜¶æ®µã€‚è¿™æ˜¯ä¸ºäº†ç¡®ä¿æˆ‘ä»¬åœ¨ä¿å­˜ä¹‹å‰ï¼Œåœ¨æ¯ä¸ªè¿›ç¨‹ä¸­éƒ½æœ‰ç›¸åŒçš„æ¨¡å‹ã€‚ç„¶åï¼Œæˆ‘ä»¬è·å– `unwrapped_model` ï¼Œå®ƒæ˜¯æˆ‘ä»¬å®šä¹‰çš„åŸºæœ¬æ¨¡å‹ã€‚ `accelerator.prepare()` æ–¹æ³•ä¼šæ›´æ”¹æ¨¡å‹æ¥é€‚åº”åˆ†å¸ƒå¼è®­ç»ƒï¼Œå› æ­¤å®ƒä¸å†å…·æœ‰ `save_pretrained()` æ–¹æ³•ï¼›ä½¿ç”¨ `accelerator.unwrap_model()` æ–¹æ³•å¯ä»¥æ’¤æ¶ˆè¿™ä¸ªæ›´æ”¹ã€‚æœ€åï¼Œæˆ‘ä»¬è°ƒç”¨ `save_pretrained()` ï¼Œä½†å‘Šè¯‰è¯¥æ–¹æ³•ä½¿ç”¨ `accelerator.save()` è€Œä¸æ˜¯ `torch.save()` ã€‚

å®Œæˆåï¼Œä½ åº”è¯¥æ‹¥æœ‰ä¸€ä¸ªäº§ç”Ÿä¸ä½¿ç”¨ `Trainer` è®­ç»ƒçš„æ¨¡å‹éå¸¸ç›¸ä¼¼çš„ç»“æœçš„æ¨¡å‹ã€‚ä½ å¯ä»¥åœ¨ [`huggingface-course/bert-finetuned-squad-accelerate`](https://huggingface.co/huggingface-course/bert-finetuned-squad-accelerate)(https://huggingface.co/huggingface-course/bert-finetuned-squad-accelerate) æŸ¥çœ‹æˆ‘ä»¬ä½¿ç”¨æ­¤ä»£ç è®­ç»ƒçš„æ¨¡å‹ã€‚å¦‚æœä½ æƒ³æµ‹è¯•å¯¹è®­ç»ƒå¾ªç¯è¿›è¡Œçš„ä»»ä½•è°ƒæ•´ï¼Œå¯ä»¥ç›´æ¥é€šè¿‡ç¼–è¾‘ä¸Šé¢æ˜¾ç¤ºçš„ä»£ç æ¥å®ç°ï¼

{/if}

### ä½¿ç”¨å¾®è°ƒæ¨¡å‹ 

æˆ‘ä»¬å·²ç»å‘ä½ å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨åœ¨æ¨¡å‹ä¸­å¿ƒä¸Šè¿›è¡Œå¾®è°ƒçš„æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨æ¨ç†å°éƒ¨ä»¶è¿›è¡Œæµ‹è¯•ã€‚è¦åœ¨æœ¬åœ°ä½¿ç”¨ `pipeline` æ¥ä½¿ç”¨å¾®è°ƒçš„æ¨¡å‹ï¼Œä½ åªéœ€æŒ‡å®šæ¨¡å‹æ ‡è¯†ç¬¦ï¼š

```python
from transformers import pipeline

## å°†å…¶æ›¿æ¢ä¸ºä½ è‡ªå·±çš„ checkpoint 
model_checkpoint = "huggingface-course/bert-finetuned-squad"
question_answerer = pipeline("question-answering", model=model_checkpoint)

context = """
Transformers is backed by the three most popular deep learning libraries â€” Jax, PyTorch and TensorFlow â€” with a seamless integration
between them. It's straightforward to train your models with one before loading them for inference with the other.
"""
question = "Which deep learning libraries back Transformers?"
question_answerer(question=question, context=context)
```

```python
{'score': 0.9979003071784973,
 'start': 78,
 'end': 105,
 'answer': 'Jax, PyTorch and TensorFlow'}
```

å¾ˆæ£’ï¼æˆ‘ä»¬çš„æ¨¡å‹ä¸ pipeline çš„é»˜è®¤æ¨¡å‹ä¸€æ ·æœ‰æ•ˆï¼

## 8.7 ç²¾é€šè‡ªç„¶è¯­è¨€å¤„ç† 

å¦‚æœä½ åœ¨è¯¾ç¨‹ä¸­åšåˆ°äº†è¿™ä¸€æ­¥ï¼Œæ­å–œä½ â€”â€”ä½ ç°åœ¨æ‹¥æœ‰äº†ç”¨ Transformers å’Œ Hugging Face ç”Ÿæ€ç³»ç»Ÿè§£å†³ï¼ˆå‡ ä¹ï¼‰ä»»ä½• NLP ä»»åŠ¡æ‰€éœ€çš„æ‰€æœ‰çŸ¥è¯†å’Œå·¥å…·ï¼

åœ¨å®Œæˆæ ¸å¿ƒ NLP ä»»åŠ¡çš„å¿«é€Ÿå…¥é—¨åï¼Œä½ åº”è¯¥ï¼š

* äº†è§£å“ªç§æ¶æ„ï¼ˆç¼–ç å™¨ã€è§£ç å™¨æˆ–ç¼–ç å™¨-è§£ç å™¨ï¼‰æœ€é€‚åˆå“ªç§ä»»åŠ¡
* äº†è§£é¢„è®­ç»ƒå’Œå¾®è°ƒè¯­è¨€æ¨¡å‹ä¹‹é—´çš„åŒºåˆ«
* äº†è§£å¦‚ä½•ä½¿ç”¨ `Trainer` API å’Œ Accelerate æˆ– TensorFlow å’Œ Keras çš„åˆ†å¸ƒå¼è®­ç»ƒåŠŸèƒ½æ¥è®­ç»ƒ Transformer æ¨¡å‹ï¼Œå…·ä½“é€‰æ‹©é‚£ä¸€ç§æ–¹æ³•å–å†³äºä½ æ‰€éœ€è¦å®Œæˆçš„ä»»åŠ¡ã€‚
* äº†è§£ ROUGE å’Œ BLEU ç­‰æŒ‡æ ‡åœ¨æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­çš„æ„ä¹‰å’Œå±€é™æ€§
* çŸ¥é“å¦‚ä½•åœ¨ Hub ä¸Šå’Œä½¿ç”¨ Transformers ä¸­çš„â€œç®¡é“â€ä¸ä½ çš„å¾®è°ƒæ¨¡å‹è¿›è¡Œäº¤äº’

å°½ç®¡æŒæ¡äº†æ‰€æœ‰è¿™äº›çŸ¥è¯†ï¼Œä½†æ€»æœ‰ä¸€å¤©ä½ ä¼šé‡åˆ°ä»£ç ä¸­çš„å›°éš¾é”™è¯¯ï¼Œæˆ–è€…å¯¹å¦‚ä½•è§£å†³ç‰¹å®šçš„ NLP é—®é¢˜æœ‰ç–‘é—®ã€‚å¹¸è¿çš„æ˜¯ï¼ŒHugging Face ç¤¾åŒºå°†éšæ—¶ä¸ºä½ æä¾›å¸®åŠ©ï¼åœ¨è¿™éƒ¨åˆ†è¯¾ç¨‹çš„æœ€åä¸€ç« ä¸­ï¼Œæˆ‘ä»¬å°†æ¢è®¨å¦‚ä½•è°ƒè¯• Transformer æ¨¡å‹å¹¶æœ‰æ•ˆåœ°å¯»æ±‚å¸®åŠ©ã€‚

### ç« æœ«æµ‹è¯• 

è®©æˆ‘ä»¬æµ‹è¯•ä¸€ä¸‹ä½ åœ¨è¿™ç« å­¦åˆ°äº†ä»€ä¹ˆï¼

####  1ï¼ä»¥ä¸‹å“ªäº›ä»»åŠ¡å¯ä»¥çœ‹ä½œä¸º token åˆ†ç±»é—®é¢˜ï¼Ÿ

1. æ‰¾å‡ºå¥å­ä¸­çš„è¯­æ³•æˆåˆ†ã€‚
2. åˆ¤æ–­ä¸€ä¸ªå¥å­çš„è¯­æ³•æ˜¯å¦æ­£ç¡®ã€‚
3. æ‰¾å‡ºå¥å­ä¸­æåˆ°çš„äººæˆ–ç‰©ã€‚
4. æ‰¾å‡ºå¥å­ä¸­å›ç­”é—®é¢˜çš„æ®µè½ã€‚

####  2ï¼ token åˆ†ç±»çš„é¢„å¤„ç†éƒ¨åˆ†ä¸å…¶ä»–é¢„å¤„ç†æµç¨‹æœ‰ä»€ä¹ˆä¸åŒï¼Ÿ

1. ä¸éœ€è¦è¿›è¡Œé¢„å¤„ç†ï¼›æ–‡æœ¬å·²ç»è¢«åˆ†è¯äº†ã€‚
2. è¾“å…¥çš„æ–‡æœ¬å°±æ˜¯å•è¯åºåˆ—ï¼Œæ‰€ä»¥æˆ‘ä»¬åªéœ€è¦è¿›è¡Œå­è¯åˆ†è¯ã€‚
3. æˆ‘ä»¬ä½¿ç”¨ <code>-100</code> æ¥æ ‡è®°ç‰¹æ®Š token ã€‚
4. åœ¨è¿›è¡Œæˆªæ–­/å¡«å……æ—¶ï¼Œæˆ‘ä»¬éœ€è¦ç¡®ä¿å°†æ ‡ç­¾æˆªæ–­æˆ–å¡«å……åˆ°ä¸è¾“å…¥ç›¸åŒçš„å¤§å°

####  3ï¼åœ¨ token åˆ†ç±»é—®é¢˜ä¸­ï¼Œå½“æˆ‘ä»¬åˆ†è¯å¹¶æƒ³è¦å­è¯åˆ†è¯æ—¶ï¼Œä¼šå‡ºç°ä»€ä¹ˆé—®é¢˜ï¼Ÿ

1. åˆ†è¯å™¨æ·»åŠ äº†ç‰¹æ®Šçš„ token ï¼Œæˆ‘ä»¬æ²¡æœ‰è¿™äº› token çš„æ ‡ç­¾ã€‚
2. æ¯ä¸ªè¯å¯èƒ½äº§ç”Ÿå¤šä¸ª token ï¼Œå› æ­¤æˆ‘ä»¬æœ€ç»ˆä¼šå¾—åˆ°æ¯”æˆ‘ä»¬æ‹¥æœ‰çš„æ ‡ç­¾æ›´å¤šçš„ token ã€‚
3. æ·»åŠ çš„ token æ²¡æœ‰æ ‡ç­¾ï¼Œæ‰€ä»¥æ²¡æœ‰é—®é¢˜ã€‚

####  4. â€œé¢†åŸŸé€‚åº”â€æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ

1. æˆ‘ä»¬åœ¨ä¸€ä¸ªæ•°æ®é›†ä¸Šè¿è¡Œæ¨¡å‹ï¼Œå¹¶è·å–è¯¥æ•°æ®é›†ä¸­æ¯ä¸ªæ ·æœ¬çš„é¢„æµ‹ç»“æœã€‚
2. å½“æˆ‘ä»¬åœ¨æ•°æ®é›†ä¸Šè®­ç»ƒæ¨¡å‹æ—¶ã€‚
3. æˆ‘ä»¬åœ¨ä¸€ä¸ªæ–°çš„æ•°æ®é›†ä¸Šå¾®è°ƒä¸€ä¸ªé¢„è®­ç»ƒçš„æ¨¡å‹ï¼Œå¹¶åœ¨æµ‹è¯•é›†ä¸Šæœ‰ä¸€å®šçš„é€‚åº”æ€§ã€‚
4. å½“æˆ‘ä»¬å°†è¢«æ¨¡å‹åˆ†ç±»é”™è¯¯çš„æ ·æœ¬æ·»åŠ åˆ°æ•°æ®é›†ä¸­ï¼Œä½¿å¾—æˆ‘ä»¬çš„æ¨¡å‹æ›´åŠ å¥å£®ã€‚

####  5ï¼æ©ç è¯­è¨€å»ºæ¨¡é—®é¢˜ä¸­çš„æ ‡ç­¾æ˜¯ä»€ä¹ˆï¼Ÿ

1. è¾“å…¥å¥å­ä¸­çš„ä¸€äº›æ ‡è®°æ˜¯éšæœºå±è”½çš„ï¼Œæ ‡ç­¾å°±æ˜¯åŸå§‹è¾“å…¥ token ã€‚
2. è¾“å…¥å¥å­ä¸­çš„ä¸€äº› token æ˜¯éšæœºå±è”½çš„ï¼Œæ ‡ç­¾æ˜¯åŸå§‹çš„è¾“å…¥ token å‘å·¦ç§»åŠ¨å½¢æˆçš„ã€‚
3. è¾“å…¥å¥å­ä¸­çš„ä¸€äº› token æ˜¯éšæœºå±è”½çš„ï¼Œæ ‡ç­¾æ˜¯è¿™ä¸ªå¥å­æ˜¯è‚¯å®šçš„è¿˜æ˜¯å¦å®šçš„ã€‚
4. ä¸¤ä¸ªå¥å­ä¸­çš„ä¸€äº› token æ˜¯éšæœºå±è”½çš„ï¼Œæ ‡ç­¾æ˜¯ä¸¤ä¸ªå¥å­æ˜¯å¦ç›¸ä¼¼ã€‚

####  6ï¼å“ªäº›ä»»åŠ¡å¯ä»¥è¢«çœ‹ä½œæ˜¯åºåˆ—åˆ°åºåˆ—çš„é—®é¢˜ï¼Ÿ

1. æ’°å†™é•¿æ–‡æ¡£çš„ç®€çŸ­è¯„è®º
2. å›ç­”å…³äºä¸€ä¸ªæ–‡æ¡£çš„é—®é¢˜ã€‚
3. å°†ä¸€æ®µä¸­æ–‡æ–‡æœ¬ç¿»è¯‘æˆè‹±æ–‡ã€‚
4. ä¿®æ­£æˆ‘ä¾„å­/æœ‹å‹å‘æ¥çš„ä¿¡æ¯ï¼Œçº æ­£ä»–ä»¬çš„è¯­æ³•é”™è¯¯

####  7ï¼å¯¹äºåºåˆ—åˆ°åºåˆ—çš„é—®é¢˜ï¼Œé¢„å¤„ç†æ•°æ®çš„æ­£ç¡®æ–¹æ³•æ˜¯ä»€ä¹ˆï¼Ÿ

1. è¾“å…¥å’Œç›®æ ‡å¿…é¡»ä¸€èµ·å‘é€åˆ° tokenizer ï¼Œä½¿ç”¨ `input = ...` å’Œ `target = ...` ã€‚
2. è¾“å…¥å’Œç›®æ ‡éƒ½å¿…é¡»åœ¨ tokenizer çš„ä¸¤æ¬¡ç‹¬ç«‹è°ƒç”¨ä¸­è¿›è¡Œé¢„å¤„ç†ã€‚
3. åƒå¾€å¸¸ä¸€æ ·ï¼Œæˆ‘ä»¬åªéœ€è¦å¯¹è¾“å…¥è¿›è¡Œ tokenize
4. è¾“å…¥åºåˆ—å’Œç›®æ ‡åºåˆ—éƒ½éœ€è¦é€šè¿‡ç‰¹æ®Šçš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨åˆ†åˆ«å‘é€ç»™ tokenizer è¿›è¡Œé¢„å¤„ç†ã€‚

####  8ï¼ä¸ºä»€ä¹ˆéœ€è¦æœ‰ä¸€ä¸ªç‰¹å®šçš„ `Trainer `å­ç±»æ¥è§£å†³åºåˆ—åˆ°åºåˆ—é—®é¢˜ï¼Ÿ

1. å› ä¸ºåºåˆ—åˆ°åºåˆ—é—®é¢˜ä½¿ç”¨è‡ªå®šä¹‰çš„æŸå¤±è®¡ç®—æ–¹æ³•ï¼Œå¿½ç•¥ <code>-100</code> çš„æ ‡ç­¾
2. å› ä¸ºåºåˆ—åˆ°åºåˆ—é—®é¢˜éœ€è¦ç‰¹æ®Šçš„è¯„ä¼°å¾ªç¯
3. å› ä¸ºè¯¥é—®é¢˜ä¸­çš„é¢„æµ‹ç›®æ ‡æ˜¯åºåˆ—åˆ°åºåˆ—ä¸­é—®é¢˜éƒ¨åˆ†çš„æ–‡æœ¬
4. å› ä¸ºæˆ‘ä»¬åœ¨åºåˆ—åˆ°åºåˆ—é—®é¢˜ä¸­ä½¿ç”¨äº†ä¸¤ä¸ªæ¨¡å‹

####  9ï¼ä¸ºä»€ä¹ˆåœ¨ Transformer æ¨¡å‹ä¸Šè°ƒç”¨ `compile()` æ—¶é€šå¸¸ä¸éœ€è¦æŒ‡å®šæŸå¤±çš„è®¡ç®—æ–¹æ³•ï¼Ÿ

1. å› ä¸º Transformer æ¨¡å‹æ˜¯ä»¥éç›‘ç£å¼å­¦ä¹ è¿›è¡Œè®­ç»ƒ
2. å› ä¸ºé»˜è®¤ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨æŸå¤±è®¡ç®—æ–¹æ³•
3. å› ä¸ºæˆ‘ä»¬åœ¨è®­ç»ƒåè®¡ç®—è¯„ä¼°æŒ‡æ ‡
4. å› ä¸ºæŸå¤±æ˜¯åœ¨`model.fit()`ä¸­æŒ‡å®šçš„

####  10ï¼ä½ åº”è¯¥åœ¨ä»€ä¹ˆæ—¶å€™é¢„å…ˆè®­ç»ƒä¸€ä¸ªæ–°çš„æ¨¡å‹ï¼Ÿ

1. å½“ä½ çš„ç‰¹å®šè¯­è¨€æ²¡æœ‰é¢„è®­ç»ƒæ¨¡å‹å¯ç”¨æ—¶
2. å½“ä½ æœ‰å¤§é‡å¯ç”¨çš„æ•°æ®æ—¶ï¼Œå³ä½¿æœ‰ä¸€ä¸ªç»è¿‡è®­ç»ƒçš„æ¨¡å‹å¯ä»¥å¤„ç†è¿™äº›æ•°æ®
3. å½“ä½ æ‹…å¿ƒä½ æ‰€ä½¿ç”¨çš„é¢„å…ˆè®­ç»ƒè¿‡çš„æ¨¡å‹çš„åå·®æ—¶
4. å½“å¯ç”¨çš„é¢„å…ˆè®­ç»ƒå¥½çš„æ¨¡å‹è¿˜ä¸å¤Ÿå¥½çš„æ—¶å€™

####  11ï¼ä¸ºä»€ä¹ˆåœ¨å¤§é‡çš„æ–‡æœ¬ä¸Šé¢„å…ˆè®­ç»ƒä¸€ä¸ªè¯­è¨€æ¨¡å‹æ˜¯å¾ˆå®¹æ˜“çš„å‘¢ï¼Ÿ

1. å½“ä½ æ‹…å¿ƒä½ æ‰€ä½¿ç”¨çš„é¢„å…ˆè®­ç»ƒçš„æ¨¡å‹çš„åå·®æ—¶
2. å› ä¸ºé¢„è®­ç»ƒä¸éœ€è¦äººå·¥æ ‡è®°æ•°æ®
3. å› ä¸ºTransformers åº“åªéœ€è¦å‡ è¡Œä»£ç å°±å¯ä»¥å¼€å§‹è®­ç»ƒ

####  12ï¼é—®ç­”ä»»åŠ¡é¢„å¤„ç†æ•°æ®æ—¶ï¼Œä¸»è¦çš„æŒ‘æˆ˜æ˜¯ä»€ä¹ˆï¼Ÿ

1. ä½ éœ€è¦å¯¹è¾“å…¥è¿›è¡Œ tokenizeã€‚
2. ä½ éœ€è¦å¤„ç†éå¸¸é•¿çš„ä¸Šä¸‹æ–‡ï¼Œè¿™äº›ä¸Šä¸‹æ–‡æä¾›äº†ä¸€äº›è®­ç»ƒç‰¹æ€§ï¼Œè¿™äº›ç‰¹æ€§å¯èƒ½æœ‰ä¹Ÿå¯èƒ½æ²¡æœ‰ç­”æ¡ˆã€‚
3. ä½ éœ€è¦å°†é—®é¢˜çš„ç­”æ¡ˆä»¥åŠè¾“å…¥è¿›è¡Œ tokenizeã€‚
4. ä½ éœ€è¦ä»æ–‡æœ¬ä¸­æ‰¾åˆ°ç­”æ¡ˆéƒ¨åˆ†åœ¨ tokenize åçš„è¾“å…¥ä¸­å¯¹åº”çš„èµ·å§‹å’Œç»“æŸä½ç½®ã€‚ã€‚

####  13ï¼é—®ç­”ä»»åŠ¡ä¸­çš„åå¤„ç†é€šå¸¸æ˜¯æ€æ ·è¿›è¡Œçš„ï¼Ÿ

1. æ¨¡å‹ç»™å‡ºäº†ç­”æ¡ˆçš„å¼€å§‹ä½ç½®å’Œç»“æŸä½ç½®ï¼Œä½ åªéœ€è¦è§£ç ç›¸å¯¹åº”ç­”æ¡ˆè·¨åº¦çš„ tokensã€‚
2. è¯¥æ¨¡å‹ä¸ºæ¯ä¸ªç¤ºä¾‹åˆ›å»ºçš„æ¯ä¸ªç‰¹å¾æä¾›äº†ç­”æ¡ˆçš„èµ·å§‹å’Œç»“æŸä½ç½®ï¼Œä½ åªéœ€åœ¨å¾—åˆ†æœ€é«˜çš„é‚£ä¸ªç‰¹å¾ä¸­è§£ç ç›¸åº”çš„ tokensã€‚
3. æ¨¡å‹ä¼šä¸ºæ¯ä¸ªæ ·æœ¬åˆ›å»ºçš„æ¯ä¸ªç‰¹å¾ç»™å‡ºäº†ç­”æ¡ˆçš„èµ·å§‹å’Œç»“æŸä½ç½®ï¼Œä½ åªéœ€è¦å°†å®ƒä»¬ä¸ä¸Šä¸‹æ–‡ä¸­çš„ç‰‡æ®µåŒ¹é…ï¼Œæ‰¾åˆ°å¾—åˆ†æœ€é«˜çš„é‚£ä¸ªã€‚
4. æ¨¡å‹ç”Ÿæˆä¸€ä¸ªç­”æ¡ˆï¼Œä½ åªéœ€è¦è§£ç å®ƒã€‚

### è§£æ

####  1ï¼ä»¥ä¸‹å“ªäº›ä»»åŠ¡å¯ä»¥çœ‹ä½œä¸º token åˆ†ç±»é—®é¢˜ï¼Ÿ

æ­£ç¡®é€‰é¡¹: 1. æ‰¾å‡ºå¥å­ä¸­çš„è¯­æ³•æˆåˆ†ã€‚

æ­£ç¡®é€‰é¡¹: 3. æ‰¾å‡ºå¥å­ä¸­æåˆ°çš„äººæˆ–ç‰©ã€‚

1. æ‰¾å‡ºå¥å­ä¸­çš„è¯­æ³•æˆåˆ†ã€‚    
è§£æ: æ­£ç¡®ï¼æˆ‘ä»¬å¯ä»¥ä¸ºæ¯ä¸ªè¯æ‰“ä¸Šåè¯ã€åŠ¨è¯ç­‰æ ‡ç­¾ã€‚
2. åˆ¤æ–­ä¸€ä¸ªå¥å­çš„è¯­æ³•æ˜¯å¦æ­£ç¡®ã€‚    
è§£æ: ä¸ï¼Œè¿™æ˜¯ä¸€ä¸ªåºåˆ—åˆ†ç±»é—®é¢˜ã€‚
3. æ‰¾å‡ºå¥å­ä¸­æåˆ°çš„äººæˆ–ç‰©ã€‚    
è§£æ: æ­£ç¡®ï¼æˆ‘ä»¬å¯ä»¥å°†æ¯ä¸ªè¯æ ‡æ³¨ä¸ºäººåæˆ–éäººåã€‚
4. æ‰¾å‡ºå¥å­ä¸­å›ç­”é—®é¢˜çš„æ®µè½ã€‚    
è§£æ: ä¸å¯¹ï¼Œé‚£åº”è¯¥æ˜¯ä¸€ä¸ªé—®é¢˜å›ç­”ï¼ˆQAï¼‰é—®é¢˜ã€‚

####  2ï¼ token åˆ†ç±»çš„é¢„å¤„ç†éƒ¨åˆ†ä¸å…¶ä»–é¢„å¤„ç†æµç¨‹æœ‰ä»€ä¹ˆä¸åŒï¼Ÿ

æ­£ç¡®é€‰é¡¹: 2. è¾“å…¥çš„æ–‡æœ¬å°±æ˜¯å•è¯åºåˆ—ï¼Œæ‰€ä»¥æˆ‘ä»¬åªéœ€è¦è¿›è¡Œå­è¯åˆ†è¯ã€‚

æ­£ç¡®é€‰é¡¹: 4. åœ¨è¿›è¡Œæˆªæ–­/å¡«å……æ—¶ï¼Œæˆ‘ä»¬éœ€è¦ç¡®ä¿å°†æ ‡ç­¾æˆªæ–­æˆ–å¡«å……åˆ°ä¸è¾“å…¥ç›¸åŒçš„å¤§å°

1. ä¸éœ€è¦è¿›è¡Œé¢„å¤„ç†ï¼›æ–‡æœ¬å·²ç»è¢«åˆ†è¯äº†ã€‚    
è§£æ: è™½ç„¶æ–‡æœ¬ç¡®å®å·²ç»è¢«åˆ†è¯ï¼Œä½†æˆ‘ä»¬ä»ç„¶éœ€è¦è¿›è¡Œå­è¯åˆ†è¯ã€‚
2. è¾“å…¥çš„æ–‡æœ¬å°±æ˜¯å•è¯åºåˆ—ï¼Œæ‰€ä»¥æˆ‘ä»¬åªéœ€è¦è¿›è¡Œå­è¯åˆ†è¯ã€‚    
è§£æ: æ­£ç¡®ï¼è¿™ä¸é€šå¸¸éœ€è¦å®Œæ•´çš„åˆ†è¯æµç¨‹é¢„å¤„ç†ä¸åŒã€‚ä½ èƒ½æƒ³åˆ°å¦ä¸€ä¸ªå·®å¼‚å—ï¼Ÿ
3. æˆ‘ä»¬ä½¿ç”¨ <code>-100</code> æ¥æ ‡è®°ç‰¹æ®Š token ã€‚    
è§£æ: è¿™ä¸æ˜¯ token åˆ†ç±»ç‰¹æœ‰çš„ â€”â€” æˆ‘ä»¬æ€»æ˜¯ç”¨ <code>-100</code> ä½œä¸ºæˆ‘ä»¬æƒ³åœ¨æŸå¤±ä¸­å¿½ç•¥çš„ token çš„æ ‡ç­¾ã€‚
4. åœ¨è¿›è¡Œæˆªæ–­/å¡«å……æ—¶ï¼Œæˆ‘ä»¬éœ€è¦ç¡®ä¿å°†æ ‡ç­¾æˆªæ–­æˆ–å¡«å……åˆ°ä¸è¾“å…¥ç›¸åŒçš„å¤§å°    
è§£æ: çš„ç¡®å¦‚æ­¤ï¼ä½†è¿™å¹¶ä¸æ˜¯å”¯ä¸€çš„åŒºåˆ«ã€‚

####  3ï¼åœ¨ token åˆ†ç±»é—®é¢˜ä¸­ï¼Œå½“æˆ‘ä»¬åˆ†è¯å¹¶æƒ³è¦å­è¯åˆ†è¯æ—¶ï¼Œä¼šå‡ºç°ä»€ä¹ˆé—®é¢˜ï¼Ÿ

æ­£ç¡®é€‰é¡¹: 2. æ¯ä¸ªè¯å¯èƒ½äº§ç”Ÿå¤šä¸ª token ï¼Œå› æ­¤æˆ‘ä»¬æœ€ç»ˆä¼šå¾—åˆ°æ¯”æˆ‘ä»¬æ‹¥æœ‰çš„æ ‡ç­¾æ›´å¤šçš„ token ã€‚

1. åˆ†è¯å™¨æ·»åŠ äº†ç‰¹æ®Šçš„ token ï¼Œæˆ‘ä»¬æ²¡æœ‰è¿™äº› token çš„æ ‡ç­¾ã€‚    
è§£æ: æˆ‘ä»¬æŠŠè¿™äº› token IDè®¾ç½®ä¸º <code>-100</code>ï¼Œæ‰€ä»¥åœ¨è®¡ç®—æŸå¤±æ—¶å®ƒä»¬ä¼šè¢«å¿½ç•¥ã€‚
2. æ¯ä¸ªè¯å¯èƒ½äº§ç”Ÿå¤šä¸ª token ï¼Œå› æ­¤æˆ‘ä»¬æœ€ç»ˆä¼šå¾—åˆ°æ¯”æˆ‘ä»¬æ‹¥æœ‰çš„æ ‡ç­¾æ›´å¤šçš„ token ã€‚    
è§£æ: è¿™æ˜¯ä¸»è¦çš„é—®é¢˜ï¼Œæˆ‘ä»¬éœ€è¦å°†åŸå§‹æ ‡ç­¾ä¸ token å¯¹é½ã€‚
3. æ·»åŠ çš„ token æ²¡æœ‰æ ‡ç­¾ï¼Œæ‰€ä»¥æ²¡æœ‰é—®é¢˜ã€‚    
è§£æ: è¿™æ˜¯ä¸æ­£ç¡®çš„ï¼›æˆ‘ä»¬éœ€è¦å’Œ token æ•°é‡ç›¸åŒçš„æ ‡ç­¾ï¼Œå¦åˆ™æˆ‘ä»¬çš„æ¨¡å‹ä¼šæŠ¥é”™ã€‚

####  4. â€œé¢†åŸŸé€‚åº”â€æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ

æ­£ç¡®é€‰é¡¹: 3. æˆ‘ä»¬åœ¨ä¸€ä¸ªæ–°çš„æ•°æ®é›†ä¸Šå¾®è°ƒä¸€ä¸ªé¢„è®­ç»ƒçš„æ¨¡å‹ï¼Œå¹¶åœ¨æµ‹è¯•é›†ä¸Šæœ‰ä¸€å®šçš„é€‚åº”æ€§ã€‚

1. æˆ‘ä»¬åœ¨ä¸€ä¸ªæ•°æ®é›†ä¸Šè¿è¡Œæ¨¡å‹ï¼Œå¹¶è·å–è¯¥æ•°æ®é›†ä¸­æ¯ä¸ªæ ·æœ¬çš„é¢„æµ‹ç»“æœã€‚    
è§£æ: ä¸ï¼Œè¿™åªæ˜¯æ¨¡å‹æ¨ç†çš„è¿‡ç¨‹ã€‚
2. å½“æˆ‘ä»¬åœ¨æ•°æ®é›†ä¸Šè®­ç»ƒæ¨¡å‹æ—¶ã€‚    
è§£æ: ä¸å¯¹ï¼Œè¿™åªæ˜¯è®­ç»ƒæ¨¡å‹çš„è¿‡ç¨‹ï¼›è¿™é‡Œæ²¡æœ‰é€‚åº”ã€‚
3. æˆ‘ä»¬åœ¨ä¸€ä¸ªæ–°çš„æ•°æ®é›†ä¸Šå¾®è°ƒä¸€ä¸ªé¢„è®­ç»ƒçš„æ¨¡å‹ï¼Œå¹¶åœ¨æµ‹è¯•é›†ä¸Šæœ‰ä¸€å®šçš„é€‚åº”æ€§ã€‚    
è§£æ: æ­£ç¡®ï¼æ¨¡å‹å°†å…¶çŸ¥è¯†é€‚åº”åˆ°äº†æ–°çš„æ•°æ®é›†ä¸Šã€‚
4. å½“æˆ‘ä»¬å°†è¢«æ¨¡å‹åˆ†ç±»é”™è¯¯çš„æ ·æœ¬æ·»åŠ åˆ°æ•°æ®é›†ä¸­ï¼Œä½¿å¾—æˆ‘ä»¬çš„æ¨¡å‹æ›´åŠ å¥å£®ã€‚    
è§£æ: å¦‚æœä½ å®šæœŸé‡æ–°è®­ç»ƒæ¨¡å‹ï¼Œçš„ç¡®åº”è¯¥è¿™æ ·åšï¼Œä½†è¿™ä¸æ˜¯é¢†åŸŸé€‚åº”ã€‚

####  5ï¼æ©ç è¯­è¨€å»ºæ¨¡é—®é¢˜ä¸­çš„æ ‡ç­¾æ˜¯ä»€ä¹ˆï¼Ÿ

æ­£ç¡®é€‰é¡¹: 1. è¾“å…¥å¥å­ä¸­çš„ä¸€äº›æ ‡è®°æ˜¯éšæœºå±è”½çš„ï¼Œæ ‡ç­¾å°±æ˜¯åŸå§‹è¾“å…¥ token ã€‚

1. è¾“å…¥å¥å­ä¸­çš„ä¸€äº›æ ‡è®°æ˜¯éšæœºå±è”½çš„ï¼Œæ ‡ç­¾å°±æ˜¯åŸå§‹è¾“å…¥ token ã€‚    
è§£æ: å°±æ˜¯è¿™æ ·ï¼
2. è¾“å…¥å¥å­ä¸­çš„ä¸€äº› token æ˜¯éšæœºå±è”½çš„ï¼Œæ ‡ç­¾æ˜¯åŸå§‹çš„è¾“å…¥ token å‘å·¦ç§»åŠ¨å½¢æˆçš„ã€‚    
è§£æ: ä¸ï¼Œå°†æ ‡ç­¾å‘å·¦ç§»åŠ¨ç›¸å½“äºé¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯ï¼Œè¿™å°±æ˜¯å› æœè¯­è¨€æ¨¡å‹ã€‚
3. è¾“å…¥å¥å­ä¸­çš„ä¸€äº› token æ˜¯éšæœºå±è”½çš„ï¼Œæ ‡ç­¾æ˜¯è¿™ä¸ªå¥å­æ˜¯è‚¯å®šçš„è¿˜æ˜¯å¦å®šçš„ã€‚    
è§£æ: è¿™æ˜¯ä¸€ä¸ªæ•°æ®å¢å¼ºçš„åºåˆ—åˆ†ç±»é—®é¢˜ï¼Œè€Œä¸æ˜¯æ©ç è¯­è¨€å»ºæ¨¡ã€‚
4. ä¸¤ä¸ªå¥å­ä¸­çš„ä¸€äº› token æ˜¯éšæœºå±è”½çš„ï¼Œæ ‡ç­¾æ˜¯ä¸¤ä¸ªå¥å­æ˜¯å¦ç›¸ä¼¼ã€‚    
è§£æ: è¿™æ˜¯ä¸€ä¸ªæ•°æ®å¢å¼ºçš„åºåˆ—åˆ†ç±»é—®é¢˜ï¼Œè€Œä¸æ˜¯æ©ç è¯­è¨€å»ºæ¨¡ã€‚

####  6ï¼å“ªäº›ä»»åŠ¡å¯ä»¥è¢«çœ‹ä½œæ˜¯åºåˆ—åˆ°åºåˆ—çš„é—®é¢˜ï¼Ÿ

æ­£ç¡®é€‰é¡¹: 1. æ’°å†™é•¿æ–‡æ¡£çš„ç®€çŸ­è¯„è®º

æ­£ç¡®é€‰é¡¹: 2. å›ç­”å…³äºä¸€ä¸ªæ–‡æ¡£çš„é—®é¢˜ã€‚

æ­£ç¡®é€‰é¡¹: 3. å°†ä¸€æ®µä¸­æ–‡æ–‡æœ¬ç¿»è¯‘æˆè‹±æ–‡ã€‚

æ­£ç¡®é€‰é¡¹: 4. ä¿®æ­£æˆ‘ä¾„å­/æœ‹å‹å‘æ¥çš„ä¿¡æ¯ï¼Œçº æ­£ä»–ä»¬çš„è¯­æ³•é”™è¯¯

1. æ’°å†™é•¿æ–‡æ¡£çš„ç®€çŸ­è¯„è®º    
è§£æ: æ˜¯çš„ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–‡æ¡£æ‘˜è¦ä»»åŠ¡ã€‚è¯•è¯•å¦ä¸€ä¸ªç­”æ¡ˆï¼
2. å›ç­”å…³äºä¸€ä¸ªæ–‡æ¡£çš„é—®é¢˜ã€‚    
è§£æ: è¿™å¯ä»¥è¢«æ„å»ºä¸ºä¸€ä¸ªåºåˆ—åˆ°åºåˆ—çš„é—®é¢˜ã€‚è¿™ä¸æ˜¯å”¯ä¸€çš„æ­£ç¡®ç­”æ¡ˆã€‚
3. å°†ä¸€æ®µä¸­æ–‡æ–‡æœ¬ç¿»è¯‘æˆè‹±æ–‡ã€‚    
è§£æ: è¿™ç»å¯¹æ˜¯ä¸€ä¸ªä»åºåˆ—åˆ°åºåˆ—çš„é—®é¢˜ã€‚ä½ èƒ½å‘ç°å¦ä¸€ä¸ªå—ï¼Ÿ
4. ä¿®æ­£æˆ‘ä¾„å­/æœ‹å‹å‘æ¥çš„ä¿¡æ¯ï¼Œçº æ­£ä»–ä»¬çš„è¯­æ³•é”™è¯¯    
è§£æ: è¿™æ˜¯ä¸€ç§ç¿»è¯‘é—®é¢˜ï¼Œæ‰€ä»¥ç»å¯¹æ˜¯ä¸€ä¸ªåºåˆ—åˆ°åºåˆ—çš„ä»»åŠ¡ã€‚è¿™ä¸æ˜¯å”¯ä¸€çš„æ­£ç¡®ç­”æ¡ˆã€‚

####  7ï¼å¯¹äºåºåˆ—åˆ°åºåˆ—çš„é—®é¢˜ï¼Œé¢„å¤„ç†æ•°æ®çš„æ­£ç¡®æ–¹æ³•æ˜¯ä»€ä¹ˆï¼Ÿ

æ­£ç¡®é€‰é¡¹: 4. è¾“å…¥åºåˆ—å’Œç›®æ ‡åºåˆ—éƒ½éœ€è¦é€šè¿‡ç‰¹æ®Šçš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨åˆ†åˆ«å‘é€ç»™ tokenizer è¿›è¡Œé¢„å¤„ç†ã€‚

1. è¾“å…¥å’Œç›®æ ‡å¿…é¡»ä¸€èµ·å‘é€åˆ° tokenizer ï¼Œä½¿ç”¨ `input = ...` å’Œ `target = ...` ã€‚    
è§£æ: è¿™å¯èƒ½æ˜¯æˆ‘ä»¬æœªæ¥è¦æ·»åŠ çš„ä¸€ä¸ª APIï¼Œä½†ç°åœ¨è¿˜ä¸è¡Œã€‚
2. è¾“å…¥å’Œç›®æ ‡éƒ½å¿…é¡»åœ¨ tokenizer çš„ä¸¤æ¬¡ç‹¬ç«‹è°ƒç”¨ä¸­è¿›è¡Œé¢„å¤„ç†ã€‚    
è§£æ: è¿™æ˜¯æ­£ç¡®çš„ï¼Œä½†æ˜¯ä¸å®Œæ•´ã€‚ä½ è¿˜éœ€è¦åšä¸€äº›äº‹æƒ…æ¥ç¡®ä¿ tokenizer æ­£ç¡®å¤„ç†ä¸¤è€…ã€‚
3. åƒå¾€å¸¸ä¸€æ ·ï¼Œæˆ‘ä»¬åªéœ€è¦å¯¹è¾“å…¥è¿›è¡Œ tokenize    
è§£æ: åœ¨ä¸€ä¸ªåºåˆ—åˆ°åºåˆ—é—®é¢˜ä¸­ï¼Œå¹¶ä¸ä»…ä»…æ˜¯è¾“å…¥æ–‡æœ¬éœ€è¦è¿›è¡Œ tokenizeï¼Œç›®æ ‡æ–‡æœ¬ä¹Ÿéœ€è¦è¿›è¡ŒåŒæ ·çš„è½¬æ¢ï¼
4. è¾“å…¥åºåˆ—å’Œç›®æ ‡åºåˆ—éƒ½éœ€è¦é€šè¿‡ç‰¹æ®Šçš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨åˆ†åˆ«å‘é€ç»™ tokenizer è¿›è¡Œé¢„å¤„ç†ã€‚    
è§£æ: è¿™æ˜¯æ­£ç¡®çš„ï¼Œ tokenizer éœ€è¦é€šè¿‡è¯¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨æ‰¾åˆ°ç›®æ ‡åºåˆ—çš„èŒƒå›´å¹¶è¿›è¡Œå¤„ç†ã€‚

####  8ï¼ä¸ºä»€ä¹ˆéœ€è¦æœ‰ä¸€ä¸ªç‰¹å®šçš„ `Trainer `å­ç±»æ¥è§£å†³åºåˆ—åˆ°åºåˆ—é—®é¢˜ï¼Ÿ

æ­£ç¡®é€‰é¡¹: 2. å› ä¸ºåºåˆ—åˆ°åºåˆ—é—®é¢˜éœ€è¦ç‰¹æ®Šçš„è¯„ä¼°å¾ªç¯

1. å› ä¸ºåºåˆ—åˆ°åºåˆ—é—®é¢˜ä½¿ç”¨è‡ªå®šä¹‰çš„æŸå¤±è®¡ç®—æ–¹æ³•ï¼Œå¿½ç•¥ <code>-100</code> çš„æ ‡ç­¾    
è§£æ: è¿™æ ¹æœ¬ä¸æ˜¯è‡ªå®šä¹‰çš„æŸå¤±è®¡ç®—æ–¹æ³•ï¼Œè€Œæ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ä¸­ä¸€ç§å¸¸è§„çš„å¿½ç•¥ç‰¹å®š token è®¡ç®—æ–¹å¼ã€‚
2. å› ä¸ºåºåˆ—åˆ°åºåˆ—é—®é¢˜éœ€è¦ç‰¹æ®Šçš„è¯„ä¼°å¾ªç¯    
è§£æ: æ²¡é”™ã€‚åºåˆ—åˆ°åºåˆ—æ¨¡å‹çš„é¢„æµ‹é€šå¸¸éœ€è¦ä½¿ç”¨ <code>generate()</code> æ–¹æ³•
3. å› ä¸ºè¯¥é—®é¢˜ä¸­çš„é¢„æµ‹ç›®æ ‡æ˜¯åºåˆ—åˆ°åºåˆ—ä¸­é—®é¢˜éƒ¨åˆ†çš„æ–‡æœ¬    
è§£æ: <code>Trainer</code> å¹¶ä¸å…³å¿ƒè¿™äº›ï¼Œå› ä¸ºè¿™äº›æ–‡æœ¬åœ¨è¿›å…¥`Tranier`ä¹‹å‰å·²ç»è¢«é¢„å¤„ç†è¿‡ã€‚
4. å› ä¸ºæˆ‘ä»¬åœ¨åºåˆ—åˆ°åºåˆ—é—®é¢˜ä¸­ä½¿ç”¨äº†ä¸¤ä¸ªæ¨¡å‹    
è§£æ: æˆ‘ä»¬ç¡®å®ä»¥æŸç§æ–¹å¼ä½¿ç”¨ä¸¤ç§æ¨¡å‹ï¼Œç¼–ç å™¨å’Œè§£ç å™¨ï¼Œä½†å®ƒä»¬è¢«ç»„åˆåœ¨ä¸€ä¸ªæ¨¡å‹ä¸­

####  9ï¼ä¸ºä»€ä¹ˆåœ¨ Transformer æ¨¡å‹ä¸Šè°ƒç”¨ `compile()` æ—¶é€šå¸¸ä¸éœ€è¦æŒ‡å®šæŸå¤±çš„è®¡ç®—æ–¹æ³•ï¼Ÿ

æ­£ç¡®é€‰é¡¹: 2. å› ä¸ºé»˜è®¤ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨æŸå¤±è®¡ç®—æ–¹æ³•

1. å› ä¸º Transformer æ¨¡å‹æ˜¯ä»¥éç›‘ç£å¼å­¦ä¹ è¿›è¡Œè®­ç»ƒ    
è§£æ: å¹¶éå¦‚æ­¤â€”â€”å³ä½¿æ˜¯æ— ç›‘ç£å­¦ä¹ ä¹Ÿéœ€è¦æŸå¤±å‡½æ•°ï¼
2. å› ä¸ºé»˜è®¤ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨æŸå¤±è®¡ç®—æ–¹æ³•    
è§£æ: æ²¡é”™ï¼
3. å› ä¸ºæˆ‘ä»¬åœ¨è®­ç»ƒåè®¡ç®—è¯„ä¼°æŒ‡æ ‡    
è§£æ: æˆ‘ä»¬ç¡®å®ç»å¸¸è¿™æ ·åšï¼Œä½†è¿™å¹¶ä¸èƒ½è§£é‡Šæˆ‘ä»¬åœ¨è®­ç»ƒä¸­ä¼˜åŒ–çš„æŸå¤±å€¼æ˜¯ä»å“ªé‡Œå¾—åˆ°çš„ã€‚
4. å› ä¸ºæŸå¤±æ˜¯åœ¨`model.fit()`ä¸­æŒ‡å®šçš„    
è§£æ: ä¸ï¼Œä¸€æ—¦è¿è¡Œ`model.compile()`ï¼ŒæŸå¤±å‡½æ•°å°±æ€»æ˜¯å›ºå®šçš„ï¼Œå¹¶ä¸”ä¸èƒ½åœ¨`model.fit()`ä¸­æ›´æ”¹ã€‚

####  10ï¼ä½ åº”è¯¥åœ¨ä»€ä¹ˆæ—¶å€™é¢„å…ˆè®­ç»ƒä¸€ä¸ªæ–°çš„æ¨¡å‹ï¼Ÿ

æ­£ç¡®é€‰é¡¹: 1. å½“ä½ çš„ç‰¹å®šè¯­è¨€æ²¡æœ‰é¢„è®­ç»ƒæ¨¡å‹å¯ç”¨æ—¶

æ­£ç¡®é€‰é¡¹: 3. å½“ä½ æ‹…å¿ƒä½ æ‰€ä½¿ç”¨çš„é¢„å…ˆè®­ç»ƒè¿‡çš„æ¨¡å‹çš„åå·®æ—¶

1. å½“ä½ çš„ç‰¹å®šè¯­è¨€æ²¡æœ‰é¢„è®­ç»ƒæ¨¡å‹å¯ç”¨æ—¶    
è§£æ: æ²¡é”™ã€‚
2. å½“ä½ æœ‰å¤§é‡å¯ç”¨çš„æ•°æ®æ—¶ï¼Œå³ä½¿æœ‰ä¸€ä¸ªç»è¿‡è®­ç»ƒçš„æ¨¡å‹å¯ä»¥å¤„ç†è¿™äº›æ•°æ®    
è§£æ: åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½ å¯èƒ½åº”è¯¥ä½¿ç”¨é¢„è®­ç»ƒçš„æ¨¡å‹å¹¶å¯¹æ•°æ®è¿›è¡Œå¾®è°ƒï¼Œä»¥é¿å…å·¨å¤§çš„è®¡ç®—æˆæœ¬ã€‚
3. å½“ä½ æ‹…å¿ƒä½ æ‰€ä½¿ç”¨çš„é¢„å…ˆè®­ç»ƒè¿‡çš„æ¨¡å‹çš„åå·®æ—¶    
è§£æ: è¿™æ˜¯æ­£ç¡®çš„ï¼Œä½†æ˜¯ä½ éœ€è¦ç¡®ä¿ä½ ç”¨äºè®­ç»ƒçš„æ•°æ®çœŸçš„æ›´å¥½ã€‚
4. å½“å¯ç”¨çš„é¢„å…ˆè®­ç»ƒå¥½çš„æ¨¡å‹è¿˜ä¸å¤Ÿå¥½çš„æ—¶å€™    
è§£æ: é‚£ä¹ˆï¼Œä½ ç¡®å®šä½ å·²ç»æ­£ç¡®åœ°è°ƒè¯•äº†ä½ çš„è®­ç»ƒå—ï¼Ÿ

####  11ï¼ä¸ºä»€ä¹ˆåœ¨å¤§é‡çš„æ–‡æœ¬ä¸Šé¢„å…ˆè®­ç»ƒä¸€ä¸ªè¯­è¨€æ¨¡å‹æ˜¯å¾ˆå®¹æ˜“çš„å‘¢ï¼Ÿ

æ­£ç¡®é€‰é¡¹: 2. å› ä¸ºé¢„è®­ç»ƒä¸éœ€è¦äººå·¥æ ‡è®°æ•°æ®

1. å½“ä½ æ‹…å¿ƒä½ æ‰€ä½¿ç”¨çš„é¢„å…ˆè®­ç»ƒçš„æ¨¡å‹çš„åå·®æ—¶    
è§£æ: è™½ç„¶è¿™æ˜¯çœŸçš„ï¼Œå’Œè¿™ä¸ªé—®é¢˜æ²¡ä»€ä¹ˆå…³ç³»ã€‚å†è¯•ä¸€æ¬¡ï¼
2. å› ä¸ºé¢„è®­ç»ƒä¸éœ€è¦äººå·¥æ ‡è®°æ•°æ®    
è§£æ: æ²¡é”™ï¼Œè¯­è¨€å»ºæ¨¡æ˜¯ä¸€ä¸ªè‡ªç›‘ç£çš„é—®é¢˜ã€‚
3. å› ä¸ºTransformers åº“åªéœ€è¦å‡ è¡Œä»£ç å°±å¯ä»¥å¼€å§‹è®­ç»ƒ    
è§£æ: è™½ç„¶è¿™æ˜¯çœŸçš„ï¼Œä½†æ˜¯å¹¶è¶³ä»¥å›ç­”è¿™ä¸ªé—®é¢˜ã€‚å†è¯•ä¸€æ¬¡ï¼

####  12ï¼é—®ç­”ä»»åŠ¡é¢„å¤„ç†æ•°æ®æ—¶ï¼Œä¸»è¦çš„æŒ‘æˆ˜æ˜¯ä»€ä¹ˆï¼Ÿ

æ­£ç¡®é€‰é¡¹: 2. ä½ éœ€è¦å¤„ç†éå¸¸é•¿çš„ä¸Šä¸‹æ–‡ï¼Œè¿™äº›ä¸Šä¸‹æ–‡æä¾›äº†ä¸€äº›è®­ç»ƒç‰¹æ€§ï¼Œè¿™äº›ç‰¹æ€§å¯èƒ½æœ‰ä¹Ÿå¯èƒ½æ²¡æœ‰ç­”æ¡ˆã€‚

æ­£ç¡®é€‰é¡¹: 4. ä½ éœ€è¦ä»æ–‡æœ¬ä¸­æ‰¾åˆ°ç­”æ¡ˆéƒ¨åˆ†åœ¨ tokenize åçš„è¾“å…¥ä¸­å¯¹åº”çš„èµ·å§‹å’Œç»“æŸä½ç½®ã€‚ã€‚

1. ä½ éœ€è¦å¯¹è¾“å…¥è¿›è¡Œ tokenizeã€‚    
è§£æ: çš„ç¡®æ˜¯éœ€è¦çš„ï¼Œä½†è¿™çœŸçš„æ˜¯ä¸€ä¸ªä¸“å±äºé—®ç­”ä»»åŠ¡çš„ä¸»è¦çš„æŒ‘æˆ˜å—ï¼Ÿ
2. ä½ éœ€è¦å¤„ç†éå¸¸é•¿çš„ä¸Šä¸‹æ–‡ï¼Œè¿™äº›ä¸Šä¸‹æ–‡æä¾›äº†ä¸€äº›è®­ç»ƒç‰¹æ€§ï¼Œè¿™äº›ç‰¹æ€§å¯èƒ½æœ‰ä¹Ÿå¯èƒ½æ²¡æœ‰ç­”æ¡ˆã€‚    
è§£æ: è¿™ç»å¯¹æ˜¯æŒ‘æˆ˜ä¹‹ä¸€ã€‚
3. ä½ éœ€è¦å°†é—®é¢˜çš„ç­”æ¡ˆä»¥åŠè¾“å…¥è¿›è¡Œ tokenizeã€‚    
è§£æ: è™½ç„¶çš„ç¡®éœ€è¦è¿™æ ·åšï¼Œä½†è¿™å¹¶ä¸èƒ½æ„æˆä¸»è¦çš„æŒ‘æˆ˜ï¼Œè¯•è¯•å¦ä¸€ä¸ªç­”æ¡ˆå§ï¼
4. ä½ éœ€è¦ä»æ–‡æœ¬ä¸­æ‰¾åˆ°ç­”æ¡ˆéƒ¨åˆ†åœ¨ tokenize åçš„è¾“å…¥ä¸­å¯¹åº”çš„èµ·å§‹å’Œç»“æŸä½ç½®ã€‚ã€‚    
è§£æ: è¿™æ˜¯æœ€éš¾çš„éƒ¨åˆ†ä¹‹ä¸€ï¼Œæ˜¯çš„ï¼

####  13ï¼é—®ç­”ä»»åŠ¡ä¸­çš„åå¤„ç†é€šå¸¸æ˜¯æ€æ ·è¿›è¡Œçš„ï¼Ÿ

æ­£ç¡®é€‰é¡¹: 3. æ¨¡å‹ä¼šä¸ºæ¯ä¸ªæ ·æœ¬åˆ›å»ºçš„æ¯ä¸ªç‰¹å¾ç»™å‡ºäº†ç­”æ¡ˆçš„èµ·å§‹å’Œç»“æŸä½ç½®ï¼Œä½ åªéœ€è¦å°†å®ƒä»¬ä¸ä¸Šä¸‹æ–‡ä¸­çš„ç‰‡æ®µåŒ¹é…ï¼Œæ‰¾åˆ°å¾—åˆ†æœ€é«˜çš„é‚£ä¸ªã€‚

1. æ¨¡å‹ç»™å‡ºäº†ç­”æ¡ˆçš„å¼€å§‹ä½ç½®å’Œç»“æŸä½ç½®ï¼Œä½ åªéœ€è¦è§£ç ç›¸å¯¹åº”ç­”æ¡ˆè·¨åº¦çš„ tokensã€‚    
è§£æ: è¿™å¯èƒ½æ˜¯ä¸€ç§æ–¹æ³•ï¼Œä½†æ˜¯æœ‰ç‚¹å¤ªç²—ç•¥äº†ï¼Œè¿˜æœ‰äº›ç»†èŠ‚æ²¡è€ƒè™‘åˆ°ã€‚
2. è¯¥æ¨¡å‹ä¸ºæ¯ä¸ªç¤ºä¾‹åˆ›å»ºçš„æ¯ä¸ªç‰¹å¾æä¾›äº†ç­”æ¡ˆçš„èµ·å§‹å’Œç»“æŸä½ç½®ï¼Œä½ åªéœ€åœ¨å¾—åˆ†æœ€é«˜çš„é‚£ä¸ªç‰¹å¾ä¸­è§£ç ç›¸åº”çš„ tokensã€‚    
è§£æ: è¿™ä¸æˆ‘ä»¬ç ”ç©¶çš„åå¤„ç†è¿‡ç¨‹å¾ˆæ¥è¿‘ï¼Œä½†å¹¶ä¸å®Œå…¨æ­£ç¡®ã€‚
3. æ¨¡å‹ä¼šä¸ºæ¯ä¸ªæ ·æœ¬åˆ›å»ºçš„æ¯ä¸ªç‰¹å¾ç»™å‡ºäº†ç­”æ¡ˆçš„èµ·å§‹å’Œç»“æŸä½ç½®ï¼Œä½ åªéœ€è¦å°†å®ƒä»¬ä¸ä¸Šä¸‹æ–‡ä¸­çš„ç‰‡æ®µåŒ¹é…ï¼Œæ‰¾åˆ°å¾—åˆ†æœ€é«˜çš„é‚£ä¸ªã€‚    
è§£æ: ç®€è€Œè¨€ä¹‹å°±æ˜¯è¿™æ ·ï¼
4. æ¨¡å‹ç”Ÿæˆä¸€ä¸ªç­”æ¡ˆï¼Œä½ åªéœ€è¦è§£ç å®ƒã€‚    
è§£æ: è™½ç„¶çš„ç¡®éœ€è¦è¿™ä¸ªè¿‡ç¨‹ï¼Œä½†æ˜¯æœ‰ç‚¹å¤ªç²—ç•¥äº†ã€‚è¯•è¯•å¦ä¸€ä¸ªç­”æ¡ˆå§ï¼

