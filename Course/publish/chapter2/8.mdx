#### 1. 自然语言处理流程的顺序是什么？

1. 首先是模型它处理文本并返回原始预测。然后 tokenizer 会对这些预测进行解释，并在将它们转换回文本
2. 首先 Tokenizer 处理文本并返回 id。模型根据这些 id 并输出预测，可以是一些文本。
3. Tokenizer 处理文本并返回 id。模型处理这些 id 并输出一个预测。然后可以再次使用 tokenizer 将这些预测转换回某些文本。

### 2. Transformer 模型的输出有的张量多少个维度，每个维度分别是什么？

1. 2 个维度，分别是：序列长度(Sequence Length)和批次大小(Batch Size)
2. 2 个维度，分别是：序列长度(Sequence Length)和隐藏层大小(Hidden Size)
3. 3 个维度，分别是：序列长度(Sequence Length)、批次大小(Batch Size)和隐藏层大小(Hidden Size)

### 3．下列哪一个是子词分词的例子（从分词的颗粒度来划分）？

1. WordPiece
2. 基于单个字符的分词
3. 基于空格和标点符号的分割
4. BPE(Byte Pair Encoding)
5. Unigram
6. 以上都不是

### 4．什么是模型头（Haed 层）？

1. 原始 Transformer 网络的一种组件，直接将张量(Tensors)输入到到正确的层
2. 也称为自注意力(self-attention)机制，它会根据序列的其他 tokens 调整一个 token 的表示
3. 一个附加组件，通常由一个或几个层组成，用于将 Transformer 的预测转换为特定于任务的输出

### 5．什么是 AutoModel？

1. 根据你的数据自动进行训练的模型
2. 一个根据 checkpoint(检查点)返回模型体系结构的对象
3. 一种可以自动检测输入语言来加载正确权重的模型

### 5．什么是 TFAutoModel？

1. 根据你的数据自动进行训练的模型
2. 一个根据 checkpoint(检查点)返回模型体系结构的对象
3. 一种可以自动检测输入语言来加载正确权重的模型

### 6．当将不同长度的句子序列在一起批处理时，需要进行哪些处理？

1. 截短
2. 直接将 Tensors 返回
3. 填充
4. 注意力掩码(Attention masking)

### 7．对序列分类(Sequence Classification)模型的 logits 输出应用 SoftMax 激活函数有什么意义？

1. 它软化了 logits 输出，使结果更可靠。
2. 它限定了上下界，使模型的输出结果可以被解释。
3. 输出的和是 1，从而产生一个可能的概率解释。

### 8.Tokenizer API 的核心方法是哪一个？

1. <code>encode</code> 因为它可以将文本编码为 ID，将预测的 ID 解码为文本
2. 直接调用 Tokenizer 对象。
3. <code>pad</code>(填充)
4. <code>tokenize</code>(tokenization)

### 9．这个代码示例中的 `result` 变量包含什么？

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
result = tokenizer.tokenize("Hello!")
```
1. 字符串列表，每个字符串都是一个 tokens (Token)
2. 一个 ID 的列表
3. 包含所有分词后的的字符串

### 10．下面的代码有什么错误吗？

```python
from transformers import AutoTokenizer, AutoModel

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
model = AutoModel.from_pretrained("gpt2")

encoded = tokenizer("Hey!", return_tensors="pt")
result = model(**encoded)
```
1. 不，看起来是对的。
2. Tokenizer 和模型应该来自相同的 checkpoint。
3. 由于模型输入需要是一个 Batch，因此可以使用 tokenizer 对其进行截断或填充来改进这段代码。

### 10．下面的代码有什么错误吗？

```python
from transformers import AutoTokenizer, TFAutoModel

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
model = TFAutoModel.from_pretrained("gpt2")

encoded = tokenizer("Hey!", return_tensors="pt")
result = model(**encoded)
```
1. 不，看起来是对的。
2. Tokenizer 和模型应该来自相同的 checkpoint。
3. 由于每个输入都是一个 Batch，因此可以使用 tokenizer (Tokenizer)对其进行平移和截断来改善这段代码。

---

### 1. 自然语言处理流程的顺序是什么？

正确选项: 3. Tokenizer 处理文本并返回 id。模型处理这些 id 并输出一个预测。然后可以再次使用 tokenizer 将这些预测转换回某些文本。

1. 首先是模型它处理文本并返回原始预测。然后 tokenizer 会对这些预测进行解释，并在将它们转换回文本    
解析: 模型不能理解文本！必须首先使用 tokenizer 将文本转换为 id，之后才可以输入给模型。
2. 首先 Tokenizer 处理文本并返回 id。模型根据这些 id 并输出预测，可以是一些文本。    
解析: 该模型的预测结果是 id 而不是文本。必须使用 tokenizer 才可以将预测转换回文本！
3. Tokenizer 处理文本并返回 id。模型处理这些 id 并输出一个预测。然后可以再次使用 tokenizer 将这些预测转换回某些文本。    
解析: 正确！Tokenizer 可以用于 id 与文本的相互转换。

### 2. Transformer 模型的输出有的张量多少个维度，每个维度分别是什么？

正确选项: 3. 3 个维度，分别是：序列长度(Sequence Length)、批次大小(Batch Size)和隐藏层大小(Hidden Size)

1. 2 个维度，分别是：序列长度(Sequence Length)和批次大小(Batch Size)    
解析: 错！该模型的张量输出还具有第三个维度：隐藏层大小(Hidden Size)。
2. 2 个维度，分别是：序列长度(Sequence Length)和隐藏层大小(Hidden Size)    
解析: 错！所有 Transformer 模型都批量进行计算，即使是单个序列；那么也会有批次大小(Batch Size)的维度并且值为 1！
3. 3 个维度，分别是：序列长度(Sequence Length)、批次大小(Batch Size)和隐藏层大小(Hidden Size)    
解析: 正确！

### 3．下列哪一个是子词分词的例子（从分词的颗粒度来划分）？

正确选项: 1. WordPiece

正确选项: 4. BPE(Byte Pair Encoding)

正确选项: 5. Unigram

1. WordPiece    
解析: 是的，这是一个子词分词的例子！
2. 基于单个字符的分词    
解析: 基于单个字符的分词和子词分词的颗粒度是不同的。
3. 基于空格和标点符号的分割    
解析: 这是一种基于单词的分词而不是子词分词！
4. BPE(Byte Pair Encoding)    
解析: 是的，这是一个子词分词的的例子！
5. Unigram    
解析: 是的，这是一个子词分词的例子！
6. 以上都不是    
解析: 不对！

### 4．什么是模型头（Haed 层）？

正确选项: 3. 一个附加组件，通常由一个或几个层组成，用于将 Transformer 的预测转换为特定于任务的输出

1. 原始 Transformer 网络的一种组件，直接将张量(Tensors)输入到到正确的层    
解析: 不对！没有这样的组件。
2. 也称为自注意力(self-attention)机制，它会根据序列的其他 tokens 调整一个 token 的表示    
解析: 不对！自注意力层确实包含“注意力头”，但是和模型头并不是同一个概念。
3. 一个附加组件，通常由一个或几个层组成，用于将 Transformer 的预测转换为特定于任务的输出    
解析: 没错。它的全名是 Adaptation Heads，也被简单地称为模型的头部，在不同的任务上有不同的形式：语言模型头，问题回答头，序列分类头．

### 5．什么是 AutoModel？

正确选项: 2. 一个根据 checkpoint(检查点)返回模型体系结构的对象

1. 根据你的数据自动进行训练的模型    
解析: 错误。你可能把 AutoModel 与 Hugging Face 的[AutoTrain](https://huggingface.co/autotrain)(https://huggingface.co/autotrain)产品相混淆了？
2. 一个根据 checkpoint(检查点)返回模型体系结构的对象    
解析: 确切地说：<code>AutoModel</code>只需要知道初始化的 checkpoint(检查点)名称就可以返回正确的体系结构。
3. 一种可以自动检测输入语言来加载正确权重的模型    
解析: 不正确；虽然有些 checkpoint(检查点)和模型能够处理多种语言，但是没有内置的工具可以根据语言自动选择 checkpoint(检查点)。你应该前往 <a href='https://huggingface.co/models'>Model Hub</a> 寻找完成所需任务的最佳 checkpoint(检查点)！

### 5．什么是 TFAutoModel？

正确选项: 2. 一个根据 checkpoint(检查点)返回模型体系结构的对象

1. 根据你的数据自动进行训练的模型    
解析: 错误。你可能把 TFAutoModel 与 Hugging Face 的[AutoTrain](https://huggingface.co/autotrain)(https://huggingface.co/autotrain)产品相混淆了？
2. 一个根据 checkpoint(检查点)返回模型体系结构的对象    
解析: 确切地说：<code>TFAutoModel</code>只需要知道初始化的 checkpoint(检查点)名称就可以返回正确的体系结构。
3. 一种可以自动检测输入语言来加载正确权重的模型    
解析: 不正确；虽然有些 checkpoint(检查点)和模型能够处理多种语言，但是没有内置的工具可以根据语言自动选择 checkpoint(检查点)。你应该前往 [Model Hub](https://huggingface.co/models)(https://huggingface.co/models) 寻找完成所需任务的最佳 checkpoint(检查点)！

### 6．当将不同长度的句子序列在一起批处理时，需要进行哪些处理？

正确选项: 1. 截短

正确选项: 3. 填充

正确选项: 4. 注意力掩码(Attention masking)

1. 截短    
解析: 是的，截断是一个正确的方式，截断可以将他们转化为一个固定长度的矩形序列。这是唯一的正确答案吗？
2. 直接将 Tensors 返回    
解析: 在批处理序列时不可以直接返回长度不一致的 Tensors。(必须是固定长度)
3. 填充    
解析: 是的，填充是一个正确的方式，可以将他们转化为一个固定长度的矩形序列。这是唯一的正确答案吗？
4. 注意力掩码(Attention masking)    
解析: 当然！当处理不同长度的序列时，注意力掩码是最重要的。然而，仅仅使用注意力遮蔽是不够的。

### 7．对序列分类(Sequence Classification)模型的 logits 输出应用 SoftMax 激活函数有什么意义？

正确选项: 2. 它限定了上下界，使模型的输出结果可以被解释。

正确选项: 3. 输出的和是 1，从而产生一个可能的概率解释。

1. 它软化了 logits 输出，使结果更可靠。    
解析: 不，SoftMax 激活函数不会影响结果的可靠性。
2. 它限定了上下界，使模型的输出结果可以被解释。    
解析: 正确！结果值被压缩到了 0 和 1 之间。不过，这并不是我们使用 SoftMax 激活函数的唯一原因。
3. 输出的和是 1，从而产生一个可能的概率解释。    
解析: 没错，但这并不是我们使用 SoftMax 激活函数的唯一原因。

### 8.Tokenizer API 的核心方法是哪一个？

正确选项: 2. 直接调用 Tokenizer 对象。

1. <code>encode</code> 因为它可以将文本编码为 ID，将预测的 ID 解码为文本    
解析: 错！虽然 <code>encode</code> 方法确实是 Tokenizer 中的方法之一，但是它并不是核心的方法，此外将预测 ID 解码为文本的是 decode。
2. 直接调用 Tokenizer 对象。    
解析: 完全正确！ tokenizer (Tokenizer) 的 <code>__call__</code>方法是一个非常强大的方法，可以处理几乎任何事情。它同时也可以从模型中获取预测。
3. <code>pad</code>(填充)    
解析: 错！<code>pad</code>(填充)非常有用，但它只是 Tokenizer API 的一部分。
4. <code>tokenize</code>(tokenization)    
解析: 可以说，<code>tokenize</code>(tokenization)方法是最有用的方法之一，但它不是 Tokenizer API 的核心方法。

### 9．这个代码示例中的 `result` 变量包含什么？

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
result = tokenizer.tokenize("Hello!")
```
正确选项: 1. 字符串列表，每个字符串都是一个 tokens (Token)

1. 字符串列表，每个字符串都是一个 tokens (Token)    
解析: 正确！把 tokens 转换成 id 后，就可以传输给模型了！
2. 一个 ID 的列表    
解析: 不正确；这是 <code>__call__</code> 或 <code>convert_tokens_to_ids</code>方法的作用！
3. 包含所有分词后的的字符串    
解析: 这将是次优的答案，因为 tokenize 方法会将字符串拆分为多个 tokens 的列表。

### 10．下面的代码有什么错误吗？

```python
from transformers import AutoTokenizer, AutoModel

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
model = AutoModel.from_pretrained("gpt2")

encoded = tokenizer("Hey!", return_tensors="pt")
result = model(**encoded)
```
正确选项: 2. Tokenizer 和模型应该来自相同的 checkpoint。

1. 不，看起来是对的。    
解析: 将一个模型与一个在不同 checkpoint 训练的 tokenizer 耦合在一起并不是一个好主意。模型没有在这个这个 tokenizer 上训练来理解 tokenizer 的输出，因此模型的输出(如果它可以运行的话)会是错乱的。
2. Tokenizer 和模型应该来自相同的 checkpoint。    
解析: 对！
3. 由于模型输入需要是一个 Batch，因此可以使用 tokenizer 对其进行截断或填充来改进这段代码。    
解析: 的确，模型输入需要是一个 Batch。然而，截断或填充这个序列并不一定有意义，这里只有一句话，而截断或填充这些技术是用来批处理一个句子列表使其长度一致的。

### 10．下面的代码有什么错误吗？

```python
from transformers import AutoTokenizer, TFAutoModel

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
model = TFAutoModel.from_pretrained("gpt2")

encoded = tokenizer("Hey!", return_tensors="pt")
result = model(**encoded)
```
正确选项: 2. Tokenizer 和模型应该来自相同的 checkpoint。

1. 不，看起来是对的。    
解析: 不幸的是，将一个模型与一个不同 checkpoint 训练的 tokenizer 耦合在并不是一个好主意。模型没有在这个这个 tokenizer 上训练来理解 tokenizer 的输出，因此模型的输出(如果它可以运行的话)会是错乱的。
2. Tokenizer 和模型应该来自相同的 checkpoint。    
解析: 对！
3. 由于每个输入都是一个 Batch，因此可以使用 tokenizer (Tokenizer)对其进行平移和截断来改善这段代码。    
解析: 的确，每个模型都需要 Batch 类型的输入。然而，截断或填充这个序列并不一定有意义，这里只有一句话，而这些技术是用来批处理一个句子列表的。

