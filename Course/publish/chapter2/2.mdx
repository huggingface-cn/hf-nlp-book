

# Pipeline çš„å†…éƒ¨ 

{#if fw === 'pt'}



{:else}



{/if}

<div custom-style="Tip-green">

è¿™æ˜¯ç¬¬ä¸€éƒ¨åˆ†ï¼Œæ ¹æ®æ‚¨ä½¿ç”¨ PyTorch æˆ–è€… TensorFlowï¼Œå†…å®¹ç•¥æœ‰ä¸åŒã€‚ç‚¹å‡»æ ‡é¢˜ä¸Šæ–¹çš„å¹³å°ï¼Œé€‰æ‹©æ‚¨å–œæ¬¢çš„å¹³å°ï¼
</div>

{#if fw === 'pt'}

{:else}

{/if}

è®©æˆ‘ä»¬ä»ä¸€ä¸ªå®Œæ•´çš„ç¤ºä¾‹å¼€å§‹ï¼Œçœ‹çœ‹åœ¨ [ç¬¬ä¸€ç« ](/course/chapter1) ä¸­æ‰§è¡Œä»¥ä¸‹ä»£ç æ—¶åœ¨å¹•åå‘ç”Ÿäº†ä»€ä¹ˆ

```python
from transformers import pipeline

classifier = pipeline("sentiment-analysis")
classifier(
    [
        "I've been waiting for a HuggingFace course my whole life.",
        "I hate this so much!",
    ]
)
```

è·å¾—å¦‚ä¸‹è¾“å‡ºï¼š

```python out
[{'label': 'POSITIVE', 'score': 0.9598047137260437},
 {'label': 'NEGATIVE', 'score': 0.9994558095932007}]
```

æ­£å¦‚æˆ‘ä»¬åœ¨ [ç¬¬ä¸€ç« ](/course/chapter1) ä¸­çœ‹åˆ°çš„ï¼Œè¿™ä¸ªç®¡é“å°†ä¸‰ä¸ªæ­¥éª¤ç»„åˆåœ¨ä¸€èµ·ï¼šé¢„å¤„ç†ã€æ¨¡å‹è®¡ç®—å’Œåå¤„ç†ï¼š

![The full NLP pipeline: tokenization of text, conversion to IDs, and inference through the Transformer model and the model head.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline.svg "The full NLP pipeline: tokenization of text, conversion to IDs, and inference through the Transformer model and the model head.")

è®©æˆ‘ä»¬å…ˆç®€å•äº†è§£ä¸€ä¸‹è¿™ä¸‰ä¸ªæ­¥éª¤ã€‚

## ä½¿ç”¨ tokenizer(æ ‡è®°å™¨)è¿›è¡Œé¢„å¤„ç† 

ä¸å…¶ä»–ç¥ç»ç½‘ç»œä¸€æ ·ï¼ŒTransformer æ¨¡å‹æ— æ³•ç›´æ¥å¤„ç†åŸå§‹æ–‡æœ¬ï¼Œå› æ­¤æˆ‘ä»¬ç®¡é“çš„ç¬¬ä¸€æ­¥æ˜¯å°†æ–‡æœ¬è¾“å…¥è½¬æ¢ä¸ºæ¨¡å‹èƒ½å¤Ÿç†è§£çš„æ•°å­—ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨*tokenizer*(æ ‡è®°å™¨)ï¼Œå®ƒå°†è´Ÿè´£ï¼š

- å°†è¾“å…¥æ‹†åˆ†ä¸ºå•è¯ã€å­å•è¯æˆ–ç¬¦å·ï¼ˆå¦‚æ ‡ç‚¹ç¬¦å·ï¼‰ï¼Œç§°ä¸ºæ ‡è®°(*token*)
- å°†æ¯ä¸ªæ ‡è®°(token)æ˜ å°„åˆ°ä¸€ä¸ªæ•°å­—ï¼Œç§°ä¸º*input ID*ï¼ˆè¾“å…¥ IDï¼‰
- æ·»åŠ æ¨¡å‹éœ€è¦çš„å…¶ä»–è¾“å…¥ï¼Œä¾‹å¦‚ç‰¹æ®Šæ ‡è®°ï¼ˆå¦‚ `[CLS]` å’Œ `[SEP]` ï¼‰æˆ–æ ‡è®°ç±»å‹ ID

åœ¨ä½¿ç”¨æ¨¡å‹æ—¶æ‰€æœ‰è¿™äº›é¢„å¤„ç†éƒ½éœ€è¦ä¸æ¨¡å‹é¢„è®­ç»ƒæ—¶çš„æ–¹å¼å®Œå…¨ç›¸åŒï¼Œå› æ­¤æˆ‘ä»¬é¦–å…ˆéœ€è¦ä» [Model Hub](https://huggingface.co/models) ä¸­ä¸‹è½½è¿™äº›ä¿¡æ¯ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨ `AutoTokenizer` ç±»å’Œå®ƒçš„ `from_pretrained()` æ–¹æ³•ã€‚å¹¶è¾“å…¥æˆ‘ä»¬æ¨¡å‹çš„æ£€æŸ¥ç‚¹åç§°ï¼Œå®ƒå°†è‡ªåŠ¨è·å–ä¸æ¨¡å‹çš„ tokenizer ç›¸å…³è”çš„æ•°æ®ï¼Œå¹¶å¯¹å…¶è¿›è¡Œç¼“å­˜ï¼ˆå› æ­¤åªæœ‰åœ¨æ‚¨ç¬¬ä¸€æ¬¡è¿è¡Œä¸‹é¢çš„ä»£ç æ—¶æ‰ä¼šä¸‹è½½ï¼‰ã€‚

 `sentiment-analysis` ï¼ˆæƒ…ç»ªåˆ†æï¼‰ç®¡é“çš„é»˜è®¤æ£€æŸ¥ç‚¹æ˜¯ `distilbert-base-uncased-finetuned-sst-2-english` ï¼ˆä½ å¯ä»¥åœ¨ [è¿™é‡Œ](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) )çœ‹åˆ°å®ƒçš„æ¨¡å‹å¡ç‰‡ï¼Œæˆ‘ä»¬è¿è¡Œä»¥ä¸‹ä»£ç ï¼š

```python
from transformers import AutoTokenizer

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
```

å½“æˆ‘ä»¬æœ‰äº† tokenizerï¼Œæˆ‘ä»¬å°±å¯ä»¥ç›´æ¥å°†æˆ‘ä»¬çš„å¥å­ä¼ é€’ç»™å®ƒï¼Œæˆ‘ä»¬å°±ä¼šå¾—åˆ°ä¸€ä¸ª*input ID*ï¼ˆè¾“å…¥ IDï¼‰åˆ—è¡¨ï¼å‰©ä¸‹è¦åšçš„å”¯ä¸€ä¸€ä»¶äº‹å°±æ˜¯å°†è¾“å…¥ ID åˆ—è¡¨è½¬æ¢ä¸º tensor(å¼ é‡)ã€‚

æ‚¨åœ¨ä½¿ç”¨ğŸ¤— Transformers æ—¶ï¼Œä¸å¿…åœ¨æ„åç«¯æ˜¯å“ªä¸ª ML æ¡†æ¶å®ç°çš„ï¼›å®ƒå¯èƒ½æ˜¯ PyTorch æˆ– TensorFlowï¼Œæˆ– Flaxã€‚ä½†æ˜¯ï¼ŒTransformers æ¨¡å‹åªæ¥å—*tensor(å¼ é‡)*ä½œä¸ºè¾“å…¥ã€‚å¦‚æœè¿™æ˜¯ä½ ç¬¬ä¸€æ¬¡å¬è¯´å¼ é‡ï¼Œä½ å¯ä»¥æŠŠå®ƒä»¬æƒ³è±¡æˆ NumPy æ•°ç»„ã€‚NumPy æ•°ç»„å¯ä»¥æ˜¯æ ‡é‡ï¼ˆ0Dï¼‰ã€å‘é‡ï¼ˆ1Dï¼‰ã€çŸ©é˜µï¼ˆ2Dï¼‰æˆ–å…·æœ‰æ›´å¤šç»´åº¦ã€‚è¿™äº›éƒ½å¯ä»¥ç§°ä¸ºå¼ é‡ï¼›å…¶ä»– ML æ¡†æ¶çš„å¼ é‡ä½¿ç”¨æ–¹æ³•ä¹Ÿç±»ä¼¼ï¼Œé€šå¸¸ä¸ NumPy æ•°ç»„ä¸€æ ·æ˜“äºå®ä¾‹åŒ–ã€‚

ä¸ºäº†æŒ‡å®šæˆ‘ä»¬æƒ³è¦å¾—åˆ°çš„å¼ é‡çš„ç±»å‹ï¼ˆPyTorchã€TensorFlow æˆ–çº¯ NumPyï¼‰ï¼Œæˆ‘ä»¬ä½¿ç”¨ return_tensors å‚æ•°ï¼š

{#if fw === 'pt'}
```python
raw_inputs = [
    "I've been waiting for a HuggingFace course my whole life.",
    "I hate this so much!",
]
inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="pt")
print(inputs)
```
{:else}
```python
raw_inputs = [
    "I've been waiting for a HuggingFace course my whole life.",
    "I hate this so much!",
]
inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="tf")
print(inputs)
```
{/if}

ç°åœ¨ä¸è¦æ‹…å¿ƒ padding(å¡«å……)å’Œ truncation(æˆªæ–­)ï¼›æˆ‘ä»¬ç¨åä¼šè§£é‡Šè¿™äº›ã€‚è¿™é‡Œè¦è®°ä½çš„æ˜¯ï¼Œæ‚¨å¯ä»¥ä¼ é€’ä¸€ä¸ªå¥å­æˆ–ä¸€ç»„å¥å­ï¼Œè¿˜å¯ä»¥æŒ‡å®šè¦è¿”å›çš„å¼ é‡ç±»å‹ï¼ˆå¦‚æœæ²¡æœ‰ä¼ é€’ç±»å‹ï¼Œé»˜è®¤è¿”å›çš„æ˜¯ python ä¸­çš„ list æ ¼å¼ï¼‰ã€‚

{#if fw === 'pt'}

ä»¥ä¸‹æ˜¯ PyTorch å¼ é‡çš„ç»“æœï¼š

```python out
{
    'input_ids': tensor([
        [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172, 2607,  2026,  2878,  2166,  1012,   102],
        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,     0,     0,     0,     0,     0,     0]
    ]), 
    'attention_mask': tensor([
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]
    ])
}
```
{:else}

ä»¥ä¸‹æ˜¯ TensorFlow å¼ é‡çš„ç»“æœï¼š

```python out
{
    'input_ids': <tf.Tensor: shape=(2, 16), dtype=int32, numpy=
        array([
            [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,  2026,  2878,  2166,  1012,   102],
            [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,     0,     0,     0,     0,     0,     0]
        ], dtype=int32)>, 
    'attention_mask': <tf.Tensor: shape=(2, 16), dtype=int32, numpy=
        array([
            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
            [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]
        ], dtype=int32)>
}
```
{/if}

è¾“å‡ºæ˜¯ä¸€ä¸ªåŒ…å«ä¸¤ä¸ªé”®ï¼Œ `input_ids` å’Œ `attention_mask` ã€‚ `input_ids` åŒ…å«ä¸¤è¡Œæ•´æ•°ï¼ˆæ¯ä¸ªå¥å­ä¸€è¡Œï¼‰ï¼Œå®ƒä»¬æ˜¯æ¯ä¸ªå¥å­ä¸­æ ‡è®°ï¼ˆtokenï¼‰çš„ IDã€‚æˆ‘ä»¬å°†åœ¨æœ¬ç« åé¢è§£é‡Šä»€ä¹ˆæ˜¯ `attention_mask` ã€‚

## æ¢ç´¢æ¨¡å‹ 

{#if fw === 'pt'}
æˆ‘ä»¬å¯ä»¥åƒä½¿ç”¨ tokenizer ä¸€æ ·ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹ã€‚ğŸ¤— Transformers æä¾›äº†ä¸€ä¸ª `AutoModel` ç±»ï¼Œå®ƒä¹Ÿæœ‰ä¸€ä¸ª from_pretrained()æ–¹æ³•ï¼š

```python
from transformers import AutoModel

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModel.from_pretrained(checkpoint)
```
{:else}
æˆ‘ä»¬å¯ä»¥åƒä½¿ç”¨ tokenizer ä¸€æ ·ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹ã€‚ğŸ¤— Transformers æä¾›äº†ä¸€ä¸ª `AutoModel` ç±»ï¼Œå®ƒä¹Ÿæœ‰ä¸€ä¸ª from_pretrained()æ–¹æ³•ï¼š

```python
from transformers import TFAutoModel

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = TFAutoModel.from_pretrained(checkpoint)
```
{/if}

åœ¨è¿™æ®µä»£ç ä¸­ï¼Œæˆ‘ä»¬å°†ä¹‹å‰åœ¨ pipeline ä¸­ä½¿ç”¨çš„ checkpointï¼ˆå®é™…ä¸Šåº”è¯¥å·²ç»è¢«ç¼“å­˜äº†ï¼‰ä¸‹è½½ä¸‹æ¥ï¼Œå¹¶ç”¨å®ƒå®ä¾‹åŒ–äº†ä¸€ä¸ªæ¨¡å‹ã€‚

è¿™ä¸ªæ¨¡å‹åªåŒ…å«åŸºæœ¬çš„ Transformer æ¨¡å—ï¼šè¾“å…¥ä¸€äº›å¥å­ï¼Œå®ƒè¾“å‡ºæˆ‘ä»¬å°†ç§°ä¸º*hidden states(éšçŠ¶æ€)*ï¼Œä¹Ÿè¢«ç§°ä¸ºç‰¹å¾ã€‚æ¯ä¸ªè¾“å…¥ï¼Œæˆ‘ä»¬éƒ½å¯ä»¥è·å–ä¸€ä¸ªé«˜ç»´å‘é‡ï¼Œä»£è¡¨ Transformer æ¨¡å‹å¯¹è¯¥è¾“å…¥çš„ä¸Šä¸‹æ–‡ç†è§£ã€‚

å¦‚æœè¿™æœ‰äº›éš¾ä»¥ç†è§£ï¼Œä¸è¦æ‹…å¿ƒã€‚æˆ‘ä»¬ä»¥åå†è§£é‡Šã€‚

è¿™äº›éšçŠ¶æ€æœ¬èº«å°±å¾ˆæœ‰ç”¨ï¼Œå®ƒä»¬è¢«ç§°ä¸º*æ¨¡å‹å¤´(head)*ï¼Œé€šå¸¸æ˜¯æ¨¡å‹å¦ä¸€éƒ¨åˆ†çš„è¾“å…¥ã€‚åœ¨ [ç¬¬ä¸€ç« ](/course/chapter1) ä¸­ï¼Œå¯ä»¥ä½¿ç”¨ç›¸åŒçš„ä½“ç³»ç»“æ„çš„æ¨¡å‹æ‰§è¡Œä¸åŒçš„ä»»åŠ¡ï¼Œè¿™æ˜¯å› ä¸ºæ¯ä¸ªä»»åŠ¡éƒ½æœ‰ä¸€ä¸ªå¯¹åº”çš„æ¨¡å‹å¤´ã€‚

### é«˜ç»´å‘é‡ï¼Ÿ

Transformers æ¨¡å—çš„çŸ¢é‡è¾“å‡ºé€šå¸¸è¾ƒå¤§ã€‚å®ƒé€šå¸¸æœ‰ä¸‰ä¸ªç»´åº¦ï¼š

- **Batch size**(æ‰¹æ¬¡å¤§å°): ä¸€æ¬¡å¤„ç†çš„åºåˆ—æ•°ï¼ˆåœ¨æˆ‘ä»¬çš„ç¤ºä¾‹ä¸­ä¸º 2ï¼‰ã€‚
- **Sequence length**ï¼ˆåºåˆ—é•¿åº¦ï¼‰: è¡¨ç¤ºåºåˆ—ï¼ˆå¥å­ï¼‰çš„é•¿åº¦ï¼ˆåœ¨æˆ‘ä»¬çš„ç¤ºä¾‹ä¸­ä¸º 16ï¼‰ã€‚
- **Hidden size**ï¼ˆéšè—å±‚å¤§å°ï¼‰: æ¯ä¸ªæ¨¡å‹è¾“å…¥çš„å‘é‡ç»´åº¦ã€‚

æ­£æ˜¯å› ä¸ºæœ€åä¸€ä¸ªç»´åº¦ï¼Œå®ƒè¢«ç§°ä¸ºâ€œé«˜ç»´â€ã€‚éšè—å±‚å¯èƒ½éå¸¸å¤§ï¼ˆå¯¹äºè¾ƒå°çš„æ¨¡å‹ï¼Œå¸¸è§çš„æ˜¯ 768ï¼Œå¯¹äºè¾ƒå¤§çš„æ¨¡å‹ï¼Œè¿™ä¸ªæ•°å­—å¯ä»¥è¾¾åˆ° 3072 æˆ–æ›´å¤šï¼‰ã€‚

å¦‚æœæˆ‘ä»¬å°†é¢„å¤„ç†çš„ä¹‹åçš„å€¼è¾“å…¥åˆ°æ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¿™ä¸€ç‚¹ï¼š

{#if fw === 'pt'}
```python
outputs = model(**inputs)
print(outputs.last_hidden_state.shape)
```

```python out
torch.Size([2, 16, 768])
```
{:else}
```py
outputs = model(inputs)
print(outputs.last_hidden_state.shape)
```

```python out
(2, 16, 768)
```
{/if}

æ³¨æ„ï¼ŒğŸ¤— Transformers æ¨¡å‹çš„è¾“å‡ºæœ‰äº›åƒ `namedtuple` æˆ–è¯å…¸ã€‚æ‚¨å¯ä»¥é€šè¿‡å±æ€§ï¼ˆå°±åƒæˆ‘ä»¬æ‰€åšçš„é‚£æ ·ï¼‰æˆ–é”®ï¼ˆ `outputs["last_hidden_state"]` ï¼‰è®¿é—®å…ƒç´ ï¼Œå¦‚æœæ‚¨ç¡®åˆ‡çŸ¥é“è¦æŸ¥æ‰¾çš„å†…å®¹åœ¨å“ªé‡Œï¼ˆ `outputs[0]` ï¼‰ï¼Œä¹Ÿå¯ä»¥é€šè¿‡ç´¢å¼•è®¿é—®å…ƒç´ ã€‚

### æ¨¡å‹å¤´ï¼šç†è§£æ•°å­—çš„æ„ä¹‰ 

Transformers æ¨¡å‹çš„è¾“å‡ºä¼šç›´æ¥å‘é€åˆ°æ¨¡å‹å¤´è¿›è¡Œå¤„ç†ã€‚

æ¨¡å‹å¤´é€šå¸¸ç”±ä¸€ä¸ªæˆ–å‡ ä¸ªçº¿æ€§å±‚ç»„æˆï¼Œå®ƒçš„è¾“å…¥æ˜¯éšçŠ¶æ€çš„é«˜ç»´å‘é‡ï¼Œå®ƒä¼šå¹¶å°†å…¶æŠ•å½±åˆ°ä¸åŒçš„ç»´åº¦ã€‚

![A Transformer network alongside its head.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/transformer_and_head.svg "A Transformer network alongside its head.")
åœ¨æ­¤å›¾ä¸­ï¼Œæ¨¡å‹ç”±å…¶åµŒå…¥å±‚å’Œåç»­å±‚è¡¨ç¤ºã€‚åµŒå…¥å±‚å°†æ ‡è®°åŒ–è¾“å…¥ä¸­çš„æ¯ä¸ªè¾“å…¥ ID è½¬æ¢ä¸ºè¡¨ç¤ºå…³è”æ ‡è®°(token)çš„å‘é‡ã€‚åç»­å±‚ä½¿ç”¨æ³¨æ„æœºåˆ¶æ“çºµè¿™äº›å‘é‡ï¼Œç”Ÿæˆå¥å­çš„æœ€ç»ˆè¡¨ç¤ºã€‚


ğŸ¤— Transformers ä¸­æœ‰è®¸å¤šä¸åŒçš„ä½“ç³»ç»“æ„ï¼Œæ¯ç§ä½“ç³»ç»“æ„éƒ½æ˜¯å›´ç»•å¤„ç†ç‰¹å®šä»»åŠ¡è€Œè®¾è®¡çš„ã€‚ä»¥ä¸‹æ˜¯ä¸€ä¸ªéè¯¦å°½çš„åˆ—è¡¨ï¼š

- `*Model` (éšçŠ¶æ€æ£€ç´¢)
- `*ForCausalLM` 
- `*ForMaskedLM` 
- `*ForMultipleChoice` 
- `*ForQuestionAnswering` 
- `*ForSequenceClassification` 
- `*ForTokenClassification` 
- ä»¥åŠå…¶ä»– ğŸ¤—

{#if fw === 'pt'}
ä»¥æƒ…æ„Ÿåˆ†ç±»ä¸ºä¾‹ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªå¸¦æœ‰åºåˆ—åˆ†ç±»å¤´çš„æ¨¡å‹ï¼ˆèƒ½å¤Ÿå°†å¥å­åˆ†ç±»ä¸ºç§¯ææˆ–æ¶ˆæï¼‰ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä¸é€‰ç”¨ `AutoModel` ç±»ï¼Œè€Œæ˜¯ä½¿ç”¨ `AutoModelForSequenceClassification` ï¼š

ï¼ˆ***ä¹Ÿå°±æ˜¯è¯´å‰é¢å†™çš„ model = AutoModel.from_pretrained(checkpoint)å¹¶ä¸èƒ½å¾—åˆ°æƒ…æ„Ÿåˆ†ç±»ä»»åŠ¡çš„ç»“æœï¼Œå› ä¸ºæ²¡æœ‰åŠ è½½ Model head***ï¼‰

```python
from transformers import AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
outputs = model(**inputs)
```
{:else}
ä»¥æƒ…æ„Ÿåˆ†ç±»ä¸ºä¾‹ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªå¸¦æœ‰åºåˆ—åˆ†ç±»å¤´çš„æ¨¡å‹ï¼ˆèƒ½å¤Ÿå°†å¥å­åˆ†ç±»ä¸ºç§¯ææˆ–æ¶ˆæï¼‰ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä¸é€‰ç”¨ `TFAutoModel` ç±»ï¼Œè€Œæ˜¯ä½¿ç”¨ `TFAutoModelForSequenceClassification` :

ï¼ˆ***ä¹Ÿå°±æ˜¯è¯´å‰é¢å†™çš„ model = AutoModel.from_pretrained(checkpoint)å¹¶ä¸èƒ½å¾—åˆ°æƒ…æ„Ÿåˆ†ç±»ä»»åŠ¡çš„ç»“æœï¼Œå› ä¸ºæ²¡æœ‰åŠ è½½ Model head***ï¼‰

```python
from transformers import TFAutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
outputs = model(inputs)
```
{/if}

ç°åœ¨å¦‚æœæˆ‘ä»¬çœ‹ä¸€ä¸‹æˆ‘ä»¬è¾“å‡ºçš„å½¢çŠ¶ï¼Œå…¶ç»´åº¦ä¼šé™ä½å¾ˆå¤šï¼šæ¨¡å‹å¤´æ¥æ”¶æˆ‘ä»¬ä¹‹å‰çœ‹åˆ°çš„é«˜ç»´å‘é‡ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¾“å‡ºåŒ…å«ä¸¤ä¸ªå€¼ï¼ˆæ¯ç§æ ‡ç­¾ä¸€ä¸ªï¼‰çš„å‘é‡ï¼š

```python
print(outputs.logits.shape)
```

{#if fw === 'pt'}

```python out
torch.Size([2, 2])
```

{:else}

```python out
(2, 2)
```

{/if}

ç”±äºæˆ‘ä»¬åªæœ‰ä¸¤ä¸ªå¥å­å’Œä¸¤é’Ÿæ ‡ç­¾ï¼Œæ‰€ä»¥æˆ‘ä»¬ä»æ¨¡å‹ä¸­å¾—åˆ°çš„ç»“æœçš„å½¢çŠ¶æ˜¯ 2 x 2ã€‚

## å¯¹è¾“å‡ºè¿›è¡Œååºå¤„ç† 

æˆ‘ä»¬ä»æ¨¡å‹ä¸­å¾—åˆ°çš„è¾“å‡ºå€¼æœ¬èº«å¹¶ä¸ä¸€å®šæœ‰æ„ä¹‰ã€‚æˆ‘ä»¬æ¥çœ‹çœ‹ï¼Œ

```python
print(outputs.logits)
```

{#if fw === 'pt'}
```python out
tensor([[-1.5607,  1.6123],
        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward>)
```
{:else}
```python out
<tf.Tensor: shape=(2, 2), dtype=float32, numpy=
    array([[-1.5606991,  1.6122842],
           [ 4.169231 , -3.3464472]], dtype=float32)>
```
{/if}

æˆ‘ä»¬çš„æ¨¡å‹é¢„æµ‹ç¬¬ä¸€å¥ä¸º `[-1.5607, 1.6123]` ï¼Œç¬¬äºŒå¥ä¸º `[ 4.1692, -3.3464]` ã€‚è¿™äº›ä¸æ˜¯æ¦‚ç‡ï¼Œè€Œæ˜¯*logits*ï¼ˆ*å¯¹æ•°å‡ ç‡*ï¼‰ï¼Œæ˜¯æ¨¡å‹æœ€åä¸€å±‚è¾“å‡ºçš„åŸå§‹çš„ã€æœªæ ‡å‡†åŒ–çš„åˆ†æ•°ã€‚è¦è½¬æ¢ä¸ºæ¦‚ç‡ï¼Œå®ƒä»¬éœ€è¦ç»è¿‡ [SoftMax](https://en.wikipedia.org/wiki/Softmax_function) å±‚ï¼ˆæ‰€æœ‰ğŸ¤—Transformers æ¨¡å‹çš„è¾“å‡ºéƒ½æ˜¯ logitsï¼Œå› ä¸ºè®­ç»ƒæ—¶çš„æŸå¤±å‡½æ•°é€šå¸¸ä¼šå°†æœ€åçš„æ¿€æ´»å‡½æ•°ï¼ˆå¦‚ SoftMaxï¼‰ä¸å®é™…çš„æŸå¤±å‡½æ•°ï¼ˆå¦‚äº¤å‰ç†µï¼‰èåˆï¼‰ï¼š

{#if fw === 'pt'}
```py
import torch

predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
print(predictions)
```
{:else}
```py
import tensorflow as tf

predictions = tf.math.softmax(outputs.logits, axis=-1)
print(predictions)
```
{/if}

{#if fw === 'pt'}
```python out
tensor([[4.0195e-02, 9.5980e-01],
        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward>)
```
{:else}
```python out
tf.Tensor(
[[4.01951671e-02 9.59804833e-01]
 [9.9945587e-01 5.4418424e-04]], shape=(2, 2), dtype=float32)
```
{/if}

ç°åœ¨æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œæ¨¡å‹é¢„æµ‹ç¬¬ä¸€å¥çš„è¾“å‡ºæ˜¯ `[0.0402, 0.9598]` ï¼Œç¬¬äºŒå¥ `[0.9995,  0.0005]` ã€‚è¿™äº›æ˜¯å¯ç›´æ¥ä½¿ç”¨çš„æ¦‚ç‡åˆ†æ•°ã€‚

ä¸ºäº†è·å¾—æ¯ä¸ªåˆ†æ•°å¯¹åº”çš„æ ‡ç­¾ï¼Œæˆ‘ä»¬å¯ä»¥æŸ¥çœ‹æ¨¡å‹é…ç½®çš„ `id2label` å±æ€§ï¼ˆä¸‹ä¸€èŠ‚å°†å¯¹æ­¤è¿›è¡Œè¯¦ç»†ä»‹ç»ï¼‰ï¼š

```python
model.config.id2label
```

```python out
{0: 'NEGATIVE', 1: 'POSITIVE'}
```

ç°åœ¨æˆ‘ä»¬å¯ä»¥å¾—å‡ºç»“è®ºï¼Œæ¨¡å‹é¢„æµ‹å¦‚ä¸‹ï¼š

- ç¬¬ä¸€å¥ï¼šæ¶ˆæï¼š0.0402ï¼Œç§¯æï¼š0.9598
- ç¬¬äºŒå¥ï¼šæ¶ˆæï¼š0.9995ï¼Œç§¯æï¼š0.0005

æˆ‘ä»¬å·²ç»æˆåŠŸåœ°å¤åˆ»äº†ç®¡é“çš„ä¸‰ä¸ªæ­¥éª¤ï¼šä½¿ç”¨ tokenizer è¿›è¡Œé¢„å¤„ç†ã€é€šè¿‡æ¨¡å‹ä¼ é€’è¾“å…¥ä»¥åŠåå¤„ç†ï¼ç°åœ¨ï¼Œè®©æˆ‘ä»¬èŠ±ä¸€äº›æ—¶é—´æ·±å…¥äº†è§£è¿™äº›æ­¥éª¤ä¸­çš„æ¯ä¸€æ­¥ã€‚

<div custom-style="Tip-green">


âœï¸ **è¯•è¯•çœ‹ï¼** é€‰æ‹©ä¸¤ä¸ªï¼ˆæˆ–æ›´å¤šï¼‰å¥å­å¹¶åˆ†åˆ«åœ¨ `sentiment-analysis` ç®¡é“å’Œè‡ªå·±å®ç°çš„ç®¡é“ä¸­è¿è¡Œå®ƒä»¬ã€‚çœ‹ä¸€çœ‹æ˜¯å¦è·å¾—çš„ç»“æœæ˜¯ä¸æ˜¯ç›¸åŒçš„ï¼

</div>
