# ç¬¬ä¹ç«  å¦‚ä½•å¯»æ±‚å¸®åŠ©

ç°åœ¨ï¼Œä½ å·²ç»çŸ¥é“å¦‚ä½•ä½¿ç”¨Transformers å¤„ç†æœ€å¸¸è§çš„ NLP ä»»åŠ¡ï¼Œå¯ä»¥å¼€å§‹è‡ªå·±çš„é¡¹ç›®äº†ï¼åœ¨æœ¬ç« ä¸­æˆ‘ä»¬å°†æ¢è®¨é‡åˆ°çš„é—®é¢˜ä»¥åŠè§£å†³æ–¹æ³•ã€‚ä½ å°†å­¦ä¹ å¦‚ä½•æˆåŠŸè°ƒè¯•ä»£ç æˆ–è®­ç»ƒï¼Œä»¥åŠåœ¨æ— æ³•è‡ªè¡Œè§£å†³é—®é¢˜æ—¶å¦‚ä½•å‘ç¤¾åŒºå¯»æ±‚å¸®åŠ©ã€‚å¦‚æœä½ å‘ç°äº† Hugging Face åº“ä¸­çš„ä¸€ä¸ª bugï¼Œæˆ‘ä»¬ä¼šå‘Šè¯‰ä½ æŠ¥å‘Š bug çš„æœ€ä½³æ–¹æ³•ï¼Œä»¥ä¾¿å°½å¿«è§£å†³é—®é¢˜ã€‚

æ›´å‡†ç¡®åœ°è¯´ï¼Œåœ¨æœ¬ç« ä¸­ï¼Œä½ å°†å­¦ä¹ ï¼š

- å‡ºç°é”™è¯¯æ—¶è¦åšçš„ç¬¬ä¸€ä»¶äº‹
- å¦‚ä½•åœ¨ [è®ºå›](https://discuss.huggingface.co)(https://discuss.huggingface.co) å¯»æ±‚å¸®åŠ© 
- å¦‚ä½•è°ƒè¯•ä½ çš„è®­ç»ƒç®¡é“
- å¦‚ä½•å†™ä¸€ä¸ªå¥½é—®é¢˜

å½“ç„¶ï¼Œæ‰€æœ‰è¿™äº›éƒ½ä¸ Transformers æˆ– Hugging Face ç”Ÿæ€æ— å…³ï¼›æœ¬ç« çš„ç»éªŒæ•™è®­é€‚ç”¨äºå¤§å¤šæ•°å¼€æºé¡¹ç›®ï¼

## 9.1 å‡ºç°é”™è¯¯æ—¶è¯¥æ€ä¹ˆåŠ 

åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†ç ”ç©¶å½“ä½ å°è¯•ä»æ–°è°ƒæ•´çš„ Transformer æ¨¡å‹ç”Ÿæˆé¢„æµ‹æ—¶å¯èƒ½å‘ç”Ÿçš„ä¸€äº›å¸¸è§é”™è¯¯ã€‚æœ¬èŠ‚ä¸ºç¬¬å››èŠ‚åšå‡†å¤‡ï¼Œæ¢ç´¢å¦‚ä½•è°ƒè¯•è®­ç»ƒé˜¶æ®µæœ¬èº«ã€‚

æˆ‘ä»¬ä¸ºè¿™ä¸€èŠ‚å‡†å¤‡äº†ä¸€ä¸ª [æ¨¡æ¿ä»“åº“](https://huggingface.co/lewtun/distilbert-base-uncased-finetuned-squad-d5716d28)(https://huggingface.co/lewtun/distilbert-base-uncased-finetuned-squad-d5716d28) ï¼Œå¦‚æœä½ æƒ³è¿è¡Œæœ¬ç« ä¸­çš„ä»£ç ï¼Œé¦–å…ˆéœ€è¦å°†æ¨¡å‹å¤åˆ¶åˆ°è‡ªå·±çš„ [Hugging Face Hub](https://huggingface.co)(https://huggingface.co) è´¦å·ã€‚è¿™éœ€è¦ä½ åœ¨ Jupyter Notebook ä¸­è¿è¡Œä»¥ä¸‹ä»»ä¸€å‘½ä»¤æ¥ç™»å½•ï¼š

```python
from huggingface_hub import notebook_login

notebook_login()
```

æˆ–åœ¨ä½ æœ€å–œæ¬¢çš„ç»ˆç«¯ä¸­æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š

```python
huggingface-cli login
```

è¿™é‡Œå°†ä¼šæç¤ºä½ è¾“å…¥ç”¨æˆ·åå’Œå¯†ç ï¼Œå¹¶åœ¨ `~/.cache/huggingface/` ä¿å­˜ä¸€ä¸ªä»¤ç‰Œ å®Œæˆç™»å½•åï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹åŠŸèƒ½å¤åˆ¶æ¨¡æ¿ä»“åº“ï¼š

```python
from distutils.dir_util import copy_tree
from huggingface_hub import Repository, snapshot_download, create_repo, get_full_repo_name

def copy_repository_template():
    # å…‹éš†ä»“åº“å¹¶æå–æœ¬åœ°è·¯å¾„
    template_repo_id = "lewtun/distilbert-base-uncased-finetuned-squad-d5716d28"
    commit_hash = "be3eaffc28669d7932492681cd5f3e8905e358b4"
    template_repo_dir = snapshot_download(template_repo_id, revision=commit_hash)
    # åœ¨ Hub ä¸Šåˆ›å»ºä¸€ä¸ªæ–°ä»“åº“
    model_name = template_repo_id.split("/")[1]
    create_repo(model_name, exist_ok=True)
    # å…‹éš†ç©ºä»“åº“
    new_repo_id = get_full_repo_name(model_name)
    new_repo_dir = model_name
    repo = Repository(local_dir=new_repo_dir, clone_from=new_repo_id)
    # å¤åˆ¶æ–‡ä»¶
    copy_tree(template_repo_dir, new_repo_dir)
    # ä¸Šä¼ åˆ° Hub ä¸Š
    repo.push_to_hub()
```

ç°åœ¨å½“ä½ è°ƒç”¨ `copy_repository_template()` æ—¶ï¼Œå®ƒå°†åœ¨ä½ çš„å¸æˆ·ä¸‹åˆ›å»ºæ¨¡æ¿ä»“åº“çš„å‰¯æœ¬ã€‚

### ä» Transformers è°ƒè¯• `pipeline` 

æ¥ä¸‹æ¥è¦å¼€å§‹æˆ‘ä»¬è°ƒè¯• Transformer æ¨¡å‹çš„å¥‡å¦™ä¸–ç•Œä¹‹æ—…ï¼Œè¯·è€ƒè™‘ä»¥ä¸‹æƒ…æ™¯ï¼šä½ æ­£åœ¨ä¸ä¸€ä½åŒäº‹åˆä½œè¿›è¡Œä¸€ä¸ªæé—®-å›ç­”çš„é¡¹ç›®ï¼Œè¿™ä¸ªé¡¹ç›®ç”¨æ¥å¸®åŠ©ç”µå­å•†åŠ¡ç½‘ç«™çš„å®¢æˆ·æ‰¾åˆ°æœ‰å…³æ¶ˆè´¹å“çš„ç­”æ¡ˆã€‚ä½ çš„åŒäº‹ç»™ä½ å‘äº†ä¸€æ¡æ¶ˆæ¯ï¼Œä¸¾ä¸ªä¾‹å­ï¼š

> å—¨ï¼æˆ‘åˆšåˆšä½¿ç”¨äº† Hugging Face è¯¾ç¨‹çš„ç¬¬å…«ç« ä¸­çš„æŠ€æœ¯è¿›è¡Œäº†ä¸€ä¸ªå®éªŒï¼Œå¹¶åœ¨ SQuAD ä¸Šè·å¾—äº†ä¸€äº›å¾ˆæ£’çš„ç»“æœï¼æˆ‘è§‰å¾—æˆ‘ä»¬å¯ä»¥ç”¨è¿™ä¸ªæ¨¡å‹ä½œä¸ºé¡¹ç›®çš„èµ·ç‚¹ã€‚Hub ä¸Šçš„æ¨¡å‹ ID æ˜¯ `lewtun/distillbert-base-uncased-finetuned-squad-d5716d28`ã€‚ä½ æ¥æµ‹è¯•ä¸€ä¸‹ ï¼‰

ä½ é¦–å…ˆæƒ³åˆ°çš„æ˜¯ä½¿ç”¨ Transformers ä¸­çš„ `pipeline` ï¼š

```python
from transformers import pipeline

model_checkpoint = get_full_repo_name("distillbert-base-uncased-finetuned-squad-d5716d28")
reader = pipeline("question-answering", model=model_checkpoint)
```

```python
"""
OSError: Can't load config for 'lewtun/distillbert-base-uncased-finetuned-squad-d5716d28'. Make sure that:

- 'lewtun/distillbert-base-uncased-finetuned-squad-d5716d28' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'lewtun/distillbert-base-uncased-finetuned-squad-d5716d28' is the correct path to a directory containing a config.json file
"""
```

å•Šå“¦ï¼Œå¥½åƒå‡ºäº†ä»€ä¹ˆé—®é¢˜ï¼å¦‚æœä½ æ˜¯ç¼–ç¨‹æ–°æ‰‹ï¼Œè¿™ç±»é”™è¯¯ä¸€å¼€å§‹çœ‹èµ·æ¥æœ‰ç‚¹ç¥ç§˜ ï¼ˆ `OSError` åˆ°åº•æ˜¯ä»€ä¹ˆï¼Ÿï¼‰ã€‚å…¶å®è¿™é‡Œæ˜¾ç¤ºçš„é”™è¯¯åªæ˜¯ä¸€ä¸ªæ›´å¤§çš„é”™è¯¯æŠ¥å‘Šä¸­æœ€åä¸€éƒ¨åˆ†ï¼Œç§°ä¸º `Python traceback` ï¼ˆåˆåå †æ ˆè·Ÿè¸ªï¼‰ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ åœ¨ Google Colab ä¸Šè¿è¡Œæ­¤ä»£ç ï¼Œä½ åº”è¯¥ä¼šçœ‹åˆ°ç±»ä¼¼äºä»¥ä¸‹å±å¹•æˆªå›¾çš„å†…å®¹ï¼š

![ä¸€ä¸ª Python çš„ traceback](./assets/traceback.png "A Python traceback.")

è¿™äº›æŠ¥å‘Šä¸­åŒ…å«å¾ˆå¤šä¿¡æ¯ï¼Œè®©æˆ‘ä»¬ä¸€èµ·æ¥çœ‹çœ‹å…³é”®éƒ¨åˆ†ã€‚è¯»å–è¿™æ ·çš„æŠ¥å‘Šæ—¶çš„é¡ºåºæ¯”è¾ƒç‰¹æ®Šï¼Œåº”è¯¥æŒ‰ç…§ä»åº•éƒ¨åˆ°é¡¶éƒ¨çš„é¡ºåºé˜…è¯»ï¼Œå¦‚æœä½ ä¹ æƒ¯äºä»ä¸Šåˆ°ä¸‹é˜…è¯»è‹±æ–‡æ–‡æœ¬ï¼Œè¿™å¯èƒ½å¬èµ·æ¥å¾ˆå¥‡æ€ªï¼Œä½†å®ƒåæ˜ äº†ä¸€ä¸ªäº‹å®ï¼štraceback æ˜¾ç¤ºäº†åœ¨ä¸‹è½½æ¨¡å‹å’Œ tokenizer æ—¶ `pipeline` è°ƒç”¨çš„å‡½æ•°åºåˆ—ã€‚ï¼ˆæŸ¥çœ‹ç¬¬ä¸‰ç« äº†è§£æœ‰å…³ `pipeline` å¦‚ä½•åœ¨åå°è¿è¡Œçš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚ï¼‰

<div custom-style="Tip-red">

ğŸš¨ çœ‹åˆ° Google Colab ä¸­ traceback å‘¨å›´ â€œ6 framesâ€ å‘¨å›´çš„è“è‰²æ¡†äº†å—ï¼Ÿè¿™æ˜¯ Colab çš„ä¸€ä¸ªç‰¹æ®ŠåŠŸèƒ½ï¼Œå®ƒå°† traceback å‹ç¼©ä¸ºâ€œframesâ€ã€‚å¦‚æœä½ æ— æ³•æ‰¾åˆ°é”™è¯¯çš„æ¥æºï¼Œå¯ä»¥é€šè¿‡å•å‡»è¿™ä¸¤ä¸ªå°ç®­å¤´æ¥å±•å¼€å®Œæ•´çš„ tracebackã€‚

</div>

è¿™æ„å‘³ç€ traceback çš„æœ€åä¸€è¡ŒæŒ‡ç¤ºæœ€åä¸€æ¡é”™è¯¯æ¶ˆæ¯å¹¶ç»™å‡ºå¼•å‘çš„å¼‚å¸¸åç§°ã€‚åœ¨è¿™é‡Œï¼Œå¼‚å¸¸ç±»å‹æ˜¯ `OSError` ï¼Œè¡¨ç¤ºä¸ç³»ç»Ÿç›¸å…³çš„é”™è¯¯ã€‚å¦‚æœæˆ‘ä»¬é˜…è¯»éšä¹‹é™„ç€çš„é”™è¯¯æ¶ˆæ¯ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ¨¡å‹çš„ `config.json` æ–‡ä»¶ä¼¼ä¹æœ‰é—®é¢˜ï¼Œè¿™é‡Œç»™å‡ºäº†ä¸¤ä¸ªä¿®å¤çš„å»ºè®®ï¼š

```python
"""
Make sure that:

- 'lewtun/distillbert-base-uncased-finetuned-squad-d5716d28' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'lewtun/distillbert-base-uncased-finetuned-squad-d5716d28' is the correct path to a directory containing a config.json file
"""
```

<div custom-style="Tip-green">

ğŸ’¡ å¦‚æœä½ é‡åˆ°éš¾ä»¥ç†è§£çš„é”™è¯¯æ¶ˆæ¯ï¼Œåªéœ€å°†è¯¥æ¶ˆæ¯å¤åˆ¶å¹¶ç²˜è´´åˆ° Google æˆ– [Stack Overflow](https://stackoverflow.com)(https://stackoverflow.com) æœç´¢æ ä¸­ã€‚ä½ å¾ˆæœ‰å¯èƒ½ä¸æ˜¯ç¬¬ä¸€ä¸ªé‡åˆ°é”™è¯¯çš„äººï¼Œè¿™å¯ä»¥åœ¨ç¤¾åŒºä¸­æ‰¾åˆ°å…¶ä»–äººå‘å¸ƒçš„è§£å†³æ–¹æ¡ˆã€‚ä¾‹å¦‚ï¼Œåœ¨ Stack Overflow ä¸Šæœç´¢ `OSError: Can't load config for` ç»™å‡ºäº†å‡ ä¸ª [ç»“æœ](https://stackoverflow.com/search?q=OSError%3A+Can%27t+load+config+for+)(https://stackoverflow.com/search?q=OSError%3A+Can%27t+load+config+for+) ï¼Œå¯ä»¥ä½œä¸ºä½ è§£å†³é—®é¢˜çš„èµ·ç‚¹ã€‚

</div>

ç¬¬ä¸€ä¸ªå»ºè®®æ˜¯æ£€æŸ¥æ¨¡å‹ ID æ˜¯å¦çœŸçš„æ­£ç¡®ï¼Œæ‰€ä»¥é¦–å…ˆè¦åšçš„å°±æ˜¯å¤åˆ¶æ ‡ç­¾å¹¶å°†å…¶ç²˜è´´åˆ° Hub çš„æœç´¢æ ä¸­ï¼š

![é”™è¯¯çš„ model æ ‡ç­¾](./assets/wrong-model-id.png "The wrong model name.")

å—¯ï¼Œçœ‹èµ·æ¥ä½ åŒäº‹çš„æ¨¡å‹ç¡®å®ä¸åœ¨ Hub ä¸Šã€‚ä½†æ˜¯ä»”ç»†çœ‹æ¨¡å‹åç§°ä¸­æœ‰ä¸€ä¸ªé”™å­—ï¼DistilBERT çš„åç§°ä¸­åªæœ‰ä¸€ä¸ª â€œlâ€ï¼Œæ‰€ä»¥è®©æˆ‘ä»¬ä¿®æ­£åå¯»æ‰¾ `lewtun/distilbert-base-uncased-finetuned-squad-d5716d28`ï¼š

![æ­£ç¡®çš„ model æ ‡ç­¾](./assets/true-model-id.png "The right model name.")

å¥½çš„ï¼Œè¿™æ¬¡æœ‰ç»“æœäº†ã€‚ç°åœ¨è®©æˆ‘ä»¬ä½¿ç”¨æ­£ç¡®çš„æ¨¡å‹ ID å†æ¬¡å°è¯•ä¸‹è½½æ¨¡å‹ï¼š

```python
model_checkpoint = get_full_repo_name("distilbert-base-uncased-finetuned-squad-d5716d28")
reader = pipeline("question-answering", model=model_checkpoint)
```

```python
"""
OSError: Can't load config for 'lewtun/distilbert-base-uncased-finetuned-squad-d5716d28'. Make sure that:

- 'lewtun/distilbert-base-uncased-finetuned-squad-d5716d28' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'lewtun/distilbert-base-uncased-finetuned-squad-d5716d28' is the correct path to a directory containing a config.json file
"""
```

å•Šï¼Œå†æ¬¡å¤±è´¥ã€‚ä¸è¦æ°”é¦ï¼Œæ¬¢è¿æ¥åˆ°æœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆçš„æ—¥å¸¸ç”Ÿæ´»ï¼å‰é¢æˆ‘ä»¬å·²ç»ä¿®æ­£äº†æ¨¡å‹ IDï¼Œæ‰€ä»¥é—®é¢˜ä¸€å®šå‡ºåœ¨ä»“åº“æœ¬èº«ã€‚è¿™é‡Œæä¾›ä¸€ç§å¿«é€Ÿè®¿é—® Hub ä¸Šä»“åº“å†…å®¹çš„æ–¹æ³•â€”â€”é€šè¿‡ `huggingface_hub` åº“çš„ `list_repo_files()` å‡½æ•°ï¼š

```python
from huggingface_hub import list_repo_files

list_repo_files(repo_id=model_checkpoint)
```

```python
['.gitattributes', 'README.md', 'pytorch_model.bin', 'special_tokens_map.json', 'tokenizer_config.json', 'training_args.bin', 'vocab.txt']
```

 æœ‰è¶£çš„æ˜¯â€”â€”ä»“åº“ä¸­ä¼¼ä¹æ²¡æœ‰é…ç½® `config.json` æ–‡ä»¶ï¼éš¾æ€ªæˆ‘ä»¬çš„ `pipeline` æ— æ³•åŠ è½½æ¨¡å‹ï¼›ä½ çš„åŒäº‹ä¸€å®šæ˜¯åœ¨å¾®è°ƒåå¿˜è®°å°†è¿™ä¸ªæ–‡ä»¶ä¸Šä¼ åˆ° Hubã€‚åœ¨è¿™ç§æƒ…å†µä¸‹é—®é¢˜ä¼¼ä¹å¾ˆå®¹æ˜“è§£å†³ï¼šè¦æ±‚ä»–æ·»åŠ æ–‡ä»¶ï¼Œæˆ–è€…ç”±äºæˆ‘ä»¬ä»æ¨¡å‹ ID ä¸­å¯ä»¥çœ‹å‡ºé¢„è®­ç»ƒæ¨¡å‹ä½¿ç”¨çš„æ˜¯ [`distilbert-base-uncased`](https://huggingface.co/distilbert-base-uncased)(https://huggingface.co/distilbert-base-uncased) ï¼Œæˆ‘ä»¬å¯ä»¥ä¸‹è½½æ­¤æ¨¡å‹çš„é…ç½®å¹¶å°†å…¶ä¸Šä¼ åˆ°ä½ ä»¬çš„ä»“åº“åæŸ¥çœ‹æ˜¯å¦å¯ä»¥è§£å†³é—®é¢˜ã€‚åœ¨è¿™é‡Œæ¶‰åŠåˆ°ç¬¬ä¸‰ç« ä¸­å­¦ä¹ çš„æŠ€å·§ï¼Œä½¿ç”¨ `AutoConfig` ç±»ä¸‹è½½æ¨¡å‹çš„é…ç½®ï¼š

```python
from transformers import AutoConfig

pretrained_checkpoint = "distilbert-base-uncased"
config = AutoConfig.from_pretrained(pretrained_checkpoint)
```

<div custom-style="Tip-red">

ğŸš¨ åœ¨è¿™é‡Œé‡‡ç”¨çš„æ–¹æ³•å¹¶ä¸æ˜¯ç™¾åˆ†ä¹‹ç™¾å¯é çš„ï¼Œå› ä¸ºä½ çš„åŒäº‹å¯èƒ½åœ¨å¾®è°ƒæ¨¡å‹ä¹‹å‰å·²ç»è°ƒæ•´äº† `distilbert-base-uncased` é…ç½®ã€‚åœ¨å®é™…æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬åº”è¯¥å…ˆä¸ä»–ä»¬æ ¸å®ï¼Œä½†åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†å‡è®¾ä»–ä»¬ä½¿ç”¨äº†é»˜è®¤é…ç½®ã€‚

</div>

ä¸Šä¸€æ­¥æˆåŠŸåå¯ä»¥ä½¿ç”¨é…ç½®çš„ `push_to_hub()` æ–¹æ³•å°†å…¶ä¸Šä¼ åˆ°æ¨¡å‹ä»“åº“ï¼š

```python
config.push_to_hub(model_checkpoint, commit_message="Add config.json")
```

ç°åœ¨å¯ä»¥é€šè¿‡ä»æœ€æ–°æäº¤çš„ `main` åˆ†æ”¯ä¸­åŠ è½½æ¨¡å‹æ¥æµ‹è¯•æ˜¯å¦æœ‰æ•ˆï¼š

```python
reader = pipeline("question-answering", model=model_checkpoint, revision="main")

context = r"""
Extractive Question Answering is the task of extracting an answer from a text
given a question. An example of a question answering dataset is the SQuAD
dataset, which is entirely based on that task. If you would like to fine-tune a
model on a SQuAD task, you may leverage the
examples/pytorch/question-answering/run_squad.py script.

Transformers is interoperable with the PyTorch, TensorFlow, and JAX
frameworks, so you can use your favourite tools for a wide variety of tasks!
"""

question = "What is extractive question answering?"
reader(question=question, context=context)
```

```python
{'score': 0.38669535517692566,
 'start': 34,
 'end': 95,
 'answer': 'the task of extracting an answer from a text given a question'}
```

æˆåŠŸäº†ï¼è®©æˆ‘ä»¬å›é¡¾ä¸€ä¸‹ä½ åˆšåˆšå­¦åˆ°çš„ä¸œè¥¿ï¼š

- Python ä¸­çš„é”™è¯¯æ¶ˆæ¯ç§°ä¸º `tracebacks` ï¼Œæ³¨æ„éœ€è¦ä»ä¸‹åˆ°ä¸Šé˜…è¯»ã€‚é”™è¯¯æ¶ˆæ¯çš„æœ€åä¸€è¡Œé€šå¸¸åŒ…å«å®šä½é—®é¢˜æ ¹æºæ‰€éœ€çš„ä¿¡æ¯ã€‚
- å¦‚æœæœ€åä¸€è¡Œæ²¡æœ‰åŒ…å«è¶³å¤Ÿçš„ä¿¡æ¯ï¼Œè¯·è¿›è¡Œ `tracebacks` ï¼Œçœ‹çœ‹æ˜¯å¦å¯ä»¥ç¡®å®šæºä»£ç ä¸­å‘ç”Ÿé”™è¯¯çš„ä½ç½®ã€‚
- å¦‚æœæ²¡æœ‰ä»»ä½•é”™è¯¯æ¶ˆæ¯å¯ä»¥å¸®åŠ©ä½ è°ƒè¯•é—®é¢˜ï¼Œè¯·å°è¯•åœ¨çº¿æœç´¢ç±»ä¼¼é—®é¢˜çš„è§£å†³æ–¹æ¡ˆã€‚
- `huggingface_hub` åº“æä¾›äº†ä¸€å¥—å·¥å…·ï¼Œä½ å¯ä»¥ä½¿ç”¨è¿™äº›å·¥å…·ä¸ Hub ä¸Šçš„ä»“åº“è¿›è¡Œäº¤äº’å’Œè°ƒè¯•ã€‚

ç°åœ¨ä½ çŸ¥é“å¦‚ä½•è°ƒè¯• `pipeline` ï¼Œè®©æˆ‘ä»¬æ¥çœ‹ä¸€ä¸ªæ›´æ£˜æ‰‹çš„ä¾‹å­ï¼Œå³æ¨¡å‹æœ¬èº«çš„å‰å‘ä¼ æ’­ã€‚

### è°ƒè¯•æ¨¡å‹çš„å‰å‘ä¼ æ’­ 

å°½ç®¡ `pipeline` å¯¹äºå¤§å¤šæ•°éœ€è¦å¿«é€Ÿç”Ÿæˆé¢„æµ‹çš„åº”ç”¨ç¨‹åºæ¥è¯´éå¸¸æœ‰ç”¨ï¼Œä½†æ˜¯æœ‰æ—¶ä½ éœ€è¦è®¿é—®æ¨¡å‹çš„ logits ï¼ˆæ¯”å¦‚ä½ æƒ³è¦å®ç°ä¸€äº›çš„è‡ªå®šä¹‰åå¤„ç†ï¼‰ã€‚ä¸ºäº†çœ‹çœ‹åœ¨è¿™ç§æƒ…å†µä¸‹ä¼šå‡ºç°ä»€ä¹ˆé—®é¢˜ï¼Œè®©æˆ‘ä»¬é¦–å…ˆä» `pipeline` ä¸­è·å–æ¨¡å‹å’Œ Tokenizers 

```python
tokenizer = reader.tokenizer
model = reader.model
```

æ¥ä¸‹æ¥æˆ‘ä»¬éœ€è¦æå‡ºä¸€ä¸ªé—®é¢˜ï¼Œçœ‹çœ‹æˆ‘ä»¬æœ€å–œæ¬¢çš„æ¡†æ¶æ˜¯å¦å—æ”¯æŒï¼š

```python
question = "Which frameworks can I use?"
```

æ­£å¦‚æˆ‘ä»¬åœ¨ç¬¬å…«ç« ä¸­å­¦ä¹ çš„ï¼Œæˆ‘ä»¬éœ€è¦é‡‡å–çš„æ­¥éª¤æ˜¯å¯¹è¾“å…¥è¿›è¡Œ tokenizeï¼Œæå–èµ·å§‹å’Œç»“æŸ token çš„ logitsï¼Œç„¶åè§£ç ç­”æ¡ˆèŒƒå›´ï¼š

```python
import torch

inputs = tokenizer(question, context, add_special_tokens=True)
input_ids = inputs["input_ids"][0]
outputs = model(**inputs)
answer_start_scores = outputs.start_logits
answer_end_scores = outputs.end_logits
# è®¡ç®—åˆ†æ•°çš„ argmax è·å–æœ€æœ‰å¯èƒ½çš„ç­”æ¡ˆå¼€å¤´
answer_start = torch.argmax(answer_start_scores)
# è®¡ç®—åˆ†æ•°çš„ argmax è·å–æœ€æœ‰å¯èƒ½çš„ç­”æ¡ˆç»“å°¾
answer_end = torch.argmax(answer_end_scores) + 1
answer = tokenizer.convert_tokens_to_string(
    tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])
)
print(f"Question: {question}")
print(f"Answer: {answer}")
```

```python
"""
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
/var/folders/28/k4cy5q7s2hs92xq7_h89_vgm0000gn/T/ipykernel_75743/2725838073.py in <module>
      1 inputs = tokenizer(question, text, add_special_tokens=True)
      2 input_ids = inputs["input_ids"]
----> 3 outputs = model(**inputs)
      4 answer_start_scores = outputs.start_logits
      5 answer_end_scores = outputs.end_logits

~/miniconda3/envs/huggingface/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
   1049         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1050                 or _global_forward_hooks or _global_forward_pre_hooks):
-> 1051             return forward_call(*input, **kwargs)
   1052         # Do not call functions when jit is used
   1053         full_backward_hooks, non_full_backward_hooks = [], []

~/miniconda3/envs/huggingface/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py in forward(self, input_ids, attention_mask, head_mask, inputs_embeds, start_positions, end_positions, output_attentions, output_hidden_states, return_dict)
    723         return_dict = return_dict if return_dict is not None else self.config.use_return_dict
    724
--> 725         distilbert_output = self.distilbert(
    726             input_ids=input_ids,
    727             attention_mask=attention_mask,

~/miniconda3/envs/huggingface/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
   1049         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1050                 or _global_forward_hooks or _global_forward_pre_hooks):
-> 1051             return forward_call(*input, **kwargs)
   1052         # Do not call functions when jit is used
   1053         full_backward_hooks, non_full_backward_hooks = [], []

~/miniconda3/envs/huggingface/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py in forward(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)
    471             raise ValueError("You cannot specify both input_ids and inputs_embeds at the same time")
    472         elif input_ids is not None:
--> 473             input_shape = input_ids.size()
    474         elif inputs_embeds is not None:
    475             input_shape = inputs_embeds.size()[:-1]

AttributeError: 'list' object has no attribute 'size'
"""
```

çœ‹èµ·æ¥æˆ‘ä»¬çš„ä»£ç ä¸­æœ‰ä¸€ä¸ªé”™è¯¯ï¼ä¸ç”¨ç´§å¼ ï¼Œä½ å¯ä»¥åœ¨ Notebook ä¸­ä½¿ç”¨ Python è°ƒè¯•å™¨ï¼š

æˆ–åœ¨ç»ˆç«¯ä¸­ï¼š

åœ¨è¿™é‡Œï¼Œé”™è¯¯æ¶ˆæ¯å‘Šè¯‰æˆ‘ä»¬ `'list' object has no attribute 'size'` ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¸€ä¸ª `-->` ç®­å¤´æŒ‡å‘ `model(**inputs)` ä¸­å¼•å‘é—®é¢˜çš„è¡Œã€‚ä½ å¯ä»¥ä½¿ç”¨ Python è°ƒè¯•å™¨ç”¨äº¤äº’æ–¹å¼æ¥è°ƒè¯•å®ƒï¼Œä½†ç°åœ¨æˆ‘ä»¬åªéœ€æ‰“å°å‡ºä¸€éƒ¨åˆ† `inputs` ï¼Œçœ‹çœ‹æˆ‘ä»¬æœ‰ä»€ä¹ˆï¼š

```python
inputs["input_ids"][:5]
```

```python
[101, 2029, 7705, 2015, 2064]
```

è¿™å½“ç„¶çœ‹èµ·æ¥åƒä¸€ä¸ªæ™®é€šçš„ Python `list` ï¼Œä½†è®©æˆ‘ä»¬ä»”ç»†æ£€æŸ¥ä¸€ä¸‹ç±»å‹ï¼š

```python
type(inputs["input_ids"])
```

```python
list
```

æ˜¯çš„ï¼Œé‚£è‚¯å®šæ˜¯ä¸€ä¸ª Python `list` ã€‚é‚£ä¹ˆå‡ºäº†ä»€ä¹ˆé—®é¢˜å‘¢ï¼Ÿå›æƒ³ä¸€ä¸‹ç¬¬ä¸‰ç« Transformers ä¸­çš„ `AutoModelForXxx` ç±»åœ¨ `tensors` ï¼ˆPyTorch æˆ–è€… TensorFlowï¼‰ è¿›è¡Œæ“ä½œï¼Œä¾‹å¦‚åœ¨ PyTorch ä¸­ï¼Œä¸€ä¸ªå¸¸è§çš„æ“ä½œæ˜¯ä½¿ç”¨ `Tensor.size()` æ–¹æ³•æå–å¼ é‡çš„ç»´åº¦ã€‚è®©æˆ‘ä»¬å†å›åˆ° traceback ä¸­ï¼Œçœ‹çœ‹å“ªä¸€è¡Œè§¦å‘äº†å¼‚å¸¸ï¼š

```python
~/miniconda3/envs/huggingface/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py in forward(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)
    471             raise ValueError("You cannot specify both input_ids and inputs_embeds at the same time")
    472         elif input_ids is not None:
--> 473             input_shape = input_ids.size()
    474         elif inputs_embeds is not None:
    475             input_shape = inputs_embeds.size()[:-1]

AttributeError: 'list' object has no attribute 'size'
```

çœ‹èµ·æ¥æˆ‘ä»¬çš„ä»£ç è¯•å›¾è°ƒç”¨ `input_ids.size()` ï¼Œä½†è¿™æ˜¾ç„¶ä¸é€‚ç”¨äº Python `list` ï¼Œè¿™åªæ˜¯ä¸€ä¸ªå®¹å™¨ã€‚æˆ‘ä»¬å¦‚ä½•è§£å†³è¿™ä¸ªé—®é¢˜å‘¢ï¼Ÿåœ¨ Stack Overflow ä¸Šæœç´¢é”™è¯¯æ¶ˆæ¯æ‰¾åˆ°äº†å¾ˆå¤šç›¸å…³çš„ [ç»“æœ](https://stackoverflow.com/search?q=AttributeError%3A+%27list%27+object+has+no+attribute+%27size%27&s=c15ec54c-63cb-481d-a749-408920073e8f)(https://stackoverflow.com/search?q=AttributeError%3A+%27list%27+object+has+no+attribute+%27size%27&s=c15ec54c-63cb-481d-a749-408920073e8f) ã€‚å•å‡»ç¬¬ä¸€ä¸ªä¼šæ˜¾ç¤ºä¸æˆ‘ä»¬ç±»ä¼¼çš„é—®é¢˜ï¼Œç­”æ¡ˆå¦‚ä¸‹é¢çš„å±å¹•æˆªå›¾æ‰€ç¤ºï¼š

![åœ¨ Stack Overflow ä¸Šçš„ä¸€ä¸ªå›ç­”](./assets/stack-overflow.png "An answer from Stack Overflow.")

è¿™é‡Œå»ºè®®æˆ‘ä»¬æ·»åŠ  `return_tensors='pt'` åˆ° Tokenizerï¼Œè®©æˆ‘ä»¬æµ‹è¯•ä¸€ä¸‹æ˜¯å¦é€‚åˆï¼š

```python
inputs = tokenizer(question, context, add_special_tokens=True, return_tensors="pt")
input_ids = inputs["input_ids"][0]
outputs = model(**inputs)
answer_start_scores = outputs.start_logits
answer_end_scores = outputs.end_logits
# è®¡ç®—åˆ†æ•°çš„ argmax è·å–æœ€æœ‰å¯èƒ½çš„ç­”æ¡ˆå¼€å¤´
answer_start = torch.argmax(answer_start_scores)
# è®¡ç®—åˆ†æ•°çš„ argmax è·å–æœ€æœ‰å¯èƒ½çš„ç­”æ¡ˆç»“å°¾
answer_end = torch.argmax(answer_end_scores) + 1
answer = tokenizer.convert_tokens_to_string(
    tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])
)
print(f"Question: {question}")
print(f"Answer: {answer}")
```

```python
"""
Question: Which frameworks can I use?
Answer: pytorch, tensorflow, and jax
"""
```

æˆåŠŸäº†ï¼è¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„ä¾‹å­ï¼Œå±•ç¤ºäº† Stack Overflow çš„å®ç”¨æ€§ï¼Œæœç´¢ç±»ä¼¼çš„é—®é¢˜ï¼Œæˆ‘ä»¬èƒ½å¤Ÿä»ç¤¾åŒºä¸­å…¶ä»–äººçš„ç»éªŒä¸­å—ç›Šã€‚ç„¶è€Œï¼Œåƒè¿™æ ·çš„æœç´¢ä¸æ€»ä¼šäº§ç”Ÿç›¸å…³çš„ç­”æ¡ˆï¼Œé‚£ä¹ˆåœ¨è¿™ç§æƒ…å†µä¸‹ä½ è¯¥æ€ä¹ˆåŠå‘¢ï¼Ÿå¹¸è¿çš„æ˜¯ï¼Œåœ¨ [Hugging Face è®ºå›](https://discuss.huggingface.co/)(https://discuss.huggingface.co/) ä¸Šæœ‰ä¸€ä¸ªå‹å¥½çš„å¼€å‘è€…ç¤¾åŒºå¯ä»¥å¸®åŠ©ä½ ï¼åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹çœ‹åœ¨è¯¥å¹³å°ä¸­å¦‚ä½•æœ€å¤§å¯èƒ½å¾—åˆ°é—®é¢˜çš„å›ç­”ã€‚

## 9.2 åœ¨è®ºå›ä¸Šå¯»æ±‚å¸®åŠ© 

[Hugging Face è®ºå›](https://discuss.huggingface.co)(https://discuss.huggingface.co) æ˜¯ä»å¼€æºå›¢é˜Ÿå’Œæ›´å¹¿æ³›çš„ Hugging Face ç¤¾åŒºè·å¾—å¸®åŠ©çš„å¥½åœ°æ–¹ã€‚ä»¥ä¸‹æ˜¯è®ºå›æŸä¸€å¤©çš„ä¸»é¡µé¢ï¼š

![Hugging Face è®ºå›](./assets/forums.png "The Hugging Face forums.")

åœ¨å·¦ä¾§ï¼Œä½ å¯ä»¥çœ‹åˆ°å„ç§ä¸»é¢˜åˆ†ç»„çš„æ‰€æœ‰ç±»åˆ«ï¼Œè€Œå³ä¾§æ˜¾ç¤ºäº†æœ€æ–°çš„ä¸»é¢˜ã€‚ä¸€ä¸ªä¸»é¢˜åŒ…å«æ ‡é¢˜ã€ç±»åˆ«å’Œæè¿°ï¼›å®ƒä¸æˆ‘ä»¬åœ¨ç¬¬å…­ç« ä¸­åˆ›å»ºè‡ªå·±çš„æ•°æ®é›†æ—¶çœ‹åˆ°çš„ GitHub issue æ ¼å¼éå¸¸ç›¸ä¼¼ é¡¾åæ€ä¹‰ï¼Œ [Beginnersï¼ˆåˆå­¦è€…ï¼‰](https://discuss.huggingface.co/c/beginners/5)(https://discuss.huggingface.co/c/beginners/5) ç±»åˆ«ä¸»è¦é¢å‘åˆšå¼€å§‹ä½¿ç”¨ Hugging Face åº“å’Œç”Ÿæ€ç³»ç»Ÿçš„äººã€‚æ¬¢è¿ä½ å¯¹ä»»ä½•åº“æå‡ºä»»ä½•é—®é¢˜ï¼Œæ— è®ºæ˜¯è°ƒè¯•ä¸€äº›ä»£ç è¿˜æ˜¯å¯»æ±‚æœ‰å…³å¦‚ä½•åšæŸäº‹çš„å¸®åŠ©ã€‚ï¼ˆè¯è™½å¦‚æ­¤ï¼Œå¦‚æœä½ çš„é—®é¢˜ç‰¹å±äºæŸä¸ªåº“ï¼Œä½ å¯èƒ½åº”è¯¥å‰å¾€è®ºå›ä¸Šå¯¹åº”çš„åº“ç±»åˆ«ã€‚ï¼‰

åŒæ ·ï¼Œ [Intermediateï¼ˆä¸­çº§ï¼‰](https://discuss.huggingface.co/c/intermediate/6)(https://discuss.huggingface.co/c/intermediate/6) å’Œ [Researchï¼ˆç ”ç©¶ï¼‰](https://discuss.huggingface.co/c/research/7)(https://discuss.huggingface.co/c/research/7) ç±»åˆ«ç”¨äºæ›´é«˜çº§çš„é—®é¢˜ï¼Œä¾‹å¦‚å…³äºåº“æˆ–ä¸€äº›æœ‰è¶£çš„æ–°è‡ªç„¶è¯­è¨€å¤„ç†ç ”ç©¶çš„è®¨è®ºã€‚

å½“ç„¶ï¼Œæˆ‘ä»¬ä¹Ÿåº”è¯¥æåˆ° [Course](https://discuss.huggingface.co/c/course/20)(https://discuss.huggingface.co/c/course/20) ç±»åˆ«ï¼Œä½ å¯ä»¥åœ¨é‡Œé¢æå‡ºä¸ Hugging Face Course ç›¸å…³çš„ä»»ä½•é—®é¢˜ï¼

é€‰æ‹©ç±»åˆ«åï¼Œå°±å¯ä»¥ç¼–å†™ç¬¬ä¸€ä¸ªä¸»é¢˜äº†ã€‚ä½ å¯ä»¥æ‰¾ä¸€äº› [æŒ‡å—](https://discuss.huggingface.co/t/how-to-request-support/3128)(https://discuss.huggingface.co/t/how-to-request-support/3128) ï¼Œæ•™ä½ å¦‚ä½•æ’°å†™ä¸»é¢˜ã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†ä¸€èµ·å­¦ä¹ ä¸€äº›ç”¨æ¥æ„æˆä¸€ä¸ªå¥½ä¸»é¢˜çš„ç‰¹ç‚¹ã€‚

### å†™ä¸€ç¯‡é«˜è´¨é‡çš„è®ºå›å¸–å­ 

ä¸¾ä¸ªä¾‹å­ï¼Œæˆ‘ä»¬è¦ä» Wikipedia æ–‡ç« ç”ŸæˆåµŒå…¥è¡¨ç¤ºç”¨æ¥åˆ›å»ºè‡ªå®šä¹‰æœç´¢å¼•æ“ã€‚é€šå¸¸æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬åŠ è½½ Tokenizer å’Œæ¨¡å‹çš„æ–¹å¼å¦‚ä¸‹ï¼š

```python
from transformers import AutoTokenizer, AutoModel

model_checkpoint = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = AutoModel.from_pretrained(model_checkpoint)
```

ç°åœ¨å‡è®¾æˆ‘ä»¬å°è¯•åµŒå…¥ç»´åŸºç™¾ç§‘æ–‡ç« ä¸­å…³äº [å˜å½¢é‡‘åˆš](https://en.wikipedia.org/wiki/Transformers)(https://en.wikipedia.org/wiki/Transformers) ï¼ˆæŒ‡çš„æ˜¯å˜å½¢é‡‘åˆšç³»åˆ—ä½œå“ï¼Œè€Œä¸æ˜¯ Transformers åº“ï¼çƒ­çŸ¥è¯†ï¼šTransformers ä½œä¸ºä¸€ä¸ª Python åº“è¢«è¶Šæ¥è¶Šå¤šäººç†ŸçŸ¥ï¼‰çš„æ•´ä¸ªéƒ¨åˆ†ï¼š

```python
text = """
Generation One is a retroactive term for the Transformers characters that
appeared between 1984 and 1993. The Transformers began with the 1980s Japanese
toy lines Micro Change and Diaclone. They presented robots able to transform
into everyday vehicles, electronic items or weapons. Hasbro bought the Micro
Change and Diaclone toys, and partnered with Takara. Marvel Comics was hired by
Hasbro to create the backstory; editor-in-chief Jim Shooter wrote an overall
story, and gave the task of creating the characthers to writer Dennis O'Neil.
Unhappy with O'Neil's work (although O'Neil created the name "Optimus Prime"),
Shooter chose Bob Budiansky to create the characters.

The Transformers mecha were largely designed by ShÅji Kawamori, the creator of
the Japanese mecha anime franchise Macross (which was adapted into the Robotech
franchise in North America). Kawamori came up with the idea of transforming
mechs while working on the Diaclone and Macross franchises in the early 1980s
(such as the VF-1 Valkyrie in Macross and Robotech), with his Diaclone mechs
later providing the basis for Transformers.

The primary concept of Generation One is that the heroic Optimus Prime, the
villainous Megatron, and their finest soldiers crash land on pre-historic Earth
in the Ark and the Nemesis before awakening in 1985, Cybertron hurtling through
the Neutral zone as an effect of the war. The Marvel comic was originally part
of the main Marvel Universe, with appearances from Spider-Man and Nick Fury,
plus some cameos, as well as a visit to the Savage Land.

The Transformers TV series began around the same time. Produced by Sunbow
Productions and Marvel Productions, later Hasbro Productions, from the start it
contradicted Budiansky's backstories. The TV series shows the Autobots looking
for new energy sources, and crash landing as the Decepticons attack. Marvel
interpreted the Autobots as destroying a rogue asteroid approaching Cybertron.
Shockwave is loyal to Megatron in the TV series, keeping Cybertron in a
stalemate during his absence, but in the comic book he attempts to take command
of the Decepticons. The TV series would also differ wildly from the origins
Budiansky had created for the Dinobots, the Decepticon turned Autobot Jetfire
(known as Skyfire on TV), the Constructicons (who combine to form
Devastator),[19][20] and Omega Supreme. The Marvel comic establishes early on
that Prime wields the Creation Matrix, which gives life to machines. In the
second season, the two-part episode The Key to Vector Sigma introduced the
ancient Vector Sigma computer, which served the same original purpose as the
Creation Matrix (giving life to Transformers), and its guardian Alpha Trion.
"""

inputs = tokenizer(text, return_tensors="pt")
logits = model(**inputs).logits
```

```python
IndexError: index out of range in self
```

ç³Ÿç³•ï¼Œæˆ‘ä»¬é‡åˆ°äº†ä¸€ä¸ªé—®é¢˜â€”â€”è€Œä¸”é”™è¯¯æ¶ˆæ¯æ¯”æˆ‘ä»¬åœ¨ç¬¬2èŠ‚ä¸­çœ‹åˆ°çš„è¦éš¾æ‡‚å¾—å¤šï¼æˆ‘ä»¬æ— æ³•ç†è§£å®Œæ•´çš„ traceback ä¿¡æ¯ï¼Œå› æ­¤æˆ‘ä»¬å†³å®šå‘ Hugging Face è®ºå›å¯»æ±‚å¸®åŠ©ã€‚æˆ‘ä»¬è¯¥å¦‚ä½•æ’°å†™ä¸»é¢˜å‘¢ï¼Ÿ

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦ç‚¹å‡»å³ä¸Šè§’çš„â€œNew Topicâ€æŒ‰é’®ï¼ˆè¯·æ³¨æ„ï¼Œè¦åˆ›å»ºä¸»é¢˜ï¼Œæˆ‘ä»¬éœ€è¦å…ˆç™»å½•ï¼‰ï¼š

![åˆ›å»ºä¸€ä¸ªæ–°çš„ topic](./assets/forums-new-topic.png "Creating a new forum topic.")

è¿™é‡Œä¼šå‡ºç°ä¸€ä¸ªå†™ä½œç•Œé¢ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨å…¶ä¸­è¾“å…¥æˆ‘ä»¬çš„ä¸»é¢˜æ ‡é¢˜ï¼Œé€‰æ‹©ä¸€ä¸ªç±»åˆ«ï¼Œå¹¶èµ·è‰å†…å®¹ï¼š

!åˆ›å»ºè®ºå› topic çš„ç•Œé¢](./assets/forum-topic01.png "The interface for creating a forum topic.")

ç”±äºé”™è¯¯ä¼¼ä¹ä»…ä¸ Transformers æœ‰å…³ï¼Œå› æ­¤æˆ‘ä»¬å°†ä¸ºé”™è¯¯é€‰æ‹©è¯¥ç±»åˆ«ã€‚ç¬¬ä¸€æ¬¡å°è¯•è§£é‡Šè¿™ä¸ªé—®é¢˜å¯èƒ½çœ‹èµ·æ¥åƒè¿™æ ·ï¼š

![èµ·è‰æ–°è®ºå› topic çš„å†…å®¹](./assets/forum-topic02.png "Drafting the content for a new forum topic.")

å°½è¿™ä¸ªä¸»é¢˜åŒ…å«è¿™ä¸ªé”™è¯¯æ¶ˆæ¯ï¼Œä½†æ’°å†™æ–¹å¼å­˜åœ¨ä¸€äº›é—®é¢˜ï¼š

1. æ ‡é¢˜æè¿°æ€§ä¸æ˜¯å¾ˆå¼ºï¼Œè¿™ä¼šå¯¼è‡´æµè§ˆè®ºå›çš„äººåœ¨æ²¡æœ‰é˜…è¯»æ­£æ–‡çš„æƒ…å†µä¸‹æ— æ³•çŸ¥é“ä¸»é¢˜æ˜¯å…³äºä»€ä¹ˆçš„ã€‚

2. æ­£æ–‡æ²¡æœ‰æä¾›è¶³å¤Ÿçš„ä¿¡æ¯è¯´æ˜é”™è¯¯çš„æ¥æºä»¥åŠå¦‚ä½•é‡ç°é”™è¯¯ã€‚

3. ä¸»é¢˜ä»¥æŸç§è¦æ±‚çš„è¯­æ°”ç›´æ¥ ï¼  äº†å‡ ä¸ªäººã€‚

åƒè¿™æ ·çš„ä¸»é¢˜ä¸å¤ªå¯èƒ½å¾ˆå¿«å¾—åˆ°ç­”æ¡ˆï¼ˆå¦‚æœæœ‰ç­”å¤çš„è¯ï¼‰ï¼Œéœ€è¦å¯¹å…¶è¿›è¡Œæ”¹è¿›ã€‚æˆ‘ä»¬å°†ä»è§£å†³ç¬¬ä¸€ä¸ªé—®é¢˜ï¼Œé€‰æ‹©ä¸€ä¸ªå¥½çš„æ ‡é¢˜å¼€å§‹ã€‚

#### é€‰æ‹©æè¿°æ€§æ ‡é¢˜ 

å¦‚æœä½ æƒ³å°±ä»£ç ä¸­çš„é”™è¯¯å¯»æ±‚å¸®åŠ©ï¼Œä¸€ä¸ªå¥½çš„ç»éªŒæ³•åˆ™æ˜¯åœ¨æ ‡é¢˜ä¸­åŒ…å«è¶³å¤Ÿçš„ä¿¡æ¯ï¼Œä»¥ä¾¿å…¶ä»–äººå¯ä»¥å¿«é€Ÿç¡®å®šä»–ä»¬æ˜¯å¦å¯ä»¥å›ç­”ä½ çš„é—®é¢˜ã€‚åœ¨æˆ‘ä»¬çš„è¿è¡Œç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬çŸ¥é“æ­£åœ¨å¼•å‘çš„å¼‚å¸¸åç§°ï¼Œå¹¶æœ‰ä¸€äº›æç¤ºè¡¨ç¤ºå®ƒæ˜¯åœ¨æ¨¡å‹çš„å‰å‘ä¼ é€’ä¸­è§¦å‘çš„ï¼Œå³æˆ‘ä»¬è°ƒç”¨ `model(**inputs)` çš„ä½ç½®ã€‚ä¸ºäº†ä¼ è¾¾è¿™ä¸€ä¿¡æ¯ï¼Œä¸€ä¸ªå¯èƒ½çš„æ ‡é¢˜å¯èƒ½æ˜¯ï¼š

> AutoModel å‰å‘ä¼ é€’ä¸­çš„ IndexError æ¥æºï¼Ÿ

è¿™ä¸ªæ ‡é¢˜å‘Šè¯‰è¯»è€…åœ¨å“ªé‡Œä½ è®¤ä¸ºé”™è¯¯æ¥è‡ªï¼Œå¦‚æœä»–ä»¬é‡åˆ°äº† `IndexError` ä»–ä»¬å¾ˆå¯èƒ½çŸ¥é“å¦‚ä½•å›ç­”ä½ ã€‚å½“ç„¶ï¼Œæ ‡é¢˜å¯ä»¥æ˜¯ä»»ä½•ä½ æƒ³è¦çš„ï¼Œå…¶ä»–å˜ä½“å¦‚ï¼š

> ä¸ºä»€ä¹ˆæˆ‘çš„æ¨¡å‹ä¼šäº§ç”Ÿ IndexErrorï¼Ÿ

è¿™ç§æ ‡é¢˜ä¹Ÿæ˜¯å¯ä»¥çš„ã€‚ç°åœ¨æˆ‘ä»¬æœ‰äº†ä¸€ä¸ªæè¿°æ€§çš„æ ‡é¢˜ï¼Œè®©æˆ‘ä»¬æ¥çœ‹çœ‹æ”¹å–„æ­£æ–‡ã€‚

#### è®¾ç½®ä»£ç æ®µçš„æ ¼å¼ 

ç°åœ¨æˆ‘ä»¬æœ‰äº†ä¸€ä¸ªæè¿°æ€§çš„æ ‡é¢˜ï¼Œè®©æˆ‘ä»¬æ¥çœ‹çœ‹å¦‚ä½•æ”¹å–„ä»£ç æ®µçš„æ ¼å¼ã€‚åœ¨ IDE ä¸­é˜…è¯»æºä»£ç å·²ç»å¤Ÿéš¾çš„äº†ï¼Œä½†æ˜¯å½“å°†ä»£ç å¤åˆ¶ç²˜è´´ä¸ºçº¯æ–‡æœ¬æ—¶å°±æ›´éš¾äº†ï¼ä¸è¿‡ Hugging Face è®ºå›æ”¯æŒä½¿ç”¨ Markdownï¼Œæ ‡å‡†æ ¼å¼æ˜¯ç”¨ä¸‰ä¸ªåå¼•å· ï¼ˆ```ï¼‰ å°†ä»£ç å—æ‹¬èµ·æ¥ã€‚è¿™å¯ä»¥è®©æ­£æ–‡æ¯”æˆ‘ä»¬çš„åŸå§‹ç‰ˆæœ¬çœ‹èµ·æ¥æ›´åŠ ç¾è§‚ï¼š

![æˆ‘ä»¬ä¿®æ”¹äº†è®ºå› topicï¼Œå¹¶å…·æœ‰æ­£ç¡®çš„ä»£ç æ ¼å¼](./assets/forum-topic03.png "Our revised forum topic, with proper code formatting.")

æ­£å¦‚ä½ åœ¨å±å¹•æˆªå›¾ä¸­çœ‹åˆ°çš„ï¼Œå°†ä»£ç å—æ‹¬åœ¨åå¼•å·ä¸­ä¼šå°†åŸå§‹æ–‡æœ¬è½¬æ¢ä¸ºå¸¦æœ‰é¢œè‰²æ ·å¼çš„æ ¼å¼åŒ–ä»£ç ï¼å¦å¤–ï¼Œå•ä¸ªåå¼•å·å¯ç”¨äºæ ¼å¼åŒ–å†…è”å˜é‡ï¼Œæ¯”å¦‚ `distilbert-base-uncased` è¿™æ ·ã€‚è¿™ä¸ªä¸»é¢˜çœ‹èµ·æ¥å¥½å¤šäº†ï¼Œæœ‰äº†ä¸€ç‚¹è¿æ°”ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šåœ¨ç¤¾åŒºä¸­æ‰¾åˆ°ä¸€äº›äººçŒœæµ‹é”™è¯¯çš„åŸå› ã€‚ç„¶è€Œï¼Œä¸å…¶ä¾é è¿æ°”ï¼Œä¸å¦‚é€šè¿‡åŒ…æ‹¬å®Œæ•´è€Œè¯¦ç»†çš„å›æº¯ä¿¡æ¯ä½¿äº‹æƒ…æ›´å®¹æ˜“ï¼

#### åŒ…æ‹¬å®Œæ•´çš„å›æº¯ä¿¡æ¯ 

ç”±äºå›æº¯çš„æœ€åä¸€è¡Œé€šå¸¸è¶³ä»¥è°ƒè¯•ä½ è‡ªå·±çš„ä»£ç ï¼Œä½†æ˜¯åªæä¾›è¿™ä¸€è¡Œä»¥â€œèŠ‚çœç©ºé—´â€å¯èƒ½ä¼šä½¿ä»–äººè°ƒè¯•é—®é¢˜å˜å¾—æ›´åŠ å›°éš¾ï¼Œå› ä¸º traceback ä¸­æ›´ä¸Šé¢çš„ä¿¡æ¯ä¹Ÿå¯èƒ½éå¸¸æœ‰ç”¨ã€‚å› æ­¤ï¼Œä¸€ä¸ªå¥½çš„åšæ³•æ˜¯å¤åˆ¶å¹¶ç²˜è´´æ•´ä¸ªçš„ tracebackï¼ŒåŒæ—¶ç¡®ä¿å®ƒçš„æ ¼å¼ä¸è¢«ç ´åã€‚ä½†æ˜¯è¿™äº› traceback å¯èƒ½ä¼šå¾ˆé•¿ï¼Œæ‰€ä»¥å¯ä»¥åœ¨å¯¹æºä»£ç è¿›è¡Œè§£é‡Šä¹‹åå†å±•ç¤ºå®ƒä»¬ã€‚å°±è¿™ä¸ªæ€è·¯ç°åœ¨æ¥å¯¹æˆ‘ä»¬çš„é—®é¢˜å¸–å­è¿›è¡Œä¿®æ”¹ï¼Œæˆ‘ä»¬çš„å¸–å­å¦‚ä¸‹æ‰€ç¤ºï¼š

![æˆ‘ä»¬çš„ç¤ºä¾‹è®ºå› topicï¼Œå…·æœ‰å®Œæ•´çš„traceback](./assets/forum-topic04.png "Our example forum topic, with the complete traceback.")

è¿™æä¾›äº†æ›´å¤šä¿¡æ¯ï¼Œç»†å¿ƒçš„è¯»è€…å¯èƒ½ä¼šæŒ‡å‡ºé—®é¢˜ä¼¼ä¹æ˜¯ç”±äº traceback ä¸­çš„è¿™è¡Œä»£ç å¯¼è‡´è¾“å…¥è¿‡é•¿ï¼š

> ä»¤ç‰Œç´¢å¼•åºåˆ—é•¿åº¦é•¿äºä¸ºæ­¤æ¨¡å‹æŒ‡å®šçš„æœ€å¤§åºåˆ—é•¿åº¦ ï¼ˆ583 > 512ï¼‰ã€‚

ç„¶è€Œï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æä¾›è§¦å‘é”™è¯¯çš„å®é™…ä»£ç è¿›ä¸€æ­¥ç®€åŒ–äº‹æƒ…ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬å°±æ¥åšè¿™ä»¶äº‹ã€‚

#### æä¾›å¯é‡å¤çš„ç¤ºä¾‹ 

å¦‚æœä½ æ›¾ç»å°è¯•è¿‡è°ƒè¯•å…¶ä»–äººçš„ä»£ç ï¼Œé‚£ä¹ˆä½ å¯èƒ½é¦–å…ˆå°è¯•é‡ç°ä»–ä»¬æŠ¥å‘Šçš„é—®é¢˜ï¼Œä»¥ä¾¿ä½ å¯ä»¥å¼€å§‹é€æ­¥æŸ¥æ‰¾é”™è¯¯ã€‚åœ¨è®ºå›ä¸Šè·å–ï¼ˆæˆ–æä¾›ï¼‰å¸®åŠ©ä¹Ÿä¸ä¾‹å¤–ï¼Œå¦‚æœä½ èƒ½æä¾›ä¸€ä¸ªé‡ç°é”™è¯¯çš„å°ä¾‹å­çœŸçš„å¾ˆæœ‰å¸®åŠ©ã€‚è¿™é‡Œæœ‰ä¸ªç¤ºä¾‹å¸–å­ï¼š

![æˆ‘ä»¬è®ºå› topic çš„æœ€ç»ˆç‰ˆæœ¬](./assets/forum-topic05.png "The final version of our forum topic.")

è¯¥å¸–å­ç›®å‰åŒ…å«ç›¸å½“å¤šçš„ä¿¡æ¯ï¼Œå¹¶ä¸”å®ƒçš„æ’°å†™æ ¼å¼æ›´å¯èƒ½å¸å¼•ç¤¾åŒºçš„æ³¨æ„ï¼Œè·å¾—æœ‰ç”¨çš„ç­”æ¡ˆã€‚æœ‰äº†è¿™äº›åŸºæœ¬æŒ‡å—ï¼Œä½ ç°åœ¨å¯ä»¥åˆ›å»ºå¾ˆæ£’çš„å¸–å­æ¥æ‰¾åˆ°é‡åˆ°çš„ Transformers é—®é¢˜çš„ç­”æ¡ˆï¼



## 9.3 è°ƒè¯• Trainer è®­ç»ƒç®¡é“

ä½ å·²ç»å°½å¯èƒ½åœ°éµå¾ªç¬¬å…«ç« ä¸­çš„å»ºè®®ï¼Œç¼–å†™äº†ä¸€æ®µæ¼‚äº®çš„ä»£ç æ¥è®­ç»ƒæˆ–å¾®è°ƒç»™å®šä»»åŠ¡çš„æ¨¡å‹ã€‚ä½†æ˜¯å½“ä½ å¯åŠ¨å‘½ä»¤ `trainer.train()` æ—¶ï¼Œå¯æ€•çš„äº‹æƒ…å‘ç”Ÿäº†ï¼šä½ å¾—åˆ°ä¸€ä¸ªé”™è¯¯ğŸ˜±ï¼æˆ–è€…æ›´ç³Ÿç³•çš„æ˜¯ï¼Œè™½ç„¶ä¸€åˆ‡ä¼¼ä¹éƒ½å¾ˆå¥½ï¼Œè®­ç»ƒè¿è¡Œæ²¡æœ‰é”™è¯¯ï¼Œä½†ç”Ÿæˆçš„æ¨¡å‹å¾ˆç³Ÿç³•ã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†å‘ä½ å±•ç¤ºå¦‚ä½•è°ƒè¯•æ­¤ç±»é—®é¢˜ã€‚

### è°ƒè¯•è®­ç»ƒç®¡é“ 

å½“ä½ åœ¨ `trainer.train()` ä¸­é‡åˆ°é”™è¯¯æ—¶ï¼Œå®ƒå¯èƒ½æ¥è‡ªå¤šä¸ªæ¥æºï¼Œå› ä¸º `Trainer` ä¼šå°†å¾ˆå¤šæ¨¡å—æ”¾åœ¨ä¸€èµ·ç»„åˆè¿è¡Œã€‚å®ƒä¼šå°† datasets è½¬æ¢ä¸º dataloaders å› æ­¤é—®é¢˜å¯èƒ½å‡ºåœ¨ datasets ä¸­ï¼Œæˆ–è€…åœ¨å°è¯•å°† datasets çš„å…ƒç´ ä¸€èµ·æ‰¹å¤„ç†æ—¶å‡ºç°é—®é¢˜ã€‚æ¥ç€å®ƒä¼šå‡†å¤‡ä¸€æ‰¹æ•°æ®å¹¶å°†å…¶æä¾›ç»™æ¨¡å‹ï¼Œå› æ­¤é—®é¢˜å¯èƒ½å‡ºåœ¨æ¨¡å‹ä»£ç ä¸­ã€‚ä¹‹åï¼Œå®ƒä¼šè®¡ç®—æ¢¯åº¦å¹¶æ‰§è¡Œä¼˜åŒ–å™¨ï¼Œå› æ­¤é—®é¢˜ä¹Ÿå¯èƒ½å‡ºåœ¨ä½ çš„ä¼˜åŒ–å™¨ä¸­ã€‚å³ä½¿è®­ç»ƒä¸€åˆ‡é¡ºåˆ©ï¼Œå¦‚æœä½ çš„è¯„ä¼°æŒ‡æ ‡æœ‰é—®é¢˜ï¼Œè¯„ä¼°æœŸé—´ä»ç„¶å¯èƒ½å‡ºç°é—®é¢˜ã€‚

è°ƒè¯• `trainer.train()` ä¸­å‡ºç°çš„é”™è¯¯çš„æœ€ä½³æ–¹æ³•æ˜¯æ‰‹åŠ¨æ£€æŸ¥æ•´ä¸ªç®¡é“ï¼Œçœ‹çœ‹å“ªé‡Œå‡ºäº†é—®é¢˜ã€‚é€šå¸¸æƒ…å†µä¸‹ï¼Œé”™è¯¯å¾ˆå®¹æ˜“è§£å†³ã€‚

ä¸ºäº†è§£é‡Šè¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å°†å°è¯•ä½¿ç”¨ä»¥ä¸‹è„šæœ¬åœ¨ [MNLI æ•°æ®é›†](https://huggingface.co/datasets/glue)(https://huggingface.co/datasets/glue) ä¸Šå¾®è°ƒ DistilBERT æ¨¡å‹ï¼š

```python
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
)

raw_datasets = evaluate.load("glue", "mnli")

model_checkpoint = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

def preprocess_function(examples):
    return tokenizer(examples["premise"], examples["hypothesis"], truncation=True)

tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)

args = TrainingArguments(
    f"distilbert-finetuned-mnli",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
)

metric = evaluate.load("glue", "mnli")

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    return metric.compute(predictions=predictions, references=labels)

trainer = Trainer(
    model,
    args,
    train_dataset=raw_datasets["train"],
    eval_dataset=raw_datasets["validation_matched"],
    compute_metrics=compute_metrics,
)
trainer.train()
```

å¦‚æœä½ å°è¯•æ‰§è¡Œå®ƒï¼Œä½ ä¼šé‡åˆ°ä¸€ä¸ªç›¸å½“æ™¦æ¶©çš„é”™è¯¯ï¼š

```python
'ValueError: You have to specify either input_ids or inputs_embeds'
```

#### æ£€æŸ¥æ•°æ® 

è¿™æ˜¯æ˜¾è€Œæ˜“è§çš„ï¼Œå¦‚æœä½ çš„æ•°æ®æŸåäº†ï¼Œ `Trainer` å°†æ— æ³•å°†æ•°æ®æ•´ç†æˆ batchï¼Œæ›´ä¸ç”¨è¯´è®­ç»ƒä½ çš„æ¨¡å‹äº†ã€‚å› æ­¤ï¼Œä½ éœ€è¦å…ˆæ£€æŸ¥ä¸€ä¸‹ä½ çš„è®­ç»ƒé›†çš„å†…å®¹ã€‚

ä¸ºäº†é¿å…èŠ±è´¹æ— æ•°å°æ—¶è¯•å›¾ä¿®å¤ä¸æ˜¯é”™è¯¯æ¥æºçš„é—®é¢˜ï¼Œæˆ‘ä»¬å»ºè®®ä½ åªä½¿ç”¨ `trainer.train_dataset` è¿›è¡Œæ£€æŸ¥ã€‚æ‰€ä»¥è®©æˆ‘ä»¬åœ¨è¿™é‡Œè¿™æ ·å°è¯•ä¸€ä¸‹ï¼š

```python
trainer.train_dataset[0]
```

```python
{'hypothesis': 'Product and geography are what make cream skimming work. ',
 'idx': 0,
 'label': 1,
 'premise': 'Conceptually cream skimming has two basic dimensions - product and geography.'}
```

ä½ æ³¨æ„åˆ°æœ‰ä»€ä¹ˆä¸å¯¹å—ï¼Ÿä¸ç¼ºå°‘ `input_ids` çš„é”™è¯¯æ¶ˆæ¯ç›¸ç»“åˆï¼Œåº”è¯¥è®©ä½ æ„è¯†åˆ°æ•°æ®é›†é‡Œæ˜¯æ–‡æœ¬ï¼Œè€Œä¸æ˜¯æ¨¡å‹å¯ä»¥ç†è§£çš„æ•°å­—ã€‚åœ¨è¿™ä¸ªä¾‹å­ï¼Œè¾“å‡ºçš„åŸå§‹é”™è¯¯ä¿¡æ¯éå¸¸å…·æœ‰è¯¯å¯¼æ€§ï¼Œå› ä¸º `Trainer` ä¼šè‡ªåŠ¨åˆ é™¤ä¸æ¨¡å‹ç­¾åä¸åŒ¹é…çš„åˆ— ï¼ˆå³æ¨¡å‹é¢„æœŸçš„è¾“å…¥å‚æ•°ï¼‰ã€‚è¿™æ„å‘³ç€åœ¨è¿™é‡Œï¼Œé™¤äº†æ ‡ç­¾ä¹‹å¤–çš„æ‰€æœ‰ä¸œè¥¿éƒ½è¢«ä¸¢å¼ƒäº†ã€‚å› æ­¤ï¼Œåˆ›å»º batch ç„¶åå°†å®ƒä»¬å‘é€åˆ°æ¨¡å‹æ—¶æ²¡æœ‰é—®é¢˜ï¼Œä½†æ˜¯æ¨¡å‹ä¼šæŠ±æ€¨å®ƒæ²¡æœ‰æ”¶åˆ°æ­£ç¡®çš„è¾“å…¥ã€‚

ä¸ºä»€ä¹ˆæ•°æ®æ²¡æœ‰è¢«å¤„ç†ï¼Ÿæˆ‘ä»¬ç¡®å®åœ¨æ•°æ®é›†ä¸Šä½¿ç”¨äº† `Dataset.map()` æ–¹æ³•æ¥ä½¿ç”¨ tokenizer å¤„ç†æ¯ä¸ªæ ·æœ¬ã€‚ä½†æ˜¯å¦‚æœä½ ä»”ç»†çœ‹ä»£ç ï¼Œä½ ä¼šå‘ç°æˆ‘ä»¬åœ¨å°†è®­ç»ƒå’Œè¯„ä¼°é›†ä¼ é€’ç»™ `Trainer` æ—¶çŠ¯äº†ä¸€ä¸ªé”™è¯¯ã€‚æˆ‘ä»¬åœ¨è¿™é‡Œæ²¡æœ‰ä½¿ç”¨ `tokenized_datasets` ï¼Œè€Œæ˜¯ä½¿ç”¨äº† `raw_datasets` ğŸ¤¦ã€‚æ‰€ä»¥è®©æˆ‘ä»¬è§£å†³è¿™ä¸ªé—®é¢˜ï¼

```python
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
)

raw_datasets = evaluate.load("glue", "mnli")

model_checkpoint = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

def preprocess_function(examples):
    return tokenizer(examples["premise"], examples["hypothesis"], truncation=True)

tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)

args = TrainingArguments(
    f"distilbert-finetuned-mnli",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
)

metric = evaluate.load("glue", "mnli")

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    return metric.compute(predictions=predictions, references=labels)

trainer = Trainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation_matched"],
    compute_metrics=compute_metrics,
)
trainer.train()
```

è¿™ä¸ªæ–°ä»£ç ç°åœ¨ä¼šç»™å‡ºä¸€ä¸ªæ–°çš„é”™è¯¯ğŸ˜¥ï¼š

```python
'ValueError: expected sequence of length 43 at dim 1 (got 37)'
```

æŸ¥çœ‹ tracebackï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°é”™è¯¯å‘ç”Ÿåœ¨æ•°æ®æ•´ç†æ­¥éª¤ä¸­ï¼š

```python
~/git/transformers/src/transformers/data/data_collator.py in torch_default_data_collator(features)
    105                 batch[k] = torch.stack([f[k] for f in features])
    106             else:
--> 107                 batch[k] = torch.tensor([f[k] for f in features])
    108 
    109     return batch
```

æ‰€ä»¥ï¼Œæˆ‘ä»¬åº”è¯¥å»ç ”ç©¶ä¸€ä¸‹é‚£ä¸ªã€‚ç„¶è€Œï¼Œåœ¨æˆ‘ä»¬è¿™æ ·åšä¹‹å‰ï¼Œè®©æˆ‘ä»¬å®Œæˆæ£€æŸ¥æˆ‘ä»¬çš„æ•°æ®ï¼Œå…ˆç¡®å®šå®ƒ 100ï¼…æ˜¯æ­£ç¡®çš„ã€‚

åœ¨è°ƒè¯•è®­ç»ƒè¿‡ç¨‹æ—¶ï¼Œä½ åº”è¯¥å§‹ç»ˆåšçš„ä¸€ä»¶äº‹æ˜¯æŸ¥çœ‹æ¨¡å‹çš„è§£ç è¾“å…¥ã€‚æˆ‘ä»¬æ— æ³•ç†è§£ç›´æ¥æä¾›ç»™å®ƒçš„æ•°å­—ï¼Œæ‰€ä»¥æˆ‘ä»¬åº”è¯¥çœ‹çœ‹è¿™äº›æ•°å­—ä»£è¡¨ä»€ä¹ˆã€‚ä¾‹å¦‚ï¼Œåœ¨è®¡ç®—æœºè§†è§‰ä¸­ï¼Œè¿™æ„å‘³ç€æŸ¥çœ‹ä½ ä¼ é€’è§£ç åçš„åƒç´ å›¾ç‰‡ï¼Œåœ¨è¯­éŸ³ä¸­æ„å‘³ç€è§£ç åçš„éŸ³é¢‘æ ·æœ¬ï¼Œå¯¹äºæˆ‘ä»¬çš„ NLP ç¤ºä¾‹ï¼Œè¿™æ„å‘³ç€ä½¿ç”¨æˆ‘ä»¬çš„ tokenizer è§£ç åçš„è¾“å…¥ï¼š

```python
tokenizer.decode(trainer.train_dataset[0]["input_ids"])
```

```python
'[CLS] conceptually cream skimming has two basic dimensions - product and geography. [SEP] product and geography are what make cream skimming work. [SEP]'
```

æ‰€ä»¥è¿™ä¼¼ä¹æ˜¯æ­£ç¡®çš„ã€‚ä½ åº”è¯¥å¯¹è¾“å…¥ä¸­çš„æ‰€æœ‰é”®éƒ½è¿™æ ·åšï¼š

```python
trainer.train_dataset[0].keys()
```

```python
dict_keys(['attention_mask', 'hypothesis', 'idx', 'input_ids', 'label', 'premise'])
```

è¯·æ³¨æ„ï¼Œä¸æ¨¡å‹æ¥å—çš„è¾“å…¥ä¸å¯¹åº”çš„é”®å°†è¢«è‡ªåŠ¨ä¸¢å¼ƒï¼Œå› æ­¤è¿™é‡Œæˆ‘ä»¬å°†ä»…ä¿ç•™ `input_ids` ã€ `attention_mask` å’Œ `label` ï¼ˆå®ƒå°†è¢«é‡å‘½åä¸º `labels` ï¼‰ã€‚ä¸ºäº†åŒé‡ä¿é™©ï¼Œä½ å¯ä»¥æ‰“å°æ¨¡å‹çš„ç±»ï¼Œç„¶åæŸ¥çœ‹å…¶æ–‡æ¡£ï¼š

```python
type(trainer.model)
```

```python
transformers.models.distilbert.modeling_distilbert.DistilBertForSequenceClassification
```

æ‰€ä»¥åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ [è¿™ä¸ªé¡µé¢](https://huggingface.co/transformers/model_doc/distilbert.html#distilbertforsequenceclassification)(https://huggingface.co/transformers/model_doc/distilbert.html#distilbertforsequenceclassification) æŸ¥çœ‹æ¨¡å‹æ¥å—çš„å‚æ•°ã€‚ `Trainer` ä¹Ÿä¼šè®°å½•å®ƒä¸¢å¼ƒçš„åˆ—ã€‚

æˆ‘ä»¬é€šè¿‡è§£ç æ£€æŸ¥äº† inputs ID æ˜¯å¦æ­£ç¡®ã€‚æ¥ä¸‹æ¥æ˜¯æ£€æŸ¥ `attention_mask` ï¼š

```python
trainer.train_dataset[0]["attention_mask"]
```

```python
[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
```

ç”±äºæˆ‘ä»¬æ²¡æœ‰åœ¨é¢„å¤„ç†ä¸­ä½¿ç”¨å¡«å……ï¼Œè¿™çœ‹èµ·æ¥æ²¡ä»€ä¹ˆé—®é¢˜ã€‚ä¸ºç¡®ä¿è¯¥æ³¨æ„æ©ç æ²¡æœ‰é—®é¢˜ï¼Œè®©æˆ‘ä»¬æ£€æŸ¥å®ƒä¸ inputs ID çš„é•¿åº¦æ˜¯å¦ç›¸åŒï¼š

```python
len(trainer.train_dataset[0]["attention_mask"]) == len(
    trainer.train_dataset[0]["input_ids"]
)
```

```python
True
```

é‚£æŒºå¥½çš„ï¼æœ€åï¼Œè®©æˆ‘ä»¬æ£€æŸ¥ä¸€ä¸‹æˆ‘ä»¬çš„æ ‡ç­¾ï¼š

```python
trainer.train_dataset[0]["label"]
```

```python
1
```

ä¸inputs ID ä¸€æ ·ï¼Œè¿™æ˜¯ä¸€ä¸ªæœ¬èº«å¹¶æ²¡æœ‰çœŸæ­£æ„ä¹‰çš„æ•°å­—ã€‚æ­£å¦‚æˆ‘ä»¬ä¹‹å‰å­¦åˆ°çš„ï¼Œæ ‡ç­¾ ID å’Œæ ‡ç­¾åä¹‹é—´çš„æ˜ å°„å­˜å‚¨åœ¨æ•°æ®é›† `features` é‡Œçš„ `names` å±æ€§ä¸­ï¼š

```python
trainer.train_dataset.features["label"].names
```

```python
['entailment', 'neutral', 'contradiction']
```

æ‰€ä»¥ `1` è¡¨ç¤º `neutral` ï¼Œè¡¨ç¤ºæˆ‘ä»¬ä¸Šé¢çœ‹åˆ°çš„ä¸¤å¥è¯å¹¶ä¸çŸ›ç›¾ï¼Œä¹Ÿæ²¡æœ‰åŒ…å«å…³ç³»ã€‚è¿™ä¼¼ä¹æ˜¯æ­£ç¡®çš„ï¼

æˆ‘ä»¬è¿™é‡Œæ²¡æœ‰ token ç±»å‹ IDï¼Œå› ä¸º DistilBERT ä¸éœ€è¦å®ƒä»¬ï¼›å¦‚æœä½ çš„æ¨¡å‹ä¸­æœ‰ä¸€äº›ï¼Œä½ è¿˜åº”è¯¥ç¡®ä¿å®ƒä»¬æ­£ç¡®åŒ¹é…è¾“å…¥ä¸­ç¬¬ä¸€å¥å’Œç¬¬äºŒå¥çš„ä½ç½®ã€‚

<div custom-style="Tip-green">

âœï¸ **è½®åˆ°ä½ äº†ï¼** æ£€æŸ¥è®­ç»ƒæ•°æ®é›†çš„ç¬¬äºŒä¸ªæ¡æ•°æ®æ˜¯å¦æ­£ç¡®ã€‚

</div>

æˆ‘ä»¬åœ¨è¿™é‡Œåªå¯¹è®­ç»ƒé›†è¿›è¡Œæ£€æŸ¥ï¼Œä½†ä½ å½“ç„¶åº”è¯¥ä»¥åŒæ ·çš„æ–¹å¼ä»”ç»†æ£€æŸ¥éªŒè¯é›†å’Œæµ‹è¯•é›†ã€‚

ç°åœ¨æˆ‘ä»¬çŸ¥é“æˆ‘ä»¬çš„æ•°æ®é›†çœ‹èµ·æ¥ä¸é”™ï¼Œæ˜¯æ—¶å€™æ£€æŸ¥è®­ç»ƒç®¡é“çš„ä¸‹ä¸€æ­¥äº†ã€‚

#### ä» datasets åˆ° dataloaders 

è®­ç»ƒç®¡é“ä¸­å¯èƒ½å‡ºé”™çš„ä¸‹ä¸€ä»¶äº‹æ˜¯å½“ `Trainer` å°è¯•ä»è®­ç»ƒæˆ–éªŒè¯é›†å½¢æˆ batch æ—¶ã€‚å½“ä½ ç¡®å®š `Trainer` çš„æ•°æ®é›†æ˜¯æ­£ç¡®çš„åï¼Œä½ å¯ä»¥å°è¯•é€šè¿‡æ‰§è¡Œä»¥ä¸‹æ“ä½œæ‰‹åŠ¨å½¢æˆä¸€ä¸ª batchï¼ˆå½“è¦æµ‹è¯•éªŒè¯é›†çš„ dataloaders æ—¶ï¼Œå¯ä»¥å°† `train` æ›¿æ¢ä¸º `eval` ï¼‰ï¼š

```python
for batch in trainer.get_train_dataloader():
    break
```

æ­¤ä»£ç å°†ä¼šåˆ›å»ºè®­ç»ƒæ•°æ®åŠ è½½å™¨ï¼Œç„¶åå¯¹å…¶è¿›è¡Œè¿­ä»£ä¸€æ¬¡ã€‚å¦‚æœä»£ç æ‰§è¡Œæ²¡æœ‰é”™è¯¯ï¼Œé‚£ä¹ˆä½ å°±æœ‰äº†å¯ä»¥æ£€æŸ¥çš„ç¬¬ä¸€ä¸ª batchï¼Œå¦‚æœä»£ç å‡ºé”™ï¼Œä½ å¯ä»¥ç¡®å®šé—®é¢˜å‡ºåœ¨æ•°æ®åŠ è½½å™¨ä¸­ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```python
~/git/transformers/src/transformers/data/data_collator.py in torch_default_data_collator(features)
    105                 batch[k] = torch.stack([f[k] for f in features])
    106             else:
--> 107                 batch[k] = torch.tensor([f[k] for f in features])
    108 
    109     return batch

ValueError: expected sequence of length 45 at dim 1 (got 76)
```

Trackback çš„æœ€åä¸€ä¸ªå †æ ˆçš„è¾“å‡ºåº”è¯¥è¶³å¤Ÿç»™ä½ ä¸€äº›çº¿ç´¢ï¼Œä½†è®©æˆ‘ä»¬å†æ·±å…¥æŒ–æ˜ä¸€ä¸‹ã€‚æ‰¹å¤„ç†åˆ›å»ºè¿‡ç¨‹ä¸­çš„å¤§å¤šæ•°é—®é¢˜æ˜¯åœ¨å°†ç¤ºä¾‹æ•´ç†åˆ°å•ä¸ª batch ä¸­è€Œå‡ºç°çš„ï¼Œå› æ­¤åœ¨æœ‰ç–‘é—®æ—¶é¦–å…ˆè¦æ£€æŸ¥çš„æ˜¯ä½ çš„ DataLoader æ­£åœ¨ä½¿ç”¨çš„ `collate_fn` ï¼š

```python
data_collator = trainer.get_train_dataloader().collate_fn
data_collator
```

```python
<function transformers.data.data_collator.default_data_collator(features: List[InputDataClass], return_tensors='pt') -> Dict[str, Any]>
```

æ‰€ä»¥ï¼Œç›®å‰ä½¿ç”¨çš„æ˜¯ `default_data_collator` ï¼Œä½†è¿™ä¸æ˜¯æˆ‘ä»¬åœ¨è¿™ä¸ªä¾‹å­ä¸­æƒ³è¦çš„ã€‚æˆ‘ä»¬å¸Œæœ›å°†ç¤ºä¾‹å¡«å……åˆ°æ‰¹å¤„ç†ä¸­æœ€é•¿çš„å¥å­ï¼Œè¿™é¡¹åŠŸèƒ½æ˜¯ç”± `DataCollatorWithPadding` æ•´ç†å™¨å®ç°çš„ã€‚è€Œè¿™ä¸ªæ•°æ®æ•´ç†å™¨åº”è¯¥æ˜¯é»˜è®¤è¢« `Trainer` ä½¿ç”¨çš„ï¼Œä¸ºä»€ä¹ˆè¿™é‡Œæ²¡æœ‰ä½¿ç”¨å‘¢ï¼Ÿ

ç­”æ¡ˆæ˜¯å› ä¸ºæˆ‘ä»¬æ²¡æœ‰å°† `tokenizer` ä¼ é€’ç»™ `Trainer` ï¼Œæ‰€ä»¥å®ƒæ— æ³•åˆ›å»ºæˆ‘ä»¬æƒ³è¦çš„ `DataCollatorWithPadding` ã€‚åœ¨å®è·µä¸­ï¼Œä½ åº”è¯¥æ˜ç¡®åœ°ä¼ é€’ä½ æƒ³è¦ä½¿ç”¨çš„æ•°æ®æ•´ç†å™¨ï¼Œä»¥ç¡®ä¿é¿å…è¿™äº›ç±»å‹çš„é”™è¯¯ã€‚è®©æˆ‘ä»¬ä¿®æ”¹ä»£ç ä»¥å®ç°è¿™ä¸€ç‚¹ï¼š

```python
from datasets import load_dataset
import evaluate
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    DataCollatorWithPadding,
    TrainingArguments,
    Trainer,
)

raw_datasets = evaluate.load("glue", "mnli")

model_checkpoint = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

def preprocess_function(examples):
    return tokenizer(examples["premise"], examples["hypothesis"], truncation=True)

tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)

args = TrainingArguments(
    f"distilbert-finetuned-mnli",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
)

metric = evaluate.load("glue", "mnli")

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    return metric.compute(predictions=predictions, references=labels)

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

trainer = Trainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation_matched"],
    compute_metrics=compute_metrics,
    data_collator=data_collator,
    tokenizer=tokenizer,
)
trainer.train()
```

å¥½æ¶ˆæ¯ï¼Ÿæˆ‘ä»¬æ²¡æœ‰å¾—åˆ°ä¸ä»¥å‰ç›¸åŒçš„é”™è¯¯ï¼Œè¿™ç»å¯¹æ˜¯è¿›æ­¥ã€‚åæ¶ˆæ¯ï¼Ÿæˆ‘ä»¬å¾—åˆ°äº†ä¸€ä¸ªè‡­åæ˜­è‘—çš„ CUDA é”™è¯¯ï¼š

```python
RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`
```

è¿™å¾ˆç³Ÿç³•ï¼Œå› ä¸º CUDA é”™è¯¯é€šå¸¸å¾ˆéš¾è°ƒè¯•ã€‚æˆ‘ä»¬ç¨åä¼šçœ‹åˆ°å¦‚ä½•è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½†é¦–å…ˆè®©æˆ‘ä»¬å®Œæˆå¯¹åˆ›å»ºæ‰¹å¤„ç†çš„åˆ†æã€‚

å¦‚æœä½ ç¡®å®šä½ çš„æ•°æ®æ•´ç†å™¨æ˜¯æ­£ç¡®çš„ï¼Œåˆ™åº”å°è¯•ç”¨å®ƒæ¥å¤„ç†æ•°æ®é›†çš„å‡ ä¸ªæ ·æœ¬ï¼š

```python
data_collator = trainer.get_train_dataloader().collate_fn
batch = data_collator([trainer.train_dataset[i] for i in range(4)])
```

æ­¤ä»£ç å°†è¿è¡Œå¤±è´¥ï¼Œå› ä¸º `train_dataset` åŒ…å«å­—ç¬¦ä¸²åˆ—ï¼Œ `Trainer` é€šå¸¸ä¼šåˆ é™¤è¿™äº›åˆ—ã€‚ä½ å¯ä»¥æ‰‹åŠ¨åˆ é™¤å®ƒä»¬ï¼Œæˆ–è€…å¦‚æœä½ ä½¿ç”¨ `Trainer` åœ¨å¹•åæ‰€åšçš„äº‹æƒ…ï¼Œä½ å¯ä»¥è°ƒç”¨ç§æœ‰çš„ `Trainer._remove_unused_columns()` æ–¹æ³•æ¥æ‰§è¡Œæ­¤æ“ä½œï¼š

```python
data_collator = trainer.get_train_dataloader().collate_fn
actual_train_set = trainer._remove_unused_columns(trainer.train_dataset)
batch = data_collator([actual_train_set[i] for i in range(4)])
```

å¦‚æœé”™è¯¯ä»ç„¶å­˜åœ¨ï¼Œä½ åº”è¯¥èƒ½å¤Ÿæ‰‹åŠ¨è°ƒè¯•æ•°æ®æ•´ç†å™¨å†…éƒ¨ä»¥ç¡®å®šå…·ä½“çš„é—®é¢˜ã€‚

ç°åœ¨æˆ‘ä»¬å·²ç»è°ƒè¯•äº†æ‰¹å¤„ç†åˆ›å»ºè¿‡ç¨‹ï¼Œæ˜¯æ—¶å€™å°†æ•°æ®ä¼ é€’ç»™æ¨¡å‹äº†ï¼

#### æ£€æŸ¥æ¨¡å‹ 

ä½ åº”è¯¥èƒ½å¤Ÿé€šè¿‡æ‰§è¡Œä»¥ä¸‹å‘½ä»¤æ¥è·å¾—ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®ï¼š

```python
for batch in trainer.get_train_dataloader():
    break
```

å¦‚æœä½ åœ¨ notebook ä¸­è¿è¡Œæ­¤ä»£ç ï¼Œä½ å¯èƒ½ä¼šæ”¶åˆ°ä¸æˆ‘ä»¬ä¹‹å‰çœ‹åˆ°çš„ç±»ä¼¼çš„ CUDA é”™è¯¯ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½ éœ€è¦é‡æ–°å¯åŠ¨ notebook å¹¶é‡æ–°æ‰§è¡Œæœ€åä¸€æ®µä»£ç ï¼Œä½†æ˜¯ä¸è¿è¡Œ `trainer.train()` è¡Œï¼è¿™æ˜¯å…³äº CUDA é”™è¯¯çš„ç¬¬äºŒä¸ªæœ€çƒ¦äººçš„äº‹æƒ…ï¼šå®ƒä»¬ä¼šç ´åä½ çš„ Cuda å†…æ ¸ï¼Œè€Œä¸”æ— æ³•æ¢å¤ã€‚å®ƒä»¬æœ€çƒ¦äººçš„äº‹æƒ…æ˜¯å®ƒä»¬å¾ˆéš¾è°ƒè¯•ã€‚

è¿™æ˜¯ä¸ºä»€ä¹ˆï¼Ÿå®ƒä¸ GPU çš„å·¥ä½œæ–¹å¼æœ‰å…³ã€‚å®ƒä»¬åœ¨å¹¶è¡Œæ‰§è¡Œå¤§é‡æ“ä½œæ–¹é¢éå¸¸æœ‰æ•ˆï¼Œä½†ç¼ºç‚¹æ˜¯å½“å…¶ä¸­ä¸€æ¡æŒ‡ä»¤å¯¼è‡´é”™è¯¯æ—¶ï¼Œä½ ä¸ä¼šç«‹å³çŸ¥é“ã€‚åªæœ‰å½“ç¨‹åºåœ¨ GPU ä¸Šè°ƒç”¨å¤šä¸ªè¿›ç¨‹çš„åŒæ­¥å¤„ç†æ—¶ï¼Œå®ƒæ‰ä¼šæ„è¯†åˆ°å‡ºç°é—®é¢˜ï¼Œå› æ­¤é”™è¯¯å®é™…ä¸Šæ˜¯åœ¨ä¸åˆ›å»ºå®ƒçš„åŸå› æ— å…³çš„åœ°æ–¹å¼•å‘çš„ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬æŸ¥çœ‹ä¹‹å‰çš„ Trackbackï¼Œé”™è¯¯æ˜¯åœ¨åå‘ä¼ æ’­æœŸé—´å¼•å‘çš„ï¼Œä½†æˆ‘ä»¬ä¼šåœ¨ä¸€åˆ†é’Ÿåçœ‹åˆ°å®ƒå®é™…ä¸Šæºäºå‰å‘ä¼ æ’­çš„æŸäº›ä¸œè¥¿ã€‚

é‚£ä¹ˆæˆ‘ä»¬å¦‚ä½•è°ƒè¯•è¿™äº›é”™è¯¯å‘¢ï¼Ÿç­”æ¡ˆå¾ˆç®€å•ï¼šä¸è°ƒè¯•ã€‚é™¤éä½ çš„ CUDA é”™è¯¯æ˜¯å†…å­˜ä¸è¶³é”™è¯¯ï¼ˆè¿™æ„å‘³ç€ä½ çš„ GPU ä¸­æ²¡æœ‰è¶³å¤Ÿçš„å†…å­˜ï¼‰ï¼Œé™¤æ­¤ä¹‹å¤–ä½ åº”è¯¥å§‹ç»ˆè¿”å›åˆ° CPU è¿›è¡Œè°ƒè¯•ã€‚

ä¸ºæ­¤ï¼Œæˆ‘ä»¬åªéœ€å°†æ¨¡å‹æ”¾å› CPU ä¸Šå¹¶åœ¨æˆ‘ä»¬çš„ batch æ•°æ®ä¸­è°ƒç”¨å®ƒâ€”â€” `DataLoader` è¿”å›çš„é‚£æ‰¹æ•°æ®å°šæœªç§»åŠ¨åˆ° GPUï¼š

```python
outputs = trainer.model.cpu()(**batch)
```

```python
~/.pyenv/versions/3.7.9/envs/base/lib/python3.7/site-packages/torch/nn/functional.py in nll_loss(input, target, weight, size_average, ignore_index, reduce, reduction)
   2386         )
   2387     if dim == 2:
-> 2388         ret = torch._C._nn.nll_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index)
   2389     elif dim == 4:
   2390         ret = torch._C._nn.nll_loss2d(input, target, weight, _Reduction.get_enum(reduction), ignore_index)

IndexError: Target 2 is out of bounds.
```

ç°åœ¨ï¼Œæƒ…å†µè¶Šæ¥è¶Šæ˜æœ—äº†ã€‚æˆ‘ä»¬ç°åœ¨åœ¨æŸå¤±è®¡ç®—ä¸­æ²¡æœ‰å‡ºç° CUDA é”™è¯¯ï¼Œè€Œæ˜¯æœ‰ä¸€ä¸ª `IndexError` ï¼ˆå› æ­¤ä¸æˆ‘ä»¬ä¹‹å‰æ‰€è¯´çš„åå‘ä¼ æ’­æ— å…³ï¼‰ã€‚æ›´å‡†ç¡®åœ°è¯´ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ˜¯ Target 2 é€ æˆäº†é”™è¯¯ï¼Œæ‰€ä»¥è¿™æ˜¯æ£€æŸ¥æ¨¡å‹æ ‡ç­¾æ•°é‡çš„å¥½æ—¶æœºï¼š

```python
trainer.model.config.num_labels
```

```python
2
```

æœ‰ä¸¤ä¸ªæ ‡ç­¾ï¼Œåªæœ‰ 0 å’Œ 1 ä½œä¸ºç›®æ ‡ï¼Œä½†æ˜¯æ ¹æ®é”™è¯¯ä¿¡æ¯æˆ‘ä»¬å¾—åˆ°ä¸€ä¸ª 2ã€‚å¾—åˆ°ä¸€ä¸ª 2 å®é™…ä¸Šæ˜¯æ­£å¸¸çš„ï¼šå¦‚æœæˆ‘ä»¬è®°å¾—æˆ‘ä»¬ä¹‹å‰æå–çš„æ ‡ç­¾åç§°ï¼Œæœ‰ä¸‰ä¸ªï¼Œæ‰€ä»¥åœ¨æˆ‘ä»¬çš„æ•°æ®é›†ä¸­æˆ‘ä»¬æœ‰ç´¢å¼• 0ã€1 å’Œ 2 é—®é¢˜æ˜¯æˆ‘ä»¬æ²¡æœ‰å‘Šè¯‰æˆ‘ä»¬çš„æ¨¡å‹ï¼Œå®ƒåº”è¯¥åˆ›å»ºä¸‰ä¸ªæ ‡ç­¾ã€‚è®©æˆ‘ä»¬è§£å†³è¿™ä¸ªé—®é¢˜ï¼

```python
from datasets import load_dataset
import evaluate
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    DataCollatorWithPadding,
    TrainingArguments,
    Trainer,
)

raw_datasets = evaluate.load("glue", "mnli")

model_checkpoint = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

def preprocess_function(examples):
    return tokenizer(examples["premise"], examples["hypothesis"], truncation=True)

tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=3)

args = TrainingArguments(
    f"distilbert-finetuned-mnli",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
)

metric = evaluate.load("glue", "mnli")

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    return metric.compute(predictions=predictions, references=labels)

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

trainer = Trainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation_matched"],
    compute_metrics=compute_metrics,
    data_collator=data_collator,
    tokenizer=tokenizer,
)
```

ä¸ºäº†æ–¹ä¾¿æ£€æŸ¥ä¸€åˆ‡æ˜¯å¦æ­£å¸¸ï¼Œæˆ‘ä»¬è¿˜æ²¡æœ‰æŠŠ `trainer.train()` åŠ è¿›å»ã€‚å¦‚æœæˆ‘ä»¬è¯·æ±‚ä¸€ä¸ª batch çš„æ•°æ®å¹¶å°†å…¶ä¼ é€’ç»™æˆ‘ä»¬çš„æ¨¡å‹ï¼Œå‡å¦‚å®ƒç°åœ¨å¯ä»¥æ­£å¸¸å·¥ä½œäº†ï¼

```python
for batch in trainer.get_train_dataloader():
    break

outputs = trainer.model.cpu()(**batch)
```

ä¸‹ä¸€æ­¥å°±å¯ä»¥å›åˆ° GPU å¹¶æ£€æŸ¥ä¸€åˆ‡æ˜¯å¦ä»ç„¶æœ‰æ•ˆï¼š

```python
import torch

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
batch = {k: v.to(device) for k, v in batch.items()}

outputs = trainer.model.to(device)(**batch)
```

å¦‚æœä»ç„¶å‡ºç°é”™è¯¯ï¼Œè¯·ç¡®ä¿é‡æ–°å¯åŠ¨ notebook å¹¶ä»…æ‰§è¡Œæœ€åä¸€ç‰ˆçš„ä»£ç ã€‚

#### æ‰§è¡Œä¸€ä¸ªä¼˜åŒ–å™¨æ­¥éª¤ 

ç°åœ¨æˆ‘ä»¬å·²ç»å¯ä»¥æ„å»ºé€šè¿‡æ¨¡å‹æ£€æŸ¥çš„æˆæ‰¹æ¬¡çš„æ•°æ®ï¼Œæˆ‘ä»¬å·²ç»ä¸ºè®­ç»ƒç®¡é“çš„ä¸‹ä¸€æ­¥åšå¥½å‡†å¤‡ï¼šè®¡ç®—æ¢¯åº¦å¹¶æ‰§è¡Œä¼˜åŒ–å™¨æ­¥éª¤ã€‚

ç¬¬ä¸€éƒ¨åˆ†åªæ˜¯åœ¨ loss ä¸Šè°ƒç”¨ `backward()` æ–¹æ³•ï¼š

```python
loss = outputs.loss
loss.backward()
```

åœ¨è¿™ä¸ªé˜¶æ®µå¾ˆå°‘å‡ºç°é”™è¯¯ï¼Œä½†å¦‚æœç¡®å®å‡ºç°é”™è¯¯ï¼Œè¯·è¿”å› CPU æ¥è·å–æœ‰ç”¨çš„é”™è¯¯æ¶ˆæ¯ã€‚

è¦æ‰§è¡Œä¼˜åŒ–æ­¥éª¤ï¼Œæˆ‘ä»¬åªéœ€è¦åˆ›å»º `optimizer` å¹¶è°ƒç”¨å®ƒçš„ `step()` æ–¹æ³•ï¼š

```python
trainer.create_optimizer()
trainer.optimizer.step()
```

åŒæ ·ï¼Œå¦‚æœä½ åœ¨ `Trainer` ä¸­ä½¿ç”¨é»˜è®¤ä¼˜åŒ–å™¨ï¼Œåˆ™åœ¨æ­¤é˜¶æ®µä½ ä¸åº”è¯¥æ”¶åˆ°é”™è¯¯ï¼Œä½†å¦‚æœä½ æœ‰è‡ªå®šä¹‰ä¼˜åŒ–å™¨ï¼Œåˆ™å¯èƒ½ä¼šå‡ºç°ä¸€äº›é—®é¢˜éœ€è¦åœ¨è¿™é‡Œè°ƒè¯•ã€‚å¦‚æœä½ åœ¨æ­¤é˜¶æ®µé‡åˆ°å¥‡æ€ªçš„ CUDA é”™è¯¯ï¼Œè¯·ä¸è¦å¿˜è®°è¿”å› CPUã€‚è¯´åˆ° CUDA é”™è¯¯ï¼Œå‰é¢æˆ‘ä»¬æåˆ°äº†ä¸€ä¸ªç‰¹æ®Šæƒ…å†µã€‚ç°åœ¨è®©æˆ‘ä»¬æ¥çœ‹çœ‹ã€‚

#### å¤„ç† CUDA out-of-memory é”™è¯¯ 

æ¯å½“ä½ æ”¶åˆ°ä»¥ `RuntimeError: CUDA out of memory` å¼€å¤´çš„é”™è¯¯æ¶ˆæ¯æ—¶ï¼Œè¿™è¡¨æ˜ä½ çš„ GPU å†…å­˜ä¸è¶³ã€‚è¿™ä¸ä½ çš„ä»£ç æ²¡æœ‰ç›´æ¥å…³è”ï¼Œå¹¶ä¸”å®ƒå¯èƒ½å‘ç”Ÿåœ¨è¿è¡Œè‰¯å¥½çš„ä»£ç ä¸­ã€‚æ­¤é”™è¯¯æ„å‘³ç€ä½ è¯•å›¾åœ¨ GPU çš„å†…éƒ¨å­˜å‚¨å™¨ä¸­æ”¾å…¥å¤ªå¤šä¸œè¥¿ï¼Œè¿™å¯¼è‡´äº†é”™è¯¯ã€‚ä¸å…¶ä»– CUDA é”™è¯¯ä¸€æ ·ï¼Œä½ éœ€è¦é‡æ–°å¯åŠ¨å†…æ ¸æ‰èƒ½å†æ¬¡è¿è¡Œè®­ç»ƒã€‚

è¦è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½ åªéœ€è¦ä½¿ç”¨æ›´å°‘çš„ GPU ç©ºé—´â€”â€”è¿™å¾€å¾€è¯´èµ·æ¥å®¹æ˜“åšèµ·æ¥éš¾ã€‚é¦–å…ˆï¼Œç¡®ä¿ä½ æ²¡æœ‰åŒæ—¶åœ¨ GPU ä¸Šè¿è¡Œä¸¤ä¸ªæ¨¡å‹ï¼ˆå½“ç„¶ï¼Œé™¤éä½ çš„é—®é¢˜éœ€è¦è¿™æ ·åšï¼‰ã€‚ç„¶åï¼Œä½ å¯èƒ½åº”è¯¥å‡å°‘ batch çš„å¤§å°ï¼Œå› ä¸ºå®ƒç›´æ¥å½±å“æ¨¡å‹çš„æ‰€æœ‰ä¸­é—´è¾“å‡ºçš„å¤§å°åŠå…¶æ¢¯åº¦ã€‚å¦‚æœé—®é¢˜ä»ç„¶å­˜åœ¨ï¼Œè¯·è€ƒè™‘ä½¿ç”¨è¾ƒå°ç‰ˆæœ¬çš„æ¨¡å‹ã€‚

<div custom-style="Tip-green">

åœ¨è¯¾ç¨‹çš„ä¸‹ä¸€éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å°†ä»‹ç»æ›´å…ˆè¿›çš„æŠ€æœ¯ï¼Œè¿™äº›æŠ€æœ¯å¯ä»¥å¸®åŠ©ä½ å‡å°‘å†…å­˜å ç”¨å¹¶è®©ä½ å¾®è°ƒæœ€å¤§çš„æ¨¡å‹ã€‚

</div>

#### è¯„ä¼°æ¨¡å‹ 

ç°åœ¨æˆ‘ä»¬å·²ç»è§£å†³äº†ä»£ç çš„æ‰€æœ‰é—®é¢˜ï¼Œä¸€åˆ‡éƒ½å¾ˆå®Œç¾ï¼Œè®­ç»ƒåº”è¯¥å¯ä»¥é¡ºåˆ©è¿›è¡Œï¼Œå¯¹å§ï¼Ÿæ²¡é‚£ä¹ˆå¿«ï¼å¦‚æœä½ è¿è¡Œ `trainer.train()` å‘½ä»¤ï¼Œä¸€å¼€å§‹ä¸€åˆ‡çœ‹èµ·æ¥éƒ½ä¸é”™ï¼Œä½†è¿‡ä¸€ä¼šå„¿ä½ ä¼šå¾—åˆ°ä»¥ä¸‹ä¿¡æ¯ï¼š

```python
# è¿™å°†èŠ±è´¹å¾ˆé•¿æ—¶é—´å¹¶ä¸”ä¼šå‡ºé”™,æ‰€ä»¥ä¸è¦ç›´æ¥è¿è¡Œè¿™ä¸ªå•å…ƒ
trainer.train()
```

```python
TypeError: only size-1 arrays can be converted to Python scalars
```

ä½ å°†æ„è¯†åˆ°æ­¤é”™è¯¯å‡ºç°åœ¨è¯„ä¼°é˜¶æ®µï¼Œå› æ­¤è¿™æ˜¯æˆ‘ä»¬éœ€è¦è°ƒè¯•çš„æœ€åä¸€ä»¶äº‹ã€‚

ä½ å¯ä»¥ç‹¬ç«‹äºè®­ç»ƒè¿è¡Œ Trainer çš„è¯„ä¼°å¾ªç¯ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```python
trainer.evaluate()
```

```python
TypeError: only size-1 arrays can be converted to Python scalars
```

<div custom-style="Tip-green">

ğŸ’¡ ä½ åº”è¯¥å§‹ç»ˆç¡®ä¿åœ¨å¯åŠ¨ `trainer.train()` ä¹‹å‰ `trainer.evaluate()` æ˜¯å¯ä»¥è¿è¡Œçš„ï¼Œä»¥é¿å…åœ¨é‡åˆ°é”™è¯¯ä¹‹å‰æµªè´¹å¤§é‡è®¡ç®—èµ„æºã€‚

</div>

åœ¨å°è¯•è°ƒè¯•è¯„ä¼°å¾ªç¯ä¸­çš„é—®é¢˜ä¹‹å‰ï¼Œä½ åº”è¯¥é¦–å…ˆç¡®ä¿ä½ å·²ç»æŸ¥çœ‹äº†æ•°æ®ï¼Œèƒ½å¤Ÿæ­£ç¡®åœ°å½¢æˆäº† batch å¹¶ä¸”å¯ä»¥åœ¨å…¶ä¸Šè¿è¡Œä½ çš„æ¨¡å‹ã€‚æˆ‘ä»¬å·²ç»å®Œæˆäº†æ‰€æœ‰è¿™äº›æ­¥éª¤ï¼Œå› æ­¤å¯ä»¥æ‰§è¡Œä»¥ä¸‹ä»£ç è€Œä¸ä¼šå‡ºé”™ï¼š

```python
for batch in trainer.get_eval_dataloader():
    break

batch = {k: v.to(device) for k, v in batch.items()}

with torch.no_grad():
    outputs = trainer.model(**batch)
```

ç¨ç­‰ä¸€ä¼šå„¿ï¼Œé”™è¯¯å‡ºç°ï¼Œåœ¨è¯„ä¼°é˜¶æ®µç»“æŸæ—¶ï¼Œå¦‚æœæˆ‘ä»¬æŸ¥çœ‹ Trackbackï¼Œæˆ‘ä»¬ä¼šçœ‹åˆ°ï¼š

```python
~/git/datasets/src/datasets/metric.py in add_batch(self, predictions, references)
    431         
    432         batch = {"predictions": predictions, "references": references}
--> 433         batch = self.info.features.encode_batch(batch)
    434         if self.writer is None:
    435             self._init_writer()
```

è¿™å‘Šè¯‰æˆ‘ä»¬é”™è¯¯æºè‡ª `datasets/metric.py` æ¨¡å—â€”â€”æ‰€ä»¥è¿™æ˜¯æˆ‘ä»¬çš„ `compute_metrics()` å‡½æ•°çš„é—®é¢˜ã€‚å®ƒéœ€è¦ä¸€ä¸ªå¸¦æœ‰ logits å’Œæ ‡ç­¾çš„å…ƒç»„ä½œä¸º NumPy æ•°ç»„ï¼Œæ‰€ä»¥è®©æˆ‘ä»¬å°è¯•å°†å…¶æä¾›ç»™å®ƒï¼š

```python
predictions = outputs.logits.cpu().numpy()
labels = batch["labels"].cpu().numpy()

compute_metrics((predictions, labels))
```

```python
TypeError: only size-1 arrays can be converted to Python scalars
```

æˆ‘ä»¬å¾—åˆ°åŒæ ·çš„é”™è¯¯ï¼Œæ‰€ä»¥é—®é¢˜è‚¯å®šå‡ºåœ¨é‚£ä¸ªå‡½æ•°ä¸Šã€‚å¦‚æœæˆ‘ä»¬å›é¡¾å®ƒçš„ä»£ç ï¼Œæˆ‘ä»¬ä¼šå‘ç°å®ƒåªæ˜¯å°† `predictions` å’Œ `labels` è½¬å‘åˆ° `metric.compute()` ã€‚é‚£ä¹ˆè¿™ç§æ–¹æ³•æœ‰é—®é¢˜å—ï¼Ÿä¸ä¸€å®šã€‚è®©æˆ‘ä»¬å¿«é€Ÿæµè§ˆä¸€ä¸‹å½¢çŠ¶ï¼š

```python
predictions.shape, labels.shape
```

```python
((8, 3), (8,))
```

æˆ‘ä»¬çš„é¢„æµ‹è¾“å‡ºæ˜¯ logits å€¼ï¼Œè€Œä¸æ˜¯éœ€è¦çš„ 3 ä¸ªæ ‡ç­¾çš„æ¦‚ç‡ï¼Œè¿™å°±æ˜¯ metrics è¿”å›è¿™ä¸ªï¼ˆæœ‰ç‚¹æ¨¡ç³Šï¼‰é”™è¯¯çš„åŸå› ã€‚ä¿®å¤å¾ˆç®€å•ï¼›æˆ‘ä»¬åªéœ€è¦åœ¨ `compute_metrics()` å‡½æ•°ä¸­æ·»åŠ ä¸€ä¸ª argmaxï¼š

```python
import numpy as np

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    return metric.compute(predictions=predictions, references=labels)

compute_metrics((predictions, labels))
```

```python
{'accuracy': 0.625}
```

ç°åœ¨æˆ‘ä»¬çš„é”™è¯¯å·²ä¿®å¤ï¼è¿™æ˜¯æœ€åä¸€ä¸ªé”™è¯¯ï¼Œæ‰€ä»¥æˆ‘ä»¬çš„è„šæœ¬ç°åœ¨å°†å¯ä»¥æ­£ç¡®åœ°è®­ç»ƒä¸€ä¸ªæ¨¡å‹ã€‚

ä½œä¸ºå‚è€ƒï¼Œè¿™é‡Œæ˜¯å®Œå…¨ä¿®æ­£å¥½çš„è„šæœ¬ï¼š

```python
import numpy as np
from datasets import load_dataset
import evaluate
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    DataCollatorWithPadding,
    TrainingArguments,
    Trainer,
)

raw_datasets = evaluate.load("glue", "mnli")

model_checkpoint = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

def preprocess_function(examples):
    return tokenizer(examples["premise"], examples["hypothesis"], truncation=True)

tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=3)

args = TrainingArguments(
    f"distilbert-finetuned-mnli",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
)

metric = evaluate.load("glue", "mnli")

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    return metric.compute(predictions=predictions, references=labels)

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

trainer = Trainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation_matched"],
    compute_metrics=compute_metrics,
    data_collator=data_collator,
    tokenizer=tokenizer,
)
trainer.train()
```

åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¦‚æœæ²¡æœ‰æ›´å¤šé”™è¯¯ï¼Œæˆ‘ä»¬çš„è„šæœ¬å°†å¾®è°ƒä¸€ä¸ªåº”è¯¥ç»™å‡ºåˆç†ç»“æœçš„æ¨¡å‹ã€‚ä½†æ˜¯ï¼Œå¦‚æœè®­ç»ƒæ²¡æœ‰ä»»ä½•é”™è¯¯ï¼Œè€Œè®­ç»ƒå‡ºæ¥çš„æ¨¡å‹æ ¹æœ¬è¡¨ç°ä¸ä½³ï¼Œæˆ‘ä»¬è¯¥æ€ä¹ˆåŠï¼Ÿè¿™æ˜¯æœºå™¨å­¦ä¹ ä¸­æœ€éš¾çš„éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†å‘ä½ å±•ç¤ºä¸€äº›å¯ä»¥å¸®åŠ©è§£å†³è¿™ç±»é—®é¢˜çš„æŠ€å·§ã€‚
<div custom-style="Tip-green">

ğŸ’¡ å¦‚æœä½ ä½¿ç”¨çš„æ˜¯æ‰‹åŠ¨è®­ç»ƒå¾ªç¯ï¼Œè°ƒè¯•è®­ç»ƒæµç¨‹æ—¶ä¹Ÿéœ€è¦éµå¾ªç›¸åŒçš„æ­¥éª¤ï¼Œè€Œä¸”æ›´å®¹æ˜“å°†è®­ç»ƒä¸­çš„å„ä¸ªæ­¥éª¤åˆ†å¼€è°ƒè¯•ã€‚ä½†æ˜¯ï¼Œè¯·ç¡®ä¿ä½ æ²¡æœ‰å¿˜è®°åœ¨åˆé€‚çš„ä½ç½®ä½¿ç”¨ `model.eval()` æˆ– `model.train()` ï¼Œä¹Ÿä¸è¦å¿˜è®°åœ¨æ¯ä¸ªæ­¥éª¤ä¸­ä½¿ç”¨ `zero_grad()` ï¼

</div>

### åœ¨è®­ç»ƒæœŸé—´è°ƒè¯•é™é»˜ï¼ˆæ²¡æœ‰ä»»ä½•é”™è¯¯æç¤ºï¼‰é”™è¯¯ 

æˆ‘ä»¬å¯ä»¥åšäº›ä»€ä¹ˆæ¥è°ƒè¯•ä¸€ä¸ªæ²¡æœ‰é”™è¯¯åœ°å®Œæˆä½†æ²¡æœ‰å¾—åˆ°å¥½çš„ç»“æœçš„è®­ç»ƒï¼Ÿæˆ‘ä»¬ä¼šåœ¨è¿™é‡Œç»™ä½ ä¸€äº›æç¤ºï¼Œä½†è¯·æ³¨æ„ï¼Œè¿™ç§è°ƒè¯•æ˜¯æœºå™¨å­¦ä¹ ä¸­æœ€éš¾çš„éƒ¨åˆ†ï¼Œå¹¶ä¸”æ²¡æœ‰çµä¸¹å¦™è¯ã€‚

#### æ£€æŸ¥ä½ çš„æ•°æ®ï¼ˆå†æ¬¡ï¼ï¼‰ 

åªæœ‰ä½ çš„æ•°æ®ç¡®å®å¯ä»¥å­¦åˆ°ä¸œè¥¿ï¼Œä½ çš„æ¨¡å‹æ‰ä¼šå­¦åˆ°ä¸€äº›ä¸œè¥¿ã€‚å¦‚æœå­˜åœ¨æŸåæ•°æ®çš„é”™è¯¯æˆ–æ ‡ç­¾æ˜¯éšæœºå±æ€§çš„ï¼Œé‚£ä¹ˆä½ å¾ˆå¯èƒ½ä¸ä¼šåœ¨æ•°æ®é›†ä¸Šè·å¾—ä»»ä½•çŸ¥è¯†ã€‚å› æ­¤ï¼Œå§‹ç»ˆé¦–å…ˆä»”ç»†æ£€æŸ¥ä½ çš„è§£ç åçš„è¾“å…¥å’Œæ ‡ç­¾ï¼Œç„¶åé—®è‡ªå·±ä»¥ä¸‹é—®é¢˜ï¼š

- è§£ç åçš„æ•°æ®æ˜¯å¦å¯ä»¥ç†è§£ï¼Ÿ
- ä½ è®¤åŒè¿™äº›æ ‡ç­¾å—ï¼Ÿ
- æœ‰æ²¡æœ‰ä¸€ä¸ªæ ‡ç­¾æ¯”å…¶ä»–æ ‡ç­¾æ›´å¸¸è§ï¼Ÿ
- å¦‚æœæ¨¡å‹é¢„æµ‹éšæœºçš„ç­”æ¡ˆ/æ€»æ˜¯ç›¸åŒçš„ç­”æ¡ˆï¼Œé‚£ä¹ˆ loss/è¯„ä¼°æŒ‡æ ‡åº”è¯¥æ˜¯å¤šå°‘ï¼Ÿ

<div custom-style="Tip-yellow">

âš ï¸ å¦‚æœä½ æ­£åœ¨è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒï¼Œè¯·åœ¨æ¯ä¸ªè¿›ç¨‹ä¸­æ‰“å°æ•°æ®é›†çš„æ ·æœ¬å¹¶ä»”ç»†æ ¸å¯¹ï¼Œç¡®ä¿ä½ å¾—åˆ°çš„æ˜¯ç›¸åŒçš„å†…å®¹ã€‚ä¸€ä¸ªå¸¸è§çš„é”™è¯¯æ˜¯åœ¨æ•°æ®åˆ›å»ºè¿‡ç¨‹ä¸­æœ‰ä¸€äº›éšæœºæ€§ï¼Œå¯¼è‡´æ¯ä¸ªè¿›ç¨‹å…·æœ‰ä¸åŒç‰ˆæœ¬çš„æ•°æ®é›†ã€‚

</div>

æŸ¥çœ‹ä½ çš„æ•°æ®åï¼ŒæŸ¥çœ‹æ¨¡å‹çš„ä¸€äº›é¢„æµ‹å¹¶å¯¹å…¶è¿›è¡Œè§£ç ã€‚å¦‚æœæ¨¡å‹æ€»æ˜¯é¢„æµ‹åŒæ ·çš„ç±»åˆ«ï¼Œé‚£å¯èƒ½æ˜¯å› ä¸ºä½ çš„æ•°æ®é›†åå‘ä¸€ä¸ªç±»åˆ«ï¼ˆé’ˆå¯¹åˆ†ç±»é—®é¢˜ï¼‰ï¼›è¿‡é‡‡æ ·ç¨€æœ‰ç±»ç­‰æŠ€æœ¯å¯èƒ½ä¼šæœ‰æ‰€å¸®åŠ©ã€‚

å¦‚æœä½ åœ¨åˆå§‹æ¨¡å‹ä¸Šè·å¾—çš„ loss/è¯„ä¼°æŒ‡æ ‡ä¸ä½ æœŸæœ›çš„éšæœºé¢„æµ‹çš„ loss/è¯„ä¼°æŒ‡æ ‡éå¸¸ä¸åŒï¼Œè¯·ä»”ç»†æ£€æŸ¥ä½ çš„ loss æˆ–è¯„ä¼°æŒ‡æ ‡çš„è®¡ç®—æ–¹å¼ï¼Œå› ä¸ºé‚£é‡Œå¯èƒ½å­˜åœ¨é”™è¯¯ã€‚å¦‚æœä½ ä½¿ç”¨æœ€åæ·»åŠ çš„å¤šä¸ª lossï¼Œå¹¶åœ¨æœ€åå°†å®ƒä»¬ç›¸åŠ ï¼Œè¯·ç¡®ä¿å®ƒä»¬å…·æœ‰ç›¸åŒçš„æ¯”ä¾‹ã€‚

å½“ä½ ç¡®å®šä½ çš„æ•°æ®æ˜¯å®Œç¾çš„æ—¶ï¼Œä½ å¯ä»¥é€šè¿‡ä¸€ä¸ªç®€å•çš„æµ‹è¯•æ¥æŸ¥çœ‹æ¨¡å‹æ˜¯å¦èƒ½å¤Ÿå¯¹å…¶è¿›è¡Œè®­ç»ƒã€‚

#### åœ¨ä¸€æ‰¹ä¸Šè¿‡åº¦æ‹Ÿåˆä½ çš„æ¨¡å‹ 

è¿‡åº¦æ‹Ÿåˆé€šå¸¸æ˜¯æˆ‘ä»¬åœ¨è®­ç»ƒæ—¶å°½é‡é¿å…çš„äº‹æƒ…ï¼Œå› ä¸ºè¿™æ„å‘³ç€æ¨¡å‹æ²¡æœ‰å­¦ä¹ è¯†åˆ«æˆ‘ä»¬æƒ³è¦çš„ä¸€èˆ¬ç‰¹å¾ï¼Œè€Œåªæ˜¯è®°ä½äº†è®­ç»ƒæ ·æœ¬ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä¸€éåˆä¸€éåœ°å°è¯•åœ¨ä¸€ä¸ªæ‰¹æ¬¡ä¸Šè®­ç»ƒä½ çš„æ¨¡å‹æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„æµ‹è¯•ï¼Œè¿™æ ·å¯ä»¥æ£€æŸ¥ä½ çš„é—®é¢˜æ˜¯å¦å¯ä»¥é€šè¿‡ä½ å°è¯•è®­ç»ƒçš„æ¨¡å‹æ¥è§£å†³ã€‚å®ƒè¿˜å°†å¸®åŠ©ä½ æŸ¥çœ‹ä½ çš„åˆå§‹å­¦ä¹ ç‡æ˜¯å¦å¤ªé«˜ã€‚

ä¸€æ—¦ä½ å®šä¹‰äº†ä½ çš„ `Trainer` ä¹‹åï¼Œè¿™æ ·åšçœŸçš„å¾ˆå®¹æ˜“ï¼›åªéœ€è·å–ä¸€æ‰¹è®­ç»ƒæ•°æ®ï¼Œç„¶åä»…ä½¿ç”¨è¯¥æ‰¹æ¬¡è¿è¡Œä¸€ä¸ªå°å‹æ‰‹åŠ¨è®­ç»ƒå¾ªç¯ï¼Œå¤§çº¦ 20 æ­¥ï¼š

```python
for batch in trainer.get_train_dataloader():
    break

batch = {k: v.to(device) for k, v in batch.items()}
trainer.create_optimizer()

for _ in range(20):
    outputs = trainer.model(**batch)
    loss = outputs.loss
    loss.backward()
    trainer.optimizer.step()
    trainer.optimizer.zero_grad()
```

<div custom-style="Tip-green">

ğŸ’¡ å¦‚æœä½ çš„è®­ç»ƒæ•°æ®ä¸å¹³è¡¡ï¼Œè¯·ç¡®ä¿æ„å»ºä¸€æ‰¹åŒ…å«æ‰€æœ‰æ ‡ç­¾çš„è®­ç»ƒæ•°æ®ã€‚

</div>

ç”Ÿæˆçš„æ¨¡å‹åœ¨ä¸€ä¸ª `batch` ä¸Šåº”è¯¥æœ‰æ¥è¿‘å®Œç¾çš„ç»“æœã€‚è®©æˆ‘ä»¬è®¡ç®—ç»“æœé¢„æµ‹çš„æŒ‡æ ‡ï¼š

```python
with torch.no_grad():
    outputs = trainer.model(**batch)
preds = outputs.logits
labels = batch["labels"]

compute_metrics((preds.cpu().numpy(), labels.cpu().numpy()))
```

```python
{'accuracy': 1.0}
```

100ï¼… å‡†ç¡®ç‡ï¼Œç°åœ¨è¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„è¿‡æ‹Ÿåˆç¤ºä¾‹ï¼ˆè¿™æ„å‘³ç€å¦‚æœä½ åœ¨ä»»ä½•å…¶ä»–å¥å­ä¸Šå°è¯•ä½ çš„æ¨¡å‹ï¼Œå®ƒå¾ˆå¯èƒ½ä¼šç»™ä½ ä¸€ä¸ªé”™è¯¯çš„ç­”æ¡ˆï¼‰ï¼

å¦‚æœä½ æ²¡æœ‰è®¾æ³•è®©ä½ çš„æ¨¡å‹è·å¾—è¿™æ ·çš„å®Œç¾ç»“æœï¼Œè¿™æ„å‘³ç€ä½ æ„å»ºé—®é¢˜æˆ–æ•°æ®çš„æ–¹å¼æœ‰é—®é¢˜ï¼Œæ‰€ä»¥ä½ åº”è¯¥ä¿®å¤å®ƒã€‚åªæœ‰å½“ä½ å¯ä»¥é€šè¿‡è¿‡æ‹Ÿåˆæµ‹è¯•æ—¶ï¼Œä½ æ‰èƒ½ç¡®å®šä½ çš„æ¨¡å‹å®é™…ä¸Šå¯ä»¥å­¦åˆ°ä¸€äº›ä¸œè¥¿ã€‚

<div custom-style="Tip-yellow">

âš ï¸ åœ¨æ­¤æµ‹è¯•ä¹‹åï¼Œä½ éœ€è¦åˆ›å»ºæ¨¡å‹å’Œ `Trainer` ï¼Œå› ä¸ºè·å¾—çš„æ¨¡å‹å¯èƒ½æ— æ³•åœ¨ä½ çš„å®Œæ•´æ•°æ®é›†ä¸Šæ¢å¤å’Œå­¦ä¹ æœ‰ç”¨çš„ä¸œè¥¿ã€‚

</div>

#### åœ¨ä½ æœ‰ç¬¬ä¸€ä¸ª baseline æ¨¡å‹ä¹‹å‰ä¸è¦è°ƒæ•´ä»»ä½•ä¸œè¥¿ 

è¶…å‚æ•°è°ƒä¼˜æ€»æ˜¯è¢«å¼ºè°ƒä¸ºæœºå™¨å­¦ä¹ ä¸­æœ€éš¾çš„éƒ¨åˆ†ï¼Œä½†è¿™åªæ˜¯å¸®åŠ©ä½ åœ¨æŒ‡æ ‡ä¸Šæœ‰æ‰€æ”¶è·çš„æœ€åä¸€æ­¥ã€‚å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œ `Trainer` çš„é»˜è®¤è¶…å‚æ•°å¯ä»¥å¾ˆå¥½åœ°ä¸ºä½ æä¾›è‰¯å¥½çš„ç»“æœï¼Œå› æ­¤åœ¨ä½ æ‹¥æœ‰æ•°æ®é›†ä¸Šçš„ baseline æ¨¡å‹ä¹‹å‰ï¼Œä¸è¦æ€¥äºè¿›è¡Œè€—æ—¶å’Œæ˜‚è´µçš„è¶…å‚æ•°æœç´¢ã€‚

ä¸€æ—¦ä½ æœ‰ä¸€ä¸ªè¶³å¤Ÿå¥½çš„æ¨¡å‹ï¼Œä½ å°±å¯ä»¥å¼€å§‹ç¨å¾®è°ƒæ•´ä¸€ä¸‹ã€‚ä¸è¦å°è¯•ä½¿ç”¨ä¸åŒçš„è¶…å‚æ•°å¯åŠ¨ä¸€åƒæ¬¡è¿è¡Œï¼Œè€Œæ˜¯æ¯”è¾ƒä¸€ä¸ªè¶…å‚æ•°çš„ä¸åŒå€¼çš„å‡ æ¬¡è¿è¡Œï¼Œä»¥äº†è§£å“ªä¸ªè¶…å‚æ•°å¯¹æŒ‡æ ‡å½±å“æœ€å¤§ã€‚

å¦‚æœä½ æ­£åœ¨è°ƒæ•´æ¨¡å‹æœ¬èº«ï¼Œè¯·ä¿æŒç®€å•ï¼Œä¸è¦å°è¯•ä»»ä½•ä½ æ— æ³•åˆç†è¯æ˜çš„äº‹æƒ…ã€‚å§‹ç»ˆç¡®ä¿ä½ è¿”å›è¿‡æ‹Ÿåˆæµ‹è¯•ä»¥éªŒè¯ä½ çš„æ›´æ”¹æ²¡æœ‰äº§ç”Ÿä»»ä½•æ„å¤–åæœã€‚

#### è¯·æ±‚å¸®å¿™ 

å¸Œæœ›ä½ ä¼šåœ¨æœ¬èŠ‚ä¸­æ‰¾åˆ°ä¸€äº›å¯ä»¥å¸®åŠ©ä½ è§£å†³é—®é¢˜çš„å»ºè®®ï¼Œä½†å¦‚æœä¸æ˜¯è¿™æ ·ï¼Œè¯·è®°ä½ä½ å¯ä»¥éšæ—¶åœ¨ [è®ºå›](https://discuss.huggingface.co/)(https://discuss.huggingface.co/) ä¸Šå‘ç¤¾åŒºæé—®ã€‚

ä»¥ä¸‹æ˜¯ä¸€äº›å¯èƒ½æœ‰ç”¨çš„é¢å¤–èµ„æºï¼š

-Joel Grus çš„ [â€œä½œä¸ºå·¥ç¨‹æœ€ä½³å®è·µå·¥å…·çš„å†ç°æ€§â€](https://docs.google.com/presentation/d/1yHLPvPhUs2KGI5ZWo0sU-PKU3GimAk3iTsI38Z-B5Gw/edit#slide=id.p)(https://docs.google.com/presentation/d/1yHLPvPhUs2KGI5ZWo0sU-PKU3GimAk3iTsI38Z-B5Gw/edit#slide=id.p) 
- Cecelia Shao çš„ [â€œç¥ç»ç½‘ç»œè°ƒè¯•æ¸…å•â€](https://towardsdatascience.com/checklist-for-debugging-neural-networks-d8b2a9434f21)(https://towardsdatascience.com/checklist-for-debugging-neural-networks-d8b2a9434f21) 
- Chase Roberts çš„ [â€œå¦‚ä½•å¯¹æœºå™¨å­¦ä¹ ä»£ç è¿›è¡Œå•å…ƒæµ‹è¯•â€](https://medium.com/@keeper6928/how-to-unit-test-machine-learning-code-57cf6fd81765)(https://medium.com/@keeper6928/how-to-unit-test-machine-learning-code-57cf6fd81765) 
- Andrej Karpathy çš„ [â€œè®­ç»ƒç¥ç»ç½‘ç»œçš„ç§˜è¯€â€](http://karpathy.github.io/2019/04/25/recipe)(http://karpathy.github.io/2019/04/25/recipe) 

å½“ç„¶ï¼Œå¹¶ä¸æ˜¯ä½ åœ¨è®­ç»ƒç¥ç»ç½‘ç»œæ—¶é‡åˆ°çš„æ¯ä¸ªé—®é¢˜éƒ½æ˜¯ä½ è‡ªå·±çš„é”™ï¼å¦‚æœä½ åœ¨Transformers æˆ–Datasets åº“ä¸­é‡åˆ°äº†ä¼¼ä¹ä¸æ­£ç¡®çš„ä¸œè¥¿ï¼Œä½ å¯èƒ½é‡åˆ°äº†ä¸€ä¸ªé”™è¯¯ã€‚ä½ åº”è¯¥å‘Šè¯‰æˆ‘ä»¬æ‰€æœ‰è¿™äº›é—®é¢˜ï¼Œåœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†è¯¦ç»†è§£é‡Šå¦‚ä½•åšåˆ°è¿™ä¸€ç‚¹ã€‚


## 9.4 è°ƒè¯•Kerasè®­ç»ƒç®¡é“ 

ä½ å·²ç»éµå¾ªç¬¬å…«ç« ä¸­çš„å»ºè®®ï¼Œç¼–å†™äº†ä¸€ä¸ªæ¼‚äº®çš„è„šæœ¬æ¥è®­ç»ƒæˆ–å¾®è°ƒç»™å®šä»»åŠ¡çš„æ¨¡å‹ã€‚ ä½†æ˜¯å½“ä½ å¯åŠ¨å‘½ä»¤ `model.fit()` æ—¶ï¼Œä½ å¾—åˆ°ä¸€ä¸ªé”™è¯¯ğŸ˜±ï¼ æˆ–è€…æ›´ç³Ÿçš„æ˜¯ä¸€åˆ‡çœ‹ä¼¼éƒ½å¾ˆæ­£å¸¸ï¼Œè®­ç»ƒè¿è¡Œæ²¡æœ‰é”™è¯¯ï¼Œä½†ç”Ÿæˆçš„æ¨¡å‹å¾ˆç³Ÿç³•ã€‚ åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†å‘ä½ å±•ç¤ºå¦‚ä½•è°ƒè¯•æ­¤ç±»é—®é¢˜ã€‚

### è°ƒè¯•è®­ç»ƒç®¡é“ 

å½“ä½ åœ¨ `model.fit()` ä¸­é‡åˆ°é”™è¯¯æ—¶ï¼Œé—®é¢˜åœ¨äºå®ƒå¯èƒ½æ¥è‡ªå¤šä¸ªä¸åŒçš„æ¥æºï¼Œå› ä¸ºè®­ç»ƒé€šå¸¸å°†ä¹‹å‰çš„è®¸å¤šå·¥ä½œæ±‡é›†åˆ°ä¸€èµ·ã€‚æ¯”å¦‚æœ‰å¯èƒ½æ˜¯ä½ çš„æ•°æ®é›†æœ‰é—®é¢˜ï¼Œæˆ–è€…å¯èƒ½æ˜¯åœ¨å°è¯•å°†æ•°æ®é›†çš„å…ƒç´ æ‰¹å¤„ç†æ—¶å‡ºç°é—®é¢˜ï¼Œåˆæˆ–è€…æ¨¡å‹ä»£ç ã€æŸå¤±å‡½æ•°æˆ–ä¼˜åŒ–å™¨ä¸­å­˜åœ¨é—®é¢˜ï¼Œå¦å¤–å³ä½¿è®­ç»ƒè¿‡ç¨‹ä¸€åˆ‡é¡ºåˆ©ï¼Œå¦‚æœæŒ‡æ ‡é€‰å–æœ‰é—®é¢˜ï¼Œè¯„ä¼°è¿‡ç¨‹ä¸­ä»ç„¶å¯èƒ½å‡ºç°é”™è¯¯ã€‚

æ‰€ä»¥è°ƒè¯• `model.fit()` ä¸­å‡ºç°çš„é”™è¯¯çš„æœ€ä½³æ–¹æ³•æ˜¯æ‰‹åŠ¨æ£€æŸ¥æ•´ä¸ªç®¡é“ï¼Œçœ‹çœ‹å“ªé‡Œå‡ºäº†é—®é¢˜ã€‚

è¿™é‡Œæˆ‘ä»¬å°†ä½¿ç”¨ä»¥ä¸‹è„šæœ¬åœ¨ [MNLI æ•°æ®é›†](https://huggingface.co/datasets/glue)(https://huggingface.co/datasets/glue)ä¸Šå¾®è°ƒ DistilBERT æ¨¡å‹ï¼š

```python
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    TFAutoModelForSequenceClassification,
)

raw_datasets = evaluate.load("glue", "mnli")

model_checkpoint = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

def preprocess_function(examples):
    return tokenizer(examples["premise"], examples["hypothesis"], truncation=True)

tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)

train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["input_ids", "labels"], batch_size=16, shuffle=True
)

validation_dataset = tokenized_datasets["validation_matched"].to_tf_dataset(
    columns=["input_ids", "labels"], batch_size=16, shuffle=True
)

model = TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint)

model.compile(loss="sparse_categorical_crossentropy", optimizer="adam")

model.fit(train_dataset)
```

å¦‚æœæ‰§è¡Œè¿™æ®µä»£ç ï¼Œåœ¨è¿›è¡Œæ•°æ®é›†è½¬æ¢æ—¶å¯èƒ½ä¼šæ”¶åˆ°ä¸€äº›`VisibleDeprecationWarning`â€”â€”è¿™æ˜¯å·²çŸ¥çš„ UX é—®é¢˜ï¼Œå¯ä»¥å¿½ç•¥ã€‚ å¦‚æœä½ åœ¨ 2021 å¹´ 11 æœˆä¹‹åå­¦ä¹ æœ¬ä¹¦æ—¶è¿˜æœ‰è¿™ä¸ªé—®é¢˜ï¼Œå¯ä»¥åœ¨æ¨ç‰¹ä¸Š @carrigmat ä¸Šå‘è¡¨æ¨æ–‡æ•¦ä¿ƒä½œè€…è¿›è¡Œä¿®å¤ã€‚

ç„¶è€Œæ›´ä¸¥é‡çš„é—®é¢˜æ˜¯ä¼šå¾—åˆ°äº†ä¸€ä¸ªæ®µå¾ˆé•¿çš„æŠ¥é”™ï¼š

```python
ValueError: No gradients provided for any variable: ['tf_distil_bert_for_sequence_classification/distilbert/embeddings/word_embeddings/weight:0', '...']
```

è¿™æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿæˆ‘ä»¬åœ¨æ•°æ®ä¸Šè®­ç»ƒæ¨¡å‹ï¼Œä½†å´æ²¡æœ‰æ¢¯åº¦ï¼Ÿ ä½ ç”šè‡³å¯èƒ½ä¸çŸ¥é“è¯¥å¦‚ä½•è¿›è¡Œè°ƒè¯•ã€‚å½“ä½ å¾—åˆ°çš„é”™è¯¯å¹¶ä¸èƒ½ç«‹å³è¡¨æ˜é—®é¢˜å‡ºåœ¨å“ªé‡Œæ—¶ï¼Œæœ€å¥½çš„è§£å†³æ–¹æ³•é€šå¸¸æ˜¯æŒ‰é¡ºåºæ£€æŸ¥æ‰€æœ‰å†…å®¹ï¼Œç¡®ä¿åœ¨æ¯ä¸ªé˜¶æ®µä¸€åˆ‡çœ‹èµ·æ¥éƒ½æ­£å¸¸ã€‚

#### æ£€æŸ¥ä½ çš„æ•°æ® 

è¿™æ˜¯ä¸è¨€è€Œå–»çš„ï¼Œå¦‚æœä½ çš„æ•°æ®å·²æŸåï¼ŒKeras æ˜¯æ— æ³•è¿›è¡Œä¿®å¤æ•°æ®çš„ã€‚ æ’æŸ¥æ•°æ®é”™è¯¯éœ€è¦é è‡ªå·±ï¼Œé¦–å…ˆè¦åšçš„äº‹æƒ…æ˜¯æŸ¥çœ‹è®­ç»ƒé›†ä¸­çš„å†…å®¹ã€‚

å°½ç®¡æŸ¥çœ‹ `raw_datasets` å’Œ `tokenized_datasets` æ¯”è¾ƒå®¹æ˜“ï¼Œä½†å¼ºçƒˆå»ºè®®ä½ åœ¨æ•°æ®å°†è¦è¿›å…¥æ¨¡å‹çš„åœ°æ–¹ç›´æ¥æŸ¥çœ‹æ•°æ®ã€‚ è¿™æ„å‘³ç€ä½ åº”è¯¥è¯•ç€è¯»å–ä½¿ç”¨ `to_tf_dataset()` å‡½æ•°åˆ›å»ºçš„ `tf.data.Dataset` çš„è¾“å‡ºï¼ é‚£åº”è¯¥æ€ä¹ˆåšå‘¢ï¼Ÿ `tf.data.Dataset` å¯¹è±¡ä¸€æ¬¡ç»™æˆ‘ä»¬æ•´ä¸ª batch çš„æ•°æ®ï¼Œå¹¶ä¸”ä¸æ”¯æŒç´¢å¼•ï¼Œæ‰€ä»¥æˆ‘ä»¬ä¸èƒ½åªè¯·æ±‚ `train_dataset[0]`ã€‚ ä½†æ˜¯æˆ‘ä»¬å¯ä»¥å…ˆå‘å®ƒè¯·æ±‚ä¸€ä¸ª batchï¼š

```python
for batch in train_dataset:
    break
```

`break` åœ¨ä¸€æ¬¡è¿­ä»£åç»“æŸå¾ªç¯ï¼Œä¼šæŠ“å–æ¥è‡ª`train_dataset` çš„ç¬¬ä¸€æ‰¹æ•°æ®å¹¶å°†å…¶ä¿å­˜ä¸º`batch`ã€‚ ç°åœ¨ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹é‡Œé¢æœ‰ä»€ä¹ˆï¼š

```python
{'attention_mask': <tf.Tensor: shape=(16, 76), dtype=int64, numpy=
 array([[1, 1, 1, ..., 0, 0, 0],
        [1, 1, 1, ..., 0, 0, 0],
        [1, 1, 1, ..., 0, 0, 0],
        ...,
        [1, 1, 1, ..., 1, 1, 1],
        [1, 1, 1, ..., 0, 0, 0],
        [1, 1, 1, ..., 0, 0, 0]])>,
 'label': <tf.Tensor: shape=(16,), dtype=int64, numpy=array([0, 2, 1, 2, 1, 1, 2, 0, 0, 0, 1, 0, 1, 2, 2, 1])>,
 'input_ids': <tf.Tensor: shape=(16, 76), dtype=int64, numpy=
 array([[ 101, 2174, 1010, ...,    0,    0,    0],
        [ 101, 3174, 2420, ...,    0,    0,    0],
        [ 101, 2044, 2048, ...,    0,    0,    0],
        ...,
        [ 101, 3398, 3398, ..., 2051, 2894,  102],
        [ 101, 1996, 4124, ...,    0,    0,    0],
        [ 101, 1999, 2070, ...,    0,    0,    0]])>}
```

çœ‹èµ·æ¥æ²¡é—®é¢˜ï¼Œå¯¹å§ï¼Ÿæˆ‘ä»¬å°† `labels` ã€`attention_mask` å’Œ `input_ids` ä¼ é€’ç»™æ¨¡å‹ï¼Œè¿™åº”è¯¥æ˜¯è®¡ç®—è¾“å‡ºå’Œè®¡ç®—æŸå¤±æ‰€éœ€çš„ã€‚é‚£ä¹ˆä¸ºä»€ä¹ˆæ²¡æœ‰æ¢¯åº¦å‘¢ï¼Ÿä»”ç»†çœ‹ï¼šæˆ‘ä»¬å°†å•ä¸ªå­—å…¸ä½œä¸ºè¾“å…¥ä¼ é€’ï¼Œä½†è®­ç»ƒæ‰¹æ¬¡é€šå¸¸æ˜¯è¾“å…¥å¼ é‡æˆ–å­—å…¸åŠ ä¸Šæ ‡ç­¾å¼ é‡ã€‚æˆ‘ä»¬çš„æ ‡ç­¾åªæ˜¯æˆ‘ä»¬è¾“å…¥å­—å…¸ä¸­çš„ä¸€ä¸ªé”®å€¼ã€‚

è¿™æ˜¯ä¸€ä¸ªé—®é¢˜å—ï¼Ÿå®é™…ä¸Šå¹¶ä¸æ€»æ˜¯ï¼ä½†æ˜¯è¿™æ˜¯ä½ åœ¨ä½¿ç”¨ TensorFlow è®­ç»ƒ Transformer æ¨¡å‹æ—¶ä¼šé‡åˆ°çš„æœ€å¸¸è§é—®é¢˜ä¹‹ä¸€ã€‚æˆ‘ä»¬çš„æ¨¡å‹éƒ½å¯ä»¥åœ¨å†…éƒ¨è®¡ç®—æŸå¤±ï¼Œä½†è¦åšåˆ°è¿™ä¸€ç‚¹éœ€è¦åœ¨è¾“å…¥å­—å…¸ä¸­ä¼ é€’æ ‡ç­¾ã€‚è¿™æ˜¯å½“æˆ‘ä»¬æ²¡æœ‰ä¸º `compile()` æŒ‡å®šæŸå¤±å€¼æ—¶ä½¿ç”¨çš„æŸå¤±ã€‚å¦ä¸€æ–¹é¢ï¼ŒKeras é€šå¸¸å¸Œæœ›æ ‡ç­¾ä¸è¾“å…¥å­—å…¸åˆ†å¼€ä¼ é€’ï¼Œå¦‚æœä¸è¿™æ ·åšæŸå¤±è®¡ç®—é€šå¸¸ä¼šå¤±è´¥ã€‚

é—®é¢˜ç°åœ¨å˜å¾—æ¸…æ™°ï¼šæˆ‘ä»¬ä¼ é€’äº†ä¸€ä¸ª`loss`å‚æ•°ï¼Œæ„å‘³ç€æˆ‘ä»¬è¦æ±‚ Keras ä¸ºæˆ‘ä»¬è®¡ç®—æŸå¤±ï¼Œä½†æˆ‘ä»¬å°†æ ‡ç­¾ä½œä¸ºè¾“å…¥ä¼ é€’ç»™äº†æ¨¡å‹ï¼Œè€Œæ²¡æœ‰æ”¾åœ¨ Keras æœŸæœ›çš„åœ°æ–¹ï¼æˆ‘ä»¬éœ€è¦äºŒé€‰ä¸€ï¼šè¦ä¹ˆä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨æŸå¤±å¹¶å°†æ ‡ç­¾ä¿ç•™åœ¨åŸå¤„ï¼Œè¦ä¹ˆç»§ç»­ä½¿ç”¨ Keras æŸå¤±ä½†å°†æ ‡ç­¾ç§»åŠ¨åˆ° Keras æœŸæœ›çš„ä½ç½®ã€‚ä¸ºäº†ç®€å•èµ·è§ï¼Œå¯ä»¥é‡‡ç”¨ç¬¬ä¸€ç§æ–¹æ³•ã€‚å°†å¯¹ `compile()` çš„è°ƒç”¨æ›´æ”¹ä¸ºï¼š

```python
model.compile(optimizer="adam")
```

ç°åœ¨æˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ¨¡å‹çš„å†…éƒ¨æŸå¤±ï¼Œè¿™ä¸ªé—®é¢˜è§£å†³äº†ï¼

<div custom-style="Tip-green">

âœï¸ **è½®åˆ°ä½ äº†ï¼** ä½œä¸ºè§£å†³å…¶ä»–é—®é¢˜åçš„å¯é€‰æŒ‘æˆ˜ï¼Œä½ å¯ä»¥å°è¯•å›åˆ°è¿™ä¸€æ­¥ï¼Œè®©æ¨¡å‹ä½¿ç”¨åŸå§‹ Keras è®¡ç®—çš„æŸå¤±è€Œä¸æ˜¯å†…éƒ¨æŸå¤±ã€‚ ä½ éœ€è¦å°† `"labels"` æ·»åŠ åˆ° `to_tf_dataset()` çš„ `label_cols` å‚æ•°ï¼Œç¡®ä¿æ­£ç¡®è¾“å‡ºæ ‡ç­¾æ¥æä¾›æ¢¯åº¦ï¼Œä½†æ˜¯æˆ‘ä»¬æŒ‡å®šçš„æŸå¤±è¿˜æœ‰ä¸€ä¸ªé—®é¢˜ã€‚å³ä½¿åœ¨è¿™ä¸ªé—®é¢˜ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå­¦ä¹ é€Ÿåº¦ä»ç„¶ä¼šéå¸¸æ…¢ï¼Œå¹¶ä¸” loss ä¼šè¾¾åˆ°ä¸€ä¸ªè¾ƒé«˜çš„å€¼ã€‚ä½ èƒ½æ‰¾å‡ºé—®é¢˜åœ¨å“ªé‡Œå—ï¼Ÿ

å¦‚æœä½ å¡ä½äº†ï¼Œè¿™æ˜¯ä¸€ä¸ª ROT13 ç¼–ç çš„æç¤ºï¼ˆå¦‚æœä½ ä¸ç†Ÿæ‚‰ ROT13ï¼Œå¯ä»¥åœ¨[è¿™é‡Œ](https://rot13.com/)(https://rot13.com/)è§£ç ã€‚ï¼‰ï¼šA ROT13-encoded hint, if you're stuck: Vs lbh ybbx ng gur bhgchgf bs FrdhraprPynffvsvpngvba zbqryf va Genafsbezref, gurve svefg bhgchg vf `ybtvgf`. Jung ner ybtvgf?ï¼ˆå¦‚æœä½ æŸ¥çœ‹ Transformers ä¸­`SequenceClassification`æ¨¡å‹çš„è¾“å‡ºï¼Œå®ƒä»¬çš„ç¬¬ä¸€ä¸ªè¾“å‡ºæ˜¯â€œlogitsâ€ã€‚ ä»€ä¹ˆæ˜¯logitsï¼Ÿï¼‰

è¿˜æœ‰ä¸€ä¸ªæç¤ºï¼š

Jura lbh fcrpvsl bcgvzvmref, npgvingvbaf be ybffrf jvgu fgevatf, Xrenf frgf nyy gur nethzrag inyhrf gb gurve qrsnhygf. Jung nethzragf qbrf FcnefrPngrtbevpnyPebffragebcl unir, naq jung ner gurve qrsnhygf?ï¼ˆå½“ä½ ä½¿ç”¨å­—ç¬¦ä¸²æŒ‡å®šä¼˜åŒ–å™¨ã€æ¿€æ´»æˆ–æŸå¤±æ—¶ï¼ŒKeras ä¼šå°†æ‰€æœ‰å‚æ•°å€¼è®¾ç½®ä¸ºå…¶é»˜è®¤å€¼ã€‚ SparseCategoricalCrossentropy æŸå¤±æœ‰å“ªäº›å‚æ•°ï¼Œå®ƒä»¬çš„é»˜è®¤å€¼æ˜¯ä»€ä¹ˆï¼Ÿï¼‰

</div>

ç°åœ¨è®©æˆ‘ä»¬å°è¯•è¿›è¡Œè®­ç»ƒã€‚ å¦‚ä»Šå·²ç»å¾—åˆ°æ¢¯åº¦ï¼Œæ‰€ä»¥å¸Œæœ›ï¼ˆæ­¤å¤„æ’­æ”¾ä»¤äººä¸å®‰çš„éŸ³ä¹ï¼‰åªéœ€è°ƒç”¨`model.fit()`ï¼Œä¸€åˆ‡éƒ½ä¼šæ­£å¸¸å·¥ä½œï¼

```python
  246/24543 [..............................] - ETA: 15:52 - loss: nan
``` 
å“¦ä¸ã€‚

`nan` ä¸æ˜¯ä¸€ä¸ªæ­£å¸¸çš„æŸå¤±å€¼ã€‚æˆ‘ä»¬å·²ç»æ£€æŸ¥äº†æˆ‘ä»¬çš„æ•°æ®ï¼Œçœ‹èµ·æ¥ä¸€åˆ‡æ­£å¸¸ã€‚å¦‚æœè¿™ä¸æ˜¯é—®é¢˜æ‰€åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥æ¥ä¸‹æ¥æ£€æŸ¥å“ªé‡Œå‘¢ï¼Ÿæ˜æ˜¾çš„ä¸‹ä¸€æ­¥æ˜¯...

#### æ£€æŸ¥æ¨¡å‹ 

`model.fit()` æ˜¯ Keras ä¸­ä¸€ä¸ªå¾ˆæ–¹ä¾¿çš„å‡½æ•°ï¼Œä½†è¿™ä¸ªå‡½æ•°ä¸€æ¬¡æ€§åšäº†å¾ˆå¤šäº‹æƒ…ï¼Œè¿™ä½¿å‡†ç¡®å®šä½é—®é¢˜å‘ç”Ÿçš„ä½ç½®å˜å¾—æ›´åŠ æ£˜æ‰‹ã€‚ å¦‚æœä½ æ­£åœ¨è°ƒè¯•æ¨¡å‹ï¼Œä¸€ä¸ªæ˜æ™ºçš„ç­–ç•¥æ˜¯è€ƒè™‘åªå°†ä¸€ä¸ªæ‰¹æ¬¡ä¼ é€’ç»™æ¨¡å‹ï¼Œå¹¶æŸ¥çœ‹è¯¥æ‰¹æ¬¡çš„è¯¦ç»†è¾“å‡ºã€‚ å¦‚æœæ¨¡å‹æŠ›å‡ºé”™è¯¯ï¼Œå¦ä¸€ä¸ªéå¸¸æœ‰ç”¨çš„æç¤ºæ˜¯å¯ä»¥å°†`run_eagerly=True`å‚æ•°ä¼ é€’ç»™ `compile()`ã€‚ è¿™ä¼šä½¿å®ƒè®­ç»ƒè¿‡ç¨‹å˜æ…¢å¾ˆå¤šï¼Œä½†å¯ä»¥ä½¿é”™è¯¯æ¶ˆæ¯æ›´å®¹æ˜“ç†è§£ï¼Œå› ä¸ºå®ƒä»¬ä¼šå‡†ç¡®åœ°æŒ‡å‡ºé—®é¢˜å‘ç”Ÿåœ¨æ¨¡å‹ä»£ç çš„å“ªä¸ªä½ç½®ã€‚

ä¸è¿‡ç›®å‰æˆ‘ä»¬è¿˜ä¸éœ€è¦ `run_eagerly`ã€‚è®©æˆ‘ä»¬å°†ä¹‹å‰å¾—åˆ°çš„ `batch` è¾“å…¥æ¨¡å‹ï¼Œå¹¶æŸ¥çœ‹è¾“å‡ºçš„ç»“æœï¼š

```python
model(batch)
```

```python
TFSequenceClassifierOutput(loss=<tf.Tensor: shape=(16,), dtype=float32, numpy=
array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan], dtype=float32)>, logits=<tf.Tensor: shape=(16, 2), dtype=float32, numpy=
array([[nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan],
       [nan, nan]], dtype=float32)>, hidden_states=None, attentions=None)
```

å—¯ï¼Œè¿™å¾ˆæ£˜æ‰‹ã€‚æ‰€æœ‰çš„å€¼éƒ½æ˜¯`nan`ï¼ä½†æ˜¯è¿™å¾ˆå¥‡æ€ªï¼Œå¯¹å§ï¼Ÿä¸ºä»€ä¹ˆæ‰€æœ‰çš„ logits éƒ½å˜æˆäº†`nan`ï¼Ÿ`nan`è¡¨ç¤ºâ€œä¸æ˜¯ä¸€ä¸ªæ•°å­—â€ã€‚ç»å¸¸å‡ºç°åœ¨æ‰§è¡Œéæ³•æ“ä½œæ—¶ï¼Œä¾‹å¦‚é™¤ä»¥é›¶ã€‚ä½†åœ¨æœºå™¨å­¦ä¹ ä¸­æœ‰å…³äº `nan` æœ‰ä¸€ä¸ªé‡è¦çš„ç»éªŒâ€”â€”è¿™ä¸ªå€¼å¾€å¾€ä¼šä¼ æ’­ã€‚å¦‚æœå°†ä¸€ä¸ªæ•°å­—ä¹˜ `nan`ï¼Œåˆ™è¾“å‡ºä¹Ÿæ˜¯ `nan`ã€‚å¦‚æœåœ¨è¾“å‡ºã€æŸå¤±æˆ–æ¢¯åº¦çš„ä»»ä½•åœ°æ–¹å¾—åˆ°ä¸€ä¸ªâ€œnanâ€ï¼Œé‚£ä¹ˆå®ƒä¼šè¿…é€Ÿä¼ æ’­åˆ°æ•´ä¸ªæ¨¡å‹ä¸­ã€‚å› ä¸ºå½“é‚£ä¸ªâ€œnanâ€å€¼é€šè¿‡ä½ çš„ç½‘ç»œä¼ æ’­å›æ¥æ—¶ï¼Œä¼šå¾—åˆ° `nan`æ¢¯åº¦ï¼Œå½“ä½¿ç”¨è¿™äº›æ¢¯åº¦è®¡ç®—æƒé‡æ›´æ–°æ—¶ï¼Œå°†è·å¾— `nan`æƒé‡ï¼Œè¿™äº›æƒé‡å°†è®¡ç®—æ›´å¤šçš„  `nan`è¾“å‡ºï¼å¾ˆå¿«æ•´ä¸ªç½‘ç»œå°±ä¼šå˜æˆä¸€ä¸ªå¤§å—`nan`ã€‚ä¸€æ—¦å‘ç”Ÿè¿™ç§æƒ…å†µï¼Œå°±å¾ˆéš¾çœ‹å‡ºé—®é¢˜æ˜¯ä»å“ªé‡Œå¼€å§‹çš„ã€‚æˆ‘ä»¬å¦‚ä½•ç¡®å®š`nan`æœ€å…ˆå‡ºç°çš„ä½ç½®å‘¢ï¼Ÿ

ç­”æ¡ˆæ˜¯â€œé‡æ–°åˆå§‹åŒ–â€æˆ‘ä»¬çš„æ¨¡å‹ã€‚ä¸€æ—¦æˆ‘ä»¬å¼€å§‹è®­ç»ƒï¼Œæˆ‘ä»¬å°±ä¼šåœ¨æŸä¸ªåœ°æ–¹å¾—åˆ°ä¸€ä¸ª `nan`ï¼Œå¹¶å¾ˆå¿«å°±ä¼šä¼ æ’­åˆ°æ•´ä¸ªæ¨¡å‹ä¸­ã€‚æ‰€ä»¥å¯ä»¥ä»æ£€æŸ¥ç‚¹åŠ è½½æ¨¡å‹è€Œä¸åšä»»ä½•æƒé‡æ›´æ–°ï¼Œè¿›è€Œæ’æŸ¥å‡ºä»å“ªé‡Œå¾—åˆ°ä¸€ä¸ª `nan` å€¼ï¼š

```python
model = TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint)
model(batch)
```

å½“æˆ‘ä»¬è¿è¡Œå®ƒæ—¶ï¼Œå¯ä»¥å¾—åˆ°ï¼š

```python
TFSequenceClassifierOutput(loss=<tf.Tensor: shape=(16,), dtype=float32, numpy=
array([0.6844486 ,        nan,        nan, 0.67127866, 0.7068601 ,
              nan, 0.69309855,        nan, 0.65531296,        nan,
              nan,        nan, 0.675402  ,        nan,        nan,
       0.69831556], dtype=float32)>, logits=<tf.Tensor: shape=(16, 2), dtype=float32, numpy=
array([[-0.04761693, -0.06509043],
       [-0.0481936 , -0.04556257],
       [-0.0040929 , -0.05848458],
       [-0.02417453, -0.0684005 ],
       [-0.02517801, -0.05241832],
       [-0.04514256, -0.0757378 ],
       [-0.02656011, -0.02646275],
       [ 0.00766164, -0.04350497],
       [ 0.02060014, -0.05655622],
       [-0.02615328, -0.0447021 ],
       [-0.05119278, -0.06928903],
       [-0.02859691, -0.04879177],
       [-0.02210129, -0.05791225],
       [-0.02363213, -0.05962167],
       [-0.05352269, -0.0481673 ],
       [-0.08141848, -0.07110836]], dtype=float32)>, hidden_states=None, attentions=None)
```

ç°åœ¨æˆ‘ä»¬åˆ°äº† logits ä¸­æ²¡æœ‰ `nan` å€¼çš„åœ°æ–¹ï¼Œè¿™ä»¤äººæ”¾å¿ƒã€‚ä½†æ˜¯æˆ‘ä»¬ç¡®å®åœ¨æŸå¤±ä¸­çœ‹åˆ°äº†ä¸€äº›â€œnanâ€å€¼ï¼Œè¿™äº›æ ·æœ¬æœ‰ä»€ä¹ˆç‰¹åˆ«ä¹‹å¤„å¯ä»¥å¯¼è‡´è¿™ä¸ªé—®é¢˜å—ï¼Ÿï¼ˆè¯·æ³¨æ„ï¼Œä½ è¿è¡Œæ­¤ä»£ç æ—¶å¯èƒ½ä¼šå¾—åˆ°ä¸åŒçš„ç´¢å¼•ï¼Œå› ä¸ºæ•°æ®é›†å·²è¢«éšæœºæ‰“ä¹±ï¼‰ï¼š

```python
import numpy as np

loss = model(batch).loss.numpy()
indices = np.flatnonzero(np.isnan(loss))
indices
```

```python
array([ 1,  2,  5,  7,  9, 10, 11, 13, 14])
```

è®©æˆ‘ä»¬çœ‹çœ‹è¿™äº›æ¥è‡ªæ ·æœ¬çš„è¾“å…¥idï¼š

```python
input_ids = batch["input_ids"].numpy()
input_ids[indices]
```

```python
array([[  101,  2007,  2032,  2001,  1037, 16480,  3917,  2594,  4135,
        23212,  3070,  2214, 10170,  1010,  2012,  4356,  1997,  3183,
         6838, 12953,  2039,  2000,  1996,  6147,  1997,  2010,  2606,
         1012,   102,  6838,  2001,  3294,  6625,  3773,  1996,  2214,
         2158,  1012,   102,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0],
       [  101,  1998,  6814,  2016,  2234,  2461,  2153,  1998, 13322,
         2009,  1012,   102,  2045,  1005,  1055,  2053,  3382,  2008,
         2016,  1005,  2222,  3046,  8103,  2075,  2009,  2153,  1012,
          102,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0],
       [  101,  1998,  2007,  1996,  3712,  4634,  1010,  2057,  8108,
         2025,  3404,  2028,  1012,  1996,  2616, 18449,  2125,  1999,
         1037,  9666,  1997,  4100,  8663, 11020,  6313,  2791,  1998,
         2431,  1011,  4301,  1012,   102,  2028,  1005,  1055,  5177,
         2110,  1998,  3977,  2000,  2832,  2106,  2025,  2689,  2104,
         2122,  6214,  1012,   102,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0],
       [  101,  1045,  2001,  1999,  1037, 13090,  5948,  2007,  2048,
         2308,  2006,  2026,  5001,  2043,  2026,  2171,  2001,  2170,
         1012,   102,  1045,  2001,  3564,  1999,  2277,  1012,   102,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0],
       [  101,  2195,  4279,  2191,  2039,  1996,  2181,  2124,  2004,
         1996,  2225,  7363,  1012,   102,  2045,  2003,  2069,  2028,
         2451,  1999,  1996,  2225,  7363,  1012,   102,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0],
       [  101,  2061,  2008,  1045,  2123,  1005,  1056,  2113,  2065,
         2009,  2428, 10654,  7347,  2030,  2009,  7126,  2256,  2495,
         2291,   102,  2009,  2003,  5094,  2256,  2495,  2291,  2035,
         2105,  1012,   102,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0],
       [  101,  2051,  1010,  2029,  3216,  2019,  2503,  3444,  1010,
         6732,  1996,  2265,  2038, 19840,  2098,  2125,  9906,  1998,
         2003,  2770,  2041,  1997,  4784,  1012,   102,  2051,  6732,
         1996,  2265,  2003,  9525,  1998,  4569,  1012,   102,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0],
       [  101,  1996, 10556,  2140, 11515,  2058,  1010,  2010,  2162,
         2252,  5689,  2013,  2010,  7223,  1012,   102,  2043,  1996,
        10556,  2140, 11515,  2058,  1010,  2010,  2252,  3062,  2000,
         1996,  2598,  1012,   102,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0],
       [  101, 13543,  1999,  2049,  6143,  2933,  2443,   102,  2025,
        13543,  1999,  6143,  2933,  2003,  2443,   102,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0]])
```

ç›®å‰æ²¡æœ‰ä»€ä¹ˆä¸å¯»å¸¸ä¹‹å¤„ã€‚ è®©æˆ‘ä»¬æ£€æŸ¥ä¸€ä¸‹æ ‡ç­¾ï¼š

```python
labels = batch['labels'].numpy()
labels[indices]
```

```python
array([2, 2, 2, 2, 2, 2, 2, 2, 2])
```

å•Šï¼æ‰€æœ‰çš„`nan` æ ·æœ¬éƒ½å…·æœ‰ç›¸åŒçš„æ ‡ç­¾ï¼Œå³æ ‡ç­¾ `2` ã€‚è¿™æ˜¯ä¸€ä¸ªéå¸¸æ˜æ˜¾çš„æç¤ºï¼Œ å½“æˆ‘ä»¬çš„æ ‡ç­¾ä¸º `2`æ—¶ï¼Œæˆ‘ä»¬ä¼šå¾—åˆ°lossä¸º `nan`ï¼Œè¿™æ˜¯æ£€æŸ¥æ¨¡å‹ä¸­æ ‡ç­¾æ•°é‡çš„å¥½æ—¶æœºï¼š

```python
model.config.num_labels
```

```python
2
```

è¿™è¡¨æ˜æ¨¡å‹è®¤ä¸ºåªæœ‰ä¸¤ä¸ªç±»ï¼Œä½†æ˜¯æ ‡ç­¾çš„å–å€¼èŒƒå›´æ˜¯ä» 0 åˆ° 2ï¼Œè¿™æ„å‘³ç€å®é™…ä¸Šæœ‰ä¸‰ä¸ªç±»åˆ«ï¼ˆå› ä¸º 0 ä¹Ÿæ˜¯ä¸€ä¸ªç±»ï¼‰ã€‚è¿™å°±æ˜¯æˆ‘ä»¬å¾—åˆ°`nan`çš„åŸå› â€”â€”å°è¯•è®¡ç®—ä¸å­˜åœ¨çš„ç±»çš„æŸå¤±ã€‚è®©æˆ‘ä»¬æ”¹å˜å®ƒå¹¶å†æ¬¡æ‹Ÿåˆæ¨¡å‹ï¼š

```python
model = TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=3)
model.compile(optimizer='adam')
model.fit(train_dataset)
```

```python
  869/24543 [>.............................] - ETA: 15:29 - loss: 1.1032
```

æˆ‘ä»¬æ­£åœ¨è®­ç»ƒï¼æ²¡æœ‰äº†`nan`ï¼ŒæŸå¤±ä¹Ÿåœ¨ä¸‹é™â€¦â€¦æœ‰ç‚¹ã€‚å¦‚æœä½ è§‚å¯Ÿä¸€æ®µæ—¶é—´ï¼Œä½ å¯èƒ½ä¼šå˜å¾—æœ‰ç‚¹ä¸è€çƒ¦ï¼Œè™½ç„¶æˆ‘ä»¬çš„æŸå¤±æ­£åœ¨ä¸€ç‚¹ç‚¹å‡å°‘ï¼Œä½†æ€»ä½“è¿˜æ˜¯ä¸€ç›´å±…é«˜ä¸ä¸‹ã€‚å…ˆåœæ­¢è®­ç»ƒå¹¶å°è¯•è€ƒè™‘å¯èƒ½å¯¼è‡´æ­¤é—®é¢˜çš„åŸå› ã€‚åœ¨è¿™ä¸€ç‚¹ä¸Šï¼Œæˆ‘ä»¬å¾ˆç¡®å®šæ•°æ®å’Œæ¨¡å‹éƒ½æ²¡æœ‰é—®é¢˜ï¼Œä½†æ˜¯æˆ‘ä»¬çš„æ¨¡å‹çš„å­¦ä¹ æ•ˆæœå¹¶ä¸æ˜¯ç‰¹åˆ«å¥½ã€‚è¿˜å‰©ä¸‹ä»€ä¹ˆï¼Ÿæ˜¯æ—¶å€™...

#### æ£€æŸ¥è¶…å‚æ•° 

å¦‚æœä½ å›å¤´çœ‹ä¸Šé¢çš„ä»£ç ï¼Œå¯èƒ½æ ¹æœ¬çœ‹ä¸åˆ°åˆ«çš„è¶…å‚æ•°ï¼Œé™¤äº†`batch_size`ï¼Œè€Œé‚£ä¼¼ä¹ä¸å¤ªå¯èƒ½æ˜¯é—®é¢˜çš„åŸå› ã€‚ä¸è¿‡ï¼Œä¸è¦è¢«è¯¯å¯¼ï¼›è¶…å‚æ•°å§‹ç»ˆå­˜åœ¨ï¼Œå¦‚æœä½ çœ‹ä¸åˆ°å®ƒä»¬ï¼Œé‚£åªæ„å‘³ç€ä½ ä¸çŸ¥é“å®ƒä»¬è®¾ç½®ä¸ºä»€ä¹ˆã€‚è¿™é‡Œå¼ºè°ƒä¸€ä¸ªå…³äº Keras çš„å…³é”®ç‚¹ï¼šå¦‚æœä½¿ç”¨å­—ç¬¦ä¸²è®¾ç½®æŸå¤±å‡½æ•°ã€ä¼˜åŒ–å™¨æˆ–æ¿€æ´»å‡½æ•°ï¼Œâ€œå®ƒçš„æ‰€æœ‰å‚æ•°éƒ½å°†è®¾ç½®ä¸ºé»˜è®¤å€¼â€ã€‚è¿™æ„å‘³ç€å³ä½¿ä½¿ç”¨å­—ç¬¦ä¸²éå¸¸æ–¹ä¾¿ï¼Œä½†åœ¨è¿™æ ·åšæ—¶åº”è¯¥éå¸¸å°å¿ƒï¼Œå› ä¸ºå®ƒå¾ˆå®¹æ˜“éšè—ä¸€äº›å…³é”®çš„é—®é¢˜ã€‚ï¼ˆå°è¯•ä¸Šé¢çš„å¯é€‰æŒ‘æˆ˜çš„ä»»ä½•äººéƒ½åº”è¯¥ç‰¹åˆ«æ³¨æ„è¿™ä¸€ç‚¹ã€‚ï¼‰

åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬åœ¨å“ªé‡Œä½¿ç”¨äº†å­—ç¬¦ä¸²å‚æ•°ï¼Ÿæœ€åˆæˆ‘ä»¬ä½¿ç”¨å­—ç¬¦ä¸²è®¾ç½®äº†æŸå¤±ï¼Œä½†ç°åœ¨æˆ‘ä»¬å·²ç»å»æ‰äº†ã€‚ä¸è¿‡ï¼Œæˆ‘ä»¬è¿˜ä½¿ç”¨å­—ç¬¦ä¸²è®¾ç½®äº†ä¼˜åŒ–å™¨ã€‚è¿™æ˜¯å¦å¯èƒ½å¯¹æˆ‘ä»¬éšè—äº†ä»€ä¹ˆå‘¢ï¼Ÿè®©æˆ‘ä»¬æŸ¥çœ‹ä¸€ä¸‹å®ƒçš„[å‚æ•°](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam)(https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam)ï¼š

è¿™é‡Œéœ€è¦æ³¨æ„å­¦ä¹ ç‡ã€‚å½“æˆ‘ä»¬åªä½¿ç”¨å­—ç¬¦ä¸²â€œadamâ€æ—¶ï¼Œå°†ä½¿ç”¨é»˜è®¤çš„å­¦ä¹ ç‡ 0.001ï¼ˆå³ 1e-3ï¼‰ã€‚è¿™å¯¹äºtransormeræ¨¡å‹æ¥è¯´å¤ªé«˜äº†ï¼Œä¸€èˆ¬æ¥è¯´ï¼Œæˆ‘ä»¬å»ºè®®å°è¯•å­¦ä¹ ç‡åœ¨ 1e-5 åˆ° 1e-4 ä¹‹é—´çš„å€¼ï¼›è¿™æ¯”å®é™…ä½¿ç”¨çš„å€¼å° 10å€ åˆ° 100å€ ä¹‹é—´ã€‚è¿™å¬èµ·æ¥å¯èƒ½æ˜¯ä¸€ä¸ªä¸»è¦é—®é¢˜ï¼Œæ‰€ä»¥è®©æˆ‘ä»¬å°è¯•å‡å°å®ƒã€‚ä¸ºæ­¤æˆ‘ä»¬éœ€è¦å¯¼å…¥`optimizer`å¯¹è±¡ã€‚è®©æˆ‘ä»¬ä»`checkpoint`é‡æ–°åˆå§‹åŒ–æ¨¡å‹ï¼Œä»¥é˜²é«˜å­¦ä¹ ç‡çš„è®­ç»ƒæŸåäº†æƒé‡ï¼š

```python
from tensorflow.keras.optimizers import Adam

model = TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint)
model.compile(optimizer=Adam(5e-5))
```

<div custom-style="Tip-green">

ğŸ’¡ä½ è¿˜å¯ä»¥ä»Transformers ä¸­å¯¼å…¥ `create_optimizer()` å‡½æ•°ï¼Œè¿™å°†æä¾›å…·æœ‰æ­£ç¡®çš„æƒé‡è¡°å‡å’Œå­¦ä¹ ç‡é¢„çƒ­å’Œè¡°å‡çš„ AdamW ä¼˜åŒ–å™¨ã€‚ æ­¤ä¼˜åŒ–å™¨é€šå¸¸ä¼šæ¯”ä½¿ç”¨é»˜è®¤ Adam ä¼˜åŒ–å™¨è·å¾—çš„ç»“æœç¨å¥½ä¸€äº›ã€‚

</div>

ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ”¹è¿›åçš„å­¦ä¹ ç‡æ¥æ‹Ÿåˆæ¨¡å‹ï¼š

```python
model.fit(train_dataset)
```

```python
319/24543 [..............................] - ETA: 16:07 - loss: 0.9718
```

ç°åœ¨è®­ç»ƒç»ˆäºçœ‹èµ·æ¥å¥æ•ˆäº†ã€‚å½“ä½ çš„æ¨¡å‹æ­£åœ¨è¿è¡Œä½†æŸå¤±æ²¡æœ‰ä¸‹é™ï¼ŒåŒæ—¶ç¡®å®šæ•°æ®æ²¡é—®é¢˜æ—¶ï¼Œå¯ä»¥æ£€æŸ¥å­¦ä¹ ç‡å’Œæƒé‡è¡°å‡ç­‰è¶…å‚æ•°ï¼Œå…¶ä¸­ä»»ä½•ä¸€ä¸ªè®¾ç½®å¾—å¤ªé«˜å¾ˆå¯èƒ½å¯¼è‡´è®­ç»ƒåœ¨é«˜æŸå¤±å€¼ä¸‹â€œåœæ»â€ã€‚

### å…¶ä»–æ½œåœ¨é—®é¢˜ 

æˆ‘ä»¬å·²ç»æ¶µç›–äº†ä¸Šé¢è„šæœ¬ä¸­æ‰€æœ‰çš„é—®é¢˜ï¼Œä½†æ˜¯è¿˜æœ‰å…¶ä»–ä¸€äº›å¸¸è§é”™è¯¯å¯èƒ½ä¼šé‡åˆ°ã€‚è®©æˆ‘ä»¬æ¥çœ‹ä¸€ä¸ªï¼ˆéå¸¸ä¸å®Œæ•´çš„ï¼‰åˆ—è¡¨ã€‚

#### å¤„ç†å†…å­˜ä¸è¶³é”™è¯¯ 

å†…å­˜ä¸è¶³çš„è¿¹è±¡æ˜¯`OOM when allocating tensor`ä¹‹ç±»çš„é”™è¯¯â€”â€”OOM æ˜¯`out of memory`çš„ç¼©å†™ã€‚ åœ¨å¤„ç†å¤§å‹è¯­è¨€æ¨¡å‹æ—¶ï¼Œè¿™æ˜¯ä¸€ä¸ªéå¸¸å¸¸è§çš„é”™è¯¯ã€‚ å¦‚é‡æ­¤ç§æƒ…å†µï¼Œå¯ä»¥å°† batch size å‡åŠå¹¶é‡è¯•ã€‚ ä½†æœ‰äº›å°ºå¯¸éå¸¸å¤§ï¼Œæ¯”å¦‚å…¨å°ºå¯¸ GPT-2 çš„å‚æ•°ä¸º 1.5Bï¼Œè¿™æ„å‘³ç€ä½ å°†éœ€è¦ 6 GB çš„å†…å­˜æ¥å­˜å‚¨æ¨¡å‹ï¼Œå¦å¤–éœ€è¦ 6 GB çš„å†…å­˜ç”¨äºæ¢¯åº¦ä¸‹é™ï¼ æ— è®ºä½ ä½¿ç”¨ä»€ä¹ˆ batch size ï¼Œè®­ç»ƒå®Œæ•´çš„ GPT-2 æ¨¡å‹é€šå¸¸éœ€è¦è¶…è¿‡ 20 GB çš„ VRAMï¼Œç„¶è€Œè¿™åªæœ‰å°‘æ•° GPU æ‰å¯ä»¥åšåˆ°ã€‚ åƒ`distilbert-base-cased`è¿™æ ·æ›´è½»é‡çº§çš„æ¨¡å‹æ›´å®¹æ˜“è¿è¡Œï¼Œè®­ç»ƒä¹Ÿæ›´å¿«ã€‚

<div custom-style="Tip-green">

åœ¨è¯¾ç¨‹çš„ä¸‹ä¸€éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å°†ä»‹ç»æ›´å…ˆè¿›çš„æŠ€æœ¯ï¼Œè¿™äº›æŠ€æœ¯å¯ä»¥å¸®åŠ©ä½ å‡å°‘å†…å­˜å ç”¨å¹¶å¾®è°ƒæœ€å¤§çš„æ¨¡å‹ã€‚

</div>

#### TensorFlow ğŸ¦›é¥¿é¥¿ 

TensorFlow ä¸€ä¸ªä¸ä¼—ä¸åŒçš„ç‚¹åœ¨äºå®ƒä¼šåœ¨ä½ åŠ è½½æ¨¡å‹æˆ–è¿›è¡Œä»»ä½•è®­ç»ƒåç«‹å³ä¸ºè‡ªå·±åˆ†é…æ‰€æœ‰çš„GPU å†…å­˜ï¼Œæ ¹æ®éœ€è¦åˆ†é…è¯¥å†…å­˜ã€‚è¿™ä¸å…¶ä»–æ¡†æ¶çš„è¡Œä¸ºä¸åŒï¼Œä¾‹å¦‚ PyTorchæ ¹æ® CUDA çš„éœ€è¦åˆ†é…å†…å­˜ï¼Œè€Œä¸æ˜¯åœ¨å†…éƒ¨è¿›è¡Œã€‚ TensorFlow æ–¹æ³•çš„ä¸€ä¸ªä¼˜ç‚¹æ˜¯å½“ä½ è€—å°½å†…å­˜æ—¶ï¼Œå®ƒé€šå¸¸ä¼šç»™å‡ºæœ‰ç”¨çš„é”™è¯¯ï¼Œå¹¶ä¸”å¯ä»¥ä»è¯¥çŠ¶æ€æ¢å¤è€Œä¸ä¼šå¯¼è‡´æ•´ä¸ª CUDA å†…æ ¸å´©æºƒã€‚åŒæ—¶ä¹Ÿä»£è¡¨ï¼šå¦‚æœåŒæ—¶è¿è¡Œä¸¤ä¸ª TensorFlow è¿›ç¨‹ï¼Œé‚£ä¹ˆåŠ¿å¿…ä¼šé‡åˆ°éº»çƒ¦ã€‚

å¦‚æœä½ åœ¨ Colab ä¸Šè¿è¡Œåˆ™æ— éœ€æ‹…å¿ƒè¿™ä¸€ç‚¹ï¼Œä½†å¦‚æœåœ¨æœ¬åœ°è¿è¡Œï¼Œè¿™ç»å¯¹æ˜¯åº”è¯¥å°å¿ƒçš„äº‹æƒ…ã€‚ç‰¹åˆ«è¦æ³¨æ„ï¼Œå…³é—­Notebooké€‰é¡¹å¡å¹¶ä¸ä¸€å®šä¼šå…³é—­è¯¥Notebook ï¼éœ€è¦é€‰æ‹©æ­£åœ¨è¿è¡Œçš„ Notebook ï¼ˆå¸¦æœ‰ç»¿è‰²å›¾æ ‡çš„ Notebook ï¼‰å¹¶åœ¨ç›®å½•åˆ—è¡¨ä¸­æ‰‹åŠ¨å…³é—­å®ƒä»¬ã€‚ä»»ä½•ä½¿ç”¨ TensorFlow çš„æ­£åœ¨è¿è¡Œçš„ Notebook ä»å¯èƒ½å ç”¨å¤§é‡ GPU å†…å­˜ï¼Œè¿™æ„å‘³ç€ä½ å¯åŠ¨çš„ä»»ä½•æ–° Notebook éƒ½å¯èƒ½ä¼šé‡åˆ°ä¸€äº›éå¸¸å¥‡æ€ªçš„é—®é¢˜ã€‚

å¦‚æœä½ å¼€å§‹è¿è¡Œä¹‹å‰æ­£ç¡®çš„ä»£ç å´æ”¶åˆ°æœ‰å…³ CUDAã€BLAS æˆ– cuBLAS çš„é”™è¯¯ï¼Œç½ªé­ç¥¸é¦–é€šå¸¸æ˜¯ç±»ä¼¼çš„ã€‚ä½ å¯ä»¥ä½¿ç”¨ç±»ä¼¼ `nvidia-smi` çš„å‘½ä»¤æ¥æ£€æŸ¥ â€”â€” å½“ä½ å…³é—­æˆ–é‡æ–°å¯åŠ¨å½“å‰ Notebook æ—¶ï¼Œå¤§éƒ¨åˆ†å†…å­˜æ˜¯å¦ç©ºé—²æˆ–è€…æ˜¯å¦ä»åœ¨ä½¿ç”¨ä¸­ï¼Ÿå¦‚æœå®ƒä»åœ¨ä½¿ç”¨ä¸­ä»£è¡¨æœ‰å…¶ä»–ä¸œè¥¿åœ¨å ç”¨ã€‚

#### æ£€æŸ¥ä½ çš„æ•°æ®ï¼ˆå†æ¬¡ï¼ï¼‰ 

åœ¨ç†è®ºä¸Šï¼Œå¦‚æœæ•°æ®ä¸­å­˜åœ¨å¯ä»¥æŒ–æ˜åˆ°çŸ¥è¯†æ—¶ï¼Œä½ çš„æ¨¡å‹æ‰ä¼šå­¦åˆ°ä¸€äº›ä¸œè¥¿ã€‚ å¦‚æœå­˜åœ¨æŸåæ•°æ®çš„é”™è¯¯æˆ–æ ‡ç­¾æ˜¯éšæœºå±æ€§çš„ï¼Œé‚£ä¹ˆå¾ˆå¯èƒ½ä¸ä¼šåœ¨æ•°æ®é›†ä¸Šè·å¾—ä»»ä½•çŸ¥è¯†ã€‚è¿™é‡Œä¸€ä¸ªæœ‰ç”¨çš„å·¥å…·æ˜¯`tokenizer.decode()`ï¼Œ å°† `input_ids` è½¬æ¢å›å­—ç¬¦ä¸²ï¼Œå¯ä»¥é€šè¿‡è¿™ä¸ªå‡½æ•°æ¥æŸ¥çœ‹æ•°æ®å’Œè®­ç»ƒæ•°æ®æ˜¯å¦æ­£åœ¨æ•™æˆä½ å¸Œæœ›å®ƒæ•™æˆçš„å†…å®¹ã€‚ ä¾‹å¦‚ï¼Œåƒæˆ‘ä»¬ä¸Šé¢æ‰€åšçš„é‚£æ ·ä» `tf.data.Dataset` ä¸­è·å– `batch` åï¼Œå¯ä»¥åƒè¿™æ ·è§£ç ç¬¬ä¸€ä¸ªå…ƒç´ ï¼š

```python
input_ids = batch["input_ids"].numpy()
tokenizer.decode(input_ids[0])
```

æ¥ç€å¯ä»¥ç”¨ç¬¬ä¸€ä¸ªæ ‡ç­¾è¿›è¡Œæ¯”è¾ƒï¼Œå°±åƒè¿™æ ·ï¼š

```python
labels = batch["labels"].numpy()
label = labels[0]
```
ä¸€æ—¦å¯ä»¥åƒè¿™æ ·æŸ¥çœ‹æ•°æ®ï¼Œå¯ä»¥æŒ‰ç…§ä»¥ä¸‹é—®é¢˜è¿›è¡Œæ£€æŸ¥ï¼š

- è§£ç åçš„æ•°æ®æ˜¯å¦å¯ä»¥ç†è§£ï¼Ÿ
- ä½ è®¤åŒè¿™äº›æ ‡ç­¾å—ï¼Ÿ
- æœ‰æ²¡æœ‰ä¸€ä¸ªæ ‡ç­¾æ¯”å…¶ä»–æ ‡ç­¾æ›´å¸¸è§ï¼Ÿ
- å¦‚æœæ¨¡å‹é¢„æµ‹éšæœºçš„ç­”æ¡ˆ/æ€»æ˜¯ç›¸åŒçš„ç­”æ¡ˆï¼Œé‚£ä¹ˆæŸå¤±/è¯„ä¼°æŒ‡æ ‡åº”è¯¥æ˜¯å¤šå°‘ï¼Ÿ

æ£€æŸ¥æ•°æ®åï¼Œå¯ä»¥æ£€æŸ¥æ¨¡å‹çš„ä¸€äº›é¢„æµ‹å¹¶å¯¹å…¶è¿›è¡Œè§£ç ã€‚ å¦‚æœæ¨¡å‹æ€»æ˜¯é¢„æµ‹åŒæ ·çš„ç±»åˆ«ï¼Œé‚£å¯èƒ½æ˜¯å› ä¸ºä½ çš„æ•°æ®é›†åå‘ä¸€ä¸ªç±»åˆ«ï¼ˆé’ˆå¯¹åˆ†ç±»é—®é¢˜ï¼‰ï¼› è¿‡é‡‡æ ·ç¨€æœ‰ç±»ç­‰æŠ€æœ¯å¯èƒ½ä¼šå¯¹è§£å†³è¿™ç§é—®é¢˜æœ‰å¸®åŠ©ã€‚æˆ–è€…ï¼Œè¿™ä¹Ÿå¯èƒ½æ˜¯ç”±äºè®­ç»ƒé—®é¢˜ï¼ˆå¦‚é”™è¯¯çš„è¶…å‚æ•°è®¾ç½®ï¼‰å¼•èµ·çš„ã€‚

å¦‚æœåœ¨åˆå§‹æ¨¡å‹ä¸Šè·å¾—çš„æŸå¤±/è¯„ä¼°æŒ‡æ ‡ä¸æœŸæœ›çš„éšæœºé¢„æµ‹çš„æŸå¤±/è¯„ä¼°æŒ‡æ ‡éå¸¸ä¸åŒï¼Œè¯·ä»”ç»†æ£€æŸ¥æŸå¤±æˆ–è¯„ä¼°æŒ‡æ ‡çš„è®¡ç®—æ–¹å¼æ˜¯å¦å­˜åœ¨é”™è¯¯ã€‚ å¦‚æœä½¿ç”¨æœ€åæ·»åŠ çš„å¤šä¸ªæŸå¤±ï¼Œè¯·ç¡®ä¿å®ƒä»¬å…·æœ‰ç›¸åŒçš„å°ºå¯¸ã€‚

å½“ä½ ç¡®å®šä½ çš„æ•°æ®æ˜¯å®Œç¾çš„ä¹‹åï¼Œå¯ä»¥é€šè¿‡ä¸€ä¸ªç®€å•çš„æµ‹è¯•æ¥æŸ¥çœ‹æ¨¡å‹æ˜¯å¦èƒ½å¤Ÿå¯¹å…¶è¿›è¡Œè®­ç»ƒã€‚

#### åœ¨ä¸€ä¸ª batch ä¸Šè¿‡æ‹Ÿåˆæ¨¡å‹ 

è¿‡æ‹Ÿåˆé€šå¸¸æ˜¯åœ¨è®­ç»ƒæ—¶å°½é‡é¿å…çš„äº‹æƒ…ï¼Œå› ä¸ºè¿™æ„å‘³ç€æ¨¡å‹æ²¡æœ‰è¯†åˆ«å¹¶å­¦ä¹ æˆ‘ä»¬æƒ³è¦çš„ä¸€èˆ¬ç‰¹å¾ï¼Œè€Œåªæ˜¯è®°ä½äº†è®­ç»ƒæ ·æœ¬ã€‚ ä½†ä¸€éåˆä¸€éåœ°å°è¯•åœ¨ä¸€ä¸ª batch ä¸Šè®­ç»ƒæ¨¡å‹å¯ä»¥æ£€æŸ¥æ„å»ºçš„é—®é¢˜æ˜¯å¦å¯ä»¥é€šè¿‡è®­ç»ƒçš„æ¨¡å‹æ¥è§£å†³ã€‚ å®ƒè¿˜å°†å¸®åŠ©æŸ¥çœ‹ä½ çš„åˆå§‹å­¦ä¹ ç‡æ˜¯å¦å¤ªé«˜ã€‚

ä¸€æ—¦ä½ å®šä¹‰äº†æ¨¡å‹ï¼Œåªéœ€è·å–ä¸€ä¸ª batch è®­ç»ƒæ•°æ®ï¼Œç„¶åå°†è¿™ä¸ª batch è§†ä¸ºä½ çš„æ•´ä¸ªæ•°æ®é›†ï¼Œå¹¶åœ¨ä¸Šé¢æ‹Ÿåˆå¤§é‡epochï¼š

```python
for batch in train_dataset:
    break

# ç¡®ä¿å·²ç»è¿è¡Œäº† model.compile() å¹¶è®¾ç½®äº†ä¼˜åŒ–å™¨å’ŒæŸå¤±/æŒ‡æ ‡

model.fit(batch, epochs=20)
```

<div custom-style="Tip-green">

ğŸ’¡ å¦‚æœè®­ç»ƒæ•°æ®ä¸å¹³è¡¡ï¼Œè¯·ç¡®ä¿æ„å»ºçš„è¿™ä¸ª batch åŒ…å«æ‰€æœ‰æ ‡ç­¾çš„è®­ç»ƒæ•°æ®ã€‚

</div>

ç”Ÿæˆçš„æ¨¡å‹åœ¨ä¸€ä¸ª batch ä¸Šåº”è¯¥æœ‰æ¥è¿‘å®Œç¾çš„ç»“æœï¼ŒæŸå¤±è¿…é€Ÿä¸‹é™åˆ° 0ï¼ˆæˆ–ä½ æ­£åœ¨ä½¿ç”¨çš„æŸå¤±çš„æœ€å°å€¼ï¼‰ã€‚

å¦‚æœä½ æ²¡æœ‰è®¾æ³•è®©ä½ çš„æ¨¡å‹è·å¾—è¿™æ ·çš„å®Œç¾ç»“æœï¼Œè¿™æ„å‘³ç€æ„å»ºé—®é¢˜æˆ–æ•°æ®çš„æ–¹å¼æœ‰é—®é¢˜ã€‚åªæœ‰å½“ä½ é€šè¿‡äº†è¿‡åº¦æ‹Ÿåˆæµ‹è¯•æ—¶ï¼Œæ‰èƒ½ç¡®å®šä½ çš„æ¨¡å‹å®é™…å¯ä»¥å­¦åˆ°ä¸€äº›ä¸œè¥¿ã€‚

<div custom-style="Tip-yellow">

âš ï¸ åœ¨æ­¤æµ‹è¯•ä¹‹åï¼Œä½ å°†ä¸å¾—ä¸é‡æ–°åˆ›å»ºæ¨¡å‹å¹¶é‡æ–°ç¼–è¯‘å› ä¸ºå¾—åˆ°çš„æ¨¡å‹å¯èƒ½æ— æ³•æ¢å¤å¹¶åœ¨å®Œæ•´æ•°æ®é›†ä¸Šå­¦åˆ°æœ‰ç”¨çš„ä¸œè¥¿ã€‚

</div>

#### åœ¨ä½ æœ‰ç¬¬ä¸€ä¸ª baseline æ¨¡å‹ä¹‹å‰ä¸è¦è°ƒæ•´ä»»ä½•ä¸œè¥¿ 

è¶…å‚æ•°è°ƒæ•´æ€»æ˜¯è¢«å¼ºè°ƒä¸ºæœºå™¨å­¦ä¹ ä¸­æœ€éš¾çš„éƒ¨åˆ†ï¼Œä½†è¿™åªæ˜¯å¸®åŠ©ä½ åœ¨æŒ‡æ ‡ä¸Šè·å¾—ä¸€ç‚¹ç‚¹æå‡çš„æœ€åä¸€æ­¥ã€‚ ä¾‹å¦‚å°†é»˜è®¤çš„ Adam å­¦ä¹ ç‡ 1e-3 ä¸ Transformer æ¨¡å‹ä¸€èµ·ä½¿ç”¨æ—¶ï¼Œå½“ç„¶ä¼šä½¿å­¦ä¹ è¿›è¡Œå¾—éå¸¸ç¼“æ…¢æˆ–å®Œå…¨åœæ­¢ï¼Œä½†å¤§å¤šæ•°æ—¶å€™åˆç†çš„è¶…å‚æ•°ï¼Œä¾‹å¦‚ä» 1e-5 åˆ° 5e-5 çš„å­¦ä¹ ç‡ä¼šå¾ˆå¥½åœ°å¸¦æ¥å¥½çš„ç»“æœã€‚æ‰€ä»¥ï¼Œåœ¨ä½ æœ‰äº† baseline æ¨¡å‹ä¹‹å‰ï¼Œè¯·ä¸è¦è¯•å›¾è¿›è¡Œè€—æ—¶ä¸”æ˜‚è´µçš„è¶…å‚æ•°æœç´¢ã€‚

ä¸€æ—¦ä½ æœ‰ä¸€ä¸ªè¶³å¤Ÿå¥½çš„æ¨¡å‹ï¼Œå°±å¯ä»¥å¼€å§‹å¾®è°ƒä¸€äº›å†…å®¹ã€‚ å°½é‡é¿å…ä½¿ç”¨ä¸åŒçš„è¶…å‚æ•°å¯åŠ¨ä¸€åƒæ¬¡è¿è¡Œï¼Œè€Œæ˜¯æ¯”è¾ƒä¸€ä¸ªè¶…å‚æ•°çš„ä¸åŒå€¼çš„å‡ æ¬¡è¿è¡Œï¼Œä»è€Œäº†è§£å“ªä¸ªå½±å“æœ€å¤§ã€‚

å¦‚æœä½ æ­£åœ¨è°ƒæ•´æ¨¡å‹æœ¬èº«ï¼Œè¯·ä¿æŒç®€å•ï¼Œä¸è¦å°è¯•ä»»ä½•ä½ æ— æ³•åˆç†è¯æ˜çš„äº‹æƒ…ï¼Œç¡®ä¿é€šè¿‡è¿‡æ‹Ÿåˆæµ‹è¯•æ¥éªŒè¯ä½ çš„æ›´æ”¹æ²¡æœ‰äº§ç”Ÿä»»ä½•æ„å¤–åæœã€‚

#### è¯·æ±‚å¸®å¿™ 

å¸Œæœ›ä½ ä¼šåœ¨æœ¬èŠ‚ä¸­æ‰¾åˆ°ä¸€äº›å¯ä»¥å¸®åŠ©ä½ è§£å†³é—®é¢˜çš„å»ºè®®ï¼Œé™¤æ­¤ä¹‹å¤–å¯ä»¥éšæ—¶åœ¨ [è®ºå›](https://discuss.huggingface.co/)(https://discuss.huggingface.co/) ä¸Šå‘ç¤¾åŒºæé—®ã€‚

ä»¥ä¸‹æ˜¯ä¸€äº›å¯èƒ½æœ‰ç”¨çš„é¢å¤–èµ„æºï¼š

-Joel Grus çš„ [â€œä½œä¸ºå·¥ç¨‹æœ€ä½³å®è·µå·¥å…·çš„å†ç°æ€§â€](https://docs.google.com/presentation/d/1yHLPvPhUs2KGI5ZWo0sU-PKU3GimAk3iTsI38Z-B5Gw/edit#slide=id.p)(https://docs.google.com/presentation/d/1yHLPvPhUs2KGI5ZWo0sU-PKU3GimAk3iTsI38Z-B5Gw/edit#slide=id.p)
- Cecelia Shao çš„ [â€œç¥ç»ç½‘ç»œè°ƒè¯•æ¸…å•â€](https://towardsdatascience.com/checklist-for-debugging-neural-networks-d8b2a9434f21)(https://towardsdatascience.com/checklist-for-debugging-neural-networks-d8b2a9434f21) 
- Chase Roberts çš„ [â€œå¦‚ä½•å¯¹æœºå™¨å­¦ä¹ ä»£ç è¿›è¡Œå•å…ƒæµ‹è¯•â€](https://medium.com/@keeper6928/how-to-unit-test-machine-learning-code-57cf6fd81765)(https://medium.com/@keeper6928/how-to-unit-test-machine-learning-code-57cf6fd81765) 
- Andrej Karpathy çš„ [â€œè®­ç»ƒç¥ç»ç½‘ç»œçš„ç§˜è¯€â€](http://karpathy.github.io/2019/04/25/recipe)(http://karpathy.github.io/2019/04/25/recipe)

å½“ç„¶ï¼Œå¹¶éä½ åœ¨è®­ç»ƒç¥ç»ç½‘ç»œæ—¶é‡åˆ°çš„æ¯ä¸ªé—®é¢˜éƒ½æ˜¯ä½ è‡ªå·±çš„é”™ï¼å¦‚æœä½ åœ¨ Transformers æˆ– Datasets åº“ä¸­é‡åˆ°çœ‹èµ·æ¥ä¸æ­£ç¡®çš„å†…å®¹è€Œå¯¼è‡´æ— æ³•è§£å†³çš„é—®é¢˜ï¼Œè¯·åŠæ—¶å‘ŠçŸ¥æˆ‘ä»¬ã€‚åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†å‡†ç¡®è§£é‡Šå¦‚ä½•è¿›è¡Œè¿™ä¸€æ­¥ã€‚

## 9.5 å¦‚ä½•å†™ä¸€ä¸ªå¥½é—®é¢˜ 

å½“ä½ åœ¨ä½¿ç”¨ Hugging Face åº“æ—¶é‡åˆ°äº†ä¸æ­£å¸¸çš„æƒ…å†µï¼Œä½ åº”è¯¥åŠæ—¶å‘Šè¯‰æˆ‘ä»¬ï¼Œè¿™æ ·æˆ‘ä»¬æ‰èƒ½ä¿®å¤å®ƒï¼ˆå¯¹äºä»»ä½•å¼€æºåº“éƒ½æ˜¯ä¸€æ ·ï¼‰ã€‚å¦‚æœä½ ä¸ç¡®å®š bug æ˜¯åœ¨ä½ è‡ªå·±çš„ä»£ç ä¸­è¿˜æ˜¯åœ¨æˆ‘ä»¬çš„åº“ä¸­ï¼Œé¦–å…ˆå¯ä»¥åœ¨ [è®ºå›](https://discuss.huggingface.co)(https://discuss.huggingface.co) è¿›è¡Œæœç´¢ã€‚è®ºå›ä¼šå¸®åŠ©ä½ æ‰¾å‡ºé—®é¢˜æ‰€åœ¨ï¼ŒHugging Face å›¢é˜Ÿä¹Ÿä¼šå¯†åˆ‡å…³æ³¨é‚£é‡Œçš„è®¨è®ºã€‚

å½“ä½ ç¡®å®šæ‰‹å¤´ä¸Šæœ‰ä¸€ä¸ª bug æ—¶ï¼Œç¬¬ä¸€æ­¥æ˜¯æ„å»ºä¸€ä¸ªæœ€å°å¯å¤ç°çš„ç¤ºä¾‹ã€‚

### åˆ›å»ºä¸€ä¸ªæœ€å°çš„å¯é‡ç°ç¤ºä¾‹ 

åˆ›å»ºä¸€ä¸ªæœ€å°å¯å¤ç°çš„ç¤ºä¾‹éå¸¸é‡è¦ï¼Œå› ä¸º Hugging Face å›¢é˜Ÿä¸­æ²¡æœ‰äººæ˜¯é­”æœ¯å¸ˆï¼ˆè‡³å°‘ç›®å‰è¿˜æ²¡æœ‰ï¼‰ï¼Œä»–ä»¬ä¸èƒ½ä¿®å¤ä»–ä»¬çœ‹ä¸åˆ°çš„é—®é¢˜ã€‚ä¸€ä¸ªæœ€å°å¯å¤ç°çš„ç¤ºä¾‹åº”è¯¥æ˜¯å¯å¤ç°çš„ï¼Œè¿™æ„å‘³ç€å®ƒä¸åº”ä¾èµ–äºä½ å¯èƒ½æœ‰çš„ä»»ä½•å¤–éƒ¨æ–‡ä»¶æˆ–æ•°æ®ã€‚å°è¯•ç”¨ä¸€äº›çœ‹èµ·æ¥åƒçœŸå®æ•°æ®çš„è™šæ‹Ÿå€¼æ›¿æ¢ä½ æ­£åœ¨ä½¿ç”¨çš„æ•°æ®ï¼Œå¹¶ä¸”ä»ç„¶äº§ç”Ÿç›¸åŒçš„é”™è¯¯ã€‚

<div custom-style="Tip-red">

ğŸš¨Transformers ä»“åº“ä¸­æœ‰å¾ˆå¤šæœªè§£å†³çš„é—®é¢˜ï¼Œå› ä¸ºæ— æ³•è®¿é—®å¤ç°è¿™äº›é—®é¢˜çš„æ•°æ®ã€‚

</div>

ä¸€æ—¦ä½ æœ‰äº†ä¸€ä¸ªè‡ªåŒ…å«çš„ç¤ºä¾‹ï¼Œä½ å¯ä»¥å°è¯•å°†å…¶è¿›ä¸€æ­¥ç®€åŒ–ï¼Œæ„å»ºæˆ‘ä»¬æ‰€è¯´çš„â€œæœ€å°å¯å¤ç°ç¤ºä¾‹â€ã€‚è™½ç„¶è¿™éœ€è¦ä½ å¤šåšä¸€äº›å·¥ä½œï¼Œä½†å¦‚æœä½ æä¾›äº†ä¸€ä¸ªç®€æ´æ˜äº†çš„ bug å¤ç°ï¼Œä½ å‡ ä¹å¯ä»¥è‚¯å®šä¼šå¾—åˆ°å¸®åŠ©å’Œä¿®å¤ã€‚

å¦‚æœä½ æ„Ÿè§‰è¶³å¤Ÿè‡ªä¿¡ï¼Œå¯ä»¥æ£€æŸ¥ä¸€ä¸‹ä½ çš„ bug å‘ç”Ÿçš„æºä»£ç ã€‚ä½ å¯èƒ½ä¼šæ‰¾åˆ°è§£å†³é—®é¢˜çš„æ–¹æ³•ï¼ˆåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½ ç”šè‡³å¯ä»¥å‘èµ·ä¸€ä¸ªä¿®å¤å®ƒçš„æ‹‰å–è¯·æ±‚ï¼‰ï¼Œä½†æ›´ä¸€èˆ¬åœ°è¯´ï¼Œè¿™å¯ä»¥å¸®åŠ©ç»´æŠ¤äººå‘˜åœ¨é˜…è¯»ä½ çš„æŠ¥å‘Šæ—¶æ›´å¥½åœ°ç†è§£æºä»£ç ã€‚

### å¡«å†™é—®é¢˜æ¨¡æ¿ 

å½“ä½ æäº¤é—®é¢˜æ—¶ï¼Œéœ€è¦å¡«å†™ä¸€ä¸ªæ¨¡æ¿ã€‚æˆ‘ä»¬å°†åœ¨è¿™é‡Œéµå¾ª [Transformers issues](https://github.com/huggingface/transformers/issues/new/choose)(https://github.com/huggingface/transformers/issues/new/choose) çš„æ¨¡æ¿ï¼Œä½†å¦‚æœä½ åœ¨å…¶ä»–ä»“åº“ä¸­æŠ¥å‘Šé—®é¢˜ï¼Œä¹Ÿéœ€è¦æä¾›ç›¸åŒç±»å‹çš„ä¿¡æ¯ã€‚è¯·ä¸è¦å°†æ¨¡æ¿ç•™ç©ºï¼ŒèŠ±æ—¶é—´å¡«å†™æ¨¡æ¿å°†æœ€å¤§ç¨‹åº¦åœ°å¢åŠ ä½ å¾—åˆ°ç­”æ¡ˆå’Œè§£å†³é—®é¢˜çš„æœºä¼šã€‚

ä¸€èˆ¬æ¥è¯´ï¼Œå½“æäº¤é—®é¢˜æ—¶ï¼Œè¦ä¿æŒç¤¼è²Œã€‚è¿™æ˜¯ä¸€ä¸ªå¼€æºé¡¹ç›®ï¼Œæ‰€ä»¥ä½ ä½¿ç”¨çš„æ˜¯å…è´¹è½¯ä»¶ï¼Œæ²¡æœ‰äººæœ‰ä¹‰åŠ¡å¸®åŠ©ä½ ã€‚ä½ å¯ä»¥åœ¨ä½ çš„é—®é¢˜ä¸­åŒ…å«ä½ è®¤ä¸ºåˆç†çš„æ‰¹è¯„ï¼Œä½†ç»´æŠ¤è€…å¯èƒ½ä¼šå¯¹æ­¤æ„Ÿåˆ°ä¸æ»¡ï¼Œå¹¶ä¸”ä¸ä¼šé¦–å…ˆå¸®åŠ©ä½ ã€‚ç¡®ä¿ä½ é˜…è¯»äº†è¯¥é¡¹ç›®çš„ [è¡Œä¸ºå‡†åˆ™](https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md)(https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md) ã€‚

#### æä¾›ç¯å¢ƒä¿¡æ¯ 

Transformers æä¾›äº†ä¸€ä¸ªå®ç”¨ç¨‹åºæ¥è·å–æœ‰å…³äºä½ ç¯å¢ƒçš„æ‰€æœ‰ä¿¡æ¯ã€‚åªéœ€åœ¨ç»ˆç«¯ä¸­è¾“å…¥ä»¥ä¸‹å†…å®¹ï¼š

```python
transformers-cli env
```

å°†å¾—åˆ°ä»¥ä¸‹è¾“å‡ºï¼š

```python
Copy-and-paste the text below in your GitHub issue and FILL OUT the two last points.

- `transformers` version: 4.12.0.dev0
- Platform: Linux-5.10.61-1-MANJARO-x86_64-with-arch-Manjaro-Linux
- Python version: 3.7.9
- PyTorch version (GPU?): 1.8.1+cu111 (True)
- Tensorflow version (GPU?): 2.5.0 (True)
- Flax version (CPU?/GPU?/TPU?): 0.3.4 (cpu)
- Jax version: 0.2.13
- JaxLib version: 0.1.65
- Using GPU in script?: <fill in>
- Using distributed or parallel set-up in script?: <fill in>
```

å¦‚æœä½ åœ¨ notebook å•å…ƒæ‰§è¡Œå®ƒï¼Œä½ è¿˜å¯ä»¥åœ¨ `transformers-cli env` å‘½ä»¤å¼€å§‹å‰æ·»åŠ ä¸€ä¸ª `!` ï¼Œç„¶åæŠŠç»“æœå¤åˆ¶åˆ°ä½ é—®é¢˜å¸–å­çš„å¼€å¤´ã€‚

#### æ ‡è®°ç›¸å…³äººå‘˜ 

ä½¿ç”¨ `@` åè·Ÿä¸Š GitHub ç”¨æˆ·åæ¥æ ‡è®°ä»–äººï¼Œå¯ä»¥å‘ä»–ä»¬å‘é€é€šçŸ¥ï¼Œè¿™æ ·ä»–ä»¬å°±ä¼šçœ‹åˆ°ä½ çš„é—®é¢˜æ¥å°½å¯èƒ½æ›´å¿«åœ°å›å¤ä½ ã€‚å¦‚æœä½ æ ‡è®°çš„äººä¸ä½ çš„é—®é¢˜æ²¡æœ‰ç›´æ¥è”ç³»è¯·è°¨æ…ä½¿ç”¨ï¼Œå› ä¸ºä»–ä»¬å¯èƒ½ä¸å–œæ¬¢æ”¶åˆ°é€šçŸ¥ã€‚å¦‚æœä½ æŸ¥çœ‹äº†ä¸ä½ çš„é”™è¯¯ç›¸å…³çš„æºæ–‡ä»¶ï¼Œå°±åº”è¯¥æ ‡è®°ä¸Šä¸€æ¬¡å¯¹ä½ è®¤ä¸ºé€ æˆé—®é¢˜çš„è¡Œè¿›è¡Œä¿®æ”¹çš„äººï¼ˆå¯ä»¥åœ¨ GitHub ä¸ŠæŸ¥çœ‹è¯¥è¡Œï¼Œé€‰æ‹©å®ƒç„¶åç‚¹å‡» â€œView git blameâ€æ¥æ‰¾åˆ°è¿™äº›ä¿¡æ¯ï¼‰ã€‚

å¦‚æœæ²¡æœ‰è¿›è¡Œæ ‡è®°ï¼Œé‚£ä¹ˆæˆ‘ä»¬çš„æ¨¡æ¿ä¼šè‡ªåŠ¨æä¾›è¦æ ‡è®°çš„äººçš„å»ºè®®ã€‚ä¸€èˆ¬ä¸è¦æ ‡è®°è¶…è¿‡ä¸‰ä¸ªäººã€‚

#### åŒ…å«ä¸€ä¸ªå¯é‡å¤çš„ç¤ºä¾‹ 

å¦‚æœä½ å·²ç»åˆ›å»ºäº†ä¸€ä¸ªäº§ç”Ÿé”™è¯¯çš„ç‹¬ç«‹ç¤ºä¾‹ï¼Œè¯·é”®å…¥ä¸€è¡ŒåŒ…å«ä¸‰ä¸ªåå¼•å·ï¼Œåè·Ÿ `python` ï¼Œåƒè¿™æ ·ï¼š

```python
```python  
```

ç„¶åç²˜è´´ä½ çš„æœ€å°å¯å¤ç°ç¤ºä¾‹ï¼Œå¹¶åœ¨æ–°çš„ä¸€è¡Œä¸Šè¾“å…¥ä¸‰ä¸ªåå¼•å·ã€‚è¿™å°†ç¡®ä¿ä½ çš„ä»£ç æ ¼å¼æ­£ç¡®ã€‚

å¦‚æœä½ æ²¡æœ‰æˆåŠŸåˆ›å»ºä¸€ä¸ªå¯å¤ç°çš„ç¤ºä¾‹ï¼Œæ¸…æ¥šåœ°æè¿°ä½ é‡åˆ°é—®é¢˜çš„æ­¥éª¤ã€‚å¦‚æœå¯ä»¥çš„è¯ï¼ŒåŒ…æ‹¬ä¸€ä¸ªä½ é‡åˆ°é”™è¯¯çš„ Google Colab ç¬”è®°æœ¬çš„é“¾æ¥ã€‚ä½ åˆ†äº«çš„ä¿¡æ¯è¶Šå¤šï¼Œç»´æŠ¤è€…å°±æ›´æœ‰å¯èƒ½å›å¤ä½ ã€‚

åœ¨æ‰€æœ‰æƒ…å†µä¸‹ï¼Œä½ éƒ½åº”è¯¥å¤åˆ¶å¹¶ç²˜è´´ä½ æ”¶åˆ°çš„æ•´ä¸ªé”™è¯¯æ¶ˆæ¯ã€‚å¦‚æœä½ åœ¨ Colab ä¸­å·¥ä½œï¼Œè¯·è®°ä½ï¼Œå †æ ˆè·Ÿè¸ªä¸­çš„æŸäº›å¸§å¯èƒ½ä¼šè‡ªåŠ¨æŠ˜å ï¼Œå› æ­¤è¯·ç¡®ä¿åœ¨å¤åˆ¶ä¹‹å‰å±•å¼€å®ƒä»¬ã€‚ä¸ä»£ç ç¤ºä¾‹ä¸€æ ·ï¼Œå°†è¯¥é”™è¯¯æ¶ˆæ¯æ”¾åœ¨ä¸¤è¡Œä¹‹é—´ï¼Œå¹¶å¸¦æœ‰ä¸‰ä¸ªåå¼•å·ï¼Œè¿™æ ·æ ¼å¼å°±æ­£ç¡®äº†ã€‚

#### æè¿°é¢„æœŸè¡Œä¸º 

ç”¨å‡ å¥è¯è§£é‡Šä¸€ä¸‹ä½ å¸Œæœ›å¾—åˆ°ä»€ä¹ˆï¼Œè¿™æ ·ç»´æŠ¤äººå‘˜å°±èƒ½å®Œå…¨ç†è§£é—®é¢˜ã€‚è¿™éƒ¨åˆ†é€šå¸¸å¾ˆæ˜æ˜¾ï¼Œæ‰€ä»¥åº”è¯¥ç”¨ä¸€å¥è¯æ¥æ¦‚æ‹¬ï¼Œä½†åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œä½ å¯èƒ½éœ€è¦è¯´å¾ˆå¤šã€‚

### æäº¤ï¼Ÿ

æäº¤ä½ çš„é—®é¢˜åï¼Œè¯·ç¡®ä¿å¿«é€Ÿæ£€æŸ¥ä¸€åˆ‡æ˜¯å¦æ­£å¸¸ã€‚å¦‚æœå‡ºç°é”™è¯¯ï¼Œä½ å¯ä»¥ç¼–è¾‘é—®é¢˜ï¼Œæˆ–è€…å¦‚æœä½ å‘ç°é—®é¢˜ä¸ä½ æœ€åˆçš„æƒ³æ³•ä¸åŒï¼Œç”šè‡³å¯ä»¥æ›´æ”¹å…¶æ ‡é¢˜ã€‚

å¦‚æœä½ æ²¡æœ‰å¾—åˆ°ç­”æ¡ˆï¼Œå°±æ²¡æœ‰å¿…è¦å»æé†’åˆ«äººã€‚å¦‚æœå‡ å¤©å†…æ²¡æœ‰äººå¸®åŠ©ä½ ï¼Œå¾ˆå¯èƒ½æ²¡æœ‰äººèƒ½ç†è§£ä½ çš„é—®é¢˜ã€‚ä¸è¦çŠ¹è±«ï¼Œå›åˆ°å¯é‡ç°çš„ä¾‹å­ã€‚ä½ èƒ½è®©å®ƒæ›´ç®€æ´æ˜äº†å—ï¼Ÿå¦‚æœåœ¨ä¸€å‘¨å†…æ²¡æœ‰å¾—åˆ°ç­”å¤ï¼Œå¯ä»¥ç•™è¨€æ¸©å’Œåœ°å¯»æ±‚å¸®åŠ©ï¼Œç‰¹åˆ«æ˜¯å¦‚æœä½ ç¼–è¾‘äº†ä½ çš„é—®é¢˜ï¼ŒåŒ…å«äº†æ›´å¤šå…³äºé—®é¢˜çš„ä¿¡æ¯ã€‚

## ç« æœ«æ€»ç»“åŠæµ‹è¯•

ä½ ç°åœ¨åº”è¯¥èƒ½å¤Ÿå¤„ç†ä¸€ç³»åˆ— NLP ä»»åŠ¡ï¼Œå¹¶å¯¹å®ƒä»¬è¿›è¡Œå¾®è°ƒæˆ–é¢„è®­ç»ƒæ¨¡å‹ã€‚ä¸è¦å¿˜è®°åœ¨ [Model Hub](https://huggingface.co/models)(https://huggingface.co/models) å’Œç¤¾åŒºåˆ†äº«ä½ çš„ç»“æœã€‚

æˆ‘ä»¬è¿«ä¸åŠå¾…åœ°æƒ³çœ‹åˆ°ä½ åˆ©ç”¨æ‰€å­¦çŸ¥è¯†åˆ›é€ å‡ºä»€ä¹ˆæ ·çš„ä½œå“ï¼

### ç« æœ«æµ‹è¯• 

####  1. åº”è¯¥æŒ‰ç…§ä»€ä¹ˆé¡ºåºè¯»å– Python å›æº¯ï¼Ÿ

1. ä»ä¸Šåˆ°ä¸‹
2. è‡ªä¸‹è€Œä¸Š

####  2. ä»€ä¹ˆæ˜¯æœ€å°å¯å†ç”Ÿç¤ºä¾‹ï¼Ÿ

1. æ¥è‡ªä¸€ç¯‡ç ”ç©¶æ–‡ç« ä¸€ä¸ªç®€å•çš„ Transformer ä½“ç³»ç»“æ„çš„å®ç°
2. ä¸€ç§ç´§å‡‘ä¸”è‡ªåŒ…å«çš„ä»£ç å—ï¼Œå¯ä»¥åœ¨ä¸ä¾èµ–äºç§æœ‰æ–‡ä»¶æˆ–æ•°æ®çš„æƒ…å†µä¸‹è¿è¡Œ
3. Python traceback çš„å±å¹•æˆªå›¾
4. ä¸€ä¸ªè®°å½•æ•´ä¸ªåˆ†æçš„ Notebookï¼ŒåŒ…æ‹¬ä¸é”™è¯¯æ— å…³çš„éƒ¨åˆ†

####  3. å‡è®¾ä½ å°è¯•è¿è¡Œä»¥ä¸‹ä»£ç ï¼Œå®ƒæŠ›å‡ºä¸€ä¸ªé”™è¯¯ï¼š

```python
from transformers import GPT3ForSequenceClassification

# ImportError: cannot import name 'GPT3ForSequenceClassification' from 'transformers' (/Users/lewtun/miniconda3/envs/huggingface/lib/python3.8/site-packages/transformers/__init__.py)
# ---------------------------------------------------------------------------
# ImportError                               Traceback (most recent call last)
# /var/folders/28/k4cy5q7s2hs92xq7_h89_vgm0000gn/T/ipykernel_30848/333858878.py in <module>
# ----> 1 from transformers import GPT3ForSequenceClassification

# ImportError: cannot import name 'GPT3ForSequenceClassification' from 'transformers' (/Users/lewtun/miniconda3/envs/huggingface/lib/python3.8/site-packages/transformers/__init__.py)
```

ä»¥ä¸‹å“ªé¡¹å¯èƒ½æ˜¯æœ‰åŠ©äºå¯»æ±‚å¸®åŠ©çš„è®ºå›ä¸»é¢˜æ ‡é¢˜ï¼Ÿ
1. importé”™è¯¯:æ— æ³•ä»'transformers'import åç§°'GPT3GPT3ForSequenceClassification'(/Users/lewtun/miniconda3/envs/huggingface/lib/python3.8/site-packages/transformers/_ init _. py) 
2. ä» transformers import gpt3forsequeclassification æ—¶å‡ºç°é—®é¢˜
3. ä¸ºä»€ä¹ˆæˆ‘ä¸èƒ½å¯¼å…¥ GPT3ForSequenceClassification ?
4. Transformers æ”¯æŒ GPT-3å—?

####  4. å‡è®¾ä½ è¯•å›¾è¿è¡Œ `trainer.train ()`ï¼Œä½†æ˜¯é‡åˆ°äº†ä¸€ä¸ªé”™è¯¯ï¼Œè¿™ä¸ªé”™è¯¯ä¸èƒ½å‡†ç¡®åœ°å‘Šè¯‰ä½ é”™è¯¯æ¥è‡ªå“ªé‡Œã€‚ä¸‹åˆ—å“ªä¸€é¡¹æ˜¯ä½ åº”è¯¥é¦–å…ˆåœ¨ä½ çš„è®­ç»ƒç®¡é“ä¸­å¯»æ‰¾é”™è¯¯çš„åœ°æ–¹ï¼Ÿ

1. è®¡ç®—æ¢¯åº¦å¹¶æ‰§è¡Œåå‘ä¼ æ’­çš„ä¼˜åŒ–æ­¥éª¤
2. è®¡ç®—æŒ‡æ ‡çš„è¯„ä¼°æ­¥éª¤
3. æ•°æ®é›†
4. Dataloader

####  5. è°ƒè¯• CUDA é”™è¯¯çš„æœ€å¥½æ–¹æ³•æ˜¯ä»€ä¹ˆï¼Ÿ

1. åœ¨è®ºå›æˆ– GitHub ä¸Šå‘å¸ƒé”™è¯¯æ¶ˆæ¯ã€‚
2. åœ¨ CPU ä¸Šæ‰§è¡Œç›¸åŒçš„ä»£ç ã€‚
3. è¯·é˜…è¯» traceback ä»¥æ‰¾å‡ºé”™è¯¯çš„åŸå› ã€‚
4. å‡å°‘ batch sizeã€‚
5. é‡æ–°å¯åŠ¨ Jupyter å†…æ ¸ã€‚

####  6. ä¿®å¤ GitHub ä¸Šçš„é—®é¢˜æœ€å¥½çš„æ–¹æ³•æ˜¯ä»€ä¹ˆï¼Ÿ

1. å‘å¸ƒè¿™ä¸ª bug çš„å®Œæ•´å¯é‡å¤çš„ä¾‹å­ã€‚
2. æ¯å¤©è¦æ±‚æ›´æ–°ã€‚
3. æ£€æŸ¥é”™è¯¯å‘¨å›´çš„æºä»£ç ï¼Œå¹¶è¯•å›¾æ‰¾å‡ºé”™è¯¯å‘ç”Ÿçš„åŸå› ã€‚åœ¨ issue ä¸Šå‘å¸ƒç»“æœã€‚

####  7. ä¸ºä»€ä¹ˆå¯¹ä¸€ä¸ª batch è¿›è¡Œè¿‡æ‹Ÿåˆé€šå¸¸æ˜¯ä¸€ç§å¥½çš„è°ƒè¯•æŠ€æœ¯ï¼Ÿ

1. ä¸æ˜¯è¿™æ ·çš„ï¼Œ è¿‡æ‹Ÿåˆæ€»æ˜¯ä¸å¥½çš„ï¼Œ åº”è¯¥é¿å…ã€‚
2. è¿™ä½¿æˆ‘ä»¬èƒ½å¤ŸéªŒè¯è¯¥æ¨¡å‹èƒ½å¤Ÿå°†æŸè€—é™ä½åˆ°é›¶ã€‚
3. è¿‡æ‹Ÿåˆå¯ä»¥æ£€æµ‹æˆ‘ä»¬éªŒè¯æˆ‘ä»¬è¾“å…¥å’Œæ ‡ç­¾çš„å¼ é‡å½¢çŠ¶æ˜¯æ­£ç¡®çš„ã€‚

####  8. ä¸ºä»€ä¹ˆåœ¨ Transformers å­˜å‚¨åº“ä¸­åˆ›å»ºæ–°é—®é¢˜æ—¶ï¼Œä½¿ç”¨ transformers-cli env åŒ…å«æœ‰å…³è®¡ç®—ç¯å¢ƒçš„è¯¦ç»†ä¿¡æ¯æ˜¯ä¸ªå¥½ä¸»æ„ï¼Ÿ

1. å®ƒå¯ä»¥ä½¿ç»´æŠ¤äººå‘˜ç†è§£ä½ æ­£åœ¨ä½¿ç”¨çš„åº“çš„å“ªä¸ªç‰ˆæœ¬ã€‚
2. å®ƒè®©ç»´æŠ¤äººå‘˜çŸ¥é“ä½ æ˜¯åœ¨ Windowsã€ macOS è¿˜æ˜¯ Linux ä¸Šè¿è¡Œä»£ç ã€‚
3. å®ƒå¯ä»¥ä½¿ç»´æŠ¤äººå‘˜çŸ¥é“ä½ æ˜¯åœ¨ GPU è¿˜æ˜¯ CPU ä¸Šè¿è¡Œä»£ç ã€‚

### è§£æ

####  1. åº”è¯¥æŒ‰ç…§ä»€ä¹ˆé¡ºåºè¯»å– Python å›æº¯ï¼Ÿ

æ­£ç¡®é€‰é¡¹: 2. è‡ªä¸‹è€Œä¸Š

1. ä»ä¸Šåˆ°ä¸‹    
è§£æ: é”™è¯¯ï¼å¤§å¤šæ•°ç¼–ç¨‹è¯­è¨€åœ¨é¡¶éƒ¨æ‰“å°å¼‚å¸¸ï¼Œä½† Python åœ¨è¿™æ–¹é¢æ˜¯ç‰¹æ®Šçš„ã€‚
2. è‡ªä¸‹è€Œä¸Š    
è§£æ: æ­£ç¡®ï¼Python åœ¨åº•éƒ¨æ˜¾ç¤ºå¼‚å¸¸å›æº¯çš„ä¸€ä¸ªä¼˜ç‚¹æ˜¯ï¼Œæ–¹ä¾¿åœ¨ç»ˆç«¯è°ƒè¯•ã€‚

####  2. ä»€ä¹ˆæ˜¯æœ€å°å¯å†ç”Ÿç¤ºä¾‹ï¼Ÿ

æ­£ç¡®é€‰é¡¹: 2. ä¸€ç§ç´§å‡‘ä¸”è‡ªåŒ…å«çš„ä»£ç å—ï¼Œå¯ä»¥åœ¨ä¸ä¾èµ–äºç§æœ‰æ–‡ä»¶æˆ–æ•°æ®çš„æƒ…å†µä¸‹è¿è¡Œ

1. æ¥è‡ªä¸€ç¯‡ç ”ç©¶æ–‡ç« ä¸€ä¸ªç®€å•çš„ Transformer ä½“ç³»ç»“æ„çš„å®ç°    
è§£æ: è™½ç„¶ä»å¤´å¼€å§‹å®ç°ä½ è‡ªå·±çš„ Transformer æ¨¡å‹æ˜¯éå¸¸æœ‰æ•™è‚²æ„ä¹‰çš„ï¼Œä½†è¿™ä¸æ˜¯æˆ‘ä»¬åœ¨è¿™é‡Œè®¨è®ºçš„å†…å®¹ã€‚
2. ä¸€ç§ç´§å‡‘ä¸”è‡ªåŒ…å«çš„ä»£ç å—ï¼Œå¯ä»¥åœ¨ä¸ä¾èµ–äºç§æœ‰æ–‡ä»¶æˆ–æ•°æ®çš„æƒ…å†µä¸‹è¿è¡Œ    
è§£æ: æ­£ç¡®çš„ï¼ æœ€å°‘çš„å¯é‡ç°ç¤ºä¾‹å¯ä»¥å¸®åŠ©åº“çš„ç»´æŠ¤äººå‘˜é‡ç°ä½ é‡åˆ°çš„é—®é¢˜ï¼Œä»¥ä¾¿ä»–ä»¬å¯ä»¥æ›´å¿«åœ°æ‰¾åˆ°è§£å†³æ–¹æ¡ˆã€‚
3. Python traceback çš„å±å¹•æˆªå›¾    
è§£æ: å†è¯•ä¸€æ¬¡ â€”â€” å°½ç®¡åœ¨æäº¤é—®é¢˜æ—¶
4. ä¸€ä¸ªè®°å½•æ•´ä¸ªåˆ†æçš„ Notebookï¼ŒåŒ…æ‹¬ä¸é”™è¯¯æ— å…³çš„éƒ¨åˆ†    
è§£æ: ä¸å®Œå…¨æ­£ç¡® â€”â€” å°½ç®¡å…±äº«ä¸€ä¸ªæ˜¾ç¤ºé”™è¯¯çš„ Google Colab Notebook ä¼šå¾ˆæœ‰å¸®åŠ©

####  3. å‡è®¾ä½ å°è¯•è¿è¡Œä»¥ä¸‹ä»£ç ï¼Œå®ƒæŠ›å‡ºä¸€ä¸ªé”™è¯¯ï¼š

```python
from transformers import GPT3ForSequenceClassification

# ImportError: cannot import name 'GPT3ForSequenceClassification' from 'transformers' (/Users/lewtun/miniconda3/envs/huggingface/lib/python3.8/site-packages/transformers/__init__.py)
# ---------------------------------------------------------------------------
# ImportError                               Traceback (most recent call last)
# /var/folders/28/k4cy5q7s2hs92xq7_h89_vgm0000gn/T/ipykernel_30848/333858878.py in <module>
# ----> 1 from transformers import GPT3ForSequenceClassification

# ImportError: cannot import name 'GPT3ForSequenceClassification' from 'transformers' (/Users/lewtun/miniconda3/envs/huggingface/lib/python3.8/site-packages/transformers/__init__.py)
```

ä»¥ä¸‹å“ªé¡¹å¯èƒ½æ˜¯æœ‰åŠ©äºå¯»æ±‚å¸®åŠ©çš„è®ºå›ä¸»é¢˜æ ‡é¢˜ï¼Ÿ
æ­£ç¡®é€‰é¡¹: 3. ä¸ºä»€ä¹ˆæˆ‘ä¸èƒ½å¯¼å…¥ GPT3ForSequenceClassification ?

æ­£ç¡®é€‰é¡¹: 4. Transformers æ”¯æŒ GPT-3å—?

1. importé”™è¯¯:æ— æ³•ä»'transformers'import åç§°'GPT3GPT3ForSequenceClassification'(/Users/lewtun/miniconda3/envs/huggingface/lib/python3.8/site-packages/transformers/_ init _. py)     
è§£æ: åŒ…æ‹¬ traceback çš„æœ€åä¸€è¡Œå¯èƒ½ä¼šæ›´å…·æè¿°æ€§ï¼Œä½†æœ€å¥½ä¿ç•™åœ¨ä¸»ä½“éƒ¨åˆ†ã€‚å†è¯•ä¸€æ¬¡ï¼
2. ä» transformers import gpt3forsequeclassification æ—¶å‡ºç°é—®é¢˜    
è§£æ: å†è¯•ä¸€æ¬¡â€”â€”å°½ç®¡è¿™æä¾›äº†æœ‰ç”¨çš„ä¿¡æ¯ï¼Œä½†æœ€å¥½è¿˜æ˜¯ä¿ç•™åœ¨æ–‡æœ¬çš„ä¸»ä½“éƒ¨åˆ†ã€‚
3. ä¸ºä»€ä¹ˆæˆ‘ä¸èƒ½å¯¼å…¥ GPT3ForSequenceClassification ?    
è§£æ: ä¸é”™çš„é€‰æ‹©ï¼è¿™ä¸ªæ ‡é¢˜æ˜¯ç®€æ´çš„ï¼Œå¹¶ç»™è¯»è€…ä¸€ä¸ªçº¿ç´¢ï¼Œä»€ä¹ˆå¯èƒ½æ˜¯é”™è¯¯çš„(å³Transformers ä¸æ”¯æŒ GPT-3)ã€‚
4. Transformers æ”¯æŒ GPT-3å—?    
è§£æ: å¥½ä¸»æ„! ç”¨é—®é¢˜ä½œä¸ºä¸»é¢˜æ ‡é¢˜æ˜¯å‘ç¤¾åŒºä¼ è¾¾é—®é¢˜çš„å¥½æ–¹æ³•ã€‚

####  4. å‡è®¾ä½ è¯•å›¾è¿è¡Œ `trainer.train ()`ï¼Œä½†æ˜¯é‡åˆ°äº†ä¸€ä¸ªé”™è¯¯ï¼Œè¿™ä¸ªé”™è¯¯ä¸èƒ½å‡†ç¡®åœ°å‘Šè¯‰ä½ é”™è¯¯æ¥è‡ªå“ªé‡Œã€‚ä¸‹åˆ—å“ªä¸€é¡¹æ˜¯ä½ åº”è¯¥é¦–å…ˆåœ¨ä½ çš„è®­ç»ƒç®¡é“ä¸­å¯»æ‰¾é”™è¯¯çš„åœ°æ–¹ï¼Ÿ

æ­£ç¡®é€‰é¡¹: 3. æ•°æ®é›†

1. è®¡ç®—æ¢¯åº¦å¹¶æ‰§è¡Œåå‘ä¼ æ’­çš„ä¼˜åŒ–æ­¥éª¤    
è§£æ: å°½ç®¡ä¼˜åŒ–å™¨ä¸­å¯èƒ½å­˜åœ¨ç¼ºé™·ï¼Œ ä½†è¿™é€šå¸¸æ˜¯è®­ç»ƒç®¡é“ä¸­çš„å‡ ä¸ªæ­¥éª¤ï¼Œ å› æ­¤é¦–å…ˆè¦æ£€æŸ¥å…¶ä»–äº‹é¡¹ã€‚å†è¯•ä¸€æ¬¡!
2. è®¡ç®—æŒ‡æ ‡çš„è¯„ä¼°æ­¥éª¤    
è§£æ: è¯„ä¼°é€šå¸¸æ˜¯åœ¨æ•´ä¸ªè®­ç»ƒç»“æŸåè¿›è¡Œçš„ï¼Œ å› æ­¤ä½ åº”è¯¥é¦–å…ˆæ£€æŸ¥è®­ç»ƒç®¡é“çš„æŸä¸ªåœ°æ–¹ã€‚
3. æ•°æ®é›†    
è§£æ: æ­£ç¡®çš„!æŸ¥çœ‹æ•°æ®å‡ ä¹æ€»æ˜¯ä½ åº”è¯¥åšçš„ç¬¬ä¸€ä»¶äº‹ï¼Œ ä»¥ç¡®ä¿æ–‡æœ¬è¿›è¡Œäº†é€‚å½“çš„ç¼–ç ï¼Œ å…·æœ‰é¢„æœŸçš„ç‰¹æ€§ï¼Œ ç­‰ç­‰ã€‚
4. Dataloader    
è§£æ: å†è¯•ä¸€æ¬¡â€”â€” è¿™éå¸¸æ¥è¿‘ä½ åº”è¯¥æ£€æŸ¥çš„ç¬¬ä¸€ä»¶äº‹ã€‚ä½ è¿˜è®°å¾—æˆ‘ä»¬äº¤ç»™ dataloader çš„æ˜¯ä»€ä¹ˆä¸œè¥¿å—ï¼Ÿ

####  5. è°ƒè¯• CUDA é”™è¯¯çš„æœ€å¥½æ–¹æ³•æ˜¯ä»€ä¹ˆï¼Ÿ

1. åœ¨è®ºå›æˆ– GitHub ä¸Šå‘å¸ƒé”™è¯¯æ¶ˆæ¯ã€‚    
è§£æ: è¿™ä¸ä¼šæœ‰ä»€ä¹ˆå¸®åŠ©
2. åœ¨ CPU ä¸Šæ‰§è¡Œç›¸åŒçš„ä»£ç ã€‚    
è§£æ: æ²¡é”™
3. è¯·é˜…è¯» traceback ä»¥æ‰¾å‡ºé”™è¯¯çš„åŸå› ã€‚    
è§£æ: å¯¹äºä»»ä½•å…¶ä»–é”™è¯¯
4. å‡å°‘ batch sizeã€‚    
è§£æ: å‡å°‘ batch size é€šå¸¸æ˜¯å¤„ç† CUDA å†…å­˜ä¸è¶³é”™è¯¯çš„ä¸€ä¸ªå¥½ç­–ç•¥
5. é‡æ–°å¯åŠ¨ Jupyter å†…æ ¸ã€‚    
è§£æ: å†è¯•ä¸€æ¬¡â€”â€”é‡æ–°å¯åŠ¨å†…æ ¸ä¸ä¼šè®©é”™è¯¯ç¥å¥‡åœ°æ¶ˆå¤±ï¼

####  6. ä¿®å¤ GitHub ä¸Šçš„é—®é¢˜æœ€å¥½çš„æ–¹æ³•æ˜¯ä»€ä¹ˆï¼Ÿ

æ­£ç¡®é€‰é¡¹: 1. å‘å¸ƒè¿™ä¸ª bug çš„å®Œæ•´å¯é‡å¤çš„ä¾‹å­ã€‚

1. å‘å¸ƒè¿™ä¸ª bug çš„å®Œæ•´å¯é‡å¤çš„ä¾‹å­ã€‚    
è§£æ: æ˜¯çš„ï¼Œè¿™æ˜¯å¸®åŠ©ç»´æŠ¤äººå‘˜æ‰¾åˆ° bug çš„æœ€å¥½æ–¹æ³•ã€‚ä½ è¿˜åº”è¯¥åšä»€ä¹ˆï¼Ÿ
2. æ¯å¤©è¦æ±‚æ›´æ–°ã€‚    
è§£æ: è¿™ä¸å¤ªå¯èƒ½ç»™ä½ ä»»ä½•å¸®åŠ©ï¼›äººä»¬æ›´å¯èƒ½ä¼šå¿½è§†ä½ ã€‚
3. æ£€æŸ¥é”™è¯¯å‘¨å›´çš„æºä»£ç ï¼Œå¹¶è¯•å›¾æ‰¾å‡ºé”™è¯¯å‘ç”Ÿçš„åŸå› ã€‚åœ¨ issue ä¸Šå‘å¸ƒç»“æœã€‚    
è§£æ: è¿™è‚¯å®šä¼šå¯¹ç»´æŠ¤äººå‘˜æœ‰å¸®åŠ©ï¼å¦‚æœä½ ç¡®å®æ‰¾åˆ°äº† bug çš„æ¥æºå’Œä¿®å¤ç¨‹åº

####  7. ä¸ºä»€ä¹ˆå¯¹ä¸€ä¸ª batch è¿›è¡Œè¿‡æ‹Ÿåˆé€šå¸¸æ˜¯ä¸€ç§å¥½çš„è°ƒè¯•æŠ€æœ¯ï¼Ÿ

æ­£ç¡®é€‰é¡¹: 2. è¿™ä½¿æˆ‘ä»¬èƒ½å¤ŸéªŒè¯è¯¥æ¨¡å‹èƒ½å¤Ÿå°†æŸè€—é™ä½åˆ°é›¶ã€‚

1. ä¸æ˜¯è¿™æ ·çš„ï¼Œ è¿‡æ‹Ÿåˆæ€»æ˜¯ä¸å¥½çš„ï¼Œ åº”è¯¥é¿å…ã€‚    
è§£æ: å½“å¯¹æ•´ä¸ªæ•°æ®é›†è¿›è¡Œè®­ç»ƒæ—¶ï¼Œ è¿‡æ‹Ÿåˆç¡®å®å¯èƒ½æ˜¯ä¸€ä¸ªä¸å¥½çš„ä¿¡å·ï¼Œ è¡¨æ˜ä½ çš„æ¨¡å‹ä¸èƒ½å¾ˆå¥½åœ°æ¨å¹¿åˆ°æ–°çš„ç¤ºä¾‹ã€‚ä½†æ˜¯ï¼Œ å¯¹äºè°ƒè¯•ï¼Œ æˆ‘ä»¬é€šå¸¸ä¸ä¼šå¯¹æ•´ä¸ªæ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚å†è¯•ä¸€æ¬¡ï¼
2. è¿™ä½¿æˆ‘ä»¬èƒ½å¤ŸéªŒè¯è¯¥æ¨¡å‹èƒ½å¤Ÿå°†æŸè€—é™ä½åˆ°é›¶ã€‚    
è§£æ: æ­£ç¡®! åªéœ€è¦ä¸¤ä¸ªæ ·æœ¬çš„å¾ˆå°çš„ batchï¼Œæˆ‘ä»¬å°±å¯ä»¥å¿«é€ŸéªŒè¯æ¨¡å‹æ˜¯å¦å…·æœ‰å­¦ä¹ èƒ½åŠ›ã€‚
3. è¿‡æ‹Ÿåˆå¯ä»¥æ£€æµ‹æˆ‘ä»¬éªŒè¯æˆ‘ä»¬è¾“å…¥å’Œæ ‡ç­¾çš„å¼ é‡å½¢çŠ¶æ˜¯æ­£ç¡®çš„ã€‚    
è§£æ: å†è¯•ä¸€æ¬¡ â€”â€” å¦‚æœä½ çš„å¼ é‡å½¢çŠ¶ä¸å¯¹é½ï¼Œ é‚£ä¹ˆä½ è‚¯å®šä¸èƒ½è®­ç»ƒï¼Œ å³ä½¿æ˜¯åœ¨ä¸€ä¸ª batch é‡Œã€‚

####  8. ä¸ºä»€ä¹ˆåœ¨ Transformers å­˜å‚¨åº“ä¸­åˆ›å»ºæ–°é—®é¢˜æ—¶ï¼Œä½¿ç”¨ transformers-cli env åŒ…å«æœ‰å…³è®¡ç®—ç¯å¢ƒçš„è¯¦ç»†ä¿¡æ¯æ˜¯ä¸ªå¥½ä¸»æ„ï¼Ÿ

æ­£ç¡®é€‰é¡¹: 1. å®ƒå¯ä»¥ä½¿ç»´æŠ¤äººå‘˜ç†è§£ä½ æ­£åœ¨ä½¿ç”¨çš„åº“çš„å“ªä¸ªç‰ˆæœ¬ã€‚

æ­£ç¡®é€‰é¡¹: 2. å®ƒè®©ç»´æŠ¤äººå‘˜çŸ¥é“ä½ æ˜¯åœ¨ Windowsã€ macOS è¿˜æ˜¯ Linux ä¸Šè¿è¡Œä»£ç ã€‚

æ­£ç¡®é€‰é¡¹: 3. å®ƒå¯ä»¥ä½¿ç»´æŠ¤äººå‘˜çŸ¥é“ä½ æ˜¯åœ¨ GPU è¿˜æ˜¯ CPU ä¸Šè¿è¡Œä»£ç ã€‚

1. å®ƒå¯ä»¥ä½¿ç»´æŠ¤äººå‘˜ç†è§£ä½ æ­£åœ¨ä½¿ç”¨çš„åº“çš„å“ªä¸ªç‰ˆæœ¬ã€‚    
è§£æ: æ­£ç¡®! ç”±äºåº“çš„æ¯ä¸ªä¸»è¦ç‰ˆæœ¬åœ¨APIä¸­éƒ½å¯èƒ½æœ‰æ›´æ”¹ï¼Œ å› æ­¤äº†è§£ä½ æ­£åœ¨ä½¿ç”¨çš„ç‰¹å®šç‰ˆæœ¬æœ‰åŠ©äºç¼©å°é—®é¢˜èŒƒå›´ã€‚å…¶ä»–å¥½å¤„æ˜¯ä»€ä¹ˆ?
2. å®ƒè®©ç»´æŠ¤äººå‘˜çŸ¥é“ä½ æ˜¯åœ¨ Windowsã€ macOS è¿˜æ˜¯ Linux ä¸Šè¿è¡Œä»£ç ã€‚    
è§£æ: æ­£ç¡®ï¼é”™è¯¯æœ‰æ—¶å¯èƒ½æ˜¯ç”±ä½ æ­£åœ¨ä½¿ç”¨çš„ç‰¹å®šæ“ä½œç³»ç»Ÿå¼•èµ·çš„ï¼Œ äº†è§£è¿™ä¸€ç‚¹æœ‰åŠ©äºç»´æŠ¤äººå‘˜åœ¨æœ¬åœ°å¤ç°è¿™äº›é”™è¯¯ã€‚ä½†è¿™å¹¶ä¸æ˜¯å”¯ä¸€çš„åŸå› ã€‚
3. å®ƒå¯ä»¥ä½¿ç»´æŠ¤äººå‘˜çŸ¥é“ä½ æ˜¯åœ¨ GPU è¿˜æ˜¯ CPU ä¸Šè¿è¡Œä»£ç ã€‚    
è§£æ: æ­£ç¡®çš„ï¼ æ­£å¦‚æˆ‘ä»¬åœ¨æœ¬ç« ä¸­æ‰€è§ï¼Œåœ¨ GPU æˆ– CPU ä¸Šè¿è¡Œçš„ä»£ç å¯èƒ½ä¼šäº§ç”Ÿä¸åŒçš„ç»“æœæˆ–é”™è¯¯ï¼Œäº†è§£ä½ ä½¿ç”¨çš„æ˜¯å“ªç§ç¡¬ä»¶æœ‰åŠ©äºå¸å¼•ç»´æŠ¤äººå‘˜çš„æ³¨æ„åŠ›ã€‚ ä½†è¿™ä¸æ˜¯å”¯ä¸€çš„å¥½å¤„...

