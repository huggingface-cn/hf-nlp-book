
## 4.2 ä½¿ç”¨ Trainer API å¾®è°ƒæ¨¡å‹ 


Transformers æä¾›äº†ä¸€ä¸ª `Trainer` ç±»ï¼Œä»¥å¸®åŠ©ä½ åœ¨è‡ªå·±çš„æ•°æ®é›†ä¸Šå¾®è°ƒå®ƒæä¾›çš„ä»»ä½•é¢„è®­ç»ƒæ¨¡å‹ã€‚å®Œæˆä¸Šä¸€èŠ‚ä¸­çš„æ‰€æœ‰æ•°æ®é¢„å¤„ç†å·¥ä½œåï¼Œä½ åªéœ€è¦æ‰§è¡Œå‡ ä¸ªæ­¥éª¤æ¥åˆ›å»º `Trainer` æœ€éš¾çš„éƒ¨åˆ†å¯èƒ½æ˜¯å‡†å¤‡è¿è¡Œ `Trainer.train()` é…ç½®ç¯å¢ƒï¼Œå› ä¸ºå®ƒåœ¨ CPU ä¸Šè¿è¡Œé€Ÿåº¦ä¼šéå¸¸æ…¢ã€‚å¦‚æœä½ æ²¡æœ‰è®¾ç½® GPUï¼Œå¯ä»¥è®¿é—®å…è´¹çš„ GPU æˆ– TPU 

Transformers æä¾›äº†ä¸€ä¸ª `Trainer` ç±»ï¼Œå¯ä»¥å¸®åŠ©ä½ åœ¨æ•°æ®é›†ä¸Šå¾®è°ƒä»»ä½•é¢„è®­ç»ƒæ¨¡å‹ã€‚åœ¨ä¸Šä¸€èŠ‚ä¸­å®Œæˆæ‰€æœ‰æ•°æ®é¢„å¤„ç†å·¥ä½œåï¼Œä½ åªéœ€å®Œæˆå‡ ä¸ªæ­¥éª¤æ¥å®šä¹‰ `Trainer` ã€‚æœ€å›°éš¾çš„éƒ¨åˆ†å¯èƒ½æ˜¯å‡†å¤‡è¿è¡Œ `Trainer.train()` æ‰€éœ€çš„ç¯å¢ƒï¼Œå› ä¸ºåœ¨ CPU ä¸Šè¿è¡Œé€Ÿåº¦éå¸¸æ…¢ã€‚å¦‚æœä½ æ²¡æœ‰è®¾ç½® GPUï¼Œå¯ä»¥ä½¿ç”¨ [Google Colab](https://colab.research.google.com/)(https://colab.research.google.com/) ï¼ˆå›½å†…ç½‘ç»œæ— æ³•ä½¿ç”¨ï¼‰ ä¸Šè·å¾—å…è´¹çš„ GPU æˆ– TPUã€‚

ä¸‹é¢çš„ç¤ºä¾‹å‡è®¾ä½ å·²ç»æ‰§è¡Œäº†ä¸Šä¸€èŠ‚ä¸­çš„ç¤ºä¾‹ã€‚ä¸‹é¢æ˜¯åœ¨å¼€å§‹å­¦ä¹ è¿™ä¸€èŠ‚ä¹‹å‰ä½ éœ€è¦è¿è¡Œçš„ä»£ç ï¼š

```python
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding

raw_datasets = load_dataset("glue", "mrpc")
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)

tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

#### Training 

åœ¨æˆ‘ä»¬å®šä¹‰ `Trainer` ä¹‹å‰ç¬¬ä¸€æ­¥è¦å®šä¹‰ä¸€ä¸ª `TrainingArguments` ç±»ï¼Œå®ƒåŒ…å« `Trainer` åœ¨è®­ç»ƒå’Œè¯„ä¼°ä¸­ä½¿ç”¨çš„æ‰€æœ‰è¶…å‚æ•°ã€‚ä½ åªéœ€è¦æä¾›çš„å‚æ•°æ˜¯ä¸€ä¸ªç”¨äºä¿å­˜è®­ç»ƒåçš„æ¨¡å‹ä»¥åŠè®­ç»ƒè¿‡ç¨‹ä¸­çš„ checkpoint çš„ç›®å½•ã€‚å¯¹äºå…¶ä½™çš„å‚æ•°ä½ å¯ä»¥ä¿ç•™é»˜è®¤å€¼ï¼Œè¿™å¯¹äºç®€å•çš„å¾®è°ƒåº”è¯¥æ•ˆæœå°±å¾ˆå¥½äº†ã€‚

```python
from transformers import TrainingArguments

training_args = TrainingArguments("test-trainer")
```

<div custom-style="Tip-green">

ğŸ’¡ å¦‚æœä½ æƒ³åœ¨è®­ç»ƒæœŸé—´è‡ªåŠ¨å°†æ¨¡å‹ä¸Šä¼ åˆ° Hubï¼Œè¯·å°† `push_to_hub=True` æ·»åŠ åˆ° TrainingArguments ä¹‹ä¸­ã€‚æˆ‘ä»¬å°†åœ¨ç¬¬äº”ç« ä¸­è¯¦ç»†ä»‹ç»è¿™éƒ¨åˆ†ã€‚

</div>

ç¬¬äºŒæ­¥æ˜¯å®šä¹‰æˆ‘ä»¬çš„æ¨¡å‹ã€‚ä¸å‰ä¸€ç« ä¸€æ ·ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ `AutoModelForSequenceClassification` ç±»ï¼Œå®ƒæœ‰ä¸¤ä¸ªå‚æ•°ï¼š

```python
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
```

ä½ ä¼šæ³¨æ„åˆ°ï¼Œå’Œç¬¬ä¸‰ç« ä¸åŒçš„æ˜¯ï¼Œåœ¨å®ä¾‹åŒ–æ­¤é¢„è®­ç»ƒæ¨¡å‹åä¼šæ”¶åˆ°è­¦å‘Šã€‚è¿™æ˜¯å› ä¸º BERT æ²¡æœ‰åœ¨å¥å­å¯¹åˆ†ç±»æ–¹é¢è¿›è¡Œè¿‡é¢„è®­ç»ƒï¼Œæ‰€ä»¥é¢„è®­ç»ƒæ¨¡å‹çš„ head å·²ç»è¢«ä¸¢å¼ƒï¼Œè€Œæ˜¯æ·»åŠ äº†ä¸€ä¸ªé€‚åˆå¥å­åºåˆ—åˆ†ç±»çš„æ–°å¤´éƒ¨ã€‚è¿™äº›è­¦å‘Šè¡¨æ˜ä¸€äº›æƒé‡æ²¡æœ‰ä½¿ç”¨ï¼ˆå¯¹åº”äºè¢«æ”¾å¼ƒçš„é¢„è®­ç»ƒå¤´çš„æƒé‡ï¼‰ï¼Œè€Œæœ‰äº›æƒé‡è¢«éšæœºåˆå§‹åŒ–ï¼ˆå¯¹åº”äºæ–° head çš„æƒé‡ï¼‰ã€‚

ä¸€æ—¦æœ‰äº†æˆ‘ä»¬çš„æ¨¡å‹ï¼Œæˆ‘ä»¬å°±å¯ä»¥å®šä¹‰ä¸€ä¸ª `Trainer` æŠŠåˆ°ç›®å‰ä¸ºæ­¢æ„å»ºçš„æ‰€æœ‰å¯¹è±¡â€”â€” `model`  `training_args` è®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†ï¼Œ `data_collator` å’Œ `tokenizer` ä¼ é€’ç»™ `Trainer` ï¼š

```python
from transformers import Trainer

trainer = Trainer(
    model,
    training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
)
```

è¯·æ³¨æ„ï¼Œå½“ä½ åœ¨è¿™é‡Œä¼ é€’ `tokenizer` æ—¶ï¼Œ `Trainer` é»˜è®¤ä½¿ç”¨çš„ `data_collator` æ˜¯ä¹‹å‰é¢„å®šä¹‰çš„ `DataCollatorWithPadding` æ‰€ä»¥ä½ å¯ä»¥åœ¨æœ¬ä¾‹ä¸­å¯ä»¥è·³è¿‡ `data_collator=data_collator` ä¸€è¡Œã€‚åœ¨ç¬¬ 2 èŠ‚ä¸­å‘ä½ å±•ç¤ºè¿™éƒ¨åˆ†å¤„ç†è¿‡ç¨‹ä»ç„¶å¾ˆé‡è¦ï¼

è¦åœ¨æˆ‘ä»¬çš„æ•°æ®é›†ä¸Šå¾®è°ƒæ¨¡å‹ï¼Œæˆ‘ä»¬åªéœ€è°ƒç”¨ `Trainer` çš„ `train()` æ–¹æ³•ï¼š

```python
trainer.train()
```

å¼€å§‹å¾®è°ƒï¼ˆåœ¨ GPU ä¸Šåº”è¯¥éœ€è¦å‡ åˆ†é’Ÿï¼‰ï¼Œæ¯ 500 æ­¥æŠ¥å‘Šä¸€æ¬¡è®­ç»ƒæŸå¤±ã€‚ç„¶è€Œå®ƒä¸ä¼šå‘Šè¯‰ä½ æ¨¡å‹çš„æ€§èƒ½ï¼ˆæˆ–è´¨é‡ï¼‰å¦‚ä½•ã€‚è¿™æ˜¯å› ä¸ºï¼š

1. æˆ‘ä»¬æ²¡æœ‰å‘Šè¯‰ Trainer åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¿›è¡Œè¯„ä¼°ï¼Œæ¯”å¦‚å°† `evaluation_strategy` è®¾ç½®ä¸ºâ€œ `step` â€ï¼ˆåœ¨æ¯ä¸ª `eval_steps` æ­¥éª¤è¯„ä¼°ä¸€æ¬¡ï¼‰æˆ–â€œ `epoch` â€ï¼ˆåœ¨æ¯ä¸ª epoch ç»“æŸæ—¶è¯„ä¼°ï¼‰ã€‚
2. æˆ‘ä»¬æ²¡æœ‰ä¸º `Trainer` æä¾›ä¸€ä¸ª `compute_metrics()` å‡½æ•°æ¥è®¡ç®—ä¸Šè¿°è¯„ä¼°è¿‡ç¨‹çš„æŒ‡æ ‡ï¼ˆå¦åˆ™è¯„ä¼°å°†åªè¾“å‡º lossï¼Œä½†è¿™ä¸æ˜¯ä¸€ä¸ªéå¸¸ç›´è§‚çš„æ•°å­—ï¼‰ã€‚

#### è¯„ä¼° 

è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•æ„å»ºä¸€ä¸ªæœ‰ç”¨çš„ `compute_metrics()` å‡½æ•°ï¼Œå¹¶åœ¨ä¸‹æ¬¡è®­ç»ƒæ—¶ä½¿ç”¨å®ƒã€‚è¯¥å‡½æ•°å¿…é¡»æ¥æ”¶ä¸€ä¸ª `EvalPrediction` å¯¹è±¡ï¼ˆå®ƒæ˜¯ä¸€ä¸ªå¸¦æœ‰ `predictions` å’Œ `label_ids` å­—æ®µçš„å‚æ•°å…ƒç»„ï¼‰ï¼Œå¹¶å°†è¿”å›ä¸€ä¸ªå­—ç¬¦ä¸²æ˜ å°„åˆ°æµ®ç‚¹æ•°çš„å­—å…¸ï¼ˆå­—ç¬¦ä¸²æ˜¯è¿”å›çš„æŒ‡æ ‡åç§°ï¼Œè€Œæµ®ç‚¹æ•°æ˜¯å…¶å€¼ï¼‰ã€‚ä¸ºäº†ä»æˆ‘ä»¬çš„æ¨¡å‹ä¸­è·å¾—é¢„æµ‹ç»“æœï¼Œå¯ä»¥ä½¿ç”¨ `Trainer.predict()` å‘½ä»¤ï¼š

```python
predictions = trainer.predict(tokenized_datasets["validation"])
print(predictions.predictions.shape, predictions.label_ids.shape)
```

```python
(408, 2) (408,)
```

`predict()` æ–¹æ³•çš„è¾“å‡ºå¦ä¸€ä¸ªå¸¦æœ‰ä¸‰ä¸ªå­—æ®µçš„å‘½åå…ƒç»„: `predictions`  `label_ids` å’Œ `metrics`  `metrics` å­—æ®µå°†åªåŒ…å«æ‰€ä¼ é€’çš„æ•°æ®é›†çš„æŸå¤±,ä»¥åŠä¸€äº›æ—¶é—´æŒ‡æ ‡(æ€»å…±èŠ±è´¹çš„æ—¶é—´å’Œå¹³å‡é¢„æµ‹æ—¶é—´)ã€‚å½“æˆ‘ä»¬å®šä¹‰äº†è‡ªå·±çš„ `compute_metrics()` å‡½æ•°å¹¶å°†å…¶ä¼ é€’ç»™ `Trainer` è¯¥å­—æ®µè¿˜å°†åŒ…å« `compute_metrics()` è¿”å›çš„ç»“æœã€‚`predict()` æ˜¯ä¸€ä¸ªäºŒç»´æ•°ç»„,å½¢çŠ¶ä¸º 408 Ã— 2(408 æ˜¯æˆ‘ä»¬ä½¿ç”¨çš„æ•°æ®é›†ä¸­çš„å…ƒç´ æ•°é‡),è¿™æ˜¯æˆ‘ä»¬ä¼ é€’ç»™ `pprdict()` çš„æ•°æ®é›†ä¸­æ¯ä¸ªå…ƒç´ çš„ logits(æ­£å¦‚åœ¨å‰ä¸€ç« ä¸­çœ‹åˆ°çš„,æ‰€æœ‰ Transformer æ¨¡å‹éƒ½è¿”å› logits)ã€‚ä¸ºäº†å°†å®ƒä»¬è½¬åŒ–ä¸ºå¯ä»¥ä¸æˆ‘ä»¬çš„æ ‡ç­¾è¿›è¡Œæ¯”è¾ƒçš„é¢„æµ‹å€¼,æˆ‘ä»¬éœ€è¦åœ¨ç¬¬äºŒè½´ä¸Šå–å€¼æœ€å¤§çš„ç´¢å¼•:

```python
import numpy as np

preds = np.argmax(predictions.predictions, axis=-1)
```

æˆ‘ä»¬ç°åœ¨å¯ä»¥å°†è¿™äº› `preds` ä¸æ ‡ç­¾è¿›è¡Œæ¯”è¾ƒã€‚ä¸ºäº†æ„å»ºæˆ‘ä»¬çš„ `compute_metric()` å‡½æ•°ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ [Evaluate](https://github.com/huggingface/evaluate/)(https://github.com/huggingface/evaluate/) åº“ä¸­çš„æŒ‡æ ‡ã€‚æˆ‘ä»¬å¯ä»¥åƒåŠ è½½æ•°æ®é›†ä¸€æ ·è½»æ¾åœ°åŠ è½½ä¸ MRPC æ•°æ®é›†å…³è”çš„æŒ‡æ ‡ï¼Œè¿™æ¬¡æ˜¯ä½¿ç”¨ `evaluate.load()` å‡½æ•°ã€‚è¿”å›çš„å¯¹è±¡æœ‰ä¸€ä¸ª `compute()` æ–¹æ³•ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨å®ƒæ¥è¿›è¡ŒæŒ‡æ ‡çš„è®¡ç®—ï¼š

```python
import evaluate

metric = evaluate.load("glue", "mrpc")
metric.compute(predictions=preds, references=predictions.label_ids)
```

```python
{'accuracy': 0.8578431372549019, 'f1': 0.8996539792387542}
```

ä½ å¾—åˆ°çš„ç¡®åˆ‡ç»“æœå¯èƒ½ä¼šæœ‰æ‰€ä¸åŒï¼Œå› ä¸ºæ¨¡å‹å¤´éƒ¨çš„éšæœºåˆå§‹åŒ–å¯èƒ½ä¼šæ”¹å˜å…¶æŒ‡æ ‡ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æˆ‘ä»¬çš„æ¨¡å‹åœ¨éªŒè¯é›†ä¸Šçš„å‡†ç¡®ç‡ä¸º 85.78ï¼…ï¼ŒF1 åˆ†æ•°ä¸º 89.97ã€‚è¿™æ˜¯ç”¨äºè¯„ä¼° MRPC æ•°æ®é›†åœ¨ GLUE åŸºå‡†æµ‹è¯•ä¸­çš„ç»“æœçš„ä¸¤ä¸ªæŒ‡æ ‡ã€‚åœ¨ BERT è®ºæ–‡ä¸­çš„è¡¨æ ¼ä¸­ï¼ŒåŸºç¡€æ¨¡å‹çš„ F1 åˆ†æ•°ä¸º 88.9ã€‚é‚£æ˜¯ uncased æ¨¡å‹ï¼Œè€Œæˆ‘ä»¬ç°åœ¨æ­£åœ¨ä½¿ç”¨ cased æ¨¡å‹ï¼Œè¿™è§£é‡Šäº†ä¸ºä»€ä¹ˆæˆ‘ä»¬å¾—åˆ°äº†æ›´å¥½çš„ç»“æœã€‚

æœ€åæŠŠæ‰€æœ‰ä¸œè¥¿æ‰“åŒ…åœ¨ä¸€èµ·ï¼Œæˆ‘ä»¬å°±å¾—åˆ°äº† `compute_metrics()` å‡½æ•°ï¼š

```python
def compute_metrics(eval_preds):
    metric = evaluate.load("glue", "mrpc")
    logits, labels = eval_preds
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)
```

ä¸ºäº†æŸ¥çœ‹æ¨¡å‹åœ¨æ¯ä¸ªè®­ç»ƒå‘¨æœŸç»“æŸæ—¶çš„å¥½åï¼Œä¸‹é¢æ˜¯æˆ‘ä»¬å¦‚ä½•ä½¿ç”¨ `compute_metrics()` å‡½æ•°å®šä¹‰ä¸€ä¸ªæ–°çš„ `Trainer` 

```python
training_args = TrainingArguments("test-trainer", evaluation_strategy="epoch")
model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)

trainer = Trainer(
    model,
    training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)
```

è¯·æ³¨æ„ï¼Œæˆ‘ä»¬è®¾ç½®äº†ä¸€ä¸ªæ–°çš„ `TrainingArguments` ï¼Œå…¶ `evaluation_strategy` è®¾ç½®ä¸º `epoch` å¹¶ä¸”åˆ›å»ºäº†ä¸€ä¸ªæ–°æ¨¡å‹ã€‚å¦‚æœä¸åˆ›å»ºæ–°çš„æ¨¡å‹å°±ç›´æ¥è®­ç»ƒï¼Œå°±åªä¼šç»§ç»­è®­ç»ƒæˆ‘ä»¬å·²ç»è®­ç»ƒè¿‡çš„æ¨¡å‹ã€‚ä¸ºäº†å¯åŠ¨æ–°çš„è®­ç»ƒï¼Œæˆ‘ä»¬æ‰§è¡Œï¼š

```python
trainer.train()
```

è¿™ä¸€æ¬¡ï¼Œå®ƒå°†åœ¨æ¯ä¸ª epoch ç»“æŸæ—¶åœ¨è®­ç»ƒæŸå¤±çš„åŸºç¡€ä¸ŠæŠ¥å‘ŠéªŒè¯æŸå¤±å’ŒæŒ‡æ ‡ã€‚åŒæ ·ï¼Œç”±äºæ¨¡å‹çš„éšæœºå¤´éƒ¨åˆå§‹åŒ–ï¼Œè¾¾åˆ°çš„å‡†ç¡®ç‡/F1 åˆ†æ•°å¯èƒ½ä¸æˆ‘ä»¬å‘ç°çš„ç•¥æœ‰ä¸åŒï¼Œè¿™æ˜¯ç”±äºæ¨¡å‹å¤´éƒ¨çš„éšæœºåˆå§‹åŒ–é€ æˆçš„ï¼Œä½†åº”è¯¥ç›¸å·®ä¸å¤šã€‚ `Trainer` å¯ä»¥åœ¨å¤šä¸ª GPU æˆ– TPU ä¸Šè¿è¡Œï¼Œå¹¶æä¾›è®¸å¤šé€‰é¡¹ï¼Œä¾‹å¦‚æ··åˆç²¾åº¦è®­ç»ƒï¼ˆåœ¨è®­ç»ƒçš„å‚æ•°ä¸­ä½¿ç”¨ `fp16 = True` ï¼‰ã€‚æˆ‘ä»¬å°†åœ¨ç¬¬åç« è®¨è®ºå®ƒæ”¯æŒçš„æ‰€æœ‰å†…å®¹ã€‚

ä½¿ç”¨ `Trainer` API å¾®è°ƒçš„ä»‹ç»åˆ°æ­¤ç»“æŸã€‚åœ¨ç¬¬å…«ç« ä¸­ä¼šç»™å‡ºä¸€ä¸ªå¯¹å¤§å¤šæ•°å¸¸è§çš„ NLP ä»»åŠ¡è¿›è¡Œè®­ç»ƒçš„ä¾‹å­ï¼Œä½†ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•åœ¨ PyTorch ä¸­åšç›¸åŒçš„æ“ä½œã€‚

<div custom-style="Tip-green">

âœï¸ **è¯•è¯•çœ‹ï¼** ä½¿ç”¨ä½ åœ¨ç¬¬ 2 èŠ‚ä¸­è¿›è¡Œçš„æ•°æ®å¤„ç†ï¼Œåœ¨ GLUE SST-2 æ•°æ®é›†ä¸Šå¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚

</div>


### ä¸€ä¸ªå®Œæ•´çš„è®­ç»ƒ 


ç°åœ¨ï¼Œæˆ‘ä»¬å°†äº†è§£å¦‚ä½•åœ¨ä¸ä½¿ç”¨ `Trainer` ç±»çš„æƒ…å†µä¸‹å®ç°ä¸ä¸Šä¸€èŠ‚ç›¸åŒçš„ç»“æœã€‚åŒæ ·ï¼Œæˆ‘ä»¬å‡è®¾ä½ å·²ç»å®Œæˆäº†ç¬¬ 2 èŠ‚ä¸­çš„æ•°æ®å¤„ç†ã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªç®€çŸ­çš„æ€»ç»“ï¼Œæ¶µç›–äº†ä½ éœ€è¦åœ¨æœ¬èŠ‚ä¹‹å‰è¿è¡Œçš„æ‰€æœ‰å†…å®¹ï¼š

```python
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding

raw_datasets = load_dataset("glue", "mrpc")
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)

tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

#### è®­ç»ƒå‰çš„å‡†å¤‡ 

åœ¨å®é™…ç¼–å†™æˆ‘ä»¬çš„è®­ç»ƒå¾ªç¯ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å®šä¹‰ä¸€äº›å¯¹è±¡ã€‚é¦–å…ˆæ˜¯æˆ‘ä»¬å°†ç”¨äºè¿­ä»£æ‰¹æ¬¡çš„æ•°æ®åŠ è½½å™¨ã€‚ä½†åœ¨å®šä¹‰è¿™äº›æ•°æ®åŠ è½½å™¨ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å¯¹æˆ‘ä»¬çš„ `tokenized_datasets` è¿›è¡Œä¸€äº›åå¤„ç†ï¼Œä»¥å¤„ç†ä¸€äº› Trainer è‡ªåŠ¨ä¸ºæˆ‘ä»¬å¤„ç†çš„å†…å®¹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬éœ€è¦ï¼š

- åˆ é™¤ä¸æ¨¡å‹ä¸éœ€è¦çš„åˆ—ï¼ˆå¦‚ `sentence1` å’Œ `sentence2` åˆ—ï¼‰ã€‚
- å°†åˆ—å `label` é‡å‘½åä¸º `labels` ï¼ˆå› ä¸ºæ¨¡å‹é»˜è®¤çš„å‚æ•°æ˜¯ `labels` ï¼‰ã€‚
- è®¾ç½®æ•°æ®é›†çš„æ ¼å¼ï¼Œä½¿å…¶è¿”å› PyTorch å¼ é‡è€Œä¸æ˜¯åˆ—è¡¨ã€‚

é’ˆå¯¹ä¸Šé¢çš„æ¯ä¸ªæ­¥éª¤ï¼Œæˆ‘ä»¬çš„ `tokenized_datasets` éƒ½æœ‰ä¸€ä¸ªæ–¹æ³•ï¼š

```python
tokenized_datasets = tokenized_datasets.remove_columns(["sentence1", "sentence2", "idx"])
tokenized_datasets = tokenized_datasets.rename_column("label", "labels")
tokenized_datasets.set_format("torch")
tokenized_datasets["train"].column_names
```

ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥æ£€æŸ¥ç»“æœä¸­æ˜¯å¦åªæœ‰æ¨¡å‹èƒ½å¤Ÿæ¥å—çš„åˆ—ï¼š

```python
["attention_mask", "input_ids", "labels", "token_type_ids"]
```

è‡³æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥è½»æ¾å®šä¹‰æ•°æ®åŠ è½½å™¨ï¼š

```python
from torch.utils.data import DataLoader

train_dataloader = DataLoader(
    tokenized_datasets["train"], shuffle=True, batch_size=8, collate_fn=data_collator
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], batch_size=8, collate_fn=data_collator
)
```

ä¸ºäº†å¿«é€Ÿæ£€éªŒæ•°æ®å¤„ç†ä¸­æ²¡æœ‰é”™è¯¯ï¼Œæˆ‘ä»¬å¯ä»¥è¿™æ ·æ£€éªŒå…¶ä¸­çš„ä¸€ä¸ª batchï¼š

```python
for batch in train_dataloader:
    break
{k: v.shape for k, v in batch.items()}
```

```python
{'attention_mask': torch.Size([8, 65]),
 'input_ids': torch.Size([8, 65]),
 'labels': torch.Size([8]),
 'token_type_ids': torch.Size([8, 65])}
```

è¯·æ³¨æ„ï¼Œå®é™…çš„å½¢çŠ¶å¯èƒ½ä¸ä½ ç•¥æœ‰ä¸åŒï¼Œå› ä¸ºæˆ‘ä»¬ä¸ºè®­ç»ƒæ•°æ®åŠ è½½å™¨è®¾ç½®äº† `shuffle=True` ï¼Œå¹¶ä¸”æ¨¡å‹ä¼šå°†å¥å­å¡«å……åˆ° `batch` ä¸­çš„æœ€å¤§é•¿åº¦ã€‚

ç°åœ¨æˆ‘ä»¬å·²ç»å®Œå…¨å®Œæˆäº†æ•°æ®é¢„å¤„ç†ï¼ˆå¯¹äºä»»ä½• ML ä»ä¸šè€…æ¥è¯´éƒ½æ˜¯ä¸€ä¸ªä»¤äººæ»¡æ„ä½†éš¾ä»¥å®ç°çš„ç›®æ ‡ï¼‰ï¼Œè®©æˆ‘ä»¬å°†æ³¨æ„åŠ›è½¬å‘æ¨¡å‹ã€‚æˆ‘ä»¬ä¼šåƒåœ¨ä¸Šä¸€èŠ‚ä¸­æ‰€åšçš„é‚£æ ·å®ä¾‹åŒ–å®ƒï¼š

```python
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
```

ä¸ºäº†ç¡®ä¿è®­ç»ƒè¿‡ç¨‹ä¸­ä¸€åˆ‡é¡ºåˆ©ï¼Œæˆ‘ä»¬å°† `batch` ä¼ é€’ç»™è¿™ä¸ªæ¨¡å‹ï¼š

```python
outputs = model(**batch)
print(outputs.loss, outputs.logits.shape)
```

```python
tensor(0.5441, grad_fn=<NllLossBackward>) torch.Size([8, 2])
```

å½“æˆ‘ä»¬æä¾› `labels` æ—¶ï¼ŒTransformers æ¨¡å‹éƒ½å°†è¿”å›è¿™ä¸ª `batch` çš„ `loss` ï¼Œæˆ‘ä»¬è¿˜å¾—åˆ°äº† `logits` ï¼ˆ `batch` ä¸­çš„æ¯ä¸ªè¾“å…¥æœ‰ä¸¤ä¸ªï¼Œæ‰€ä»¥å¼ é‡å¤§å°ä¸º 8 x 2ï¼‰ã€‚

æˆ‘ä»¬å‡ ä¹å‡†å¤‡å¥½ç¼–å†™æˆ‘ä»¬çš„è®­ç»ƒå¾ªç¯äº†ï¼æˆ‘ä»¬åªæ˜¯ç¼ºå°‘ä¸¤ä»¶äº‹ï¼šä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨ã€‚ç”±äºæˆ‘ä»¬è¯•å›¾æ‰‹åŠ¨å®ç° `Trainer` çš„åŠŸèƒ½ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ç›¸åŒçš„ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨ã€‚ `Trainer` ä½¿ç”¨çš„ä¼˜åŒ–å™¨æ˜¯ `AdamW` ï¼Œå®ƒä¸ `Adam` ç›¸åŒï¼Œä½†åŠ å…¥äº†æƒé‡è¡°å‡æ­£åˆ™åŒ–çš„ä¸€ç‚¹å˜åŒ–ï¼ˆå‚è§ Ilya Loshchilov å’Œ Frank Hutter çš„ [â€œDecoupled Weight Decay Regularizationâ€](https://arxiv.org/abs/1711.05101)(https://arxiv.org/abs/1711.05101) ï¼‰ï¼š

```python
from transformers import AdamW

optimizer = AdamW(model.parameters(), lr=5e-5)
```

æœ€åï¼Œé»˜è®¤ä½¿ç”¨çš„å­¦ä¹ ç‡è°ƒåº¦å™¨åªæ˜¯ä»æœ€å¤§å€¼ ï¼ˆ5e-5ï¼‰ åˆ° 0 çš„çº¿æ€§è¡°å‡ã€‚ä¸ºäº†å®šä¹‰å®ƒï¼Œæˆ‘ä»¬éœ€è¦çŸ¥é“æˆ‘ä»¬è®­ç»ƒçš„æ¬¡æ•°ï¼Œå³æ‰€æœ‰æ•°æ®è®­ç»ƒçš„æ¬¡æ•°ï¼ˆepochsï¼‰ä¹˜ä»¥çš„ batch çš„æ•°é‡ï¼ˆå³æˆ‘ä»¬è®­ç»ƒæ•°æ®åŠ è½½å™¨çš„é•¿åº¦ï¼‰ã€‚ `Trainer` é»˜è®¤æƒ…å†µä¸‹ä½¿ç”¨ä¸‰ä¸ª `epochs` ï¼Œå› æ­¤æˆ‘ä»¬å®šä¹‰è®­ç»ƒè¿‡ç¨‹å¦‚ä¸‹ï¼š

```python
from transformers import get_scheduler

num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
print(num_training_steps)
```

```python
1377
```

#### è®­ç»ƒå¾ªç¯ 

æœ€åä¸€ä»¶äº‹ï¼šå¦‚æœæˆ‘ä»¬å¯ä»¥è®¿é—® GPUï¼Œæˆ‘ä»¬å°†å¸Œæœ›ä½¿ç”¨ GPUï¼ˆåœ¨ CPU ä¸Šï¼Œè®­ç»ƒå¯èƒ½éœ€è¦å‡ ä¸ªå°æ—¶è€Œä¸æ˜¯å‡ åˆ†é’Ÿï¼‰ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ª `device` ï¼Œå®ƒåœ¨ GPU å¯ç”¨çš„æƒ…å†µä¸‹æŒ‡å‘ GPU æˆ‘ä»¬å°†æŠŠæˆ‘ä»¬çš„æ¨¡å‹å’Œ `batch` æ”¾åœ¨ `device` ä¸Šï¼š

```python
import torch

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)
device
```

```python
device(type='cuda')
```

æˆ‘ä»¬ç°åœ¨å‡†å¤‡å¥½è®­ç»ƒäº†ï¼ä¸ºäº†çŸ¥é“è®­ç»ƒä½•æ—¶ç»“æŸï¼Œæˆ‘ä»¬ä½¿ç”¨ `tqdm` åº“ï¼Œåœ¨è®­ç»ƒæ­¥éª¤æ•°ä¸Šæ·»åŠ äº†ä¸€ä¸ªè¿›åº¦æ¡ï¼š

```python
from tqdm.auto import tqdm

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

ä½ å¯ä»¥çœ‹åˆ°è®­ç»ƒå¾ªç¯çš„æ ¸å¿ƒä¸ä»‹ç»ä¸­çš„éå¸¸ç›¸ä¼¼ã€‚æˆ‘ä»¬æ²¡æœ‰è¦æ±‚ä»»ä½•æ£€éªŒï¼Œæ‰€ä»¥è¿™ä¸ªè®­ç»ƒå¾ªç¯ä¸ä¼šå‘Šè¯‰æˆ‘ä»¬ä»»ä½•å…³äºæ¨¡å‹ç›®å‰çš„çŠ¶æ€ã€‚æˆ‘ä»¬éœ€è¦ä¸ºæ­¤æ·»åŠ ä¸€ä¸ªè¯„ä¼°å¾ªç¯ã€‚

#### è¯„ä¼°å¾ªç¯ 

æ­£å¦‚æˆ‘ä»¬ä¹‹å‰æ‰€åšçš„é‚£æ ·ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ Evaluate åº“æä¾›çš„æŒ‡æ ‡ã€‚æˆ‘ä»¬å·²ç»äº†è§£äº† `metric.compute()` æ–¹æ³•ï¼Œå½“æˆ‘ä»¬ä½¿ç”¨ `add_batch()` æ–¹æ³•è¿›è¡Œé¢„æµ‹å¾ªç¯æ—¶ï¼Œå®é™…ä¸Šè¯¥æŒ‡æ ‡å¯ä»¥ä¸ºæˆ‘ä»¬ç´¯ç§¯æ‰€æœ‰ `batch` çš„ç»“æœã€‚ä¸€æ—¦æˆ‘ä»¬ç´¯ç§¯äº†æ‰€æœ‰ `batch` ï¼Œæˆ‘ä»¬å°±å¯ä»¥ä½¿ç”¨ `metric.compute()` å¾—åˆ°æœ€ç»ˆç»“æœã€‚ä»¥ä¸‹æ˜¯å¦‚ä½•åœ¨è¯„ä¼°å¾ªç¯ä¸­å®ç°æ‰€æœ‰è¿™äº›çš„æ–¹æ³•ï¼š

```python
import evaluate

metric = evaluate.load("glue", "mrpc")
model.eval()
for batch in eval_dataloader:
    batch = {k: v.to(device) for k, v in batch.items()}
    with torch.no_grad():
        outputs = model(**batch)

    logits = outputs.logits
    predictions = torch.argmax(logits, dim=-1)
    metric.add_batch(predictions=predictions, references=batch["labels"])

metric.compute()
```

```python
{'accuracy': 0.8431372549019608, 'f1': 0.8907849829351535}
```

åŒæ ·ï¼Œç”±äºæ¨¡å‹å¤´éƒ¨åˆå§‹åŒ–å’Œæ•°æ®æ‰“ä¹±çš„éšæœºæ€§ï¼Œä½ çš„ç»“æœä¼šç•¥æœ‰ä¸åŒï¼Œä½†åº”è¯¥ç›¸å·®ä¸å¤šã€‚

<div custom-style="Tip-green">

âœï¸ **è¯•è¯•çœ‹ï¼** ä¿®æ”¹ä¹‹å‰çš„è®­ç»ƒå¾ªç¯ä»¥åœ¨ SST-2 æ•°æ®é›†ä¸Šå¾®è°ƒä½ çš„æ¨¡å‹ã€‚

</div>

#### ä½¿ç”¨Accelerate åŠ é€Ÿä½ çš„è®­ç»ƒå¾ªç¯ 

æˆ‘ä»¬ä¹‹å‰å®šä¹‰çš„è®­ç»ƒå¾ªç¯åœ¨å•ä¸ª CPU æˆ– GPU ä¸Šè¿è¡Œè‰¯å¥½ã€‚ä½†æ˜¯ä½¿ç”¨ [Accelerate](https://github.com/huggingface/accelerate)(https://github.com/huggingface/accelerate) åº“ï¼Œåªéœ€è¿›è¡Œä¸€äº›è°ƒæ•´ï¼Œæˆ‘ä»¬å°±å¯ä»¥åœ¨å¤šä¸ª GPU æˆ– TPU ä¸Šå¯ç”¨åˆ†å¸ƒå¼è®­ç»ƒã€‚ä»åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æ•°æ®åŠ è½½å™¨å¼€å§‹ï¼Œæˆ‘ä»¬çš„æ‰‹åŠ¨è®­ç»ƒå¾ªç¯å¦‚ä¸‹æ‰€ç¤ºï¼š

```python
from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
optimizer = AdamW(model.parameters(), lr=3e-5)

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)

num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

ä»¥ä¸‹æ˜¯æ›´æ”¹çš„éƒ¨åˆ†ï¼š

```diff
+ from accelerate import Accelerator
  from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

+ accelerator = Accelerator()

  model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
  optimizer = AdamW(model.parameters(), lr=3e-5)

- device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
- model.to(device)

+ train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(
+     train_dataloader, eval_dataloader, model, optimizer
+ )

  num_epochs = 3
  num_training_steps = num_epochs * len(train_dataloader)
  lr_scheduler = get_scheduler(
      "linear",
      optimizer=optimizer,
      num_warmup_steps=0,
      num_training_steps=num_training_steps
  )

  progress_bar = tqdm(range(num_training_steps))

  model.train()
  for epoch in range(num_epochs):
      for batch in train_dataloader:
-         batch = {k: v.to(device) for k, v in batch.items()}
          outputs = model(**batch)
          loss = outputs.loss
-         loss.backward()
+         accelerator.backward(loss)

          optimizer.step()
          lr_scheduler.step()
          optimizer.zero_grad()
          progress_bar.update(1)
```

è¦æ·»åŠ çš„ç¬¬ä¸€è¡Œæ˜¯å¯¼å…¥ `Accelerator` ã€‚ç¬¬äºŒè¡Œå®ä¾‹åŒ–ä¸€ä¸ª `Accelerator` å¯¹è±¡ å®ƒå°†æŸ¥çœ‹ç¯å¢ƒå¹¶åˆå§‹åŒ–é€‚å½“çš„åˆ†å¸ƒå¼è®¾ç½®ã€‚Accelerate ä¸ºä½ å¤„ç†æ•°æ®åœ¨è®¾å¤‡é—´çš„ä¼ é€’ï¼Œå› æ­¤ä½ å¯ä»¥åˆ é™¤å°†æ¨¡å‹æ”¾åœ¨è®¾å¤‡ä¸Šçš„é‚£è¡Œä»£ç ï¼ˆæˆ–è€…ï¼Œå¦‚æœä½ æ„¿æ„ï¼Œå¯ä½¿ç”¨ `accelerator.device` ä»£æ›¿ `device` ï¼‰ã€‚

ç„¶åå¤§éƒ¨åˆ†å·¥ä½œä¼šåœ¨å°†æ•°æ®åŠ è½½å™¨ã€æ¨¡å‹å’Œä¼˜åŒ–å™¨å‘é€åˆ°çš„ `accelerator.prepare()` ä¸­å®Œæˆã€‚è¿™å°†ä¼šæŠŠè¿™äº›å¯¹è±¡åŒ…è£…åœ¨é€‚å½“çš„å®¹å™¨ä¸­ï¼Œä»¥ç¡®ä¿ä½ çš„åˆ†å¸ƒå¼è®­ç»ƒæŒ‰é¢„æœŸå·¥ä½œã€‚è¦è¿›è¡Œçš„å…¶ä½™æ›´æ”¹æ˜¯åˆ é™¤å°† `batch` æ”¾åœ¨ `device` çš„é‚£è¡Œä»£ç ï¼ˆåŒæ ·ï¼Œå¦‚æœä½ æƒ³ä¿ç•™å®ƒï¼Œä½ å¯ä»¥å°†å…¶æ›´æ”¹ä¸ºä½¿ç”¨ `accelerator.device` ï¼‰ å¹¶å°† `loss.backward()` æ›¿æ¢ä¸º `accelerator.backward(loss)` ã€‚

<div custom-style="Tip-yellow">

âš ï¸ ä¸ºäº†ä½¿äº‘ç«¯ TPU æä¾›çš„åŠ é€Ÿä¸­å‘æŒ¥æœ€å¤§çš„æ•ˆç›Šï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨ tokenizer çš„ `padding=max_length` å’Œ `max_length` å‚æ•°å°†ä½ çš„æ ·æœ¬å¡«å……åˆ°å›ºå®šé•¿åº¦ã€‚

</div>

å¦‚æœä½ æƒ³å¤åˆ¶å¹¶ç²˜è´´æ¥ç›´æ¥è¿è¡Œï¼Œä»¥ä¸‹æ˜¯ Accelerate çš„å®Œæ•´è®­ç»ƒå¾ªç¯ï¼š

```python
from accelerate import Accelerator
from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

accelerator = Accelerator()

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
optimizer = AdamW(model.parameters(), lr=3e-5)

train_dl, eval_dl, model, optimizer = accelerator.prepare(
    train_dataloader, eval_dataloader, model, optimizer
)

num_epochs = 3
num_training_steps = num_epochs * len(train_dl)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dl:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

æŠŠè¿™ä¸ªæ”¾åœ¨ `train.py` æ–‡ä»¶ä¸­ï¼Œå¯ä»¥è®©å®ƒåœ¨ä»»ä½•ç±»å‹çš„åˆ†å¸ƒå¼è®¾ç½®ä¸Šè¿è¡Œã€‚è¦åœ¨åˆ†å¸ƒå¼è®¾ç½®ä¸­è¯•ç”¨å®ƒï¼Œè¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

```python
accelerate config
```

è¿™å°†è¯¢é—®ä½ å‡ ä¸ªé…ç½®çš„é—®é¢˜å¹¶å°†ä½ çš„å›ç­”ä¿å­˜åˆ°æ­¤å‘½ä»¤ä½¿ç”¨çš„é…ç½®æ–‡ä»¶ä¸­ï¼š

```python
accelerate launch train.py
```

è¿™å°†å¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒ

è¿™å°†å¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒã€‚å¦‚æœä½ æƒ³åœ¨ Notebook ä¸­å°è¯•æ­¤æ“ä½œï¼ˆä¾‹å¦‚ï¼Œåœ¨ Colab ä¸Šä½¿ç”¨ TPU è¿›è¡Œæµ‹è¯•ï¼‰ï¼Œåªéœ€å°†ä»£ç ç²˜è´´åˆ°ä¸€ä¸ª `training_function()` å‡½æ•°ä¸­ï¼Œå¹¶åœ¨æœ€åä¸€ä¸ªå•å…ƒæ ¼ä¸­è¿è¡Œï¼š

```python
from accelerate import notebook_launcher

notebook_launcher(training_function)
```

ä½ å¯ä»¥åœ¨ [Accelerate repo](https://github.com/huggingface/accelerate/tree/main/examples)(https://github.com/huggingface/accelerate/tree/main/examples) æ‰¾åˆ°æ›´å¤šçš„ç¤ºä¾‹ã€‚
