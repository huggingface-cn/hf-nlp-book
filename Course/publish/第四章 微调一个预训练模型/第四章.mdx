# ç¬¬å››ç«  å¾®è°ƒä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹

åœ¨ç¬¬ä¸‰ç« æˆ‘ä»¬æ¢ç´¢äº†å¦‚ä½•ä½¿ç”¨ Tokenizer å’Œé¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œé¢„æµ‹ã€‚é‚£ä¹ˆå¦‚ä½•ä½¿ç”¨è‡ªå·±çš„æ•°æ®é›†å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹å‘¢ï¼Ÿæœ¬ç« å°†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ä½ å°†å­¦åˆ°ï¼š

{#if fw === 'pt'}

* å¦‚ä½•ä»æ¨¡å‹ä¸­å¿ƒï¼ˆhubï¼‰å‡†å¤‡å¤§å‹æ•°æ®é›†
* å¦‚ä½•ä½¿ç”¨é«˜çº§çš„ `Trainer` API å¾®è°ƒä¸€ä¸ªæ¨¡å‹
* å¦‚ä½•ä½¿ç”¨è‡ªå®šä¹‰è®­ç»ƒè¿‡ç¨‹
* å¦‚ä½•åˆ©ç”¨Accelerate åº“åœ¨æ‰€æœ‰åˆ†å¸ƒå¼è®¾å¤‡ä¸Šè½»æ¾è¿è¡Œè‡ªå®šä¹‰è®­ç»ƒè¿‡ç¨‹

{:else}

* å¦‚ä½•ä»æ¨¡å‹ä¸­å¿ƒï¼ˆhubï¼‰å‡†å¤‡å¤§å‹æ•°æ®é›†
* å¦‚ä½•ä½¿ç”¨ Keras å¾®è°ƒæ¨¡å‹
* å¦‚ä½•ä½¿ç”¨ Keras è¿›è¡Œé¢„æµ‹
* å¦‚ä½•ä½¿ç”¨è‡ªå®šä¹‰æŒ‡æ ‡

{/if}

ä¸ºäº†å°†ç»è¿‡è®­ç»ƒçš„å‚æ•°ä¸Šä¼ åˆ° Hugging Face Hubï¼Œä½ éœ€è¦ä¸€ä¸ª huggingface.co å¸æˆ·ï¼š [åˆ›å»ºä¸€ä¸ªè´¦æˆ·](https://huggingface.co/join)(https://huggingface.co/join) 



## 4.1 å¤„ç†æ•°æ® 

åœ¨è¿™ä¸€å°èŠ‚ä½ å°†å­¦ä¹ ç¬¬ä¸€å°èŠ‚ä¸­æåˆ°çš„â€œå¦‚ä½•ä½¿ç”¨æ¨¡å‹ä¸­å¿ƒï¼ˆhubï¼‰å¤§å‹æ•°æ®é›†â€ï¼Œä¸‹é¢æ˜¯ç”¨æ¨¡å‹ä¸­å¿ƒçš„æ•°æ®åœ¨ PyTorch ä¸Šè®­ç»ƒå¥å­åˆ†ç±»å™¨çš„ä¸€ä¸ªä¾‹å­ï¼š

```python
import torch
from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification

# å’Œä¹‹å‰ä¸€æ ·
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    "This course is amazing!",
]
batch = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")

# æ–°å¢éƒ¨åˆ†
batch["labels"] = torch.tensor([1, 1])

optimizer = AdamW(model.parameters())
loss = model(**batch).loss
loss.backward()
optimizer.step()
```

{:else}

åœ¨è¿™ä¸€å°èŠ‚ä½ å°†å­¦ä¹ ç¬¬ä¸€å°èŠ‚ä¸­æåˆ°çš„â€œå¦‚ä½•ä½¿ç”¨æ¨¡å‹ä¸­å¿ƒï¼ˆhubï¼‰å¤§å‹æ•°æ®é›†â€ï¼Œä¸‹é¢æ˜¯ç”¨æ¨¡å‹ä¸­å¿ƒçš„æ•°æ®åœ¨ TensorFlow ä¸Šè®­ç»ƒå¥å­åˆ†ç±»å™¨çš„ä¸€ä¸ªä¾‹å­ï¼š

```python
import tensorflow as tf
import numpy as np
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

# å’Œä¹‹å‰ä¸€æ ·
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    "This course is amazing!",
]
batch = dict(tokenizer(sequences, padding=True, truncation=True, return_tensors="tf"))

# æ–°å¢éƒ¨åˆ†
model.compile(optimizer="adam", loss="sparse_categorical_crossentropy")
labels = tf.convert_to_tensor([1, 1])
model.train_on_batch(batch, labels)
```

{/if}

ç„¶è€Œä»…ç”¨ä¸¤å¥è¯è®­ç»ƒæ¨¡å‹ä¸ä¼šäº§ç”Ÿå¾ˆå¥½çš„æ•ˆæœï¼Œä½ éœ€è¦å‡†å¤‡ä¸€ä¸ªæ›´å¤§çš„æ•°æ®é›†æ‰èƒ½å¾—åˆ°æ›´å¥½çš„è®­ç»ƒç»“æœã€‚

åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬ä»¥ MRPCï¼ˆå¾®è½¯ç ”ç©¶é™¢é‡Šä¹‰è¯­æ–™åº“ï¼‰æ•°æ®é›†ä¸ºä¾‹ï¼Œè¯¥æ•°æ®é›†ç”±å¨å»‰Â·å¤šå…°å’Œå…‹é‡Œæ–¯Â·å¸ƒç½—å…‹ç‰¹åœ¨è¿™ç¯‡æ–‡ç« å‘å¸ƒï¼Œç”± 5801 å¯¹å¥å­ç»„æˆï¼Œæ¯ä¸ªå¥å­å¯¹å¸¦æœ‰ä¸€ä¸ªæ ‡ç­¾æ¥æŒ‡ç¤ºå®ƒä»¬æ˜¯å¦ä¸ºåŒä¹‰ï¼ˆå³ä¸¤ä¸ªå¥å­çš„æ„æ€ç›¸åŒï¼‰ã€‚åœ¨æœ¬ç« é€‰æ‹©è¯¥æ•°æ®é›†çš„åŸå› æ˜¯å®ƒçš„æ•°æ®ä½“é‡å°ï¼Œå®¹æ˜“å¯¹å…¶è¿›è¡Œè®­ç»ƒã€‚

### ä»æ¨¡å‹ä¸­å¿ƒï¼ˆHubï¼‰åŠ è½½æ•°æ®é›† 

{#if fw === 'pt'}

{:else}

{/if}

æ¨¡å‹ä¸­å¿ƒï¼ˆhubï¼‰ä¸ä»…ä»…åŒ…å«æ¨¡å‹ï¼Œè¿˜æœ‰è®¸å¤šåˆ«çš„è¯­è¨€çš„æ•°æ®é›†ã€‚è®¿é—® [Datasets](https://huggingface.co/datasets)(https://huggingface.co/datasets) çš„é“¾æ¥å³å¯è¿›è¡Œæµè§ˆã€‚æˆ‘ä»¬å»ºè®®ä½ åœ¨å®Œæˆæœ¬èŠ‚çš„å­¦ä¹ åé˜…è¯»ä¸€ä¸‹ [åŠ è½½å’Œå¤„ç†æ–°çš„æ•°æ®é›†](https://huggingface.co/docs/datasets/loading_datasets.html#from-the-huggingface-hub)(https://huggingface.co/docs/datasets/loading_datasets.html#from-the-huggingface-hub) è¿™ç¯‡æ–‡ç« ï¼Œè¿™ä¼šè®©ä½ å¯¹ huggingface çš„æ•°æ®é›†ç†è§£æ›´åŠ æ¸…æ™°ã€‚ç°åœ¨è®©æˆ‘ä»¬ä½¿ç”¨ MRPC æ•°æ®é›†ä¸­çš„ [GLUE åŸºå‡†æµ‹è¯•æ•°æ®é›†](https://gluebenchmark.com)(https://gluebenchmark.com) ï¼Œå®ƒæ˜¯æ„æˆ MRPC æ•°æ®é›†çš„ 10 ä¸ªæ•°æ®é›†ä¹‹ä¸€ï¼Œä½œä¸ºä¸€ä¸ªç”¨äºè¡¡é‡æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨ 10 ä¸ªä¸åŒæ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸­æ€§èƒ½çš„å­¦æœ¯åŸºå‡†ã€‚

Datasets åº“æä¾›äº†ä¸€æ¡éå¸¸ä¾¿æ·çš„å‘½ä»¤ï¼Œå¯ä»¥åœ¨æ¨¡å‹ä¸­å¿ƒï¼ˆhubï¼‰ä¸Šä¸‹è½½å’Œç¼“å­˜æ•°æ®é›†ã€‚ä½ å¯ä»¥ä»¥ä¸‹ä»£ç ä¸‹è½½ MRPC æ•°æ®é›†ï¼š

```python
from datasets import load_dataset

raw_datasets = load_dataset("glue", "mrpc")
raw_datasets
```

```python
DatasetDict({
    train: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 408
    })
    test: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 1725
    })
})
```

ç°åœ¨æˆ‘ä»¬è·å¾—äº†ä¸€ä¸ª `DatasetDict` å¯¹è±¡ï¼Œè¿™ä¸ªå¯¹è±¡åŒ…å«è®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†ã€‚æ¯ä¸€ä¸ªé›†åˆéƒ½åŒ…å« 4 ä¸ªåˆ—ï¼ˆ `sentence1` ï¼Œ `sentence2` ï¼Œ `label` å’Œ `idx` ï¼‰ä»¥åŠä¸€ä¸ªä»£è¡¨è¡Œæ•°çš„å˜é‡ï¼ˆæ¯ä¸ªé›†åˆä¸­çš„è¡Œçš„ä¸ªæ•°ï¼‰ã€‚è¿è¡Œç»“æœæ˜¾ç¤ºè¯¥è®­ç»ƒé›†ä¸­æœ‰ 3668 å¯¹å¥å­ï¼ŒéªŒè¯é›†ä¸­æœ‰ 408 å¯¹ï¼Œæµ‹è¯•é›†ä¸­æœ‰ 1725 å¯¹ã€‚

é»˜è®¤æƒ…å†µä¸‹ï¼Œè¯¥å‘½ä»¤ä¼šä¸‹è½½æ•°æ®é›†å¹¶ç¼“å­˜åˆ° `~/.cache/huggingface/datasets` ã€‚å›æƒ³åœ¨ç¬¬ 2 ç« ä¸­å­¦åˆ°çš„æˆ‘ä»¬å¯ä»¥é€šè¿‡è®¾ç½® `HF_HOME` ç¯å¢ƒå˜é‡æ¥è‡ªå®šä¹‰ç¼“å­˜çš„æ–‡ä»¶å¤¹ã€‚

æˆ‘ä»¬å¯ä»¥è®¿é—®è¯¥æ•°æ®é›†ä¸­çš„æ¯ä¸€ä¸ª `raw_train_dataset` å¯¹è±¡ï¼Œä¾‹å¦‚ä½¿ç”¨å­—å…¸ï¼š

```python
raw_train_dataset = raw_datasets["train"]
raw_train_dataset[0]
```

```python
{'idx': 0,
 'label': 1,
 'sentence1': 'Amrozi accused his brother , whom he called " the witness " , of deliberately distorting his evidence .',
 'sentence2': 'Referring to him as only " the witness " , Amrozi accused his brother of deliberately distorting his evidence .'}
```

ç°åœ¨å¯ä»¥çœ‹åˆ°æ ‡ç­¾å·²ç»æ˜¯æ•´æ•°äº†ï¼Œå› æ­¤ä¸éœ€è¦å¯¹æ ‡ç­¾åšä»»ä½•é¢„å¤„ç†ã€‚å¦‚æœæƒ³è¦çŸ¥é“å“ªä¸ªæ•°å­—å¯¹åº”äºå“ªä¸ªæ ‡ç­¾ï¼Œæˆ‘ä»¬å¯ä»¥æŸ¥çœ‹ `raw_train_dataset` çš„ `features` ã€‚è¿™å‘Šè¯‰æˆ‘ä»¬æ¯åˆ—çš„ç±»å‹ï¼š

```python
raw_train_dataset.features
```

```python
{'sentence1': Value(dtype='string', id=None),
 'sentence2': Value(dtype='string', id=None),
 'label': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], names_file=None, id=None),
 'idx': Value(dtype='int32', id=None)}
```

åœ¨ä¸Šé¢çš„ä¾‹å­ä¹‹ä¸­ï¼Œ `Labelï¼ˆæ ‡ç­¾ï¼‰` æ˜¯ä¸€ç§ `ClassLabelï¼ˆåˆ†ç±»æ ‡ç­¾ï¼‰` ï¼Œä¹Ÿå°±æ˜¯ä½¿ç”¨æ•´æ•°å»ºç«‹èµ·ç±»åˆ«æ ‡ç­¾çš„æ˜ å°„å…³ç³»ã€‚ `0` å¯¹åº”äº `not_equivalentï¼ˆéåŒä¹‰ï¼‰` ï¼Œ `1` å¯¹åº”äº `equivalentï¼ˆåŒä¹‰ï¼‰` ã€‚

<div custom-style="Tip-green">

âœï¸ **è¯•è¯•çœ‹ï¼** æŸ¥çœ‹è®­ç»ƒé›†çš„ç¬¬ 15 è¡Œå…ƒç´ å’ŒéªŒè¯é›†çš„ 87 è¡Œå…ƒç´ ã€‚ä»–ä»¬çš„æ ‡ç­¾æ˜¯ä»€ä¹ˆï¼Ÿ

</div>

### é¢„å¤„ç†æ•°æ®é›† 

ä¸ºäº†é¢„å¤„ç†æ•°æ®é›†ï¼Œæˆ‘ä»¬éœ€è¦å°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹èƒ½å¤Ÿç†è§£çš„æ•°å­—ã€‚åœ¨ç¬¬ä¸‰ç« æˆ‘ä»¬å·²ç»å­¦ä¹ è¿‡ã€‚è¿™æ˜¯é€šè¿‡ä¸€ä¸ª Tokenizer å®Œæˆçš„ï¼Œæˆ‘ä»¬å¯ä»¥å‘ Tokenizer è¾“å…¥ä¸€ä¸ªå¥å­æˆ–ä¸€ä¸ªå¥å­åˆ—è¡¨ã€‚ä»¥ä¸‹ä»£ç è¡¨ç¤ºå¯¹æ¯å¯¹å¥å­ä¸­çš„æ‰€æœ‰ç¬¬ä¸€å¥å’Œæ‰€æœ‰ç¬¬äºŒå¥è¿›è¡Œ tokenizeï¼š

```python
from transformers import AutoTokenizer

checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
tokenized_sentences_1 = tokenizer(raw_datasets["train"]["sentence1"])
tokenized_sentences_2 = tokenizer(raw_datasets["train"]["sentence2"])
```

ä¸è¿‡åœ¨å°†ä¸¤å¥è¯ä¼ é€’ç»™æ¨¡å‹ï¼Œé¢„æµ‹è¿™ä¸¤å¥è¯æ˜¯å¦æ˜¯åŒä¹‰ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦ç»™è¿™ä¸¤å¥è¯ä¾æ¬¡è¿›è¡Œé€‚å½“çš„é¢„å¤„ç†ã€‚Tokenizer ä¸ä»…ä»…å¯ä»¥è¾“å…¥å•ä¸ªå¥å­ï¼Œè¿˜å¯ä»¥è¾“å…¥ä¸€ç»„å¥å­ï¼Œå¹¶æŒ‰ç…§ BERT æ¨¡å‹æ‰€éœ€è¦çš„è¾“å…¥è¿›è¡Œå¤„ç†ï¼š

```python
inputs = tokenizer("This is the first sentence.", "This is the second one.")
inputs
```

```python
{ 
  'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102],
  'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],
  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
}
```

æˆ‘ä»¬åœ¨ç¬¬ä¸‰ç« è®¨è®ºäº† `è¾“å…¥è¯id(input_ids)` å’Œ `æ³¨æ„åŠ›é®ç½©(attention_mask)` ï¼Œä½†å°šæœªè®¨è®º `ç±»å‹æ ‡è®°ID(token_type_ids)` ã€‚åœ¨æœ¬ä¾‹ä¸­ï¼Œ `ç±»å‹æ ‡è®°ID(token_type_ids)` çš„ä½œç”¨å°±æ˜¯å‘Šè¯‰æ¨¡å‹è¾“å…¥çš„å“ªä¸€éƒ¨åˆ†æ˜¯ç¬¬ä¸€å¥ï¼Œå“ªä¸€éƒ¨åˆ†æ˜¯ç¬¬äºŒå¥ã€‚

<div custom-style="Tip-green">

âœï¸ ** è¯•è¯•çœ‹ï¼** é€‰å–è®­ç»ƒé›†ä¸­çš„ç¬¬ 15 ä¸ªå…ƒç´ ï¼Œå°†ä¸¤å¥è¯åˆ†åˆ«æ ‡è®°ä¸ºä¸€å¯¹ã€‚ç»“æœå’Œä¸Šæ–¹çš„ä¾‹å­æœ‰ä»€ä¹ˆä¸åŒï¼Ÿ

</div>

å¦‚æœå°† `input_ids` ä¸­çš„ id è½¬æ¢å›æ–‡å­—ï¼š

```python
tokenizer.convert_ids_to_tokens(inputs["input_ids"])
```

å°†å¾—åˆ°ï¼š

```python
['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']
```

æ‰€ä»¥æˆ‘ä»¬çœ‹åˆ°æ¨¡å‹éœ€è¦è¾“å…¥çš„å½¢å¼æ˜¯ `[CLS] sentence1 [SEP] sentence2 [SEP]` ã€‚æ‰€ä»¥å½“æœ‰ä¸¤å¥è¯çš„æ—¶å€™ï¼Œ `ç±»å‹æ ‡è®°ID(token_type_ids)` çš„å€¼æ˜¯ï¼š

```python
['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']
[      0,      0,    0,     0,       0,          0,   0,       0,      1,    1,     1,        1,     1,   1,       1]
```

ç°åœ¨è¾“å…¥ä¸­ `[CLS] sentence1 [SEP]` å®ƒä»¬çš„ `token_type_ids` å‡ä¸º `0` ï¼Œè€Œå…¶ä»–éƒ¨åˆ†å¯¹åº” `sentence2 [SEP]` ï¼Œæ‰€æœ‰çš„ `token_type_ids` å‡ä¸º `1` ã€‚

è¯·æ³¨æ„ï¼Œå¦‚æœé€‰æ‹©å…¶ä»–çš„ checkpointï¼Œä¸ä¸€å®šå…·æœ‰ `token_type_ids` ï¼Œæ¯”å¦‚ï¼ŒDistilBERT æ¨¡å‹å°±ä¸ä¼šè¿”å›ã€‚åªæœ‰å½“ tokenizer åœ¨é¢„è®­ç»ƒæœŸé—´ä½¿ç”¨è¿‡è¿™ä¸€å±‚ï¼Œä¹Ÿå°±æ˜¯æ¨¡å‹åœ¨æ„å»ºæ—¶ä¾èµ–å®ƒä»¬æ—¶ï¼Œæ‰ä¼šè¿”å› `token_type_ids` ç±»å‹ã€‚

åœ¨è¿™é‡Œï¼ŒBERT ä½¿ç”¨äº†å¸¦æœ‰ `token_type_ids` çš„é¢„è®­ç»ƒ tokenizerï¼Œé™¤äº†æˆ‘ä»¬åœ¨ç¬¬äºŒç« ä¸­è®¨è®ºçš„æ©ç è¯­è¨€å»ºæ¨¡ï¼Œè¿˜æœ‰ä¸€ä¸ªé¢å¤–çš„åº”ç”¨ç±»å‹ç§°ä¸ºâ€œä¸‹ä¸€å¥é¢„æµ‹â€ã€‚è¿™ä¸ªä»»åŠ¡çš„ç›®æ ‡æ˜¯å¯¹å¥å­å¯¹ä¹‹é—´çš„å…³ç³»è¿›è¡Œå»ºæ¨¡ã€‚

åœ¨ä¸‹ä¸€å¥é¢„æµ‹ä»»åŠ¡ä¸­ï¼Œä¼šç»™æ¨¡å‹è¾“å…¥æˆå¯¹çš„å¥å­ï¼ˆå¸¦æœ‰éšæœºé®ç½©çš„ tokenï¼‰ï¼Œå¹¶è¦æ±‚é¢„æµ‹ç¬¬äºŒä¸ªå¥å­æ˜¯å¦ç´§è·Ÿç¬¬ä¸€ä¸ªå¥å­ã€‚ä¸ºäº†ä½¿ä»»åŠ¡å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œæ•°æ®é›†ä¸­ä¸€æœ‰ä¸€åŠå¥å­å¯¹ä¸­çš„å¥å­åœ¨åŸå§‹æ–‡æ¡£ä¸­é¡ºåºæ’åˆ—ï¼Œå¦ä¸€åŠå¥å­å¯¹ä¸­çš„ä¸¤ä¸ªå¥å­æ¥è‡ªä¸¤ä¸ªä¸åŒçš„æ–‡æ¡£ã€‚

ä¸€èˆ¬æ¥è¯´æ— éœ€è¦æ‹…å¿ƒåœ¨ä½ çš„è¾“å…¥ä¸­æ˜¯å¦éœ€è¦æœ‰ `token_type_ids` ã€‚åªè¦ä½ ä½¿ç”¨ç›¸åŒçš„ checkpoint çš„ Tokenizer å’Œæ¨¡å‹ï¼ŒTokenizer å°±ä¼šçŸ¥é“å‘æ¨¡å‹æä¾›ä»€ä¹ˆï¼Œä¸€åˆ‡éƒ½ä¼šé¡ºåˆ©è¿›è¡Œã€‚

ç°åœ¨æˆ‘ä»¬å·²ç»äº†è§£äº† Tokenizer å¦‚ä½•å¤„ç†ä¸€å¯¹å¥å­ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨å®ƒæ¥å¤„ç†æ•´ä¸ªæ•°æ®é›†ï¼šå°±åƒåœ¨ç¬¬ä¸‰ç« ä¸­ä¸€æ ·ï¼Œæˆ‘ä»¬å¯ä»¥ç»™ Tokenizer æä¾›ä¸€å¯¹å¥å­ï¼Œç¬¬ä¸€ä¸ªå‚æ•°æ˜¯å®ƒç¬¬ä¸€ä¸ªå¥å­çš„åˆ—è¡¨ï¼Œç¬¬äºŒä¸ªå‚æ•°æ˜¯ç¬¬äºŒä¸ªå¥å­çš„åˆ—è¡¨ã€‚è¿™ä¹Ÿä¸æˆ‘ä»¬åœ¨ç¬¬ä¸‰ç« ä¸­çœ‹åˆ°çš„å¡«å……å’Œæˆªæ–­é€‰é¡¹å…¼å®¹ã€‚å› æ­¤é¢„å¤„ç†è®­ç»ƒæ•°æ®é›†çš„ä¸€ç§æ–¹æ³•æ˜¯ï¼š

```python
tokenized_dataset = tokenizer(
    raw_datasets["train"]["sentence1"],
    raw_datasets["train"]["sentence2"],
    padding=True,
    truncation=True,
)
```

è¿™ç§æ–¹æ³•è™½ç„¶æœ‰æ•ˆï¼Œä½†æœ‰ä¸€ä¸ªç¼ºç‚¹æ˜¯å®ƒè¿”å›çš„æ˜¯ä¸€ä¸ªå­—å…¸ï¼ˆå­—å…¸çš„é”®æ˜¯ `è¾“å…¥è¯id(input_ids)` ï¼Œ `æ³¨æ„åŠ›é®ç½©(attention_mask)` å’Œ `ç±»å‹æ ‡è®°ID(token_type_ids)` ï¼Œå­—å…¸çš„å€¼æ˜¯é”®æ‰€å¯¹åº”å€¼çš„åˆ—è¡¨ï¼‰ã€‚è¿™æ„å‘³ç€åœ¨è½¬æ¢è¿‡ç¨‹ä¸­è¦æœ‰è¶³å¤Ÿçš„å†…å­˜æ¥å­˜å‚¨æ•´ä¸ªæ•°æ®é›†æ‰ä¸ä¼šå‡ºé”™ã€‚ä¸è¿‡æ¥è‡ªDatasets åº“ä¸­çš„æ•°æ®é›†æ˜¯ä»¥ [Apache Arrow](https://arrow.apache.org)(https://arrow.apache.org) æ ¼å¼å­˜å‚¨åœ¨ç£ç›˜ä¸Šçš„ï¼Œå› æ­¤ä½ åªéœ€å°†æ¥ä¸‹æ¥è¦ç”¨çš„æ•°æ®åŠ è½½åœ¨å†…å­˜ä¸­ï¼Œè¿™å¯¹å†…å­˜å®¹é‡çš„éœ€æ±‚æ¯”è¾ƒå‹å¥½ã€‚

æˆ‘ä»¬å°†ä½¿ç”¨ [Dataset.map()](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map)(https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map) æ–¹æ³•å°†æ•°æ®ä¿å­˜ä¸º dataset æ ¼å¼ï¼Œå¦‚æœæˆ‘ä»¬éœ€è¦åšæ›´å¤šçš„é¢„å¤„ç†è€Œä¸ä»…ä»…æ˜¯ tokenization å®ƒè¿˜æ”¯æŒäº†ä¸€äº›é¢å¤–çš„è‡ªå®šä¹‰çš„æ–¹æ³•ã€‚ `map()` æ–¹æ³•çš„å·¥ä½œåŸç†æ˜¯ä½¿ç”¨ä¸€ä¸ªå‡½æ•°å¤„ç†æ•°æ®é›†çš„æ¯ä¸ªå…ƒç´ ã€‚è®©æˆ‘ä»¬å®šä¹‰ä¸€ä¸ªå¯¹è¾“å…¥è¿›è¡Œ tokenize çš„å‡½æ•°ï¼š

```python
def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)
```

è¯¥å‡½æ•°æ¥æ”¶ä¸€ä¸ªå­—å…¸ï¼ˆä¸ dataset çš„é¡¹ç±»ä¼¼ï¼‰å¹¶è¿”å›ä¸€ä¸ªåŒ…å« `è¾“å…¥è¯id(input_ids)` ï¼Œ `æ³¨æ„åŠ›é®ç½©(attention_mask)` å’Œ `token_type_ids` é”®çš„æ–°å­—å…¸ã€‚è¯·æ³¨æ„ï¼Œå¦‚æœ `example` å­—å…¸æ‰€å¯¹åº”çš„å€¼åŒ…å«å¤šä¸ªå¥å­ï¼ˆæ¯ä¸ªé”®ä½œä¸ºä¸€ä¸ªå¥å­åˆ—è¡¨ï¼‰ï¼Œé‚£ä¹ˆå®ƒä¾ç„¶å¯ä»¥è¿è¡Œï¼Œå°±åƒå‰é¢çš„ä¾‹å­ä¸€æ ·ï¼Œ `tokenizer` å¯ä»¥å¤„ç†æˆå¯¹çš„å¥å­åˆ—è¡¨ï¼Œè¿™æ ·çš„è¯æˆ‘ä»¬å¯ä»¥åœ¨è°ƒç”¨ `map()` æ—¶ä½¿ç”¨è¯¥é€‰é¡¹ `batched=True` ï¼Œè¿™å°†æ˜¾è‘—åŠ å¿«å¤„ç†çš„é€Ÿåº¦ã€‚ `tokenizer` æ¥è‡ª [Tokenizers](https://github.com/huggingface/tokenizers)(https://github.com/huggingface/tokenizers) åº“ï¼Œç”± Rust ç¼–å†™è€Œæˆã€‚å½“ä¸€æ¬¡ç»™å®ƒå¾ˆå¤šè¾“å…¥æ—¶ï¼Œè¿™ä¸ª `tokenizer` å¯ä»¥å¤„ç†åœ°éå¸¸å¿«ã€‚

è¯·æ³¨æ„ï¼Œæˆ‘ä»¬æš‚æ—¶åœ¨ `tokenize_function` ä¸­çœç•¥äº† padding å‚æ•°ã€‚è¿™æ˜¯å› ä¸ºå°†æ‰€æœ‰çš„æ ·æœ¬å¡«å……åˆ°æœ€å¤§é•¿åº¦å¹¶ä¸é«˜æ•ˆã€‚ä¸€ä¸ªæ›´å¥½çš„åšæ³•æ˜¯ï¼šåœ¨æ„å»º batch çš„æ—¶å€™ã€‚è¿™æ ·æˆ‘ä»¬åªéœ€è¦å¡«å……åˆ°æ¯ä¸ª batch ä¸­çš„æœ€å¤§é•¿åº¦ï¼Œè€Œä¸æ˜¯æ•´ä¸ªæ•°æ®é›†çš„æœ€å¤§é•¿åº¦ã€‚å½“è¾“å…¥é•¿åº¦ä¸ç¨³å®šæ—¶ï¼Œè¿™å¯ä»¥èŠ‚çœå¤§é‡æ—¶é—´å’Œå¤„ç†èƒ½åŠ›ï¼

ä¸‹é¢æ˜¯æˆ‘ä»¬å¦‚ä½•ä½¿ç”¨ä¸€æ¬¡æ€§ `tokenize_function` å¤„ç†æ•´ä¸ªæ•°æ®é›†ã€‚æˆ‘ä»¬åœ¨è°ƒç”¨ `map` æ—¶ä½¿ç”¨äº† `batch =True` ï¼Œè¿™æ ·å‡½æ•°å°±å¯ä»¥åŒæ—¶å¤„ç†æ•°æ®é›†çš„å¤šä¸ªå…ƒç´ ï¼Œè€Œä¸æ˜¯åˆ†åˆ«å¤„ç†æ¯ä¸ªå…ƒç´ ï¼Œè¿™æ ·å¯ä»¥æ›´å¿«è¿›è¡Œé¢„å¤„ç†ã€‚

ä»¥ä¸‹æ˜¯å¦‚ä½•ä½¿ç”¨ tokenization å‡½æ•°å¤„ç†æˆ‘ä»¬çš„æ•´ä¸ªæ•°æ®é›†çš„æ–¹æ³•ã€‚æˆ‘ä»¬åœ¨è°ƒç”¨ map æ—¶ä½¿ç”¨äº† `batched=True` ï¼Œå› æ­¤è¯¥å‡½æ•°ä¼šä¸€æ¬¡æ€§å¤„ç†æ•°æ®é›†çš„å¤šä¸ªå…ƒç´ ï¼Œè€Œä¸æ˜¯å•ç‹¬å¤„ç†æ¯ä¸ªå…ƒç´ ã€‚è¿™æ ·å¯ä»¥å®ç°æ›´å¿«çš„é¢„å¤„ç†ã€‚

```python
tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
tokenized_datasets
```

Datasets åº“è¿›è¡Œè¿™ç§å¤„ç†çš„æ–¹å¼æ˜¯å‘æ•°æ®é›†æ·»åŠ æ–°çš„å­—æ®µï¼Œæ¯ä¸ªå­—æ®µå¯¹åº”é¢„å¤„ç†å‡½æ•°è¿”å›çš„å­—å…¸ä¸­çš„æ¯ä¸ªé”®ï¼š

```python
DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 408
    })
    test: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 1725
    })
})
```

åœ¨ä½¿ç”¨é¢„å¤„ç†å‡½æ•° `map()` æ—¶ï¼Œç”šè‡³å¯ä»¥é€šè¿‡ä¼ é€’ `num_proc` å‚æ•°å¹¶è¡Œå¤„ç†ã€‚æˆ‘ä»¬åœ¨è¿™é‡Œæ²¡æœ‰è¿™æ ·åšï¼Œå› ä¸ºåœ¨è¿™ä¸ªä¾‹å­ä¸­Tokenizers åº“å·²ç»ä½¿ç”¨å¤šçº¿ç¨‹æ¥æ›´å¿«åœ°å¯¹æ ·æœ¬ tokenizeï¼Œä½†æ˜¯å¦‚æœæ²¡æœ‰ä½¿ç”¨è¯¥åº“æ”¯æŒçš„å¿«é€Ÿ tokenizerï¼Œä½¿ç”¨ `num_proc` å¯èƒ½ä¼šåŠ å¿«é¢„å¤„ç†ã€‚

æˆ‘ä»¬çš„ `tokenize_function` è¿”å›åŒ…å« `è¾“å…¥è¯id(input_ids)` ï¼Œ `æ³¨æ„åŠ›é®ç½©(attention_mask)` å’Œ `token_type_ids` é”®çš„å­—å…¸ï¼Œè¿™ä¸‰ä¸ªå­—æ®µè¢«æ·»åŠ åˆ°æ•°æ®é›†çš„ä¸‰ä¸ªé›†åˆé‡Œï¼ˆè®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†ï¼‰ã€‚è¯·æ³¨æ„ï¼Œå¦‚æœé¢„å¤„ç†å‡½æ•° `map()` ä¸ºç°æœ‰é”®è¿”å›ä¸€ä¸ªæ–°å€¼ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿ç”¨ `map()` å‡½æ•°è¿”å›çš„æ–°å€¼ä¿®æ”¹ç°æœ‰çš„å­—æ®µã€‚

æˆ‘ä»¬æœ€åéœ€è¦åšçš„æ˜¯å°†æ‰€æœ‰ç¤ºä¾‹å¡«å……åˆ°è¯¥ batch ä¸­æœ€é•¿å…ƒç´ çš„é•¿åº¦ï¼Œè¿™ç§æŠ€æœ¯è¢«ç§°ä¸ºåŠ¨æ€å¡«å……ã€‚

### åŠ¨æ€å¡«å…… 

{#if fw === 'pt'}

è´Ÿè´£åœ¨æ‰¹å¤„ç†ä¸­å°†æ•°æ®æ•´ç†ä¸ºä¸€ä¸ª batch çš„å‡½æ•°ç§°ä¸º `collate å‡½æ•°` ã€‚è¿™æ˜¯ä¸€ä¸ªå¯ä»¥åœ¨æ„å»º `DataLoader` æ—¶ä¼ é€’çš„ä¸€ä¸ªå‚æ•°ï¼Œé»˜è®¤æ˜¯ä¸€ä¸ªå°†ä½ çš„æ•°æ®é›†è½¬æ¢ä¸º PyTorch å¼ é‡å¹¶å°†å®ƒä»¬æ‹¼æ¥èµ·æ¥çš„å‡½æ•°ï¼ˆå¦‚æœä½ çš„å…ƒç´ æ˜¯åˆ—è¡¨ã€å…ƒç»„æˆ–å­—å…¸ï¼Œåˆ™ä¼šä½¿ç”¨é€’å½’è¿›è¡Œæ‹¼æ¥ï¼‰ã€‚è¿™åœ¨æœ¬ä¾‹å­ä¸­ä¸‹æ˜¯ä¸å¯è¡Œçš„ï¼Œå› ä¸ºæˆ‘ä»¬çš„è¾“å…¥çš„å¤§å°å¯èƒ½æ˜¯ä¸ç›¸åŒçš„ã€‚æˆ‘ä»¬ç‰¹æ„æ¨è¿Ÿäº†å¡«å……çš„æ—¶é—´ï¼Œåªåœ¨æ¯ä¸ª batch ä¸Šè¿›è¡Œå¿…è¦çš„å¡«å……ï¼Œä»¥é¿å…å‡ºç°æœ‰å¤§é‡å¡«å……çš„è¿‡é•¿è¾“å…¥ã€‚è¿™å°†å¤§å¤§åŠ å¿«è®­ç»ƒé€Ÿåº¦ï¼Œä½†è¯·æ³¨æ„ï¼Œå¦‚æœä½ åœ¨ TPU ä¸Šè®­ç»ƒï¼Œéœ€è¦æ³¨æ„ä¸€ä¸ªé—®é¢˜â€”â€”TPU å–œæ¬¢å›ºå®šçš„å½¢çŠ¶ï¼Œå³ä½¿è¿™éœ€è¦é¢å¤–å¡«å……å¾ˆå¤šæ— ç”¨çš„ tokenã€‚

{:else}

è´Ÿè´£åœ¨æ‰¹å¤„ç†ä¸­å°†æ•°æ®æ•´ç†ä¸ºä¸€ä¸ª batch çš„å‡½æ•°ç§°ä¸º `collate å‡½æ•°` ã€‚é»˜è®¤çš„æ‹¼åˆå‡½æ•°åªä¼šå°†ä½ çš„æ ·æœ¬è½¬æ¢ä¸º `tf.Tensor` å¹¶å°†å®ƒä»¬æ‹¼æ¥èµ·æ¥ï¼ˆå¦‚æœä½ çš„å…ƒç´ æ˜¯åˆ—è¡¨ã€å…ƒç»„æˆ–å­—å…¸ï¼Œåˆ™ä¼šä½¿ç”¨é€’å½’è¿›è¡Œæ‹¼æ¥ï¼‰ã€‚è¿™åœ¨æœ¬ä¾‹ä¸­æ˜¯ä¸å¯è¡Œçš„ï¼Œå› ä¸ºæˆ‘ä»¬çš„è¾“å…¥ä¸æ˜¯éƒ½æ˜¯ç›¸åŒå¤§å°çš„ã€‚æˆ‘ä»¬ç‰¹æ„æ¨è¿Ÿäº†å¡«å……æ—¶é—´ï¼Œåªåœ¨æ¯ä¸ª batch ä¸Šè¿›è¡Œå¡«å……ï¼Œä»¥é¿å…æœ‰å¤ªå¤šå¡«å……çš„è¿‡é•¿çš„è¾“å…¥ã€‚è¿™å°†å¤§å¤§åŠ å¿«è®­ç»ƒé€Ÿåº¦ï¼Œä½†è¯·æ³¨æ„ï¼Œå¦‚æœä½ åœ¨ TPU ä¸Šè®­ç»ƒï¼Œéœ€è¦æ³¨æ„ä¸€ä¸ªé—®é¢˜â€”â€”TPU å–œæ¬¢å›ºå®šçš„å½¢çŠ¶ï¼Œå³ä½¿è¿™éœ€è¦é¢å¤–å¡«å……å¾ˆå¤šæ— ç”¨çš„ tokenã€‚

{/if}

ä¸ºäº†è§£å†³å¥å­é•¿åº¦ä¸ç»Ÿä¸€çš„é—®é¢˜ï¼Œæˆ‘ä»¬å¿…é¡»å®šä¹‰ä¸€ä¸ª collate å‡½æ•°ï¼Œè¯¥å‡½æ•°ä¼šå°†æ¯ä¸ª batch å¥å­å¡«å……åˆ°æ­£ç¡®çš„é•¿åº¦ã€‚å¹¸è¿çš„æ˜¯ï¼Œtransformer åº“é€šè¿‡ `DataCollatorWithPadding` ä¸ºæˆ‘ä»¬æä¾›äº†è¿™æ ·ä¸€ä¸ªå‡½æ•°ã€‚å½“ä½ å®ä¾‹åŒ–å®ƒæ—¶ï¼Œå®ƒéœ€è¦ä¸€ä¸ª tokenizer ï¼ˆç”¨æ¥çŸ¥é“ä½¿ç”¨å“ªç§å¡«å…… token ä»¥åŠæ¨¡å‹æœŸæœ›åœ¨è¾“å…¥çš„å·¦è¾¹å¡«å……è¿˜æ˜¯å³è¾¹å¡«å……ï¼‰ï¼Œç„¶åå®ƒä¼šè‡ªåŠ¨å®Œæˆæ‰€æœ‰éœ€è¦çš„æ“ä½œï¼š

{#if fw === 'pt'}

```python
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

{:else}

```python
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="tf")
```

{/if}

ä¸ºäº†æµ‹è¯•è¿™ä¸ªæ–°ç©å…·ï¼Œè®©æˆ‘ä»¬ä»æˆ‘ä»¬çš„è®­ç»ƒé›†ä¸­æŠ½å–å‡ ä¸ªæ ·æœ¬ï¼Œåœ¨è¿™é‡Œï¼Œæˆ‘ä»¬åˆ é™¤åˆ— `idx` ï¼Œ `sentence1` å’Œ `sentence2` ï¼Œå› ä¸ºä¸éœ€è¦å®ƒä»¬ï¼Œè€Œä¸”åˆ é™¤åŒ…å«å­—ç¬¦ä¸²çš„åˆ—ï¼ˆæˆ‘ä»¬ä¸èƒ½ç”¨å­—ç¬¦ä¸²åˆ›å»ºå¼ é‡ï¼‰ï¼Œç„¶åæŸ¥çœ‹ä¸€ä¸ª batch ä¸­æ¯ä¸ªæ¡ç›®çš„é•¿åº¦ï¼š

```python
samples = tokenized_datasets["train"][:8]
samples = {k: v for k, v in samples.items() if k not in ["idx", "sentence1", "sentence2"]}
[len(x) for x in samples["input_ids"]]
```

```python
[50, 59, 47, 67, 59, 50, 62, 32]
```

ä¸å‡ºæ‰€æ–™ï¼Œæˆ‘ä»¬å¾—åˆ°äº†ä¸åŒé•¿åº¦çš„æ ·æœ¬ï¼Œä» 32 åˆ° 67ã€‚åŠ¨æ€å¡«å……æ„å‘³ç€è¿™ä¸ª batch éƒ½åº”è¯¥å¡«å……åˆ°é•¿åº¦ä¸º 67ï¼Œè¿™æ˜¯è¿™ä¸ª batch ä¸­çš„æœ€å¤§é•¿åº¦ã€‚å¦‚æœæ²¡æœ‰åŠ¨æ€å¡«å……ï¼Œæ‰€æœ‰çš„æ ·æœ¬éƒ½å¿…é¡»å¡«å……åˆ°æ•´ä¸ªæ•°æ®é›†ä¸­çš„æœ€å¤§é•¿åº¦ï¼Œæˆ–è€…æ¨¡å‹å¯ä»¥æ¥å—çš„æœ€å¤§é•¿åº¦ã€‚è®©æˆ‘ä»¬å†æ¬¡æ£€æŸ¥ `data_collator` æ˜¯å¦æ­£ç¡®åœ°åŠ¨æ€å¡«å……äº†è¿™æ‰¹æ ·æœ¬ï¼š

```python
batch = data_collator(samples)
{k: v.shape for k, v in batch.items()}
```
{#if fw === 'tf'}

```python
{'attention_mask': TensorShape([8, 67]),
 'input_ids': TensorShape([8, 67]),
 'token_type_ids': TensorShape([8, 67]),
 'labels': TensorShape([8])}
```

{:else}

```python
{'attention_mask': torch.Size([8, 67]),
 'input_ids': torch.Size([8, 67]),
 'token_type_ids': torch.Size([8, 67]),
 'labels': torch.Size([8])}
```

çœ‹èµ·æ¥ä¸é”™ï¼ç°åœ¨ï¼Œæˆ‘ä»¬å·²ç»ä»åŸå§‹æ–‡æœ¬è½¬åŒ–ä¸ºäº†æ¨¡å‹å¯ä»¥å¤„ç†çš„æ•°æ®ï¼Œæˆ‘ä»¬å‡†å¤‡å¥½å¯¹å…¶è¿›è¡Œå¾®è°ƒã€‚

{/if}

<div custom-style="Tip-green">

âœï¸ ** è¯•è¯•çœ‹ï¼** åœ¨ GLUE SST-2 æ•°æ®é›†ä¸Šå¤åˆ»ä¸Šè¿°é¢„å¤„ç†ã€‚å®ƒæœ‰ç‚¹ä¸åŒï¼Œå› ä¸ºå®ƒæ˜¯ç”±å•å¥è€Œä¸æ˜¯æˆå¯¹çš„å¥å­ç»„æˆçš„ï¼Œä½†æ˜¯æˆ‘ä»¬æ‰€åšçš„å…¶ä»–äº‹æƒ…çœ‹èµ·æ¥åº”è¯¥æ˜¯ä¸€æ ·çš„ã€‚å¦ä¸€ä¸ªè¿›é˜¶çš„æŒ‘æˆ˜æ˜¯å°è¯•ç¼–å†™ä¸€ä¸ªå¯ç”¨äºä»»ä½• GLUE ä»»åŠ¡çš„é¢„å¤„ç†å‡½æ•°ã€‚

</div>

{#if fw === 'tf'}

ç°åœ¨æˆ‘ä»¬æœ‰äº† dataset å’Œ data collatorï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨ data collator æ‰¹é‡åœ°å¤„ç† datasetã€‚æˆ‘ä»¬å¯ä»¥æ‰‹åŠ¨åŠ è½½æ‰¹æ¬¡å¹¶è¿›è¡Œæ•´åˆï¼Œä½†è¿™éœ€è¦å¤§é‡å·¥ä½œï¼Œæ€§èƒ½ä¹Ÿæœ‰å¯èƒ½ä¸å¥½ã€‚ç›¸åï¼Œæœ‰ä¸€ä¸ªç®€å•çš„æ–¹æ³•ä¸ºè¿™ä¸ªé—®é¢˜æä¾›é«˜æ•ˆçš„è§£å†³æ–¹æ¡ˆï¼š `to_tf_dataset()` ã€‚å®ƒå°†æŠŠä½ çš„æ•°æ®é›†åŒ…è£…ä¸€ä¸ª `tf.data.Dataset` ç±»ä¸­ï¼Œè¿™ä¸ªæ–¹æ³•å¸¦æœ‰ä¸€ä¸ªå¯é€‰çš„ data collator åŠŸèƒ½ã€‚ `tf.data.Dataset` æ˜¯ TensorFlow çš„æœ¬åœ°æ ¼å¼ï¼ŒKeras å¯ä»¥ç›´æ¥ç”¨å®ƒæ¥è¿›è¡Œ `model.fit()` ï¼Œå› æ­¤è¿™ç§æ–¹æ³•ä¼šç«‹å³å°†Dataset è½¬æ¢ä¸ºå¯ç”¨äºè®­ç»ƒçš„æ ¼å¼ã€‚è®©æˆ‘ä»¬ç”¨æˆ‘ä»¬çš„æ•°æ®é›†æ¼”ç¤ºä¸€ä¸‹è¿™ä¸ªæ–¹æ³•ï¼

```py
tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)

tf_validation_dataset = tokenized_datasets["validation"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=False,
    collate_fn=data_collator,
    batch_size=8,
)
```

å°±æ˜¯è¿™æ ·ï¼æˆ‘ä»¬å¯ä»¥æŠŠè¿™äº›æ•°æ®é›†å¸¦å…¥ä¸‹ä¸€èŠ‚ï¼Œåœ¨ç»è¿‡æ‰€æœ‰è‰°è‹¦çš„æ•°æ®é¢„å¤„ç†å·¥ä½œä¹‹åï¼Œè®­ç»ƒå°†å˜å¾—éå¸¸ç®€å•å’Œæ„‰å¿«ã€‚

{/if}

## 4.2 ä½¿ç”¨ Trainer API å¾®è°ƒæ¨¡å‹ 


Transformers æä¾›äº†ä¸€ä¸ª `Trainer` ç±»ï¼Œä»¥å¸®åŠ©ä½ åœ¨è‡ªå·±çš„æ•°æ®é›†ä¸Šå¾®è°ƒå®ƒæä¾›çš„ä»»ä½•é¢„è®­ç»ƒæ¨¡å‹ã€‚å®Œæˆä¸Šä¸€èŠ‚ä¸­çš„æ‰€æœ‰æ•°æ®é¢„å¤„ç†å·¥ä½œåï¼Œä½ åªéœ€è¦æ‰§è¡Œå‡ ä¸ªæ­¥éª¤æ¥åˆ›å»º `Trainer` æœ€éš¾çš„éƒ¨åˆ†å¯èƒ½æ˜¯å‡†å¤‡è¿è¡Œ `Trainer.train()` é…ç½®ç¯å¢ƒï¼Œå› ä¸ºå®ƒåœ¨ CPU ä¸Šè¿è¡Œé€Ÿåº¦ä¼šéå¸¸æ…¢ã€‚å¦‚æœä½ æ²¡æœ‰è®¾ç½® GPUï¼Œå¯ä»¥è®¿é—®å…è´¹çš„ GPU æˆ– TPU 

Transformers æä¾›äº†ä¸€ä¸ª `Trainer` ç±»ï¼Œå¯ä»¥å¸®åŠ©ä½ åœ¨æ•°æ®é›†ä¸Šå¾®è°ƒä»»ä½•é¢„è®­ç»ƒæ¨¡å‹ã€‚åœ¨ä¸Šä¸€èŠ‚ä¸­å®Œæˆæ‰€æœ‰æ•°æ®é¢„å¤„ç†å·¥ä½œåï¼Œä½ åªéœ€å®Œæˆå‡ ä¸ªæ­¥éª¤æ¥å®šä¹‰ `Trainer` ã€‚æœ€å›°éš¾çš„éƒ¨åˆ†å¯èƒ½æ˜¯å‡†å¤‡è¿è¡Œ `Trainer.train()` æ‰€éœ€çš„ç¯å¢ƒï¼Œå› ä¸ºåœ¨ CPU ä¸Šè¿è¡Œé€Ÿåº¦éå¸¸æ…¢ã€‚å¦‚æœä½ æ²¡æœ‰è®¾ç½® GPUï¼Œå¯ä»¥ä½¿ç”¨ [Google Colab](https://colab.research.google.com/)(https://colab.research.google.com/) ï¼ˆå›½å†…ç½‘ç»œæ— æ³•ä½¿ç”¨ï¼‰ ä¸Šè·å¾—å…è´¹çš„ GPU æˆ– TPUã€‚

ä¸‹é¢çš„ç¤ºä¾‹å‡è®¾ä½ å·²ç»æ‰§è¡Œäº†ä¸Šä¸€èŠ‚ä¸­çš„ç¤ºä¾‹ã€‚ä¸‹é¢æ˜¯åœ¨å¼€å§‹å­¦ä¹ è¿™ä¸€èŠ‚ä¹‹å‰ä½ éœ€è¦è¿è¡Œçš„ä»£ç ï¼š

```python
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding

raw_datasets = load_dataset("glue", "mrpc")
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)

tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

#### Training 

åœ¨æˆ‘ä»¬å®šä¹‰ `Trainer` ä¹‹å‰ç¬¬ä¸€æ­¥è¦å®šä¹‰ä¸€ä¸ª `TrainingArguments` ç±»ï¼Œå®ƒåŒ…å« `Trainer` åœ¨è®­ç»ƒå’Œè¯„ä¼°ä¸­ä½¿ç”¨çš„æ‰€æœ‰è¶…å‚æ•°ã€‚ä½ åªéœ€è¦æä¾›çš„å‚æ•°æ˜¯ä¸€ä¸ªç”¨äºä¿å­˜è®­ç»ƒåçš„æ¨¡å‹ä»¥åŠè®­ç»ƒè¿‡ç¨‹ä¸­çš„ checkpoint çš„ç›®å½•ã€‚å¯¹äºå…¶ä½™çš„å‚æ•°ä½ å¯ä»¥ä¿ç•™é»˜è®¤å€¼ï¼Œè¿™å¯¹äºç®€å•çš„å¾®è°ƒåº”è¯¥æ•ˆæœå°±å¾ˆå¥½äº†ã€‚

```python
from transformers import TrainingArguments

training_args = TrainingArguments("test-trainer")
```

<div custom-style="Tip-green">

ğŸ’¡ å¦‚æœä½ æƒ³åœ¨è®­ç»ƒæœŸé—´è‡ªåŠ¨å°†æ¨¡å‹ä¸Šä¼ åˆ° Hubï¼Œè¯·å°† `push_to_hub=True` æ·»åŠ åˆ° TrainingArguments ä¹‹ä¸­ã€‚æˆ‘ä»¬å°†åœ¨ç¬¬äº”ç« ä¸­è¯¦ç»†ä»‹ç»è¿™éƒ¨åˆ†ã€‚

</div>

ç¬¬äºŒæ­¥æ˜¯å®šä¹‰æˆ‘ä»¬çš„æ¨¡å‹ã€‚ä¸å‰ä¸€ç« ä¸€æ ·ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ `AutoModelForSequenceClassification` ç±»ï¼Œå®ƒæœ‰ä¸¤ä¸ªå‚æ•°ï¼š

```python
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
```

ä½ ä¼šæ³¨æ„åˆ°ï¼Œå’Œç¬¬ä¸‰ç« ä¸åŒçš„æ˜¯ï¼Œåœ¨å®ä¾‹åŒ–æ­¤é¢„è®­ç»ƒæ¨¡å‹åä¼šæ”¶åˆ°è­¦å‘Šã€‚è¿™æ˜¯å› ä¸º BERT æ²¡æœ‰åœ¨å¥å­å¯¹åˆ†ç±»æ–¹é¢è¿›è¡Œè¿‡é¢„è®­ç»ƒï¼Œæ‰€ä»¥é¢„è®­ç»ƒæ¨¡å‹çš„ head å·²ç»è¢«ä¸¢å¼ƒï¼Œè€Œæ˜¯æ·»åŠ äº†ä¸€ä¸ªé€‚åˆå¥å­åºåˆ—åˆ†ç±»çš„æ–°å¤´éƒ¨ã€‚è¿™äº›è­¦å‘Šè¡¨æ˜ä¸€äº›æƒé‡æ²¡æœ‰ä½¿ç”¨ï¼ˆå¯¹åº”äºè¢«æ”¾å¼ƒçš„é¢„è®­ç»ƒå¤´çš„æƒé‡ï¼‰ï¼Œè€Œæœ‰äº›æƒé‡è¢«éšæœºåˆå§‹åŒ–ï¼ˆå¯¹åº”äºæ–° head çš„æƒé‡ï¼‰ã€‚

ä¸€æ—¦æœ‰äº†æˆ‘ä»¬çš„æ¨¡å‹ï¼Œæˆ‘ä»¬å°±å¯ä»¥å®šä¹‰ä¸€ä¸ª `Trainer` æŠŠåˆ°ç›®å‰ä¸ºæ­¢æ„å»ºçš„æ‰€æœ‰å¯¹è±¡â€”â€” `model`  `training_args` è®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†ï¼Œ `data_collator` å’Œ `tokenizer` ä¼ é€’ç»™ `Trainer` ï¼š

```python
from transformers import Trainer

trainer = Trainer(
    model,
    training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
)
```

è¯·æ³¨æ„ï¼Œå½“ä½ åœ¨è¿™é‡Œä¼ é€’ `tokenizer` æ—¶ï¼Œ `Trainer` é»˜è®¤ä½¿ç”¨çš„ `data_collator` æ˜¯ä¹‹å‰é¢„å®šä¹‰çš„ `DataCollatorWithPadding` æ‰€ä»¥ä½ å¯ä»¥åœ¨æœ¬ä¾‹ä¸­å¯ä»¥è·³è¿‡ `data_collator=data_collator` ä¸€è¡Œã€‚åœ¨ç¬¬ 2 èŠ‚ä¸­å‘ä½ å±•ç¤ºè¿™éƒ¨åˆ†å¤„ç†è¿‡ç¨‹ä»ç„¶å¾ˆé‡è¦ï¼

è¦åœ¨æˆ‘ä»¬çš„æ•°æ®é›†ä¸Šå¾®è°ƒæ¨¡å‹ï¼Œæˆ‘ä»¬åªéœ€è°ƒç”¨ `Trainer` çš„ `train()` æ–¹æ³•ï¼š

```python
trainer.train()
```

å¼€å§‹å¾®è°ƒï¼ˆåœ¨ GPU ä¸Šåº”è¯¥éœ€è¦å‡ åˆ†é’Ÿï¼‰ï¼Œæ¯ 500 æ­¥æŠ¥å‘Šä¸€æ¬¡è®­ç»ƒæŸå¤±ã€‚ç„¶è€Œå®ƒä¸ä¼šå‘Šè¯‰ä½ æ¨¡å‹çš„æ€§èƒ½ï¼ˆæˆ–è´¨é‡ï¼‰å¦‚ä½•ã€‚è¿™æ˜¯å› ä¸ºï¼š

1. æˆ‘ä»¬æ²¡æœ‰å‘Šè¯‰ Trainer åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¿›è¡Œè¯„ä¼°ï¼Œæ¯”å¦‚å°† `evaluation_strategy` è®¾ç½®ä¸ºâ€œ `step` â€ï¼ˆåœ¨æ¯ä¸ª `eval_steps` æ­¥éª¤è¯„ä¼°ä¸€æ¬¡ï¼‰æˆ–â€œ `epoch` â€ï¼ˆåœ¨æ¯ä¸ª epoch ç»“æŸæ—¶è¯„ä¼°ï¼‰ã€‚
2. æˆ‘ä»¬æ²¡æœ‰ä¸º `Trainer` æä¾›ä¸€ä¸ª `compute_metrics()` å‡½æ•°æ¥è®¡ç®—ä¸Šè¿°è¯„ä¼°è¿‡ç¨‹çš„æŒ‡æ ‡ï¼ˆå¦åˆ™è¯„ä¼°å°†åªè¾“å‡º lossï¼Œä½†è¿™ä¸æ˜¯ä¸€ä¸ªéå¸¸ç›´è§‚çš„æ•°å­—ï¼‰ã€‚

#### è¯„ä¼° 

è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•æ„å»ºä¸€ä¸ªæœ‰ç”¨çš„ `compute_metrics()` å‡½æ•°ï¼Œå¹¶åœ¨ä¸‹æ¬¡è®­ç»ƒæ—¶ä½¿ç”¨å®ƒã€‚è¯¥å‡½æ•°å¿…é¡»æ¥æ”¶ä¸€ä¸ª `EvalPrediction` å¯¹è±¡ï¼ˆå®ƒæ˜¯ä¸€ä¸ªå¸¦æœ‰ `predictions` å’Œ `label_ids` å­—æ®µçš„å‚æ•°å…ƒç»„ï¼‰ï¼Œå¹¶å°†è¿”å›ä¸€ä¸ªå­—ç¬¦ä¸²æ˜ å°„åˆ°æµ®ç‚¹æ•°çš„å­—å…¸ï¼ˆå­—ç¬¦ä¸²æ˜¯è¿”å›çš„æŒ‡æ ‡åç§°ï¼Œè€Œæµ®ç‚¹æ•°æ˜¯å…¶å€¼ï¼‰ã€‚ä¸ºäº†ä»æˆ‘ä»¬çš„æ¨¡å‹ä¸­è·å¾—é¢„æµ‹ç»“æœï¼Œå¯ä»¥ä½¿ç”¨ `Trainer.predict()` å‘½ä»¤ï¼š

```python
predictions = trainer.predict(tokenized_datasets["validation"])
print(predictions.predictions.shape, predictions.label_ids.shape)
```

```python
(408, 2) (408,)
```

`predict()` æ–¹æ³•çš„è¾“å‡ºå¦ä¸€ä¸ªå¸¦æœ‰ä¸‰ä¸ªå­—æ®µçš„å‘½åå…ƒç»„: `predictions`  `label_ids` å’Œ `metrics`  `metrics` å­—æ®µå°†åªåŒ…å«æ‰€ä¼ é€’çš„æ•°æ®é›†çš„æŸå¤±,ä»¥åŠä¸€äº›æ—¶é—´æŒ‡æ ‡(æ€»å…±èŠ±è´¹çš„æ—¶é—´å’Œå¹³å‡é¢„æµ‹æ—¶é—´)ã€‚å½“æˆ‘ä»¬å®šä¹‰äº†è‡ªå·±çš„ `compute_metrics()` å‡½æ•°å¹¶å°†å…¶ä¼ é€’ç»™ `Trainer` è¯¥å­—æ®µè¿˜å°†åŒ…å« `compute_metrics()` è¿”å›çš„ç»“æœã€‚`predict()` æ˜¯ä¸€ä¸ªäºŒç»´æ•°ç»„,å½¢çŠ¶ä¸º 408 Ã— 2(408 æ˜¯æˆ‘ä»¬ä½¿ç”¨çš„æ•°æ®é›†ä¸­çš„å…ƒç´ æ•°é‡),è¿™æ˜¯æˆ‘ä»¬ä¼ é€’ç»™ `pprdict()` çš„æ•°æ®é›†ä¸­æ¯ä¸ªå…ƒç´ çš„ logits(æ­£å¦‚åœ¨å‰ä¸€ç« ä¸­çœ‹åˆ°çš„,æ‰€æœ‰ Transformer æ¨¡å‹éƒ½è¿”å› logits)ã€‚ä¸ºäº†å°†å®ƒä»¬è½¬åŒ–ä¸ºå¯ä»¥ä¸æˆ‘ä»¬çš„æ ‡ç­¾è¿›è¡Œæ¯”è¾ƒçš„é¢„æµ‹å€¼,æˆ‘ä»¬éœ€è¦åœ¨ç¬¬äºŒè½´ä¸Šå–å€¼æœ€å¤§çš„ç´¢å¼•:

```python
import numpy as np

preds = np.argmax(predictions.predictions, axis=-1)
```

æˆ‘ä»¬ç°åœ¨å¯ä»¥å°†è¿™äº› `preds` ä¸æ ‡ç­¾è¿›è¡Œæ¯”è¾ƒã€‚ä¸ºäº†æ„å»ºæˆ‘ä»¬çš„ `compute_metric()` å‡½æ•°ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ [Evaluate](https://github.com/huggingface/evaluate/)(https://github.com/huggingface/evaluate/) åº“ä¸­çš„æŒ‡æ ‡ã€‚æˆ‘ä»¬å¯ä»¥åƒåŠ è½½æ•°æ®é›†ä¸€æ ·è½»æ¾åœ°åŠ è½½ä¸ MRPC æ•°æ®é›†å…³è”çš„æŒ‡æ ‡ï¼Œè¿™æ¬¡æ˜¯ä½¿ç”¨ `evaluate.load()` å‡½æ•°ã€‚è¿”å›çš„å¯¹è±¡æœ‰ä¸€ä¸ª `compute()` æ–¹æ³•ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨å®ƒæ¥è¿›è¡ŒæŒ‡æ ‡çš„è®¡ç®—ï¼š

```python
import evaluate

metric = evaluate.load("glue", "mrpc")
metric.compute(predictions=preds, references=predictions.label_ids)
```

```python
{'accuracy': 0.8578431372549019, 'f1': 0.8996539792387542}
```

ä½ å¾—åˆ°çš„ç¡®åˆ‡ç»“æœå¯èƒ½ä¼šæœ‰æ‰€ä¸åŒï¼Œå› ä¸ºæ¨¡å‹å¤´éƒ¨çš„éšæœºåˆå§‹åŒ–å¯èƒ½ä¼šæ”¹å˜å…¶æŒ‡æ ‡ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æˆ‘ä»¬çš„æ¨¡å‹åœ¨éªŒè¯é›†ä¸Šçš„å‡†ç¡®ç‡ä¸º 85.78ï¼…ï¼ŒF1 åˆ†æ•°ä¸º 89.97ã€‚è¿™æ˜¯ç”¨äºè¯„ä¼° MRPC æ•°æ®é›†åœ¨ GLUE åŸºå‡†æµ‹è¯•ä¸­çš„ç»“æœçš„ä¸¤ä¸ªæŒ‡æ ‡ã€‚åœ¨ BERT è®ºæ–‡ä¸­çš„è¡¨æ ¼ä¸­ï¼ŒåŸºç¡€æ¨¡å‹çš„ F1 åˆ†æ•°ä¸º 88.9ã€‚é‚£æ˜¯ uncased æ¨¡å‹ï¼Œè€Œæˆ‘ä»¬ç°åœ¨æ­£åœ¨ä½¿ç”¨ cased æ¨¡å‹ï¼Œè¿™è§£é‡Šäº†ä¸ºä»€ä¹ˆæˆ‘ä»¬å¾—åˆ°äº†æ›´å¥½çš„ç»“æœã€‚

æœ€åæŠŠæ‰€æœ‰ä¸œè¥¿æ‰“åŒ…åœ¨ä¸€èµ·ï¼Œæˆ‘ä»¬å°±å¾—åˆ°äº† `compute_metrics()` å‡½æ•°ï¼š

```python
def compute_metrics(eval_preds):
    metric = evaluate.load("glue", "mrpc")
    logits, labels = eval_preds
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)
```

ä¸ºäº†æŸ¥çœ‹æ¨¡å‹åœ¨æ¯ä¸ªè®­ç»ƒå‘¨æœŸç»“æŸæ—¶çš„å¥½åï¼Œä¸‹é¢æ˜¯æˆ‘ä»¬å¦‚ä½•ä½¿ç”¨ `compute_metrics()` å‡½æ•°å®šä¹‰ä¸€ä¸ªæ–°çš„ `Trainer` 

```python
training_args = TrainingArguments("test-trainer", evaluation_strategy="epoch")
model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)

trainer = Trainer(
    model,
    training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)
```

è¯·æ³¨æ„ï¼Œæˆ‘ä»¬è®¾ç½®äº†ä¸€ä¸ªæ–°çš„ `TrainingArguments` ï¼Œå…¶ `evaluation_strategy` è®¾ç½®ä¸º `epoch` å¹¶ä¸”åˆ›å»ºäº†ä¸€ä¸ªæ–°æ¨¡å‹ã€‚å¦‚æœä¸åˆ›å»ºæ–°çš„æ¨¡å‹å°±ç›´æ¥è®­ç»ƒï¼Œå°±åªä¼šç»§ç»­è®­ç»ƒæˆ‘ä»¬å·²ç»è®­ç»ƒè¿‡çš„æ¨¡å‹ã€‚ä¸ºäº†å¯åŠ¨æ–°çš„è®­ç»ƒï¼Œæˆ‘ä»¬æ‰§è¡Œï¼š

```python
trainer.train()
```

è¿™ä¸€æ¬¡ï¼Œå®ƒå°†åœ¨æ¯ä¸ª epoch ç»“æŸæ—¶åœ¨è®­ç»ƒæŸå¤±çš„åŸºç¡€ä¸ŠæŠ¥å‘ŠéªŒè¯æŸå¤±å’ŒæŒ‡æ ‡ã€‚åŒæ ·ï¼Œç”±äºæ¨¡å‹çš„éšæœºå¤´éƒ¨åˆå§‹åŒ–ï¼Œè¾¾åˆ°çš„å‡†ç¡®ç‡/F1 åˆ†æ•°å¯èƒ½ä¸æˆ‘ä»¬å‘ç°çš„ç•¥æœ‰ä¸åŒï¼Œè¿™æ˜¯ç”±äºæ¨¡å‹å¤´éƒ¨çš„éšæœºåˆå§‹åŒ–é€ æˆçš„ï¼Œä½†åº”è¯¥ç›¸å·®ä¸å¤šã€‚ `Trainer` å¯ä»¥åœ¨å¤šä¸ª GPU æˆ– TPU ä¸Šè¿è¡Œï¼Œå¹¶æä¾›è®¸å¤šé€‰é¡¹ï¼Œä¾‹å¦‚æ··åˆç²¾åº¦è®­ç»ƒï¼ˆåœ¨è®­ç»ƒçš„å‚æ•°ä¸­ä½¿ç”¨ `fp16 = True` ï¼‰ã€‚æˆ‘ä»¬å°†åœ¨ç¬¬åç« è®¨è®ºå®ƒæ”¯æŒçš„æ‰€æœ‰å†…å®¹ã€‚

ä½¿ç”¨ `Trainer` API å¾®è°ƒçš„ä»‹ç»åˆ°æ­¤ç»“æŸã€‚åœ¨ç¬¬å…«ç« ä¸­ä¼šç»™å‡ºä¸€ä¸ªå¯¹å¤§å¤šæ•°å¸¸è§çš„ NLP ä»»åŠ¡è¿›è¡Œè®­ç»ƒçš„ä¾‹å­ï¼Œä½†ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•åœ¨ PyTorch ä¸­åšç›¸åŒçš„æ“ä½œã€‚

<div custom-style="Tip-green">

âœï¸ **è¯•è¯•çœ‹ï¼** ä½¿ç”¨ä½ åœ¨ç¬¬ 2 èŠ‚ä¸­è¿›è¡Œçš„æ•°æ®å¤„ç†ï¼Œåœ¨ GLUE SST-2 æ•°æ®é›†ä¸Šå¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚

</div>


### ä¸€ä¸ªå®Œæ•´çš„è®­ç»ƒ 


ç°åœ¨ï¼Œæˆ‘ä»¬å°†äº†è§£å¦‚ä½•åœ¨ä¸ä½¿ç”¨ `Trainer` ç±»çš„æƒ…å†µä¸‹å®ç°ä¸ä¸Šä¸€èŠ‚ç›¸åŒçš„ç»“æœã€‚åŒæ ·ï¼Œæˆ‘ä»¬å‡è®¾ä½ å·²ç»å®Œæˆäº†ç¬¬ 2 èŠ‚ä¸­çš„æ•°æ®å¤„ç†ã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªç®€çŸ­çš„æ€»ç»“ï¼Œæ¶µç›–äº†ä½ éœ€è¦åœ¨æœ¬èŠ‚ä¹‹å‰è¿è¡Œçš„æ‰€æœ‰å†…å®¹ï¼š

```python
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding

raw_datasets = load_dataset("glue", "mrpc")
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)

tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

#### è®­ç»ƒå‰çš„å‡†å¤‡ 

åœ¨å®é™…ç¼–å†™æˆ‘ä»¬çš„è®­ç»ƒå¾ªç¯ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å®šä¹‰ä¸€äº›å¯¹è±¡ã€‚é¦–å…ˆæ˜¯æˆ‘ä»¬å°†ç”¨äºè¿­ä»£æ‰¹æ¬¡çš„æ•°æ®åŠ è½½å™¨ã€‚ä½†åœ¨å®šä¹‰è¿™äº›æ•°æ®åŠ è½½å™¨ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å¯¹æˆ‘ä»¬çš„ `tokenized_datasets` è¿›è¡Œä¸€äº›åå¤„ç†ï¼Œä»¥å¤„ç†ä¸€äº› Trainer è‡ªåŠ¨ä¸ºæˆ‘ä»¬å¤„ç†çš„å†…å®¹ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬éœ€è¦ï¼š

- åˆ é™¤ä¸æ¨¡å‹ä¸éœ€è¦çš„åˆ—ï¼ˆå¦‚ `sentence1` å’Œ `sentence2` åˆ—ï¼‰ã€‚
- å°†åˆ—å `label` é‡å‘½åä¸º `labels` ï¼ˆå› ä¸ºæ¨¡å‹é»˜è®¤çš„å‚æ•°æ˜¯ `labels` ï¼‰ã€‚
- è®¾ç½®æ•°æ®é›†çš„æ ¼å¼ï¼Œä½¿å…¶è¿”å› PyTorch å¼ é‡è€Œä¸æ˜¯åˆ—è¡¨ã€‚

é’ˆå¯¹ä¸Šé¢çš„æ¯ä¸ªæ­¥éª¤ï¼Œæˆ‘ä»¬çš„ `tokenized_datasets` éƒ½æœ‰ä¸€ä¸ªæ–¹æ³•ï¼š

```python
tokenized_datasets = tokenized_datasets.remove_columns(["sentence1", "sentence2", "idx"])
tokenized_datasets = tokenized_datasets.rename_column("label", "labels")
tokenized_datasets.set_format("torch")
tokenized_datasets["train"].column_names
```

ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥æ£€æŸ¥ç»“æœä¸­æ˜¯å¦åªæœ‰æ¨¡å‹èƒ½å¤Ÿæ¥å—çš„åˆ—ï¼š

```python
["attention_mask", "input_ids", "labels", "token_type_ids"]
```

è‡³æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥è½»æ¾å®šä¹‰æ•°æ®åŠ è½½å™¨ï¼š

```python
from torch.utils.data import DataLoader

train_dataloader = DataLoader(
    tokenized_datasets["train"], shuffle=True, batch_size=8, collate_fn=data_collator
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], batch_size=8, collate_fn=data_collator
)
```

ä¸ºäº†å¿«é€Ÿæ£€éªŒæ•°æ®å¤„ç†ä¸­æ²¡æœ‰é”™è¯¯ï¼Œæˆ‘ä»¬å¯ä»¥è¿™æ ·æ£€éªŒå…¶ä¸­çš„ä¸€ä¸ª batchï¼š

```python
for batch in train_dataloader:
    break
{k: v.shape for k, v in batch.items()}
```

```python
{'attention_mask': torch.Size([8, 65]),
 'input_ids': torch.Size([8, 65]),
 'labels': torch.Size([8]),
 'token_type_ids': torch.Size([8, 65])}
```

è¯·æ³¨æ„ï¼Œå®é™…çš„å½¢çŠ¶å¯èƒ½ä¸ä½ ç•¥æœ‰ä¸åŒï¼Œå› ä¸ºæˆ‘ä»¬ä¸ºè®­ç»ƒæ•°æ®åŠ è½½å™¨è®¾ç½®äº† `shuffle=True` ï¼Œå¹¶ä¸”æ¨¡å‹ä¼šå°†å¥å­å¡«å……åˆ° `batch` ä¸­çš„æœ€å¤§é•¿åº¦ã€‚

ç°åœ¨æˆ‘ä»¬å·²ç»å®Œå…¨å®Œæˆäº†æ•°æ®é¢„å¤„ç†ï¼ˆå¯¹äºä»»ä½• ML ä»ä¸šè€…æ¥è¯´éƒ½æ˜¯ä¸€ä¸ªä»¤äººæ»¡æ„ä½†éš¾ä»¥å®ç°çš„ç›®æ ‡ï¼‰ï¼Œè®©æˆ‘ä»¬å°†æ³¨æ„åŠ›è½¬å‘æ¨¡å‹ã€‚æˆ‘ä»¬ä¼šåƒåœ¨ä¸Šä¸€èŠ‚ä¸­æ‰€åšçš„é‚£æ ·å®ä¾‹åŒ–å®ƒï¼š

```python
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
```

ä¸ºäº†ç¡®ä¿è®­ç»ƒè¿‡ç¨‹ä¸­ä¸€åˆ‡é¡ºåˆ©ï¼Œæˆ‘ä»¬å°† `batch` ä¼ é€’ç»™è¿™ä¸ªæ¨¡å‹ï¼š

```python
outputs = model(**batch)
print(outputs.loss, outputs.logits.shape)
```

```python
tensor(0.5441, grad_fn=<NllLossBackward>) torch.Size([8, 2])
```

å½“æˆ‘ä»¬æä¾› `labels` æ—¶ï¼ŒTransformers æ¨¡å‹éƒ½å°†è¿”å›è¿™ä¸ª `batch` çš„ `loss` ï¼Œæˆ‘ä»¬è¿˜å¾—åˆ°äº† `logits` ï¼ˆ `batch` ä¸­çš„æ¯ä¸ªè¾“å…¥æœ‰ä¸¤ä¸ªï¼Œæ‰€ä»¥å¼ é‡å¤§å°ä¸º 8 x 2ï¼‰ã€‚

æˆ‘ä»¬å‡ ä¹å‡†å¤‡å¥½ç¼–å†™æˆ‘ä»¬çš„è®­ç»ƒå¾ªç¯äº†ï¼æˆ‘ä»¬åªæ˜¯ç¼ºå°‘ä¸¤ä»¶äº‹ï¼šä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨ã€‚ç”±äºæˆ‘ä»¬è¯•å›¾æ‰‹åŠ¨å®ç° `Trainer` çš„åŠŸèƒ½ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ç›¸åŒçš„ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨ã€‚ `Trainer` ä½¿ç”¨çš„ä¼˜åŒ–å™¨æ˜¯ `AdamW` ï¼Œå®ƒä¸ `Adam` ç›¸åŒï¼Œä½†åŠ å…¥äº†æƒé‡è¡°å‡æ­£åˆ™åŒ–çš„ä¸€ç‚¹å˜åŒ–ï¼ˆå‚è§ Ilya Loshchilov å’Œ Frank Hutter çš„ [â€œDecoupled Weight Decay Regularizationâ€](https://arxiv.org/abs/1711.05101)(https://arxiv.org/abs/1711.05101) ï¼‰ï¼š

```python
from transformers import AdamW

optimizer = AdamW(model.parameters(), lr=5e-5)
```

æœ€åï¼Œé»˜è®¤ä½¿ç”¨çš„å­¦ä¹ ç‡è°ƒåº¦å™¨åªæ˜¯ä»æœ€å¤§å€¼ ï¼ˆ5e-5ï¼‰ åˆ° 0 çš„çº¿æ€§è¡°å‡ã€‚ä¸ºäº†å®šä¹‰å®ƒï¼Œæˆ‘ä»¬éœ€è¦çŸ¥é“æˆ‘ä»¬è®­ç»ƒçš„æ¬¡æ•°ï¼Œå³æ‰€æœ‰æ•°æ®è®­ç»ƒçš„æ¬¡æ•°ï¼ˆepochsï¼‰ä¹˜ä»¥çš„ batch çš„æ•°é‡ï¼ˆå³æˆ‘ä»¬è®­ç»ƒæ•°æ®åŠ è½½å™¨çš„é•¿åº¦ï¼‰ã€‚ `Trainer` é»˜è®¤æƒ…å†µä¸‹ä½¿ç”¨ä¸‰ä¸ª `epochs` ï¼Œå› æ­¤æˆ‘ä»¬å®šä¹‰è®­ç»ƒè¿‡ç¨‹å¦‚ä¸‹ï¼š

```python
from transformers import get_scheduler

num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
print(num_training_steps)
```

```python
1377
```

#### è®­ç»ƒå¾ªç¯ 

æœ€åä¸€ä»¶äº‹ï¼šå¦‚æœæˆ‘ä»¬å¯ä»¥è®¿é—® GPUï¼Œæˆ‘ä»¬å°†å¸Œæœ›ä½¿ç”¨ GPUï¼ˆåœ¨ CPU ä¸Šï¼Œè®­ç»ƒå¯èƒ½éœ€è¦å‡ ä¸ªå°æ—¶è€Œä¸æ˜¯å‡ åˆ†é’Ÿï¼‰ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ª `device` ï¼Œå®ƒåœ¨ GPU å¯ç”¨çš„æƒ…å†µä¸‹æŒ‡å‘ GPU æˆ‘ä»¬å°†æŠŠæˆ‘ä»¬çš„æ¨¡å‹å’Œ `batch` æ”¾åœ¨ `device` ä¸Šï¼š

```python
import torch

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)
device
```

```python
device(type='cuda')
```

æˆ‘ä»¬ç°åœ¨å‡†å¤‡å¥½è®­ç»ƒäº†ï¼ä¸ºäº†çŸ¥é“è®­ç»ƒä½•æ—¶ç»“æŸï¼Œæˆ‘ä»¬ä½¿ç”¨ `tqdm` åº“ï¼Œåœ¨è®­ç»ƒæ­¥éª¤æ•°ä¸Šæ·»åŠ äº†ä¸€ä¸ªè¿›åº¦æ¡ï¼š

```python
from tqdm.auto import tqdm

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

ä½ å¯ä»¥çœ‹åˆ°è®­ç»ƒå¾ªç¯çš„æ ¸å¿ƒä¸ä»‹ç»ä¸­çš„éå¸¸ç›¸ä¼¼ã€‚æˆ‘ä»¬æ²¡æœ‰è¦æ±‚ä»»ä½•æ£€éªŒï¼Œæ‰€ä»¥è¿™ä¸ªè®­ç»ƒå¾ªç¯ä¸ä¼šå‘Šè¯‰æˆ‘ä»¬ä»»ä½•å…³äºæ¨¡å‹ç›®å‰çš„çŠ¶æ€ã€‚æˆ‘ä»¬éœ€è¦ä¸ºæ­¤æ·»åŠ ä¸€ä¸ªè¯„ä¼°å¾ªç¯ã€‚

#### è¯„ä¼°å¾ªç¯ 

æ­£å¦‚æˆ‘ä»¬ä¹‹å‰æ‰€åšçš„é‚£æ ·ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ Evaluate åº“æä¾›çš„æŒ‡æ ‡ã€‚æˆ‘ä»¬å·²ç»äº†è§£äº† `metric.compute()` æ–¹æ³•ï¼Œå½“æˆ‘ä»¬ä½¿ç”¨ `add_batch()` æ–¹æ³•è¿›è¡Œé¢„æµ‹å¾ªç¯æ—¶ï¼Œå®é™…ä¸Šè¯¥æŒ‡æ ‡å¯ä»¥ä¸ºæˆ‘ä»¬ç´¯ç§¯æ‰€æœ‰ `batch` çš„ç»“æœã€‚ä¸€æ—¦æˆ‘ä»¬ç´¯ç§¯äº†æ‰€æœ‰ `batch` ï¼Œæˆ‘ä»¬å°±å¯ä»¥ä½¿ç”¨ `metric.compute()` å¾—åˆ°æœ€ç»ˆç»“æœã€‚ä»¥ä¸‹æ˜¯å¦‚ä½•åœ¨è¯„ä¼°å¾ªç¯ä¸­å®ç°æ‰€æœ‰è¿™äº›çš„æ–¹æ³•ï¼š

```python
import evaluate

metric = evaluate.load("glue", "mrpc")
model.eval()
for batch in eval_dataloader:
    batch = {k: v.to(device) for k, v in batch.items()}
    with torch.no_grad():
        outputs = model(**batch)

    logits = outputs.logits
    predictions = torch.argmax(logits, dim=-1)
    metric.add_batch(predictions=predictions, references=batch["labels"])

metric.compute()
```

```python
{'accuracy': 0.8431372549019608, 'f1': 0.8907849829351535}
```

åŒæ ·ï¼Œç”±äºæ¨¡å‹å¤´éƒ¨åˆå§‹åŒ–å’Œæ•°æ®æ‰“ä¹±çš„éšæœºæ€§ï¼Œä½ çš„ç»“æœä¼šç•¥æœ‰ä¸åŒï¼Œä½†åº”è¯¥ç›¸å·®ä¸å¤šã€‚

<div custom-style="Tip-green">

âœï¸ **è¯•è¯•çœ‹ï¼** ä¿®æ”¹ä¹‹å‰çš„è®­ç»ƒå¾ªç¯ä»¥åœ¨ SST-2 æ•°æ®é›†ä¸Šå¾®è°ƒä½ çš„æ¨¡å‹ã€‚

</div>

#### ä½¿ç”¨Accelerate åŠ é€Ÿä½ çš„è®­ç»ƒå¾ªç¯ 

æˆ‘ä»¬ä¹‹å‰å®šä¹‰çš„è®­ç»ƒå¾ªç¯åœ¨å•ä¸ª CPU æˆ– GPU ä¸Šè¿è¡Œè‰¯å¥½ã€‚ä½†æ˜¯ä½¿ç”¨ [Accelerate](https://github.com/huggingface/accelerate)(https://github.com/huggingface/accelerate) åº“ï¼Œåªéœ€è¿›è¡Œä¸€äº›è°ƒæ•´ï¼Œæˆ‘ä»¬å°±å¯ä»¥åœ¨å¤šä¸ª GPU æˆ– TPU ä¸Šå¯ç”¨åˆ†å¸ƒå¼è®­ç»ƒã€‚ä»åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æ•°æ®åŠ è½½å™¨å¼€å§‹ï¼Œæˆ‘ä»¬çš„æ‰‹åŠ¨è®­ç»ƒå¾ªç¯å¦‚ä¸‹æ‰€ç¤ºï¼š

```python
from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
optimizer = AdamW(model.parameters(), lr=3e-5)

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)

num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

ä»¥ä¸‹æ˜¯æ›´æ”¹çš„éƒ¨åˆ†ï¼š

```diff
+ from accelerate import Accelerator
  from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

+ accelerator = Accelerator()

  model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
  optimizer = AdamW(model.parameters(), lr=3e-5)

- device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
- model.to(device)

+ train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(
+     train_dataloader, eval_dataloader, model, optimizer
+ )

  num_epochs = 3
  num_training_steps = num_epochs * len(train_dataloader)
  lr_scheduler = get_scheduler(
      "linear",
      optimizer=optimizer,
      num_warmup_steps=0,
      num_training_steps=num_training_steps
  )

  progress_bar = tqdm(range(num_training_steps))

  model.train()
  for epoch in range(num_epochs):
      for batch in train_dataloader:
-         batch = {k: v.to(device) for k, v in batch.items()}
          outputs = model(**batch)
          loss = outputs.loss
-         loss.backward()
+         accelerator.backward(loss)

          optimizer.step()
          lr_scheduler.step()
          optimizer.zero_grad()
          progress_bar.update(1)
```

è¦æ·»åŠ çš„ç¬¬ä¸€è¡Œæ˜¯å¯¼å…¥ `Accelerator` ã€‚ç¬¬äºŒè¡Œå®ä¾‹åŒ–ä¸€ä¸ª `Accelerator` å¯¹è±¡ å®ƒå°†æŸ¥çœ‹ç¯å¢ƒå¹¶åˆå§‹åŒ–é€‚å½“çš„åˆ†å¸ƒå¼è®¾ç½®ã€‚Accelerate ä¸ºä½ å¤„ç†æ•°æ®åœ¨è®¾å¤‡é—´çš„ä¼ é€’ï¼Œå› æ­¤ä½ å¯ä»¥åˆ é™¤å°†æ¨¡å‹æ”¾åœ¨è®¾å¤‡ä¸Šçš„é‚£è¡Œä»£ç ï¼ˆæˆ–è€…ï¼Œå¦‚æœä½ æ„¿æ„ï¼Œå¯ä½¿ç”¨ `accelerator.device` ä»£æ›¿ `device` ï¼‰ã€‚

ç„¶åå¤§éƒ¨åˆ†å·¥ä½œä¼šåœ¨å°†æ•°æ®åŠ è½½å™¨ã€æ¨¡å‹å’Œä¼˜åŒ–å™¨å‘é€åˆ°çš„ `accelerator.prepare()` ä¸­å®Œæˆã€‚è¿™å°†ä¼šæŠŠè¿™äº›å¯¹è±¡åŒ…è£…åœ¨é€‚å½“çš„å®¹å™¨ä¸­ï¼Œä»¥ç¡®ä¿ä½ çš„åˆ†å¸ƒå¼è®­ç»ƒæŒ‰é¢„æœŸå·¥ä½œã€‚è¦è¿›è¡Œçš„å…¶ä½™æ›´æ”¹æ˜¯åˆ é™¤å°† `batch` æ”¾åœ¨ `device` çš„é‚£è¡Œä»£ç ï¼ˆåŒæ ·ï¼Œå¦‚æœä½ æƒ³ä¿ç•™å®ƒï¼Œä½ å¯ä»¥å°†å…¶æ›´æ”¹ä¸ºä½¿ç”¨ `accelerator.device` ï¼‰ å¹¶å°† `loss.backward()` æ›¿æ¢ä¸º `accelerator.backward(loss)` ã€‚

<div custom-style="Tip-yellow">

âš ï¸ ä¸ºäº†ä½¿äº‘ç«¯ TPU æä¾›çš„åŠ é€Ÿä¸­å‘æŒ¥æœ€å¤§çš„æ•ˆç›Šï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨ tokenizer çš„ `padding=max_length` å’Œ `max_length` å‚æ•°å°†ä½ çš„æ ·æœ¬å¡«å……åˆ°å›ºå®šé•¿åº¦ã€‚

</div>

å¦‚æœä½ æƒ³å¤åˆ¶å¹¶ç²˜è´´æ¥ç›´æ¥è¿è¡Œï¼Œä»¥ä¸‹æ˜¯ Accelerate çš„å®Œæ•´è®­ç»ƒå¾ªç¯ï¼š

```python
from accelerate import Accelerator
from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

accelerator = Accelerator()

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
optimizer = AdamW(model.parameters(), lr=3e-5)

train_dl, eval_dl, model, optimizer = accelerator.prepare(
    train_dataloader, eval_dataloader, model, optimizer
)

num_epochs = 3
num_training_steps = num_epochs * len(train_dl)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dl:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
```

æŠŠè¿™ä¸ªæ”¾åœ¨ `train.py` æ–‡ä»¶ä¸­ï¼Œå¯ä»¥è®©å®ƒåœ¨ä»»ä½•ç±»å‹çš„åˆ†å¸ƒå¼è®¾ç½®ä¸Šè¿è¡Œã€‚è¦åœ¨åˆ†å¸ƒå¼è®¾ç½®ä¸­è¯•ç”¨å®ƒï¼Œè¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

```python
accelerate config
```

è¿™å°†è¯¢é—®ä½ å‡ ä¸ªé…ç½®çš„é—®é¢˜å¹¶å°†ä½ çš„å›ç­”ä¿å­˜åˆ°æ­¤å‘½ä»¤ä½¿ç”¨çš„é…ç½®æ–‡ä»¶ä¸­ï¼š

```python
accelerate launch train.py
```

è¿™å°†å¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒ

è¿™å°†å¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒã€‚å¦‚æœä½ æƒ³åœ¨ Notebook ä¸­å°è¯•æ­¤æ“ä½œï¼ˆä¾‹å¦‚ï¼Œåœ¨ Colab ä¸Šä½¿ç”¨ TPU è¿›è¡Œæµ‹è¯•ï¼‰ï¼Œåªéœ€å°†ä»£ç ç²˜è´´åˆ°ä¸€ä¸ª `training_function()` å‡½æ•°ä¸­ï¼Œå¹¶åœ¨æœ€åä¸€ä¸ªå•å…ƒæ ¼ä¸­è¿è¡Œï¼š

```python
from accelerate import notebook_launcher

notebook_launcher(training_function)
```

ä½ å¯ä»¥åœ¨ [Accelerate repo](https://github.com/huggingface/accelerate/tree/main/examples)(https://github.com/huggingface/accelerate/tree/main/examples) æ‰¾åˆ°æ›´å¤šçš„ç¤ºä¾‹ã€‚


## 4.3 ä½¿ç”¨ Keras å¾®è°ƒæ¨¡å‹ 

ä¸€æ—¦å®Œæˆä¸Šä¸€èŠ‚ä¸­çš„æ‰€æœ‰æ•°æ®é¢„å¤„ç†å·¥ä½œåï¼Œä½ åªå‰©ä¸‹æœ€åçš„å‡ ä¸ªæ­¥éª¤æ¥è®­ç»ƒæ¨¡å‹ã€‚ ä½†æ˜¯è¯·æ³¨æ„ï¼Œ`model.fit()` å‘½ä»¤åœ¨ CPU ä¸Šè¿è¡Œä¼šéå¸¸ç¼“æ…¢ã€‚ å¦‚æœä½ æ²¡æœ‰GPUï¼Œä½ å¯ä»¥åœ¨ [Google Colab](https://colab.research.google.com)(https://colab.research.google.com)ï¼ˆå›½å†…ç½‘ç»œæ— æ³•ä½¿ç”¨ï¼‰ ä¸Šä½¿ç”¨å…è´¹çš„ GPU æˆ– TPUã€‚

ä¸‹é¢çš„ä»£ç ç¤ºä¾‹å‡è®¾ä½ å·²ç»è¿è¡Œäº†ä¸Šä¸€èŠ‚ä¸­çš„ä»£ç ç¤ºä¾‹ã€‚ ä¸‹é¢æ˜¯åœ¨å¼€å§‹å­¦ä¹ è¿™ä¸€èŠ‚ä¹‹å‰ä½ éœ€è¦è¿è¡Œçš„ä»£ç ï¼š

```python
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding
import numpy as np

raw_datasets = load_dataset("glue", "mrpc")
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)

tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)

data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="tf")

tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)

tf_validation_dataset = tokenized_datasets["validation"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=False,
    collate_fn=data_collator,
    batch_size=8,
)
```

### è®­ç»ƒæ¨¡å‹ 

ä»Transformers å¯¼å…¥çš„ TensorFlow æ¨¡å‹å·²ç»æ˜¯ Keras æ¨¡å‹ã€‚

è¿™æ„å‘³ç€ä¸€æ—¦æˆ‘ä»¬æœ‰äº†æ•°æ®ï¼Œåªéœ€è¦å¾ˆå°‘çš„å·¥ä½œå°±å¯ä»¥å¼€å§‹å¯¹å…¶è¿›è¡Œè®­ç»ƒã€‚

å’Œç¬¬ä¸‰ç« ä½¿ç”¨çš„æ–¹æ³•ä¸€æ ·, æˆ‘ä»¬å°†ä½¿ç”¨äºŒåˆ†ç±»çš„ `TFAutoModelForSequenceClassification`ç±»ï¼Œæˆ‘ä»¬å°†æœ‰ä¸¤ä¸ªæ ‡ç­¾: 

```python
from transformers import TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
```

ä½ ä¼šæ³¨æ„åˆ°ï¼Œä¸ç¬¬ä¸‰ç« ä¸åŒçš„æ˜¯ï¼Œåœ¨å®ä¾‹åŒ–è¿™ä¸ªé¢„è®­ç»ƒçš„æ¨¡å‹åä¼šæ”¶åˆ°è­¦å‘Šã€‚ è¿™æ˜¯å› ä¸º BERT æ²¡æœ‰å¯¹å¥å­å¯¹çš„åˆ†ç±»è¿›è¡Œé¢„è®­ç»ƒï¼Œæ‰€ä»¥é¢„è®­ç»ƒæ¨¡å‹çš„ head å·²ç»è¢«ä¸¢å¼ƒï¼Œå¹¶ä¸”æ’å…¥äº†ä¸€ä¸ªé€‚åˆåºåˆ—åˆ†ç±»çš„æ–° headã€‚ è­¦å‘Šè¡¨æ˜ï¼Œè¿™äº›æƒé‡æ²¡æœ‰ä½¿ç”¨ï¼ˆå¯¹åº”äºä¸¢å¼ƒçš„é¢„è®­ç»ƒ head æƒé‡ï¼‰ï¼Œè€Œå…¶ä»–ä¸€äº›æƒé‡æ˜¯éšæœºåˆå§‹åŒ–çš„ï¼ˆå¯¹åº”äºæ–° head çš„æƒé‡ï¼‰ã€‚ æœ€åå®ƒå»ºè®®ä½ è®­ç»ƒæ¨¡å‹ï¼Œè¿™æ­£æ˜¯æˆ‘ä»¬ç°åœ¨è¦åšçš„ã€‚

ä¸ºäº†åœ¨æˆ‘ä»¬çš„æ•°æ®é›†ä¸Šå¾®è°ƒæ¨¡å‹ï¼Œæˆ‘ä»¬åªéœ€è¦åœ¨æˆ‘ä»¬çš„æ¨¡å‹ä¸Šè°ƒç”¨ `compile()` æ–¹æ³•ï¼Œç„¶åå°†æˆ‘ä»¬çš„æ•°æ®ä¼ é€’ç»™ `fit()` æ–¹æ³•ã€‚ è¿™å°†å¯åŠ¨å¾®è°ƒè¿‡ç¨‹ï¼ˆåœ¨ GPU ä¸Šåº”è¯¥éœ€è¦å‡ åˆ†é’Ÿï¼‰å¹¶è¾“å‡ºè®­ç»ƒæŸå¤±ï¼Œä»¥åŠæ¯ä¸ª epoch ç»“æŸæ—¶çš„éªŒè¯æŸå¤±ã€‚

<div custom-style="Tip-green">

è¯·æ³¨æ„Transformers æ¨¡å‹å…·æœ‰å¤§å¤šæ•° Keras æ¨¡å‹æ‰€æ²¡æœ‰çš„ç‰¹æ®Šèƒ½åŠ›â€”â€”å®ƒä»¬å¯ä»¥è‡ªåŠ¨ä½¿ç”¨å†…éƒ¨è®¡ç®—çš„æŸå¤±ã€‚ å¦‚æœä½ æ²¡æœ‰åœ¨ `compile()` ä¸­è®¾ç½®æŸå¤±å‚æ•°ï¼Œå®ƒä»¬å¯ä»¥è‡ªåŠ¨ä½¿ç”¨é€‚å½“çš„æŸå¤±å‡½æ•°ï¼Œå¹¶åœ¨å†…éƒ¨è®¡ç®—ã€‚ è¯·æ³¨æ„ï¼Œè¦ä½¿ç”¨å†…éƒ¨æŸå¤±ï¼Œä½ éœ€è¦å°†æ ‡ç­¾ä½œä¸ºè¾“å…¥çš„ä¸€éƒ¨åˆ†ä¼ å…¥æ¨¡å‹ï¼Œè€Œä¸æ˜¯ä½œä¸ºå•ç‹¬çš„æ ‡ç­¾ï¼ˆè¿™æ˜¯åœ¨ Keras æ¨¡å‹ä¸­ä½¿ç”¨æ ‡ç­¾çš„å¸¸è§„æ–¹å¼ï¼‰ã€‚ ä½ å°†åœ¨è¯¾ç¨‹çš„ç¬¬ 2 éƒ¨åˆ†ä¸­çœ‹åˆ°è¿™æ–¹é¢çš„ç¤ºä¾‹ï¼Œæ­£ç¡®å®šä¹‰æŸå¤±å‡½æ•°å¯èƒ½ä¼šæœ‰äº›æ£˜æ‰‹ã€‚ ç„¶è€Œå¯¹äºåºåˆ—åˆ†ç±»æ¥è¯´ï¼Œæ ‡å‡†çš„ Keras æŸå¤±å‡½æ•°æ•ˆæœå¾ˆå¥½ï¼Œå› æ­¤æˆ‘ä»¬å°†åœ¨è¿™é‡Œä½¿ç”¨å®ƒã€‚

</div>

```python
from tensorflow.keras.losses import SparseCategoricalCrossentropy

model.compile(
    optimizer="adam",
    loss=SparseCategoricalCrossentropy(from_logits=True),
    metrics=["accuracy"],
)
model.fit(
    tf_train_dataset,
    validation_data=tf_validation_dataset,
)
```

<div custom-style="Tip-green">

è¯·æ³¨æ„è¿™é‡Œæœ‰ä¸€ä¸ªéå¸¸å¸¸è§çš„é™·é˜±â€”â€”ä½ å¯ä»¥æŠŠæŸå¤±çš„åç§°ä½œä¸ºä¸€ä¸ªå­—ç¬¦ä¸²ä¼ é€’ç»™ Kerasï¼Œä½†é»˜è®¤æƒ…å†µä¸‹ï¼ŒKeras ä¼šå‡è®¾ä½ å·²ç»å¯¹è¾“å‡ºè¿›è¡Œäº† softmaxã€‚ ç„¶è€Œï¼Œè®¸å¤šæ¨¡å‹åœ¨ç»è¿‡ softmax å‡½æ•°ä¹‹å‰è¾“å‡ºçš„æ˜¯è¢«ç§°ä¸º `logits` çš„å€¼ã€‚ æˆ‘ä»¬éœ€è¦å‘Šè¯‰æŸå¤±å‡½æ•°ï¼Œæˆ‘ä»¬çš„æ¨¡å‹æ˜¯å¦å·²ç»ä½¿ç”¨ softmax å‡½æ•°è¿›è¡Œäº†å¤„ç†ï¼Œå”¯ä¸€çš„æ–¹æ³•æ˜¯ä¼ é€’ä¸€ä¸ªæŸå¤±å‡½æ•°å¹¶ä¸”åœ¨å‚æ•°çš„éƒ¨åˆ†å‘Šè¯‰æ¨¡å‹ï¼Œè€Œä¸æ˜¯åªä¼ é€’ä¸€ä¸ªå­—ç¬¦ä¸²ã€‚

</div>

### æ”¹å–„è®­ç»ƒçš„æ•ˆæœ 

å¦‚æœä½ å°è¯•ä¸Šè¿°ä»£ç ï¼Œå®ƒçš„ç¡®å¯ä»¥è¿è¡Œï¼Œä½†ä½ ä¼šå‘ç°æŸå¤±ä¸‹é™å¾—å¾ˆæ…¢æˆ–è€…ä¸è§„å¾‹ã€‚ ä¸»è¦åŸå› æ˜¯`å­¦ä¹ ç‡`ã€‚ ä¸æŸå¤±ä¸€æ ·ï¼Œå½“æˆ‘ä»¬æŠŠä¼˜åŒ–å™¨çš„åç§°ä½œä¸ºå­—ç¬¦ä¸²ä¼ é€’ç»™ Keras æ—¶ï¼ŒKeras ä¼šåˆå§‹åŒ–è¯¥ä¼˜åŒ–å™¨å…·æœ‰æ‰€æœ‰å‚æ•°çš„é»˜è®¤å€¼ï¼ŒåŒ…æ‹¬å­¦ä¹ ç‡ã€‚ ä¸è¿‡æ ¹æ®é•¿æœŸç»éªŒï¼Œæˆ‘ä»¬çŸ¥é“Transformer æ¨¡å‹çš„é€šå¸¸çš„æœ€ä½³å­¦ä¹ ç‡æ¯” Adam çš„é»˜è®¤å€¼ï¼ˆå³1e-3ï¼Œä¹Ÿå†™æˆä¸º 10 çš„ -3 æ¬¡æ–¹ï¼Œæˆ– 0.001ï¼‰ä½å¾—å¤šã€‚ 5e-5ï¼ˆ0.00005ï¼‰ï¼Œæ˜¯ä¸€ä¸ªæ›´å¥½çš„èµ·å§‹ç‚¹ã€‚

é™¤äº†é™ä½å­¦ä¹ ç‡ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜æœ‰ç¬¬äºŒä¸ªæŠ€å·§ï¼šæˆ‘ä»¬å¯ä»¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ…¢æ…¢é™ä½å­¦ä¹ ç‡ã€‚åœ¨æ–‡çŒ®ä¸­ï¼Œä½ æœ‰æ—¶ä¼šçœ‹åˆ°è¿™è¢«ç§°ä¸º å­¦ä¹ ç‡çš„`è¡°å‡ï¼ˆdecayingï¼‰` æˆ– `é€€ç«ï¼ˆannealingï¼‰`ã€‚ åœ¨ Keras ä¸­ï¼Œæœ€å¥½çš„æ–¹æ³•æ˜¯ä½¿ç”¨ `å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆlearning rate schedulerï¼‰`ã€‚ ä¸€ä¸ªå¥½ç”¨çš„è°ƒåº¦å™¨æ˜¯`PolynomialDecay`â€”â€”å°½ç®¡å®ƒçš„åå­—å«`PolynomialDecayï¼ˆå¤šé¡¹å¼è¡°å‡ï¼‰`ï¼Œä½†åœ¨é»˜è®¤è®¾ç½®ä¸‹ï¼Œå®ƒåªæ˜¯ç®€å•å°†å­¦ä¹ ç‡ä»åˆå§‹å€¼çº¿æ€§è¡°å‡åˆ°æœ€ç»ˆå€¼ï¼Œè¿™æ­£æ˜¯æˆ‘ä»¬æƒ³è¦çš„ã€‚ä¸è¿‡ä¸ºäº†æ­£ç¡®ä½¿ç”¨è°ƒåº¦ç¨‹åºï¼Œæˆ‘ä»¬éœ€è¦å‘Šè¯‰å®ƒè®­ç»ƒçš„æ¬¡æ•°ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡ä¸‹é¢çš„`num_train_steps`è®¡ç®—å¾—åˆ°ã€‚

```python
from tensorflow.keras.optimizers.schedules import PolynomialDecay

batch_size = 8
num_epochs = 3

# è®­ç»ƒæ­¥æ•°æ˜¯æ•°æ®é›†ä¸­çš„æ ·æœ¬æ•°é‡,é™¤ä»¥ batch å¤§å°,ç„¶åä¹˜ä»¥æ€»çš„ epoch æ•°ã€‚
# æ³¨æ„è¿™é‡Œçš„ tf_train_dataset æ˜¯ batch å½¢å¼çš„ tf.data.Dataset,
# è€Œä¸æ˜¯åŸå§‹çš„ Hugging Face Dataset ,æ‰€ä»¥ä½¿ç”¨ len() è®¡ç®—å®ƒçš„é•¿åº¦å·²ç»æ˜¯ num_samples // batch_sizeã€‚
num_train_steps = len(tf_train_dataset) * num_epochs
lr_scheduler = PolynomialDecay(
    initial_learning_rate=5e-5, end_learning_rate=0.0, decay_steps=num_train_steps
)
from tensorflow.keras.optimizers import Adam

opt = Adam(learning_rate=lr_scheduler)
```

<div custom-style="Tip-green">

Transformers åº“è¿˜æœ‰ä¸€ä¸ª `create_optimizer()` å‡½æ•°ï¼Œå®ƒå°†åˆ›å»ºä¸€ä¸ªå…·æœ‰å­¦ä¹ ç‡è¡°å‡çš„ `AdamW` ä¼˜åŒ–å™¨ã€‚ è¿™æ˜¯ä¸€ä¸ªå¿«æ·çš„æ–¹å¼ï¼Œä½ å°†åœ¨æœ¬è¯¾ç¨‹çš„åç»­éƒ¨åˆ†ä¸­è¯¦ç»†äº†è§£ã€‚

</div>

ç°åœ¨æˆ‘ä»¬æœ‰äº†å…¨æ–°çš„ä¼˜åŒ–å™¨ï¼Œæˆ‘ä»¬å¯ä»¥å°è¯•ä½¿ç”¨å®ƒè¿›è¡Œè®­ç»ƒã€‚ é¦–å…ˆï¼Œè®©æˆ‘ä»¬é‡æ–°åŠ è½½æ¨¡å‹ï¼Œé‡æ–°è®¾ç½®åˆšåˆšè®­ç»ƒæ—¶çš„æƒé‡ï¼Œç„¶åæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ–°çš„ä¼˜åŒ–å™¨å¯¹å…¶è¿›è¡Œç¼–è¯‘ï¼š

```python
import tensorflow as tf

model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
model.compile(optimizer=opt, loss=loss, metrics=["accuracy"])
```

ç°åœ¨ï¼Œæˆ‘ä»¬å†æ¬¡è¿›è¡Œfitï¼š

```python
model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=3)
```

<div custom-style="Tip-green">

ğŸ’¡ å¦‚æœä½ æƒ³åœ¨è®­ç»ƒæœŸé—´è‡ªåŠ¨å°†æ¨¡å‹ä¸Šä¼ åˆ° Hubï¼Œä½ å¯ä»¥åœ¨ `model.fit()` æ–¹æ³•ä¸­ä¼ é€’ä¸€ä¸ª `PushToHubCallback`ã€‚ æˆ‘ä»¬å°†åœ¨ç¬¬äº”ç« ä¸­è¿›ä¸€æ­¥äº†è§£è¿™ä¸ªé—®é¢˜ã€‚

</div>

### æ¨¡å‹é¢„æµ‹ 


è®­ç»ƒå’Œè§‚å¯ŸæŸå¤±çš„ä¸‹é™éƒ½æ˜¯éå¸¸å¥½ï¼Œä½†å¦‚æœæˆ‘ä»¬æƒ³ä»è®­ç»ƒåçš„æ¨¡å‹ä¸­å¾—åˆ°è¾“å‡ºï¼Œæˆ–è€…è®¡ç®—ä¸€äº›æŒ‡æ ‡ï¼Œæˆ–è€…åœ¨ç”Ÿäº§ä¸­ä½¿ç”¨æ¨¡å‹ï¼Œè¯¥æ€ä¹ˆåŠå‘¢ï¼Ÿä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨`predict()` æ–¹æ³•ã€‚ è¿™å°†è¿”å›æ¨¡å‹çš„è¾“å‡ºå¤´çš„`logits`æ•°å€¼ï¼Œæ¯ä¸ªç±»ä¸€ä¸ªã€‚

```python
preds = model.predict(tf_validation_dataset)["logits"]
```

æˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿ç”¨ argmax å°†è¿™äº› logits è½¬æ¢ä¸ºæ¨¡å‹çš„ç±»åˆ«é¢„æµ‹ï¼Œæœ€é«˜çš„ logitsï¼Œå¯¹åº”äºæœ€æœ‰å¯èƒ½çš„ç±»åˆ«

```python
class_preds = np.argmax(preds, axis=1)
print(preds.shape, class_preds.shape)
```

```python
(408, 2) (408,)
```

ç°åœ¨ï¼Œè®©æˆ‘ä»¬ä½¿ç”¨è¿™äº› `preds` æ¥è®¡ç®—ä¸€äº›æŒ‡æ ‡ï¼ æˆ‘ä»¬å¯ä»¥åƒåŠ è½½æ•°æ®é›†ä¸€æ ·è½»æ¾åœ°åŠ è½½ä¸ MRPC æ•°æ®é›†ç›¸å…³çš„æŒ‡æ ‡ï¼Œè¿™æ¬¡ä½¿ç”¨çš„æ˜¯ `evaluate.load()` å‡½æ•°ã€‚ è¿”å›çš„å¯¹è±¡æœ‰ä¸€ä¸ª `compute()` æ–¹æ³•ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å®ƒæ¥è¿›è¡ŒæŒ‡æ ‡çš„è®¡ç®—ï¼š

```python
import evaluate

metric = evaluate.load("glue", "mrpc")
metric.compute(predictions=class_preds, references=raw_datasets["validation"]["label"])
```

```python
{'accuracy': 0.8578431372549019, 'f1': 0.8996539792387542}
```

ç”±äºæ¨¡å‹å¤´çš„éšæœºåˆå§‹åŒ–å¯èƒ½ä¼šæ”¹å˜æ¨¡å‹è¾¾åˆ°çš„æŒ‡æ ‡ï¼Œå› æ­¤ä½ å¾—åˆ°çš„æœ€ç»ˆç»“æœå¯èƒ½ä¼šæœ‰æ‰€ä¸åŒã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æˆ‘ä»¬çš„æ¨¡å‹åœ¨éªŒè¯é›†ä¸Šçš„å‡†ç¡®ç‡ä¸º85.78%ï¼ŒF1åˆ†æ•°ä¸º89.97ã€‚ è¿™å°±æ˜¯ç”¨äºè¯„ä¼° GLUE åŸºå‡†çš„ MRPC æ•°æ®é›†ä¸Šçš„ç»“æœä¸¤ä¸ªæŒ‡æ ‡ã€‚ [BERT è®ºæ–‡](https://arxiv.org/pdf/1810.04805.pdf)(https://arxiv.org/pdf/1810.04805.pdf) ä¸­çš„è¡¨æ ¼æŠ¥å‘Šäº†åŸºæœ¬æ¨¡å‹çš„ F1 åˆ†æ•°ä¸º 88.9ã€‚ é‚£æ˜¯ `uncased` æ¨¡å‹ï¼Œè€Œæˆ‘ä»¬ç›®å‰ä½¿ç”¨çš„æ˜¯ `cased` æ¨¡å‹ï¼Œè¿™è§£é‡Šäº†ä¸ºä»€ä¹ˆæˆ‘ä»¬ä¼šå¾—åˆ°äº†æ›´å¥½çš„ç»“æœã€‚

å…³äºä½¿ç”¨ Keras API è¿›è¡Œå¾®è°ƒçš„ä»‹ç»åˆ°æ­¤ç»“æŸã€‚ åœ¨ç¬¬å…«ç« å°†ç»™å‡ºå¯¹å¤§å¤šæ•°å¸¸è§ NLP ä»»åŠ¡å¾®è°ƒæˆ–è®­ç»ƒçš„ç¤ºä¾‹ã€‚å¦‚æœä½ æƒ³åœ¨ Keras API ä¸Šç£¨ç»ƒè‡ªå·±çš„æŠ€èƒ½ï¼Œè¯·å°è¯•ä½¿ç¬¬2èŠ‚æ‰€å­¦çš„çš„æ•°æ®å¤„ç†æŠ€å·§åœ¨ GLUE SST-2 æ•°æ®é›†ä¸Šå¾®è°ƒæ¨¡å‹ã€‚

## 4.4 ç« æœ«æ€»ç»“åŠç« æœ«æµ‹è¯•

è¿™æ˜¯éå¸¸ä»¤äººé«˜å…´çš„ï¼åœ¨å‰ä¸¤ç« ä¸­ï¼Œä½ äº†è§£äº†æ¨¡å‹å’Œ Tokenizerï¼Œç°åœ¨ä½ çŸ¥é“å¦‚ä½•é’ˆå¯¹ä½ è‡ªå·±çš„æ•°æ®å¯¹å®ƒä»¬è¿›è¡Œå¾®è°ƒã€‚å›é¡¾ä¸€ä¸‹ï¼Œåœ¨æœ¬ç« ä¸­ï¼Œä½ ï¼š

{#if fw === 'pt'}

* äº†è§£äº† [Hub](https://huggingface.co/datasets)(https://huggingface.co/datasets)(https://huggingface.co/datasets) ä¸­çš„æ•°æ®é›†
* å­¦ä¹ äº†å¦‚ä½•åŠ è½½å’Œé¢„å¤„ç†æ•°æ®é›†ï¼ŒåŒ…æ‹¬ä½¿ç”¨åŠ¨æ€å¡«å……å’Œæ•°æ®æ•´ç†å™¨
* å®ç°ä½ è‡ªå·±çš„æ¨¡å‹å¾®è°ƒå’Œè¯„ä¼°
* å®ç°äº†ä¸€ä¸ªè¾ƒä¸ºåº•å±‚çš„è®­ç»ƒå¾ªç¯
* ä½¿ç”¨ Accelerate è½»æ¾è°ƒæ•´ä½ çš„è®­ç»ƒå¾ªç¯ï¼Œä½¿å…¶é€‚ç”¨äºå¤šä¸ª GPU æˆ– TPU

{:else}

* äº†è§£äº† [Hub](https://huggingface.co/datasets)(https://huggingface.co/datasets)(https://huggingface.co/datasets) ä¸­çš„æ•°æ®é›†
* å­¦ä¹ äº†å¦‚ä½•åŠ è½½å’Œé¢„å¤„ç†æ•°æ®é›†
* å­¦ä¹ äº†å¦‚ä½•ä½¿ç”¨ Keras å¾®è°ƒå’Œè¯„ä¼°æ¨¡å‹
* å®ç°äº†è‡ªå®šä¹‰æŒ‡æ ‡

{/if}


### ç« æœ«æµ‹è¯• 

####  1. â€œemotionâ€æ•°æ®é›†åŒ…å«å¸¦æœ‰æƒ…ç»ªæ ‡è®°çš„ Twitter æ¶ˆæ¯ã€‚è¯·åœ¨ [ Hub ]( https://huggingface.co/datasets)( https://huggingface.co/datasets)( https://huggingface.co/datasets) ä¸­è¿›è¡Œæœç´¢å¹¶è¯»å–æ•°æ®é›†çš„æ•°æ®å¡ç‰‡ã€‚åˆ¤æ–­å“ªä¸€ä¸ªåŸºæœ¬æƒ…æ„Ÿä¸åœ¨è¿™ä¸ªæ•°æ®é›†ä¸­ï¼Ÿ

1. Joy
2. Love
3. Confusion
4. Surprise

####  2. åœ¨ [ Hub ]( https://huggingface.co/datasets)( https://huggingface.co/datasets)( https://huggingface.co/datasets) ä¸­æœç´¢`ar_sarcasm`æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†æ”¯æŒå“ªä¸ªä»»åŠ¡ï¼Ÿ

1. æƒ…ç»ªåˆ†ç±»
2. æœºå™¨ç¿»è¯‘
3. å‘½åå®ä½“è¯†åˆ«
4. å›ç­”é—®é¢˜

####  3. å½“è¾“å…¥ä¸€å¯¹å¥å­æ—¶ BERT æ¨¡å‹ä¼šéœ€è¦è¿›è¡Œæ€ä¹ˆæ ·çš„é¢„å¤„ç†ï¼Ÿ

1. Tokens_of_sentence_1 [ SEP ] Tokens_of_sentence_2
2. [CLS] Tokens_of_sentence_1 Tokens_of_sentence_2
3. [CLS] Tokens_of_sentence_1 [SEP] Tokens_of_sentence_2 [SEP]
4. [CLS] Tokens_of_sentence_1 [SEP] Tokens_of_sentence_2

####  4. `Dataset.map ()`æ–¹æ³•çš„å¥½å¤„æ˜¯ä»€ä¹ˆï¼Ÿ

1. è¯¥å‡½æ•°æ‰§è¡Œåçš„ç»“æœè¢«ç¼“å­˜ï¼Œé‡æ–°æ‰§è¡Œä»£ç æ—¶ä¸ä¼šèŠ±è´¹å¤šä½™æ—¶é—´ã€‚
2. å®ƒå¯ä»¥è¿›è¡Œå¹¶è¡ŒåŒ–å¤„ç†ï¼Œæ¯”åœ¨æ•°æ®é›†çš„æ¯ä¸ªå…ƒç´ ä¸Šä¾æ¬¡ä½¿ç”¨å‡½æ•°è¿›è¡Œå¤„ç†æ›´å¿«ã€‚
3. å®ƒä¸ä¼šå°†æ•´ä¸ªæ•°æ®é›†åŠ è½½åˆ°å†…å­˜ä¸­ï¼Œè€Œæ˜¯åœ¨å¤„ç†ä¸€ä¸ªå…ƒç´ åç«‹å³ä¿å­˜ç»“æœã€‚

####  5. ä»€ä¹ˆæ˜¯åŠ¨æ€å¡«å……ï¼Ÿ

1. å°±æ˜¯å°†æ¯ä¸ªæ‰¹å¤„ç†çš„è¾“å…¥å¡«å……åˆ°æ•´ä¸ªæ•°æ®é›†ä¸­çš„æœ€å¤§é•¿åº¦ã€‚
2. è¿™æ˜¯å½“ä½ åœ¨åˆ›å»º batch æ—¶å°†è¾“å…¥å¡«å……åˆ°è¯¥ batch å†…å¥å­çš„æœ€å¤§é•¿åº¦ã€‚
3. å½“ä½ å°†æ¯ä¸ªå¥å­å¡«å……åˆ°ä¸æ•°æ®é›†ä¸­çš„å‰ä¸€ä¸ªå¥å­ç›¸åŒæ•°é‡çš„ token æ—¶ã€‚

####  6. collate å‡½æ•°çš„ç”¨é€”æ˜¯ä»€ä¹ˆï¼Ÿ

1. å®ƒç¡®ä¿æ•°æ®é›†ä¸­çš„æ‰€æœ‰åºåˆ—å…·æœ‰ç›¸åŒçš„é•¿åº¦ã€‚
2. å®ƒæŠŠæ‰€æœ‰çš„æ ·æœ¬åœ°æ”¾åœ¨ä¸€ä¸ª batch é‡Œã€‚
3. å®ƒé¢„å¤„ç†æ•´ä¸ªæ•°æ®é›†ã€‚
4. å®ƒæˆªæ–­æ•°æ®é›†ä¸­çš„åºåˆ—ã€‚

####  7. å½“ä½ ç”¨ä¸€ä¸ªé¢„å…ˆè®­ç»ƒè¿‡çš„è¯­è¨€æ¨¡å‹ï¼ˆä¾‹å¦‚ `bert-base-uncased`ï¼‰å®ä¾‹åŒ–ä¸€ä¸ª`AutoModelForXxx`ç±»ï¼Œè¿™ä¸ªç±»ä¸å®ƒæ‰€è¢«è®­ç»ƒçš„ä»»åŠ¡ä¸åŒ¹é…æ—¶ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ

1. ä»€ä¹ˆéƒ½æ²¡æœ‰ï¼Œä½†ä¼šå‡ºç°ä¸€ä¸ªè­¦å‘Šã€‚
2. ä¸¢å¼ƒé¢„è®­ç»ƒæ¨¡å‹çš„å¤´éƒ¨ï¼Œå–è€Œä»£ä¹‹çš„æ˜¯ä¸€ä¸ªé€‚åˆè¯¥ä»»åŠ¡çš„æ–°å¤´éƒ¨ã€‚
3. ä¸¢å¼ƒé¢„å…ˆè®­ç»ƒå¥½çš„æ¨¡å‹å¤´éƒ¨ã€‚
4. æ²¡æœ‰ï¼Œå› ä¸ºæ¨¡å‹ä»ç„¶å¯ä»¥é’ˆå¯¹ä¸åŒçš„ä»»åŠ¡è¿›è¡Œå¾®è°ƒã€‚

####  8ï¼`TrainingArguments`çš„ç”¨é€”æ˜¯ä»€ä¹ˆï¼Ÿ

1. å®ƒåŒ…å«äº†æ‰€æœ‰ç”¨äºè®­ç»ƒå’Œè¯„ä¼°çš„è¶…å‚æ•°ã€‚
2. å®ƒæŒ‡å®šæ¨¡å‹çš„å¤§å°ã€‚
3. å®ƒåªåŒ…å«ç”¨äºè¯„ä¼°çš„è¶…å‚æ•°ã€‚

####  9ï¼ä¸ºä»€ä¹ˆè¦ä½¿ç”¨Accelerate åº“ï¼Ÿ

1. å®ƒå¯ä»¥å¯¹æ›´å¿«åœ°è®¿é—®çš„æ¨¡å‹ã€‚
2. å®ƒæä¾›äº†ä¸€ä¸ªé«˜çº§ APIï¼Œå› æ­¤æˆ‘ä¸å¿…å®ç°è‡ªå·±çš„è®­ç»ƒå¾ªç¯ã€‚
3. å®ƒä½¿æˆ‘ä»¬çš„è®­ç»ƒå¾ªç¯è¿è¡Œåœ¨åˆ†å¸ƒå¼æ¶æ„ä¸Š
4. å®ƒæä¾›äº†æ›´å¤šçš„ä¼˜åŒ–åŠŸèƒ½ã€‚

####  4ï¼å½“æ¨¡å‹ä¸é¢„è®­ç»ƒçš„ä»»åŠ¡ä¸åŒ¹é…æ—¶ï¼Œä¾‹å¦‚ä½¿ç”¨é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ï¼ˆä¾‹å¦‚â€œ`bert-base-uncased`â€ï¼‰å®ä¾‹åŒ–â€œ`TFAutoModelForXxx`â€ç±»æ—¶ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ

1. ä»€ä¹ˆéƒ½ä¸ä¼šå‘ç”Ÿï¼Œä½†æ˜¯ä½ ä¼šå¾—åˆ°ä¸€ä¸ªè­¦å‘Šã€‚
2. ä¸¢å¼ƒé¢„è®­ç»ƒæ¨¡å‹çš„å¤´éƒ¨ï¼Œå¹¶æ’å…¥ä¸€ä¸ªæ–°çš„å¤´éƒ¨ä»¥é€‚åº”æ–°çš„ä»»åŠ¡ã€‚
3. ä¸¢å¼ƒé¢„å…ˆè®­ç»ƒå¥½çš„æ¨¡å‹å¤´éƒ¨ã€‚
4. æ²¡æœ‰ï¼Œå› ä¸ºæ¨¡å‹ä»ç„¶å¯ä»¥é’ˆå¯¹ä¸åŒçš„ä»»åŠ¡è¿›è¡Œå¾®è°ƒã€‚

####  5ï¼æ¥è‡ª `transformers` çš„ TensorFlow æ¨¡å‹å·²ç»æ˜¯ Keras æ¨¡å‹ï¼Œè¿™æœ‰ä»€ä¹ˆå¥½å¤„ï¼Ÿ

1. è¿™äº›æ¨¡å‹å¯åœ¨å¼€ç®±å³ç”¨çš„ TPU ä¸Šè¿è¡Œã€‚
2. ä½ å¯ä»¥åˆ©ç”¨ç°æœ‰æ–¹æ³•ï¼Œä¾‹å¦‚ <code>compile()</code>ã€<code>fit()</code> å’Œ <code>predict()</code>ã€‚
3. ä½ å¯ä»¥å­¦ä¹  Keras ä»¥åŠ Transformerã€‚
4. ä½ å¯ä»¥è½»æ¾è®¡ç®—ä¸æ•°æ®é›†ç›¸å…³çš„æŒ‡æ ‡ã€‚

####  6ï¼å¦‚ä½•å®šä¹‰è‡ªå·±çš„è‡ªå®šä¹‰æŒ‡æ ‡ï¼Ÿ

1. ä½¿ç”¨å­ç±»åŒ– tf.keras.metrics.Metricã€‚
2. ä½¿ç”¨ Keras å‡½æ•° APIã€‚
3. ä½¿ç”¨ä½¿ç”¨å¸¦æœ‰ç­¾åçš„ <code>metric_fn(y_true, y_pred)</code> å‡½æ•°ã€‚
4. ä½¿ç”¨è°·æ­Œæœç´¢ã€‚

### è§£æ

####  1. â€œemotionâ€æ•°æ®é›†åŒ…å«å¸¦æœ‰æƒ…ç»ªæ ‡è®°çš„ Twitter æ¶ˆæ¯ã€‚è¯·åœ¨ [ Hub ]( https://huggingface.co/datasets)( https://huggingface.co/datasets)( https://huggingface.co/datasets) ä¸­è¿›è¡Œæœç´¢å¹¶è¯»å–æ•°æ®é›†çš„æ•°æ®å¡ç‰‡ã€‚åˆ¤æ–­å“ªä¸€ä¸ªåŸºæœ¬æƒ…æ„Ÿä¸åœ¨è¿™ä¸ªæ•°æ®é›†ä¸­ï¼Ÿ

æ­£ç¡®é€‰é¡¹: 3. Confusion

1. Joy    
è§£æ: å†è¯•ä¸€æ¬¡â€”â€”è¿™ç§æƒ…ç»ªåœ¨è¿™ä¸ªæ•°æ®é›†ä¸­ï¼
2. Love    
è§£æ: å†è¯•ä¸€æ¬¡â€”â€”è¿™ç§æƒ…ç»ªåœ¨è¿™ä¸ªæ•°æ®é›†ä¸­ï¼
3. Confusion    
è§£æ: æ­£ç¡®! Confusion ä¸æ˜¯å…­ç§åŸºæœ¬æƒ…ç»ªä¹‹ä¸€ã€‚
4. Surprise    
è§£æ: Surprise! å†è¯•ä¸€æ¬¡ï¼

####  2. åœ¨ [ Hub ](https://huggingface.co/datasets)(https://huggingface.co/datasets)( https://huggingface.co/datasets) ä¸­æœç´¢`ar_sarcasm`æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†æ”¯æŒå“ªä¸ªä»»åŠ¡ï¼Ÿ

æ­£ç¡®é€‰é¡¹: 1. æƒ…ç»ªåˆ†ç±»

1. æƒ…ç»ªåˆ†ç±»    
è§£æ: æ²¡é”™! å¤šäºè¿™äº›æ ‡ç­¾ã€‚
2. æœºå™¨ç¿»è¯‘    
è§£æ: ä¸å¯¹ï¼Œè¯·å†çœ‹çœ‹[æ•°æ®å¡ç‰‡](https://huggingface.co/datasets/ar_sarcasm)(https://huggingface.co/datasets/ar_sarcasm)(https://huggingface.co/datasets/ar_sarcasm)ï¼
3. å‘½åå®ä½“è¯†åˆ«    
è§£æ: ä¸å¯¹ï¼Œè¯·å†çœ‹çœ‹[æ•°æ®å¡ç‰‡](https://huggingface.co/datasets/ar_sarcasm)(https://huggingface.co/datasets/ar_sarcasm)(https://huggingface.co/datasets/ar_sarcasm) ï¼
4. å›ç­”é—®é¢˜    
è§£æ: ä¸å¯¹ï¼Œ å†è¯•ä¸€æ¬¡ï¼

####  3. å½“è¾“å…¥ä¸€å¯¹å¥å­æ—¶ BERT æ¨¡å‹ä¼šéœ€è¦è¿›è¡Œæ€ä¹ˆæ ·çš„é¢„å¤„ç†ï¼Ÿ

æ­£ç¡®é€‰é¡¹: 3. [CLS] Tokens_of_sentence_1 [SEP] Tokens_of_sentence_2 [SEP]

1. Tokens_of_sentence_1 [ SEP ] Tokens_of_sentence_2    
è§£æ: éœ€è¦ä½¿ç”¨ä¸€ä¸ªç‰¹æ®Šçš„ `[SEP]` token æ¥åˆ†éš”ä¸¤ä¸ªå¥å­ï¼Œä½†æ˜¯åªæœ‰è¿™ä¸€ä¸ªè¿˜ä¸å¤Ÿã€‚
2. [CLS] Tokens_of_sentence_1 Tokens_of_sentence_2    
è§£æ: éœ€è¦ä¸€ä¸ªç‰¹æ®Šçš„ `[CLS]` token æ¥æŒ‡ç¤ºå¥å­çš„å¼€å¤´ï¼Œä½†æ˜¯åªæœ‰è¿™ä¸€ä¸ªè¿˜ä¸å¤Ÿã€‚
3. [CLS] Tokens_of_sentence_1 [SEP] Tokens_of_sentence_2 [SEP]    
è§£æ: æ­£ç¡®ï¼
4. [CLS] Tokens_of_sentence_1 [SEP] Tokens_of_sentence_2    
è§£æ: éœ€è¦ä¸€ä¸ªç‰¹æ®Šçš„ `[CLS]` token æ¥æŒ‡ç¤ºå¥å­çš„å¼€å¤´ï¼Œè¿˜éœ€è¦ä¸€ä¸ªç‰¹æ®Šçš„ `[SEP]` token æ¥åˆ†éš”ä¸¤ä¸ªå¥å­ï¼Œä½†è¿™è¿˜ä¸æ˜¯éœ€è¦å…¨éƒ¨çš„é¢„å¤„ç†ã€‚

####  4. `Dataset.map ()`æ–¹æ³•çš„å¥½å¤„æ˜¯ä»€ä¹ˆï¼Ÿ

æ­£ç¡®é€‰é¡¹: 1. è¯¥å‡½æ•°æ‰§è¡Œåçš„ç»“æœè¢«ç¼“å­˜ï¼Œé‡æ–°æ‰§è¡Œä»£ç æ—¶ä¸ä¼šèŠ±è´¹å¤šä½™æ—¶é—´ã€‚

æ­£ç¡®é€‰é¡¹: 2. å®ƒå¯ä»¥è¿›è¡Œå¹¶è¡ŒåŒ–å¤„ç†ï¼Œæ¯”åœ¨æ•°æ®é›†çš„æ¯ä¸ªå…ƒç´ ä¸Šä¾æ¬¡ä½¿ç”¨å‡½æ•°è¿›è¡Œå¤„ç†æ›´å¿«ã€‚

æ­£ç¡®é€‰é¡¹: 3. å®ƒä¸ä¼šå°†æ•´ä¸ªæ•°æ®é›†åŠ è½½åˆ°å†…å­˜ä¸­ï¼Œè€Œæ˜¯åœ¨å¤„ç†ä¸€ä¸ªå…ƒç´ åç«‹å³ä¿å­˜ç»“æœã€‚

1. è¯¥å‡½æ•°æ‰§è¡Œåçš„ç»“æœè¢«ç¼“å­˜ï¼Œé‡æ–°æ‰§è¡Œä»£ç æ—¶ä¸ä¼šèŠ±è´¹å¤šä½™æ—¶é—´ã€‚    
è§£æ: è¿™ç¡®å®æ˜¯è¿™ç§æ–¹æ³•çš„ä¼˜ç‚¹ä¹‹ä¸€! ä¸è¿‡è¿˜æœ‰åˆ«çš„ä¼˜ç‚¹...
2. å®ƒå¯ä»¥è¿›è¡Œå¹¶è¡ŒåŒ–å¤„ç†ï¼Œæ¯”åœ¨æ•°æ®é›†çš„æ¯ä¸ªå…ƒç´ ä¸Šä¾æ¬¡ä½¿ç”¨å‡½æ•°è¿›è¡Œå¤„ç†æ›´å¿«ã€‚    
è§£æ: è¿™æ˜¯è¯¥æ–¹æ³•ä¸€ä¸ªæ¯”è¾ƒä¼˜é›…çš„ç‰¹ç‚¹ï¼ ä¸è¿‡è¿˜æœ‰åˆ«çš„ä¼˜ç‚¹...
3. å®ƒä¸ä¼šå°†æ•´ä¸ªæ•°æ®é›†åŠ è½½åˆ°å†…å­˜ä¸­ï¼Œè€Œæ˜¯åœ¨å¤„ç†ä¸€ä¸ªå…ƒç´ åç«‹å³ä¿å­˜ç»“æœã€‚    
è§£æ: è¿™æ˜¯è¿™ç§æ–¹æ³•çš„ä¸€ä¸ªä¼˜ç‚¹ï¼ ä¸è¿‡è¿˜æœ‰åˆ«çš„ä¼˜ç‚¹...

####  5. ä»€ä¹ˆæ˜¯åŠ¨æ€å¡«å……ï¼Ÿ

æ­£ç¡®é€‰é¡¹: 2. è¿™æ˜¯å½“ä½ åœ¨åˆ›å»º batch æ—¶å°†è¾“å…¥å¡«å……åˆ°è¯¥ batch å†…å¥å­çš„æœ€å¤§é•¿åº¦ã€‚

1. å°±æ˜¯å°†æ¯ä¸ªæ‰¹å¤„ç†çš„è¾“å…¥å¡«å……åˆ°æ•´ä¸ªæ•°æ®é›†ä¸­çš„æœ€å¤§é•¿åº¦ã€‚    
è§£æ: å®ƒç¡®å®æ„å‘³ç€åˆ›å»º batch æ—¶è¿›è¡Œå¡«å……ï¼Œä½†ä¸æ˜¯æ•´ä¸ªæ•°æ®é›†ä¸­çš„æœ€å¤§é•¿åº¦ã€‚
2. è¿™æ˜¯å½“ä½ åœ¨åˆ›å»º batch æ—¶å°†è¾“å…¥å¡«å……åˆ°è¯¥ batch å†…å¥å­çš„æœ€å¤§é•¿åº¦ã€‚    
è§£æ: æ²¡é”™ï¼â€œåŠ¨æ€â€éƒ¨åˆ†å³æ¯ä¸ª batch çš„å¤§å°æ˜¯åœ¨åˆ›å»ºæ—¶ç¡®å®šçš„ï¼Œå› æ­¤ä¸åŒçš„ batch å¯èƒ½å…·æœ‰ä¸åŒçš„å½¢çŠ¶ã€‚
3. å½“ä½ å°†æ¯ä¸ªå¥å­å¡«å……åˆ°ä¸æ•°æ®é›†ä¸­çš„å‰ä¸€ä¸ªå¥å­ç›¸åŒæ•°é‡çš„ token æ—¶ã€‚    
è§£æ: é”™è¯¯ï¼Œè€Œä¸”ç”±äºæˆ‘ä»¬åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯¹æ•°æ®é›†è¿›è¡Œäº†éšæœºçš„æ‰“ä¹±ï¼Œå› æ­¤æ•°æ®é›†ä¸­çš„é¡ºåºæ˜¯æ²¡æœ‰æ„ä¹‰çš„ã€‚

####  6. collate å‡½æ•°çš„ç”¨é€”æ˜¯ä»€ä¹ˆï¼Ÿ

1. å®ƒç¡®ä¿æ•°æ®é›†ä¸­çš„æ‰€æœ‰åºåˆ—å…·æœ‰ç›¸åŒçš„é•¿åº¦ã€‚    
è§£æ: collate å‡½æ•°ç”¨äºå¤„ç†å•ä¸ª batch å¤„ç†ï¼Œè€Œä¸æ˜¯æ•´ä¸ªæ•°æ®é›†ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è®¨è®ºçš„æ˜¯æ‰€æœ‰ collate å‡½æ•°é€šå¸¸çš„ç”¨é€”ï¼Œè€Œä¸æ˜¯ç‰¹å®šçš„  <code>DataCollatorWithPadding</code> 
2. å®ƒæŠŠæ‰€æœ‰çš„æ ·æœ¬åœ°æ”¾åœ¨ä¸€ä¸ª batch é‡Œã€‚    
è§£æ: æ­£ç¡®ï¼ä½ å¯å°† collate å‡½æ•°ä½œä¸º <code>DataLoader</code>å‡½æ•°çš„ä¸€ä¸ªå‚æ•°ã€‚ æˆ‘ä»¬ä½¿ç”¨äº† <code>DataCollatorWithPadding</code> å‡½æ•°
3. å®ƒé¢„å¤„ç†æ•´ä¸ªæ•°æ®é›†ã€‚    
è§£æ: é¢„å¤„ç†å‡½æ•°ï¼ˆpreprocessingï¼‰ç”¨äºé¢„å¤„ç†æ•´ä¸ªæ•°æ®é›†ï¼Œè€Œä¸æ˜¯ collate å‡½æ•°ã€‚
4. å®ƒæˆªæ–­æ•°æ®é›†ä¸­çš„åºåˆ—ã€‚    
è§£æ: collate å‡½æ•°ç”¨äºå¤„ç†å•ä¸ª batch çš„å¤„ç†ï¼Œè€Œä¸æ˜¯æ•´ä¸ªæ•°æ®é›†ã€‚å¦‚æœä½ å¯¹æˆªæ–­æ„Ÿå…´è¶£ï¼Œå¯ä»¥ä½¿ç”¨ <code> tokenizer </code> çš„<code> truncate </code> å‚æ•°ã€‚

####  7. å½“ä½ ç”¨ä¸€ä¸ªé¢„å…ˆè®­ç»ƒè¿‡çš„è¯­è¨€æ¨¡å‹ï¼ˆä¾‹å¦‚ `bert-base-uncased`ï¼‰å®ä¾‹åŒ–ä¸€ä¸ª`AutoModelForXxx`ç±»ï¼Œè¿™ä¸ªç±»ä¸å®ƒæ‰€è¢«è®­ç»ƒçš„ä»»åŠ¡ä¸åŒ¹é…æ—¶ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ

æ­£ç¡®é€‰é¡¹: 2. ä¸¢å¼ƒé¢„è®­ç»ƒæ¨¡å‹çš„å¤´éƒ¨ï¼Œå–è€Œä»£ä¹‹çš„æ˜¯ä¸€ä¸ªé€‚åˆè¯¥ä»»åŠ¡çš„æ–°å¤´éƒ¨ã€‚

1. ä»€ä¹ˆéƒ½æ²¡æœ‰ï¼Œä½†ä¼šå‡ºç°ä¸€ä¸ªè­¦å‘Šã€‚    
è§£æ: ç¡®å®å‡ºç°è­¦å‘Šï¼Œä½†è¿™è¿˜ä¸æ˜¯å…¨éƒ¨ï¼
2. ä¸¢å¼ƒé¢„è®­ç»ƒæ¨¡å‹çš„å¤´éƒ¨ï¼Œå–è€Œä»£ä¹‹çš„æ˜¯ä¸€ä¸ªé€‚åˆè¯¥ä»»åŠ¡çš„æ–°å¤´éƒ¨ã€‚    
è§£æ: æ­£ç¡®çš„ã€‚ ä¾‹å¦‚ï¼Œå½“æˆ‘ä»¬å°† AutoModelForSequenceClassification ä¸ bert-base-uncased ç»“åˆä½¿ç”¨æ—¶ï¼Œæˆ‘ä»¬åœ¨å®ä¾‹åŒ–æ¨¡å‹æ—¶å°†æ”¶åˆ°è­¦å‘Šã€‚ é¢„è®­ç»ƒçš„å¤´ä¸ç”¨äºåºåˆ—åˆ†ç±»ä»»åŠ¡ï¼Œå› æ­¤å®ƒè¢«ä¸¢å¼ƒï¼Œå¹¶ä½¿ç”¨éšæœºåˆå§‹åŒ–æƒé‡çš„ç”¨äºåºåˆ—åˆ†ç±»ä»»åŠ¡çš„å¤´ã€‚
3. ä¸¢å¼ƒé¢„å…ˆè®­ç»ƒå¥½çš„æ¨¡å‹å¤´éƒ¨ã€‚    
è§£æ: è¿˜éœ€è¦åšå…¶ä»–äº‹æƒ…ï¼Œå†è¯•ä¸€æ¬¡ï¼
4. æ²¡æœ‰ï¼Œå› ä¸ºæ¨¡å‹ä»ç„¶å¯ä»¥é’ˆå¯¹ä¸åŒçš„ä»»åŠ¡è¿›è¡Œå¾®è°ƒã€‚    
è§£æ: è¿™ä¸ªç»è¿‡è®­ç»ƒçš„æ¨¡ç‰¹çš„å¤´æ²¡æœ‰ç»è¿‡è®­ç»ƒæ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæ‰€ä»¥æˆ‘ä»¬åº”è¯¥ä¸¢å¼ƒè¯¥å¤´éƒ¨ï¼

####  8ï¼`TrainingArguments`çš„ç”¨é€”æ˜¯ä»€ä¹ˆï¼Ÿ

æ­£ç¡®é€‰é¡¹: 1. å®ƒåŒ…å«äº†æ‰€æœ‰ç”¨äºè®­ç»ƒå’Œè¯„ä¼°çš„è¶…å‚æ•°ã€‚

1. å®ƒåŒ…å«äº†æ‰€æœ‰ç”¨äºè®­ç»ƒå’Œè¯„ä¼°çš„è¶…å‚æ•°ã€‚    
è§£æ: æ­£ç¡®ï¼
2. å®ƒæŒ‡å®šæ¨¡å‹çš„å¤§å°ã€‚    
è§£æ: æ¨¡å‹å¤§å°æ˜¯ç”±æ¨¡å‹é…ç½®å®šä¹‰çš„ï¼Œè€Œä¸æ˜¯ç”± `TrainingArguments` ç±» ã€‚
3. å®ƒåªåŒ…å«ç”¨äºè¯„ä¼°çš„è¶…å‚æ•°ã€‚    
è§£æ: åœ¨ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬è¿˜æŒ‡å®šäº†æ¨¡å‹çš„è¶…å‚æ•°åŠå…¶æ£€æŸ¥ç‚¹çš„ä¿å­˜ä½ç½®ã€‚ å†è¯•ä¸€æ¬¡ï¼

####  9ï¼ä¸ºä»€ä¹ˆè¦ä½¿ç”¨Accelerate åº“ï¼Ÿ

æ­£ç¡®é€‰é¡¹: 3. å®ƒä½¿æˆ‘ä»¬çš„è®­ç»ƒå¾ªç¯è¿è¡Œåœ¨åˆ†å¸ƒå¼æ¶æ„ä¸Š

1. å®ƒå¯ä»¥å¯¹æ›´å¿«åœ°è®¿é—®çš„æ¨¡å‹ã€‚    
è§£æ: ä¸ï¼ŒAccelerate åº“ä¸æä¾›ä»»ä½•æ¨¡å‹ã€‚
2. å®ƒæä¾›äº†ä¸€ä¸ªé«˜çº§ APIï¼Œå› æ­¤æˆ‘ä¸å¿…å®ç°è‡ªå·±çš„è®­ç»ƒå¾ªç¯ã€‚    
è§£æ: è¿™æ˜¯æˆ‘ä»¬ä½¿ç”¨ <code>Trainer</code> æ‰€åšçš„äº‹æƒ…ï¼Œè€Œä¸æ˜¯ Accelerate åº“ã€‚ å†è¯•ä¸€æ¬¡
3. å®ƒä½¿æˆ‘ä»¬çš„è®­ç»ƒå¾ªç¯è¿è¡Œåœ¨åˆ†å¸ƒå¼æ¶æ„ä¸Š    
è§£æ: æ­£ç¡®! ä½¿ç”¨Accelerate åº“ï¼Œä½ çš„è®­ç»ƒå¾ªç¯å¯ä»¥åœ¨å¤šä¸ª GPU å’Œ TPUs ä¸Šè¿è¡Œã€‚
4. å®ƒæä¾›äº†æ›´å¤šçš„ä¼˜åŒ–åŠŸèƒ½ã€‚    
è§£æ: ä¸ï¼ŒAccelerate åº“ä¸æä¾›ä»»ä½•ä¼˜åŒ–åŠŸèƒ½ã€‚

####  4ï¼å½“æ¨¡å‹ä¸é¢„è®­ç»ƒçš„ä»»åŠ¡ä¸åŒ¹é…æ—¶ï¼Œä¾‹å¦‚ä½¿ç”¨é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ï¼ˆä¾‹å¦‚â€œ`bert-base-uncased`â€ï¼‰å®ä¾‹åŒ–â€œ`TFAutoModelForXxx`â€ç±»æ—¶ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ

æ­£ç¡®é€‰é¡¹: 2. ä¸¢å¼ƒé¢„è®­ç»ƒæ¨¡å‹çš„å¤´éƒ¨ï¼Œå¹¶æ’å…¥ä¸€ä¸ªæ–°çš„å¤´éƒ¨ä»¥é€‚åº”æ–°çš„ä»»åŠ¡ã€‚

1. ä»€ä¹ˆéƒ½ä¸ä¼šå‘ç”Ÿï¼Œä½†æ˜¯ä½ ä¼šå¾—åˆ°ä¸€ä¸ªè­¦å‘Šã€‚    
è§£æ: ä½ ç¡®å®å¾—åˆ°äº†è­¦å‘Šï¼Œä½†è¿™è¿˜ä¸æ˜¯å…¨éƒ¨ï¼
2. ä¸¢å¼ƒé¢„è®­ç»ƒæ¨¡å‹çš„å¤´éƒ¨ï¼Œå¹¶æ’å…¥ä¸€ä¸ªæ–°çš„å¤´éƒ¨ä»¥é€‚åº”æ–°çš„ä»»åŠ¡ã€‚    
è§£æ: æ­£ç¡®çš„ã€‚ ä¾‹å¦‚ï¼Œå½“æˆ‘ä»¬å°† `TFAutoModelForSequenceClassification `ä¸ `bert-base-uncased` ç»“åˆä½¿ç”¨æ—¶ï¼Œæˆ‘ä»¬åœ¨å®ä¾‹åŒ–æ¨¡å‹æ—¶æ”¶åˆ°è­¦å‘Šã€‚ é¢„è®­ç»ƒçš„å¤´ä¸ç”¨äºåºåˆ—åˆ†ç±»ä»»åŠ¡ï¼Œå› æ­¤å®ƒè¢«ä¸¢å¼ƒï¼Œä½¿ç”¨æ–°çš„å¤´å¹¶ä¸”éšæœºåˆå§‹åŒ–æƒé‡ã€‚
3. ä¸¢å¼ƒé¢„å…ˆè®­ç»ƒå¥½çš„æ¨¡å‹å¤´éƒ¨ã€‚    
è§£æ: é™¤æ­¤ä¹‹å¤–è¿˜æœ‰ä¸€äº›äº‹æƒ…ä¼šå‘ç”Ÿï¼Œ å†è¯•ä¸€æ¬¡ï¼
4. æ²¡æœ‰ï¼Œå› ä¸ºæ¨¡å‹ä»ç„¶å¯ä»¥é’ˆå¯¹ä¸åŒçš„ä»»åŠ¡è¿›è¡Œå¾®è°ƒã€‚    
è§£æ: è¿™ä¸ªç»è¿‡è®­ç»ƒçš„æ¨¡ç‰¹çš„å¤´æ²¡æœ‰ç»è¿‡è®­ç»ƒæ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæ‰€ä»¥æˆ‘ä»¬åº”è¯¥ä¸¢æ‰è¿™ä¸ªå¤´ï¼

####  5ï¼æ¥è‡ª `transformers` çš„ TensorFlow æ¨¡å‹å·²ç»æ˜¯ Keras æ¨¡å‹ï¼Œè¿™æœ‰ä»€ä¹ˆå¥½å¤„ï¼Ÿ

æ­£ç¡®é€‰é¡¹: 2. ä½ å¯ä»¥åˆ©ç”¨ç°æœ‰æ–¹æ³•ï¼Œä¾‹å¦‚ <code>compile()</code>ã€<code>fit()</code> å’Œ <code>predict()</code>ã€‚

æ­£ç¡®é€‰é¡¹: 3. ä½ å¯ä»¥å­¦ä¹  Keras ä»¥åŠ Transformerã€‚

1. è¿™äº›æ¨¡å‹å¯åœ¨å¼€ç®±å³ç”¨çš„ TPU ä¸Šè¿è¡Œã€‚    
è§£æ: å·®ä¸å¤šï¼ä½†æ˜¯è¿˜éœ€è¦è¿›è¡Œä¸€äº›å°çš„é¢å¤–ä¿®æ”¹ã€‚ä¾‹å¦‚ï¼Œä½ éœ€è¦åœ¨ <code>TPUStrategy</code> èŒƒå›´å†…è¿è¡Œæ‰€æœ‰å†…å®¹ï¼ŒåŒ…æ‹¬æ¨¡å‹çš„åˆå§‹åŒ–ã€‚
2. ä½ å¯ä»¥åˆ©ç”¨ç°æœ‰æ–¹æ³•ï¼Œä¾‹å¦‚ <code>compile()</code>ã€<code>fit()</code> å’Œ <code>predict()</code>ã€‚    
è§£æ: æ­£ç¡®! ä¸€æ—¦ä½ æœ‰äº†æ•°æ®ï¼Œåªéœ€è¦å¾ˆå°‘çš„å·¥ä½œå°±å¯ä»¥åœ¨è¿™äº›æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒã€‚
3. ä½ å¯ä»¥å­¦ä¹  Keras ä»¥åŠ Transformerã€‚    
è§£æ: æ²¡é”™ï¼Œä½†æˆ‘ä»¬è¦æ‰¾çš„æ˜¯åˆ«çš„ä¸œè¥¿:)
4. ä½ å¯ä»¥è½»æ¾è®¡ç®—ä¸æ•°æ®é›†ç›¸å…³çš„æŒ‡æ ‡ã€‚    
è§£æ: Keras å¸®åŠ©æˆ‘ä»¬è®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹ï¼Œè€Œä¸æ˜¯è®¡ç®—ä¸æ•°æ®é›†ç›¸å…³çš„æŒ‡æ ‡ã€‚

####  6ï¼å¦‚ä½•å®šä¹‰è‡ªå·±çš„è‡ªå®šä¹‰æŒ‡æ ‡ï¼Ÿ

æ­£ç¡®é€‰é¡¹: 1. ä½¿ç”¨å­ç±»åŒ– tf.keras.metrics.Metricã€‚

æ­£ç¡®é€‰é¡¹: 3. ä½¿ç”¨ä½¿ç”¨å¸¦æœ‰ç­¾åçš„ <code>metric_fn(y_true, y_pred)</code> å‡½æ•°ã€‚

æ­£ç¡®é€‰é¡¹: 4. ä½¿ç”¨è°·æ­Œæœç´¢ã€‚
