

# ç¿»è¯‘ 

{#if fw === 'pt'}



{:else}



{/if}

ç°åœ¨è®©æˆ‘ä»¬æ·±å…¥ç ”ç©¶ç¿»è¯‘ã€‚è¿™æ˜¯å¦ä¸€ä¸ª [sequence-to-sequence ä»»åŠ¡](/course/chapter1/7)(/course/chapter1/7) ï¼Œè¿™æ„å‘³ç€è¿™æ˜¯ä¸€ä¸ªå¯ä»¥è¡¨è¿°ä¸ºä»ä¸€ä¸ªåºåˆ—åˆ°å¦ä¸€ä¸ªåºåˆ—çš„é—®é¢˜ã€‚ä»è¿™ä¸ªæ„ä¹‰ä¸Šè¯´ï¼Œè¿™ä¸ªé—®é¢˜éå¸¸ç±»ä¼¼ [æ–‡æœ¬æ‘˜è¦](/course/chapter7/6)(/course/chapter7/6) ï¼Œå¹¶ä¸”ä½ å¯ä»¥å°†æˆ‘ä»¬å°†åœ¨æ­¤å¤„å­¦ä¹ åˆ°çš„ä¸€äº›å†…å®¹è¿ç§»åˆ°å…¶ä»–çš„åºåˆ—åˆ°åºåˆ—é—®é¢˜ï¼Œä¾‹å¦‚ï¼š

- **é£æ ¼è¿ç§»** : åˆ›å»ºä¸€ä¸ªæ¨¡å‹å°†æŸç§é£æ ¼è¿ç§»åˆ°ä¸€æ®µæ–‡æœ¬ï¼ˆä¾‹å¦‚ï¼Œæ­£å¼çš„é£æ ¼è¿ç§»åˆ°ä¼‘é—²çš„é£æ ¼ï¼Œæˆ–èå£«æ¯”äºšè‹±è¯­åˆ°ç°ä»£è‹±è¯­ï¼‰ã€‚
- **ç”Ÿæˆé—®é¢˜çš„å›ç­”** åˆ›å»ºä¸€ä¸ªæ¨¡å‹ï¼Œåœ¨ç»™å®šä¸Šä¸‹æ–‡çš„æƒ…å†µä¸‹ç”Ÿæˆé—®é¢˜çš„ç­”æ¡ˆã€‚



å¦‚æœä½ æœ‰è¶³å¤Ÿå¤§çš„ä¸¤ç§ï¼ˆæˆ–æ›´å¤šï¼‰è¯­è¨€çš„æ–‡æœ¬è¯­æ–™åº“ï¼Œä½ å¯ä»¥ä»å¤´å¼€å§‹è®­ç»ƒä¸€ä¸ªæ–°çš„ç¿»è¯‘æ¨¡å‹ï¼Œå°±åƒæˆ‘ä»¬åœ¨ [å› æœè¯­è¨€å»ºæ¨¡](/course/chapter7/6)(/course/chapter7/6) éƒ¨åˆ†ä¸­æ‰€åšçš„é‚£æ ·ã€‚ç„¶è€Œï¼Œå¾®è°ƒç°æœ‰çš„ç¿»è¯‘æ¨¡å‹ä¼šæ›´å¿«ï¼Œæ— è®ºæ˜¯ä»åƒ mT5 æˆ– mBART è¿™æ ·çš„å¤šè¯­è¨€æ¨¡å‹å¾®è°ƒåˆ°ç‰¹å®šçš„è¯­è¨€å¯¹ï¼Œè¿˜æ˜¯ä½ æƒ³å¾®è°ƒåˆ°ç‰¹å®šè¯­æ–™åº“çš„ä¸€ç§è¯­è¨€åˆ°å¦ä¸€ç§è¯­è¨€çš„ä¸“ç”¨ç¿»è¯‘æ¨¡å‹ã€‚

åœ¨è¿™ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†å¾®è°ƒä¸€ä¸ªé¢„è®­ç»ƒçš„ Marian æ¨¡å‹ï¼Œè¯¥æ¨¡å‹æ˜¯ç”¨æ¥ä»è‹±è¯­ç¿»è¯‘æˆæ³•è¯­çš„ï¼ˆå› ä¸ºå¾ˆå¤š Hugging Face çš„å‘˜å·¥éƒ½ä¼šè¯´è¿™ä¸¤ç§è¯­è¨€ï¼‰ï¼Œå¹¶åœ¨ [KDE4 æ•°æ®é›†](https://huggingface.co/datasets/kde4)(https://huggingface.co/datasets/kde4)(https://huggingface.co/datasets/kde4) ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äº [KDE åº”ç”¨](https://apps.kde.org/)(https://apps.kde.org/) çš„æœ¬åœ°åŒ–æ–‡ä»¶æ•°æ®é›†ã€‚æˆ‘ä»¬å°†ä½¿ç”¨çš„æ¨¡å‹å·²ç»åœ¨ä» [Opus æ•°æ®é›†](https://opus.nlpl.eu/)(https://opus.nlpl.eu/) (å®é™…ä¸ŠåŒ…å« KDE4 æ•°æ®é›†)ä¸­æå–çš„æ³•è¯­å’Œè‹±è¯­æ–‡æœ¬çš„å¤§å‹è¯­æ–™åº“ä¸Šè¿›è¡Œäº†é¢„å…ˆè®­ç»ƒã€‚ä½†æ˜¯ï¼Œå³ä½¿æˆ‘ä»¬ä½¿ç”¨çš„é¢„è®­ç»ƒæ¨¡å‹åœ¨å…¶é¢„è®­ç»ƒæœŸé—´ä½¿ç”¨äº†è¿™éƒ¨åˆ†æ•°æ®é›†ï¼Œæˆ‘ä»¬ä¹Ÿä¼šçœ‹åˆ°ï¼Œç»è¿‡å¾®è°ƒåï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°ä¸€ä¸ªæ›´å¥½çš„ç‰ˆæœ¬ã€‚

å®Œæˆåï¼Œæˆ‘ä»¬å°†æ‹¥æœ‰ä¸€ä¸ªæ¨¡å‹ï¼Œå¯ä»¥è¿›è¡Œè¿™æ ·çš„ç¿»è¯‘ï¼š



<a class="flex justify-center" href="/huggingface-course/marian-finetuned-kde4-en-to-fr">
<img class="block dark:hidden lg:w-3/5" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/modeleval-marian-finetuned-kde4-en-to-fr.png" alt="One-hot encoded labels for question answering."/>
<img class="hidden dark:block lg:w-3/5" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/modeleval-marian-finetuned-kde4-en-to-fr-dark.png" alt="One-hot encoded labels for question answering."/>
</a>

ä¸å‰é¢çš„éƒ¨åˆ†ä¸€æ ·ï¼Œä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹ä»£ç æ‰¾åˆ°æˆ‘ä»¬å°†è®­ç»ƒå¹¶ä¸Šä¼ åˆ° Hub çš„å®é™…æ¨¡å‹ï¼Œå¹¶ [åœ¨è¿™é‡Œ](https://huggingface.co/huggingface-course/marian-finetuned-kde4-en-to-fr?text=This+plugin+allows+you+to+automatically+translate+web+pages+between+several+languages.)(https://huggingface.co/huggingface-course/marian-finetuned-kde4-en-to-fr?text=This+plugin+allows+you+to+automatically+translate+web+pages+between+several+languages.) æŸ¥çœ‹æ¨¡å‹è¾“å‡ºçš„ç»“æœã€‚

## å‡†å¤‡æ•°æ® 

ä¸ºäº†ä»å¤´å¼€å§‹å¾®è°ƒæˆ–è®­ç»ƒç¿»è¯‘æ¨¡å‹ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªé€‚åˆè¯¥ä»»åŠ¡çš„æ•°æ®é›†ã€‚å¦‚å‰æ‰€è¿°ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ [KDE4 æ•°æ®é›†](https://huggingface.co/datasets/kde4)(https://huggingface.co/datasets/kde4)(https://huggingface.co/datasets/kde4) ã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œä½†ä½ å¯ä»¥å¾ˆå®¹æ˜“åœ°è°ƒæ•´ä»£ç ä»¥ä½¿ç”¨ä½ è‡ªå·±çš„æ•°æ®ï¼Œåªè¦ä½ æœ‰è¦äº’è¯‘çš„ä¸¤ç§è¯­è¨€çš„å¥å­å¯¹ã€‚å¦‚æœä½ éœ€è¦å¤ä¹ å¦‚ä½•å°†è‡ªå®šä¹‰æ•°æ®åŠ è½½åˆ° **Dataset** å¯ä»¥å¤ä¹ ä¸€ä¸‹ç¬¬äº”ç« ã€‚

### KDE4 æ•°æ®é›† 

åƒå¾€å¸¸ä¸€æ ·ï¼Œæˆ‘ä»¬ä½¿ç”¨ `load_dataset()` å‡½æ•°ä¸‹è½½æˆ‘ä»¬çš„æ•°æ®é›†ï¼š

```py
from datasets import load_dataset

raw_datasets = load_dataset("kde4", lang1="en", lang2="fr")
```

å¦‚æœä½ æƒ³ä½¿ç”¨ä¸åŒçš„è¯­è¨€å¯¹ï¼Œä½ å¯ä»¥ä½¿ç”¨å®ƒä»¬çš„ä»£ç æ¥æŒ‡å®šå®ƒä»¬ã€‚è¯¥æ•°æ®é›†å…±æœ‰ 92 ç§è¯­è¨€å¯ç”¨ï¼›ä½ å¯ä»¥é€šè¿‡ [æ•°æ®é›†å¡ç‰‡](https://huggingface.co/datasets/kde4)(https://huggingface.co/datasets/kde4) å±•å¼€å…¶ä¸Šçš„è¯­è¨€æ ‡ç­¾æ¥æŸ¥çœ‹å®ƒä»¬ã€‚

<img src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/language_tags.png" alt="Language available for the KDE4 dataset." width="100%">

æˆ‘ä»¬æ¥çœ‹çœ‹æ•°æ®é›†ï¼š

```py
raw_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['id', 'translation'],
        num_rows: 210173
    })
})
```

æˆ‘ä»¬æœ‰ 210,173 å¯¹å¥å­ï¼Œä½†åœ¨ä¸€æ¬¡è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬éœ€è¦åˆ›å»ºè‡ªå·±çš„éªŒè¯é›†ã€‚æ­£å¦‚æˆ‘ä»¬åœ¨ç¬¬äº”ç« å­¦çš„çš„é‚£æ ·ï¼Œ `Dataset` æœ‰ä¸€ä¸ª `train_test_split()` æ–¹æ³•å¯ä»¥å¸®åŠ©æˆ‘ä»¬ã€‚æˆ‘ä»¬å°†è®¾ç½®ä¸€ä¸ªéšæœºæ•°ç§å­ä»¥ä¿è¯ç»“æœçš„å¯å¤ç°æ€§ï¼š

```py
split_datasets = raw_datasets["train"].train_test_split(train_size=0.9, seed=20)
split_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['id', 'translation'],
        num_rows: 189155
    })
    test: Dataset({
        features: ['id', 'translation'],
        num_rows: 21018
    })
})
```

æˆ‘ä»¬å¯ä»¥è¿™æ ·å°† "test" é”®é‡å‘½åä¸º "validation"ï¼š

```py
split_datasets["validation"] = split_datasets.pop("test")
```

ç°åœ¨è®©æˆ‘ä»¬çœ‹ä¸€ä¸‹æ•°æ®é›†çš„ä¸€ä¸ªå…ƒç´ ï¼š

```py
split_datasets["train"][1]["translation"]
```

```python out
{'en': 'Default to expanded threads',
 'fr': 'Par dÃ©faut, dÃ©velopper les fils de discussion'}
```

æˆ‘ä»¬å¾—åˆ°ä¸€ä¸ªåŒ…å«æˆ‘ä»¬é€‰æ‹©çš„ä¸¤ç§è¯­è¨€çš„ä¸¤ä¸ªå¥å­çš„å­—å…¸ã€‚è¿™ä¸ªå……æ»¡æŠ€æœ¯è®¡ç®—æœºç§‘å­¦æœ¯è¯­çš„æ•°æ®é›†çš„ä¸€ä¸ªç‰¹æ®Šä¹‹å¤„åœ¨äºå®ƒä»¬éƒ½å®Œå…¨ç”¨æ³•è¯­ç¿»è¯‘ã€‚ç„¶è€Œï¼Œæ³•å›½å·¥ç¨‹å¸ˆé€šå¸¸å¾ˆæ‡’æƒ°ï¼Œåœ¨äº¤è°ˆæ—¶ï¼Œå¤§å¤šæ•°è®¡ç®—æœºç§‘å­¦ä¸“ç”¨è¯æ±‡éƒ½ç”¨è‹±è¯­è¡¨è¿°ã€‚ä¾‹å¦‚ï¼Œâ€œthreadsâ€è¿™ä¸ªè¯å¾ˆå¯èƒ½å‡ºç°åœ¨æ³•è¯­å¥å­ä¸­ï¼Œå°¤å…¶æ˜¯åœ¨æŠ€æœ¯å¯¹è¯ä¸­ï¼›ä½†åœ¨è¿™ä¸ªæ•°æ®é›†ä¸­ï¼Œå®ƒè¢«ç¿»è¯‘æˆæ›´å‡†ç¡®çš„â€œfils de Discussionâ€ã€‚æˆ‘ä»¬ä½¿ç”¨çš„é¢„è®­ç»ƒæ¨¡å‹å·²ç»åœ¨ä¸€ä¸ªæ›´å¤§çš„æ³•è¯­å’Œè‹±è¯­å¥å­è¯­æ–™åº“ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒï¼Œé€‰æ‹©äº†æ›´ä¸ºç®€å•çš„ä¿ç•™åŸè¯çš„æ–¹å¼ï¼š

```py
from transformers import pipeline

model_checkpoint = "Helsinki-NLP/opus-mt-en-fr"
translator = pipeline("translation", model=model_checkpoint)
translator("Default to expanded threads")
```

```python out
[{'translation_text': 'Par dÃ©faut pour les threads Ã©largis'}]
```

è¿™ç§æƒ…å†µçš„å¦ä¸€ä¸ªä¾‹å­å¯ä»¥åœ¨"plugin"è¿™ä¸ªè¯ä¸Šçœ‹åˆ°ï¼Œå®ƒå¹¶éæ­£å¼çš„æ³•è¯­è¯æ±‡ï¼Œä½†å¤§å¤šæ•°æ¯è¯­æ˜¯æ³•è¯­çš„äººéƒ½ä¼šç†è§£å¹¶ä¸”ä¸ä¼šå»ç¿»è¯‘å®ƒã€‚åœ¨ KDE4 æ•°æ®é›†ä¸­ï¼Œè¿™ä¸ªè¯è¢«ç¿»è¯‘æˆäº†æ›´æ­£å¼çš„æ³•è¯­è¯æ±‡â€œmodule d'extensionâ€ï¼š

```py
split_datasets["train"][172]["translation"]
```

```python out
{'en': 'Unable to import %1 using the OFX importer plugin. This file is not the correct format.',
 'fr': "Impossible d'importer %1 en utilisant le module d'extension d'importation OFX. Ce fichier n'a pas un format correct."}
```

ç„¶è€Œï¼Œæˆ‘ä»¬çš„é¢„è®­ç»ƒæ¨¡å‹åšæŒä½¿ç”¨ç®€ç»ƒè€Œç†Ÿæ‚‰çš„è‹±æ–‡å•è¯ï¼š

```py
translator(
    "Unable to import %1 using the OFX importer plugin. This file is not the correct format."
)
```

```python out
[{'translation_text': "Impossible d'importer %1 en utilisant le plugin d'importateur OFX. Ce fichier n'est pas le bon format."}]
```

çœ‹çœ‹æˆ‘ä»¬çš„å¾®è°ƒæ¨¡å‹æ˜¯å¦èƒ½è¯†åˆ«æ•°æ®é›†çš„è¿™äº›ç‰¹æ®Šæ€§ã€‚ï¼ˆå‰§é€è­¦å‘Šï¼šå®ƒä¼šçš„ï¼‰ã€‚



<div custom-style="Tip-green">


âœï¸ **è½®åˆ°ä½ äº†ï¼** å¦ä¸€ä¸ªåœ¨æ³•è¯­ä¸­ç»å¸¸ä½¿ç”¨çš„è‹±è¯­å•è¯æ˜¯â€œemailâ€ã€‚åœ¨è®­ç»ƒæ•°æ®é›†ä¸­æ‰¾åˆ°ä½¿ç”¨è¿™ä¸ªè¯çš„ç¬¬ä¸€ä¸ªæ ·æœ¬ã€‚å®ƒæ˜¯å¦‚ä½•ç¿»è¯‘çš„ï¼Ÿé¢„è®­ç»ƒæ¨¡å‹å¦‚ä½•ç¿»è¯‘åŒä¸€ä¸ªè‹±æ–‡å¥å­ï¼Ÿ

</div>

### å¤„ç†æ•°æ® 



ä½ ç°åœ¨åº”è¯¥çŸ¥é“æˆ‘ä»¬çš„ä¸‹ä¸€æ­¥è¯¥åšäº›ä»€ä¹ˆäº†ï¼šæ‰€æœ‰æ–‡æœ¬éƒ½éœ€è¦è½¬æ¢ä¸º token IDs çš„é›†åˆï¼Œä»¥ä¾¿æ¨¡å‹èƒ½å¤Ÿç†è§£å®ƒä»¬ã€‚å¯¹äºè¿™ä¸ªä»»åŠ¡ï¼Œæˆ‘ä»¬éœ€è¦åŒæ—¶å¯¹è¾“å…¥å’Œç›®æ ‡è¯å…ƒåŒ–ã€‚æˆ‘ä»¬çš„é¦–è¦ä»»åŠ¡æ˜¯åˆ›å»ºæˆ‘ä»¬çš„ `tokenizer` å¯¹è±¡ã€‚å¦‚å‰æ‰€è¿°ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ Marian è‹±è¯­åˆ°æ³•è¯­çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚å¦‚æœä½ ä½¿ç”¨å¦ä¸€å¯¹è¯­è¨€å°è¯•æ­¤ä»£ç ï¼Œè¯·ç¡®ä¿è°ƒæ•´æ¨¡å‹ checkpointã€‚ [Helsinki-NLP](https://huggingface.co/Helsinki-NLP)(https://huggingface.co/Helsinki-NLP) ç»„ç»‡æä¾›äº†è¶…è¿‡ä¸€åƒä¸ªå¤šè¯­è¨€æ¨¡å‹ã€‚

```python
from transformers import AutoTokenizer

model_checkpoint = "Helsinki-NLP/opus-mt-en-fr"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors="pt")
```

ä½ ä¹Ÿå¯ä»¥å°† `model_checkpoint` æ›¿æ¢ä¸ºä½ ä» [Hub](https://huggingface.co/models)(https://huggingface.co/models) ä¸­é€‰æ‹©çš„å…¶ä»–æ¨¡å‹ï¼Œæˆ–è€…ä¸€ä¸ªä½ ä¿å­˜äº†é¢„è®­ç»ƒæ¨¡å‹å’Œ tokenizer çš„æœ¬åœ°æ–‡ä»¶å¤¹ã€‚

<div custom-style="Tip-green">


ğŸ’¡ å¦‚æœä½ åœ¨ä½¿ç”¨ä¸€ä¸ªå¤šè¯­è¨€çš„ tokenizerï¼Œæ¯”å¦‚ mBARTï¼ŒmBART-50ï¼Œæˆ–è€… M2M100ï¼Œä½ éœ€è¦é€šè¿‡è®¾ç½® `tokenizer.src_lang` å’Œ `tokenizer.tgt_lang` æ¥åœ¨ tokenizer ä¸­æŒ‡å®šä½ çš„è¾“å…¥å’Œç›®æ ‡çš„è¯­è¨€ä»£ç ã€‚

</div>

æˆ‘ä»¬çš„æ•°æ®å‡†å¤‡ç›¸å½“ç›´æ¥ã€‚åªæœ‰ä¸€ç‚¹è¦è®°ä½ï¼›ä½ éœ€è¦ç¡®ä¿ tokenizer å¤„ç†çš„ç›®æ ‡æ˜¯è¾“å‡ºè¯­è¨€ï¼ˆåœ¨è¿™é‡Œæ˜¯æ³•è¯­ï¼‰ã€‚ä½ å¯ä»¥é€šè¿‡å°†ç›®æ ‡ä¼ é€’ç»™ tokenizer çš„ `__call__` æ–¹æ³•çš„ `text_targets` å‚æ•°æ¥å®Œæˆæ­¤æ“ä½œã€‚

ä¸ºäº†çœ‹çœ‹è¿™æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Œè®©æˆ‘ä»¬å¤„ç†è®­ç»ƒé›†ä¸­æ¯ç§è¯­è¨€çš„ä¸€ä¸ªæ ·æœ¬ï¼š

```python
en_sentence = split_datasets["train"][1]["translation"]["en"]
fr_sentence = split_datasets["train"][1]["translation"]["fr"]

inputs = tokenizer(en_sentence, text_target=fr_sentence)
inputs
```

```python out
{'input_ids': [47591, 12, 9842, 19634, 9, 0], 'attention_mask': [1, 1, 1, 1, 1, 1], 'labels': [577, 5891, 2, 3184, 16, 2542, 5, 1710, 0]}
```

å¦‚æˆ‘ä»¬æ‰€è§ï¼Œè¾“å‡ºåŒ…å«äº†ä¸è‹±è¯­å¥å­ç›¸å…³è”çš„è¾“å…¥ IDsï¼Œè€Œä¸æ³•è¯­å¥å­ç›¸å…³è”çš„ IDs å­˜å‚¨åœ¨ `labels` å­—æ®µä¸­ã€‚å¦‚æœä½ å¿˜è®°æŒ‡ç¤ºä½ æ­£åœ¨å¯¹ labels è¿›è¡Œè¯å…ƒåŒ–ï¼Œå®ƒä»¬å°†ç”±è¾“å…¥ tokenizer è¿›è¡Œè¯å…ƒåŒ–ï¼Œè¿™åœ¨ Marian æ¨¡å‹çš„æƒ…å†µä¸‹å°†ä¼šå‡ºç°é—®é¢˜ï¼š

```python
wrong_targets = tokenizer(fr_sentence)
print(tokenizer.convert_ids_to_tokens(wrong_targets["input_ids"]))
print(tokenizer.convert_ids_to_tokens(inputs["labels"]))
```

```python out
['â–Par', 'â–dÃ©', 'f', 'aut', ',', 'â–dÃ©', 've', 'lop', 'per', 'â–les', 'â–fil', 's', 'â–de', 'â–discussion', '</s>']
['â–Par', 'â–dÃ©faut', ',', 'â–dÃ©velopper', 'â–les', 'â–fils', 'â–de', 'â–discussion', '</s>']
```

å¦‚ä½ æ‰€è§ï¼Œå¦‚æœç”¨è‹±è¯­çš„ tokenizer æ¥é¢„å¤„ç†æ³•è¯­å¥å­ï¼Œä¼šäº§ç”Ÿæ›´å¤šçš„ tokensï¼Œå› ä¸ºè¿™ä¸ª tokenizer ä¸è®¤è¯†ä»»ä½•æ³•è¯­å•è¯ï¼ˆé™¤äº†é‚£äº›åœ¨è‹±è¯­é‡Œä¹Ÿå‡ºç°çš„ï¼Œæ¯”å¦‚"discussion"ï¼‰ã€‚

ç”±äºâ€œinputsâ€æ˜¯ä¸€ä¸ªåŒ…å«æˆ‘ä»¬å¸¸ç”¨é”®ï¼ˆè¾“å…¥ IDã€æ³¨æ„æ©ç ç­‰ï¼‰çš„å­—å…¸ï¼Œæœ€åä¸€æ­¥æ˜¯å®šä¹‰æˆ‘ä»¬å°†åº”ç”¨äºæ•°æ®é›†çš„é¢„å¤„ç†å‡½æ•°ï¼š

```python
max_length = 128

def preprocess_function(examples):
    inputs = [ex["en"] for ex in examples["translation"]]
    targets = [ex["fr"] for ex in examples["translation"]]
    model_inputs = tokenizer(
        inputs, text_target=targets, max_length=max_length, truncation=True
    )
```

è¯·æ³¨æ„ï¼Œæˆ‘ä»¬ä¸ºè¾“å…¥å’Œè¾“å‡ºè®¾ç½®äº†ç›¸åŒçš„æœ€å¤§é•¿åº¦ã€‚ç”±äºæˆ‘ä»¬å¤„ç†çš„æ–‡æœ¬çœ‹èµ·æ¥å¾ˆçŸ­ï¼Œæˆ‘ä»¬ä½¿ç”¨ 128ã€‚

<div custom-style="Tip-green">


ğŸ’¡ å¦‚æœä½ æ­£åœ¨ä½¿ç”¨ T5 æ¨¡å‹ï¼ˆæ›´å…·ä½“åœ°è¯´ï¼Œä¸€ä¸ª `t5-xxx` æ£€æŸ¥ç‚¹ï¼‰ï¼Œæ¨¡å‹ä¼šæœŸæœ›æ–‡æœ¬è¾“å…¥æœ‰ä¸€ä¸ªå‰ç¼€æŒ‡ç¤ºç€æ‰‹å¤´çš„ä»»åŠ¡ï¼Œæ¯”å¦‚ `translate: English to French:` ã€‚

</div>

<div custom-style="Tip-yellow">


âš ï¸ æˆ‘ä»¬ä¸å…³æ³¨ç›®æ ‡çš„æ³¨æ„åŠ›æ©ç ï¼Œå› ä¸ºæ¨¡å‹ä¸ä¼šéœ€è¦å®ƒã€‚ç›¸åï¼Œå¯¹åº”äºå¡«å……æ ‡è®°çš„æ ‡ç­¾åº”è®¾ç½®ä¸º `-100` ï¼Œä»¥ä¾¿åœ¨ loss è®¡ç®—ä¸­å¿½ç•¥å®ƒä»¬ã€‚è¿™å°†åœ¨ç¨åç”±æˆ‘ä»¬çš„æ•°æ®æ•´ç†å™¨å®Œæˆï¼Œå› ä¸ºæˆ‘ä»¬æ­£åœ¨åº”ç”¨åŠ¨æ€å¡«å……ï¼Œä½†æ˜¯å¦‚æœä½ åœ¨æ­¤å¤„ä½¿ç”¨å¡«å……ï¼Œä½ åº”è¯¥è°ƒæ•´é¢„å¤„ç†å‡½æ•°ä»¥å°†ä¸å¡«å……è¯å…ƒå¯¹åº”çš„æ‰€æœ‰æ ‡ç­¾è®¾ç½®ä¸º `-100` ã€‚

</div>

æˆ‘ä»¬ç°åœ¨å¯ä»¥å¯¹æ•°æ®é›†çš„æ‰€æœ‰æ•°æ®ä¸€æ¬¡æ€§è¿›è¡Œè¯¥é¢„å¤„ç†ï¼š

```py
tokenized_datasets = split_datasets.map(
    preprocess_function,
    batched=True,
    remove_columns=split_datasets["train"].column_names,
)
```

ç°åœ¨æ•°æ®å·²ç»è¿‡é¢„å¤„ç†ï¼Œæˆ‘ä»¬å‡†å¤‡å¥½å¾®è°ƒæˆ‘ä»¬çš„é¢„è®­ç»ƒæ¨¡å‹äº†ï¼

{#if fw === 'pt'}

## ä½¿ç”¨ `Trainer` API å¾®è°ƒæ¨¡å‹ 

ä½¿ç”¨ `Trainer` çš„å®é™…ä»£ç å°†ä¸ä»¥å‰ç›¸åŒï¼Œåªæ˜¯ç¨ä½œæ”¹åŠ¨ï¼šæˆ‘ä»¬åœ¨è¿™é‡Œä½¿ç”¨ [`Seq2SeqTrainer`](https://huggingface.co/transformers/main_classes/trainer.html#seq2seqtrainer)(https://huggingface.co/transformers/main_classes/trainer.html#seq2seqtrainer) ï¼Œå®ƒæ˜¯ `Trainer` çš„å­ç±»ï¼Œå®ƒä½¿ç”¨ `generate()` æ–¹æ³•æ¥é¢„æµ‹è¾“å…¥çš„è¾“å‡ºï¼Œå¹¶ä¸”å¯ä»¥æ­£ç¡®å¤„ç†è¿™ç§åºåˆ—åˆ°åºåˆ—çš„è¯„ä¼°ã€‚å½“æˆ‘ä»¬è®¨è®ºè¯„ä¼°æŒ‡æ ‡æ—¶ï¼Œæˆ‘ä»¬å°†æ›´è¯¦ç»†åœ°æ¢è®¨è¿™ä¸€ç‚¹ã€‚

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªå®é™…çš„æ¨¡å‹æ¥è¿›è¡Œå¾®è°ƒã€‚æˆ‘ä»¬å°†ä½¿ç”¨å¸¸ç”¨çš„ `AutoModel` APIï¼š

```py
from transformers import AutoModelForSeq2SeqLM

model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)
```

{:else}

## ä½¿ç”¨ Keras å¾®è°ƒæ¨¡å‹ 

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªå®é™…çš„æ¨¡å‹æ¥è¿›è¡Œå¾®è°ƒã€‚æˆ‘ä»¬å°†ä½¿ç”¨å¸¸ç”¨çš„ `AutoModel` APIï¼š

```py
from transformers import TFAutoModelForSeq2SeqLM

model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint, from_pt=True)
```

<div custom-style="Tip-green">


ğŸ’¡ `Helsinki-NLP/opus-mt-en-fr` checkpoint åªæœ‰ PyTorch çš„æƒé‡ï¼Œæ‰€ä»¥å¦‚æœä½ å°è¯•åŠ è½½æ¨¡å‹è€Œæ²¡æœ‰ä½¿ç”¨ `from_pt=True` å‚æ•°åœ¨ `from_pretrained()` æ–¹æ³•ä¸­ï¼Œä½ ä¼šå¾—åˆ°ä¸€ä¸ªé”™è¯¯ã€‚å½“ä½ æŒ‡å®š `from_pt=True` ï¼Œåº“ä¼šè‡ªåŠ¨ä¸‹è½½å¹¶ä¸ºä½ è½¬æ¢ PyTorch æƒé‡ã€‚å¦‚ä½ æ‰€è§ï¼Œä½¿ç”¨transormer åœ¨ä¸¤è€…ä¹‹é—´åˆ‡æ¢éå¸¸ç®€å•

</div>

{/if}

æ³¨æ„ï¼Œè¿™æ¬¡æˆ‘ä»¬ä½¿ç”¨çš„æ˜¯ä¸€ä¸ªå·²ç»åœ¨ç¿»è¯‘ä»»åŠ¡ä¸Šè¿›è¡Œè¿‡è®­ç»ƒçš„æ¨¡å‹ï¼Œå®é™…ä¸Šå·²ç»å¯ä»¥ä½¿ç”¨äº†ï¼Œæ‰€ä»¥æ²¡æœ‰å…³äºç¼ºå°‘æƒé‡æˆ–æ–°åˆå§‹åŒ–çš„æƒé‡çš„è­¦å‘Šã€‚

### æ•°æ®æ•´ç† 

æˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ•°æ®æ•´ç†å™¨æ¥å¤„ç†åŠ¨æ€æ‰¹å¤„ç†çš„å¡«å……ã€‚åœ¨æœ¬ä¾‹ä¸­ï¼Œæˆ‘ä»¬ä¸èƒ½åƒç¬¬ä¸‰ç« é‚£æ ·åªä½¿ç”¨ `DataCollatorWithPadding` ï¼Œå› ä¸ºå®ƒåªå¡«å……è¾“å…¥ï¼ˆè¾“å…¥ IDã€æ³¨æ„æ©ç å’Œè¯å…ƒç±»å‹ IDï¼‰ã€‚æˆ‘ä»¬çš„æ ‡ç­¾ä¹Ÿåº”è¯¥å¡«å……åˆ°æ ‡ç­¾ä¸­é‡åˆ°çš„æœ€å¤§é•¿åº¦ã€‚è€Œä¸”ï¼Œå¦‚å‰æ‰€è¿°ï¼Œç”¨äºå¡«å……æ ‡ç­¾çš„å¡«å……å€¼åº”ä¸º `-100` ï¼Œè€Œä¸æ˜¯ tokenizer çš„å¡«å……è¯å…ƒï¼Œä»¥ç¡®ä¿åœ¨æŸå¤±è®¡ç®—ä¸­å¿½ç•¥è¿™äº›å¡«å……å€¼ã€‚

è¿™ä¸€åˆ‡éƒ½å¯ä»¥ç”± [`DataCollatorForSeq2Seq`](https://huggingface.co/transformers/main_classes/data_collator.html#datacollatorforseq2seq)(https://huggingface.co/transformers/main_classes/data_collator.html#datacollatorforseq2seq) å®Œæˆã€‚ä¸ `DataCollatorWithPadding` ä¸€æ ·ï¼Œå®ƒæ¥æ”¶ç”¨äºé¢„å¤„ç†è¾“å…¥çš„ `tokenizer` ï¼Œä½†å®ƒä¹Ÿæ¥æ”¶ `model` ã€‚è¿™æ˜¯å› ä¸ºæ•°æ®æ•´ç†å™¨è¿˜å°†è´Ÿè´£å‡†å¤‡è§£ç å™¨è¾“å…¥ IDï¼Œå®ƒä»¬æ˜¯æ ‡ç­¾åç§»ä¹‹åå½¢æˆçš„ï¼Œå¼€å¤´å¸¦æœ‰ç‰¹æ®Šè¯å…ƒã€‚ç”±äºå¯¹äºä¸åŒçš„æ¶æ„æœ‰ç¨å¾®ä¸åŒçš„åç§»æ–¹å¼ï¼Œ `DataCollatorForSeq2Seq` éœ€è¦çŸ¥é“ `model` å¯¹è±¡ï¼š

{#if fw === 'pt'}

```py
from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)
```

{:else}

```py
from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors="tf")
```

{/if}

ä¸ºäº†åœ¨å‡ ä¸ªæ ·æœ¬ä¸Šè¿›è¡Œæµ‹è¯•ï¼Œæˆ‘ä»¬åªéœ€åœ¨æˆ‘ä»¬å·²ç»å®Œæˆè¯å…ƒåŒ–çš„è®­ç»ƒé›†ä¸­çš„éƒ¨åˆ†æ•°æ®ä¸Šè°ƒç”¨å®ƒï¼š

```py
batch = data_collator([tokenized_datasets["train"][i] for i in range(1, 3)])
batch.keys()
```

```python out
dict_keys(['attention_mask', 'input_ids', 'labels', 'decoder_input_ids'])
```

æˆ‘ä»¬å¯ä»¥æ£€æŸ¥æˆ‘ä»¬çš„æ ‡ç­¾æ˜¯å¦å·²ç»ç”¨ `-100` å¡«å……åˆ° batch çš„æœ€å¤§é•¿åº¦ï¼š

```py
batch["labels"]
```

```python out
tensor([[  577,  5891,     2,  3184,    16,  2542,     5,  1710,     0,  -100,
          -100,  -100,  -100,  -100,  -100,  -100],
        [ 1211,     3,    49,  9409,  1211,     3, 29140,   817,  3124,   817,
           550,  7032,  5821,  7907, 12649,     0]])
```

æˆ‘ä»¬è¿˜å¯ä»¥æŸ¥çœ‹è§£ç å™¨è¾“å…¥ IDï¼Œå¯ä»¥çœ‹åˆ°å®ƒä»¬æ˜¯æ ‡ç­¾ç»è¿‡åç§»å½¢æˆçš„ç‰ˆæœ¬ï¼š

```py
batch["decoder_input_ids"]
```

```python out
tensor([[59513,   577,  5891,     2,  3184,    16,  2542,     5,  1710,     0,
         59513, 59513, 59513, 59513, 59513, 59513],
        [59513,  1211,     3,    49,  9409,  1211,     3, 29140,   817,  3124,
           817,   550,  7032,  5821,  7907, 12649]])
```

ä»¥ä¸‹æ˜¯æˆ‘ä»¬æ•°æ®é›†ä¸­ç¬¬ä¸€ä¸ªå’Œç¬¬äºŒä¸ªå…ƒç´ çš„æ ‡ç­¾ï¼š

```py
for i in range(1, 3):
    print(tokenized_datasets["train"][i]["labels"])
```

```python out
[577, 5891, 2, 3184, 16, 2542, 5, 1710, 0]
[1211, 3, 49, 9409, 1211, 3, 29140, 817, 3124, 817, 550, 7032, 5821, 7907, 12649, 0]
```

{#if fw === 'pt'}

æˆ‘ä»¬å°†æŠŠè¿™ä¸ª `data_collator` ä¼ é€’ç»™ `Seq2SeqTrainer` ã€‚æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬çœ‹ä¸€ä¸‹è¯„ä¼°æŒ‡æ ‡ã€‚

{:else}

æˆ‘ä»¬ç°åœ¨å¯ä»¥ä½¿ç”¨ `data_collator` å°†æˆ‘ä»¬çš„æ¯ä¸ªæ•°æ®é›†è½¬æ¢ä¸º `tf.data.Dataset` ï¼Œå‡†å¤‡å¥½è¿›è¡Œè®­ç»ƒï¼š

```python
tf_train_dataset = model.prepare_tf_dataset(
    tokenized_datasets["train"],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=32,
)
tf_eval_dataset = model.prepare_tf_dataset(
    tokenized_datasets["validation"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=16,
)
```

{/if}


### è¯„ä¼°æŒ‡æ ‡ 



{#if fw === 'pt'}

`Seq2SeqTrainer` å¯¹å…¶è¶…ç±» `Trainer` çš„å¢å¼ºåŠŸèƒ½æ˜¯åœ¨è¯„ä¼°æˆ–é¢„æµ‹æ—¶ä½¿ç”¨ `generate()` æ–¹æ³•ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹ä¼šä½¿ç”¨ `decoder_input_ids` ï¼Œå¹¶é€šè¿‡ä¸€ä¸ªæ³¨æ„åŠ›æ©ç ç¡®ä¿å®ƒä¸ä½¿ç”¨åœ¨é¢„æµ‹çš„ token ä¹‹åçš„ tokenï¼Œä»¥åŠ é€Ÿè®­ç»ƒã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬æ— æ³•ä½¿ç”¨è¿™äº›æ ‡ç­¾ï¼Œå› æ­¤ï¼Œä½¿ç”¨åŒæ ·çš„è®¾ç½®è¯„ä¼°æˆ‘ä»¬çš„æ¨¡å‹æ˜¯ä¸€ä¸ªå¥½ä¸»æ„ã€‚

æ­£å¦‚æˆ‘ä»¬åœ¨ç¬¬ä¸€ç« çœ‹åˆ°çš„ï¼Œè§£ç å™¨é€šè¿‡ä¸€ä¸ªä¸€ä¸ªåœ°é¢„æµ‹è¯å…ƒæ¥æ‰§è¡Œæ¨ç†â€”â€”è¿™æ˜¯Transformers åœ¨å¹•åé€šè¿‡ **generate()** æ–¹æ³•å®ç°çš„ã€‚ `Seq2SeqTrainer` å°†å…è®¸æˆ‘ä»¬åœ¨è®¾ç½® `predict_with_generate=True` æ—¶ï¼Œä½¿ç”¨è¯¥æ–¹æ³•è¿›è¡Œè¯„ä¼°ã€‚


{/if}

ç”¨äºç¿»è¯‘çš„ä¼ ç»ŸæŒ‡æ ‡æ˜¯ [BLEU åˆ†æ•°](https://en.wikipedia.org/wiki/BLEU)(https://en.wikipedia.org/wiki/BLEU) , å®ƒæœ€åˆåœ¨ 2002 å¹´ç”± Kishore Papineni ç­‰äººçš„ä¸€ç¯‡æ–‡ç« ä¸­è¢«å¼•å…¥ã€‚BLEU åˆ†æ•°è¯„ä¼°ç¿»è¯‘ä¸å…¶æ ‡ç­¾çš„æ¥è¿‘ç¨‹åº¦ã€‚å®ƒä¸è¡¡é‡æ¨¡å‹ç”Ÿæˆè¾“å‡ºçš„å¯ç†è§£æ€§æˆ–è¯­æ³•æ­£ç¡®æ€§ï¼Œè€Œæ˜¯ä½¿ç”¨ç»Ÿè®¡è§„åˆ™æ¥ç¡®ä¿ç”Ÿæˆè¾“å‡ºä¸­çš„æ‰€æœ‰å•è¯ä¹Ÿå‡ºç°åœ¨ç›®æ ‡ä¸­ã€‚æ­¤å¤–ï¼Œè¿˜æœ‰ä¸€äº›è§„åˆ™å¯¹é‡å¤çš„è¯è¿›è¡Œæƒ©ç½šï¼Œå¦‚æœè¿™äº›è¯åœ¨ç›®æ ‡ä¸­æ²¡æœ‰é‡å¤ï¼ˆé¿å…æ¨¡å‹è¾“å‡ºåƒâ€œthe the the the theâ€è¿™æ ·çš„å¥å­ï¼‰ï¼›ä»¥åŠå¯¹è¾“å‡ºçš„å¥å­é•¿åº¦æ¯”ç›®æ ‡ä¸­çš„çŸ­ï¼ˆé¿å…æ¨¡å‹è¾“å‡ºåƒâ€œtheâ€è¿™æ ·çš„å¥å­ï¼‰è¿›è¡Œæƒ©ç½šã€‚

BLEU çš„ä¸€ä¸ªç¼ºç‚¹æ˜¯å®ƒéœ€è¦æ–‡æœ¬å·²ç»è¢«åˆ†è¯ï¼Œè¿™ä½¿å¾—æ¯”è¾ƒä½¿ç”¨ä¸åŒåˆ†è¯å™¨çš„æ¨¡å‹ä¹‹é—´çš„åˆ†æ•°å˜å¾—å›°éš¾ã€‚å› æ­¤ï¼Œå½“ä»Šç”¨äºåŸºå‡†ç¿»è¯‘æ¨¡å‹çš„æœ€å¸¸ç”¨æŒ‡æ ‡æ˜¯ [SacreBLEU](https://github.com/mjpost/sacrebleu)(https://github.com/mjpost/sacrebleu) ï¼Œå®ƒé€šè¿‡å¯¹åˆ†è¯æ­¥éª¤æ ‡å‡†åŒ–è§£å†³äº†è¿™ä¸ªç¼ºç‚¹ï¼ˆå’Œå…¶ä»–çš„ä¸€äº›ç¼ºç‚¹ï¼‰ã€‚è¦ä½¿ç”¨æ­¤æŒ‡æ ‡ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦å®‰è£… SacreBLEU åº“ï¼š

```py
!pip install sacrebleu
```

ç„¶åæˆ‘ä»¬å¯ä»¥å°±åƒæˆ‘ä»¬åœ¨ç¬¬ä¸‰ç« é‚£æ ·é€šè¿‡ `evaluate.load()` åŠ è½½å®ƒ 

```py
import evaluate

metric = evaluate.load("sacrebleu")
```

è¿™ä¸ªæŒ‡æ ‡å°†æ–‡æœ¬ä½œä¸ºè¾“å…¥å’Œç›®æ ‡ã€‚å®ƒçš„è®¾è®¡æ˜¯ä¸ºäº†æ¥å—å¤šä¸ªå¯æ¥å—çš„ç›®æ ‡ï¼Œå› ä¸ºåŒä¸€å¥è¯é€šå¸¸æœ‰å¤šç§å¯æ¥å—çš„ç¿»è¯‘â€”â€”æˆ‘ä»¬ä½¿ç”¨çš„æ•°æ®é›†åªæä¾›ä¸€ä¸ªï¼Œä½†åœ¨ NLP ä¸­æ‰¾åˆ°å°†å¤šä¸ªå¥å­ä½œä¸ºæ ‡ç­¾çš„æ•°æ®é›†ä¸æ˜¯ä¸€ä¸ªéš¾é¢˜ã€‚å› æ­¤ï¼Œé¢„æµ‹ç»“æœåº”è¯¥æ˜¯ä¸€ä¸ªå¥å­åˆ—è¡¨ï¼Œè€Œå‚è€ƒåº”è¯¥æ˜¯ä¸€ä¸ªå¥å­åˆ—è¡¨çš„åˆ—è¡¨ã€‚

è®©æˆ‘ä»¬å°è¯•ä¸€ä¸ªä¾‹å­ï¼š

```py
predictions = [
    "This plugin lets you translate web pages between several languages automatically."
]
references = [
    [
        "This plugin allows you to automatically translate web pages between several languages."
    ]
]
metric.compute(predictions=predictions, references=references)
```

```python out
{'score': 46.750469682990165,
 'counts': [11, 6, 4, 3],
 'totals': [12, 11, 10, 9],
 'precisions': [91.67, 54.54, 40.0, 33.33],
 'bp': 0.9200444146293233,
 'sys_len': 12,
 'ref_len': 13}
```

è¿™å¾—åˆ°äº† 46.75 çš„ BLEU åˆ†æ•°ï¼Œè¿™æ˜¯ç›¸å½“ä¸é”™çš„â€”â€”ä½œä¸ºå‚è€ƒï¼ŒåŸå§‹ Transformer æ¨¡å‹åœ¨ [â€œAttention Is All You Needâ€ è®ºæ–‡](https://arxiv.org/pdf/1706.03762.pdf)(https://arxiv.org/pdf/1706.03762.pdf) ç±»ä¼¼çš„è‹±è¯­å’Œæ³•è¯­ç¿»è¯‘ä»»åŠ¡ä¸­è·å¾—äº† 41.8 çš„ BLEU åˆ†æ•°ï¼ï¼ˆå…³äºä¸ªåˆ«æŒ‡æ ‡ï¼Œå¦‚ `counts` å’Œ `bp` ï¼Œå¯ä»¥å‚è§ [SacreBLEUä»“åº“](https://github.com/mjpost/sacrebleu/blob/078c440168c6adc89ba75fe6d63f0d922d42bcfe/sacrebleu/metrics/bleu.py#L74)(https://github.com/mjpost/sacrebleu/blob/078c440168c6adc89ba75fe6d63f0d922d42bcfe/sacrebleu/metrics/bleu.py#L74) ã€‚) å¦ä¸€æ–¹é¢ï¼Œå¦‚æœæˆ‘ä»¬å°è¯•ä½¿ç”¨ç¿»è¯‘æ¨¡å‹ä¸­ç»å¸¸å‡ºç°çš„ä¸¤ç§ç³Ÿç³•çš„é¢„æµ‹ç±»å‹ï¼ˆå¤§é‡é‡å¤æˆ–å¤ªçŸ­ï¼‰ï¼Œæˆ‘ä»¬å°†å¾—åˆ°ç›¸å½“ç³Ÿç³•çš„ BLEU åˆ†æ•°ï¼š

```py
predictions = ["This This This This"]
references = [
    [
        "This plugin allows you to automatically translate web pages between several languages."
    ]
]
metric.compute(predictions=predictions, references=references)
```

```python out
{'score': 1.683602693167689,
 'counts': [1, 0, 0, 0],
 'totals': [4, 3, 2, 1],
 'precisions': [25.0, 16.67, 12.5, 12.5],
 'bp': 0.10539922456186433,
 'sys_len': 4,
 'ref_len': 13}
```

```py
predictions = ["This plugin"]
references = [
    [
        "This plugin allows you to automatically translate web pages between several languages."
    ]
]
metric.compute(predictions=predictions, references=references)
```

```python out
{'score': 0.0,
 'counts': [2, 1, 0, 0],
 'totals': [2, 1, 0, 0],
 'precisions': [100.0, 100.0, 0.0, 0.0],
 'bp': 0.004086771438464067,
 'sys_len': 2,
 'ref_len': 13}
```

åˆ†æ•°å¯ä»¥ä» 0 åˆ° 100ï¼Œè¶Šé«˜è¶Šå¥½ã€‚

{#if fw === 'tf'}

ä¸ºäº†å°†æ¨¡å‹çš„è¾“å‡ºè½¬åŒ–ä¸ºè¯„ä¼°æŒ‡æ ‡å¯ä»¥ä½¿ç”¨çš„æ–‡æœ¬ï¼Œæˆ‘ä»¬å°†åˆ©ç”¨ `tokenizer.batch_decode()` æ–¹æ³•ã€‚æˆ‘ä»¬åªéœ€è¦æ¸…ç†æ‰€æœ‰æ ‡ç­¾ä¸­çš„ `-100` ï¼Œè¯å…ƒåˆ†æå™¨ä¼šè‡ªåŠ¨å¯¹å¡«å……è¯å…ƒåšåŒæ ·çš„å¤„ç†ã€‚è®©æˆ‘ä»¬å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œè¿™ä¸ªå‡½æ•°ä¼šæ¥æ”¶æˆ‘ä»¬çš„æ¨¡å‹å’Œä¸€ä¸ªæ•°æ®é›†ï¼Œå¹¶åœ¨å…¶ä¸Šè®¡ç®—æŒ‡æ ‡ã€‚æˆ‘ä»¬è¿˜å°†ä½¿ç”¨ä¸€ä¸ªæ˜¾è‘—æå‡æ€§èƒ½çš„æŠ€å·§ - ä½¿ç”¨ [XLA](https://www.tensorflow.org/xla)(https://www.tensorflow.org/xla) ï¼ŒTensorFlow çš„åŠ é€Ÿçº¿æ€§ä»£æ•°ç¼–è¯‘å™¨ï¼Œç¼–è¯‘æˆ‘ä»¬çš„ç”Ÿæˆä»£ç ã€‚XLA å¯¹æ¨¡å‹çš„è®¡ç®—å›¾è¿›è¡Œäº†å„ç§ä¼˜åŒ–ï¼Œä»è€Œæ˜¾è‘—æå‡äº†é€Ÿåº¦å’Œå†…å­˜ä½¿ç”¨ç‡ã€‚å¦‚ Hugging Face çš„ [åšå®¢](https://huggingface.co/blog/tf-xla-generate)(https://huggingface.co/blog/tf-xla-generate) æ‰€è¿°ï¼Œå½“æˆ‘ä»¬çš„è¾“å…¥å½¢çŠ¶ä¸å˜åŒ–å¤ªå¤§æ—¶ï¼ŒXLA å·¥ä½œå¾—æœ€å¥½ã€‚ä¸ºäº†å¤„ç†è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å°†æŠŠè¾“å…¥è¡¥é½åˆ° 128 çš„å€æ•°ï¼Œç„¶åç”¨å¡«å……æ•´ç†å™¨åˆ¶ä½œä¸€ä¸ªæ–°çš„æ•°æ®é›†ï¼Œç„¶åæˆ‘ä»¬å°†åº”ç”¨ `@tf.function(jit_compile=True)` è£…é¥°å™¨åˆ°æˆ‘ä»¬çš„ç”Ÿæˆå‡½æ•°ä¸Šï¼Œè¿™å°†æ ‡è®°æ•´ä¸ªå‡½æ•°ç”¨ XLA ç¼–è¯‘ã€‚

```py
import numpy as np
import tensorflow as tf
from tqdm import tqdm

generation_data_collator = DataCollatorForSeq2Seq(
    tokenizer, model=model, return_tensors="tf", pad_to_multiple_of=128
)

tf_generate_dataset = model.prepare_tf_dataset(
    tokenized_datasets["validation"],
    collate_fn=generation_data_collator,
    shuffle=False,
    batch_size=8,
)

@tf.function(jit_compile=True)
def generate_with_xla(batch):
    return model.generate(
        input_ids=batch["input_ids"],
        attention_mask=batch["attention_mask"],
        max_new_tokens=128,
    )

def compute_metrics():
    all_preds = []
    all_labels = []
    sampled_dataset = tokenized_datasets["validation"].shuffle().select(range(200))
    tf_generate_dataset = sampled_dataset.to_tf_dataset(
        columns=["input_ids", "attention_mask", "labels"],
        collate_fn=data_collator,
        shuffle=False,
        batch_size=4,
    )
    for batch, labels in tqdm(tf_generate_dataset):
        predictions = generate_with_xla(batch)
        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
        labels = labels.numpy()
        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
        decoded_preds = [pred.strip() for pred in decoded_preds]
        decoded_labels = [[label.strip()] for label in decoded_labels]
        all_preds.extend(decoded_preds)
        all_labels.extend(decoded_labels)

    result = metric.compute(predictions=all_preds, references=all_labels)
    return {"bleu": result["score"]}
```

{:else}

ä¸ºäº†å°†æ¨¡å‹çš„è¾“å‡ºè½¬åŒ–ä¸ºè¯„ä¼°æŒ‡æ ‡å¯ä»¥ä½¿ç”¨çš„æ–‡æœ¬ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ `tokenizer.batch_decode()` æ–¹æ³•ã€‚æˆ‘ä»¬åªéœ€è¦æ¸…ç†æ ‡ç­¾ä¸­çš„æ‰€æœ‰ `-100` ï¼ˆè¯å…ƒåˆ†æå™¨å°†è‡ªåŠ¨å¯¹å¡«å……çš„è¯å…ƒæ‰§è¡Œç›¸åŒæ“ä½œï¼‰ï¼š

```py
import numpy as np

def compute_metrics(eval_preds):
    preds, labels = eval_preds
    # å¦‚æœæ¨¡å‹è¿”å›çš„å†…å®¹è¶…è¿‡äº†é¢„æµ‹çš„logits
    if isinstance(preds, tuple):
        preds = preds[0]

    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)

    # ç”±äºæˆ‘ä»¬æ— æ³•è§£ç  -100ï¼Œå› æ­¤å°†æ ‡ç­¾ä¸­çš„ -100 æ›¿æ¢æ‰
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # ä¸€äº›ç®€å•çš„åå¤„ç†
    decoded_preds = [pred.strip() for pred in decoded_preds]
    decoded_labels = [[label.strip()] for label in decoded_labels]

    result = metric.compute(predictions=decoded_preds, references=decoded_labels)
    return {"bleu": result["score"]}
```

{/if}

ç°åœ¨è¿™å·²ç»å®Œæˆäº†ï¼Œæˆ‘ä»¬å·²ç»å‡†å¤‡å¥½å¾®è°ƒæˆ‘ä»¬çš„æ¨¡å‹äº†ï¼

### å¾®è°ƒæ¨¡å‹ 

ç¬¬ä¸€æ­¥æ˜¯ç™»å½• Hugging Faceï¼Œè¿™æ ·ä½ å°±å¯ä»¥å°†ç»“æœä¸Šä¼ åˆ°æ¨¡å‹ä¸­å¿ƒã€‚æœ‰ä¸€ä¸ªæ–¹ä¾¿çš„åŠŸèƒ½å¯ä»¥å¸®åŠ©ä½ åœ¨ notebook ä¸­å®Œæˆæ­¤æ“ä½œï¼š

```python
from huggingface_hub import notebook_login

notebook_login()
```

è¿™å°†æ˜¾ç¤ºä¸€ä¸ªå°éƒ¨ä»¶ï¼Œä½ å¯ä»¥åœ¨å…¶ä¸­è¾“å…¥ä½ çš„ Hugging Face ç™»å½•å‡­æ®ã€‚

å¦‚æœä½ ä¸æ˜¯åœ¨ notebook ä¸Šè¿è¡Œä»£ç ï¼Œåªéœ€åœ¨ç»ˆç«¯ä¸­è¾“å…¥ä»¥ä¸‹è¡Œï¼š

```bash
huggingface-cli login
```

{#if fw === 'tf'}

åœ¨æˆ‘ä»¬å¼€å§‹ä¹‹å‰ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬åœ¨æ²¡æœ‰ä»»ä½•è®­ç»ƒçš„æƒ…å†µä¸‹ä»æˆ‘ä»¬çš„æ¨¡å‹ä¸­å¾—åˆ°äº†ä»€ä¹ˆæ ·çš„ç»“æœï¼š

```py
print(compute_metrics())
```

```
{'bleu': 33.26983701454733}
```

ä¸€æ—¦å®Œæˆï¼Œæˆ‘ä»¬å°±å¯ä»¥å‡†å¤‡ç¼–è¯‘å’Œè®­ç»ƒæ¨¡å‹æ‰€éœ€çš„ä¸€åˆ‡ã€‚è¯·æ³¨æ„å½“ä½¿ç”¨ `tf.keras.mixed_precision.set_global_policy("mixed_float16")` æ—¶â€”â€”è¿™å°†å‘Šè¯‰ Keras ä½¿ç”¨ float16 è¿›è¡Œè®­ç»ƒï¼Œè¿™å¯ä»¥æ˜¾ç€æé«˜æ”¯æŒå®ƒçš„ GPUï¼ˆNvidia 20xx/V100 æˆ–æ›´é«˜ç‰ˆæœ¬ï¼‰çš„é€Ÿåº¦ã€‚

```python
from transformers import create_optimizer
from transformers.keras_callbacks import PushToHubCallback
import tensorflow as tf

# è®­ç»ƒæ­¥éª¤çš„æ•°é‡æ˜¯æ•°æ®é›†ä¸­æ ·æœ¬çš„æ•°é‡ï¼Œé™¤ä»¥batchå¤§å°ç„¶åä¹˜ä»¥æ€»çš„epochsæ•°ã€‚
# è®­ç»ƒæ­¥æ•°æ˜¯æ•°æ®é›†ä¸­çš„æ ·æœ¬æ•°é‡ï¼Œé™¤ä»¥ batch å¤§å°ï¼Œç„¶åä¹˜ä»¥æ€»çš„ epoch æ•°ã€‚
# æ³¨æ„è¿™é‡Œçš„ tf_train_dataset æ˜¯ batch å½¢å¼çš„ tf.data.Datasetï¼Œ
# è€Œä¸æ˜¯åŸå§‹çš„ Hugging Face Dataset ï¼Œæ‰€ä»¥ len() æ±‚å–å®ƒçš„é•¿åº¦å·²ç»æ˜¯ num_samples // batch_sizeã€‚

num_epochs = 3
num_train_steps = len(tf_train_dataset) * num_epochs

optimizer, schedule = create_optimizer(
    init_lr=5e-5,
    num_warmup_steps=0,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)
model.compile(optimizer=optimizer)

# ä½¿ç”¨æ··åˆç²¾åº¦float16è¿›è¡Œè®­ç»ƒ
tf.keras.mixed_precision.set_global_policy("mixed_float16")
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å®šä¹‰ä¸€ä¸ª `PushToHubCallback` ä»¥ä¾¿åœ¨è®­ç»ƒæœŸé—´å°†æˆ‘ä»¬çš„æ¨¡å‹ä¸Šä¼ åˆ° Hubï¼Œæ­£å¦‚æˆ‘ä»¬åœ¨ ç¬¬ 2 èŠ‚) ä¸­çœ‹åˆ°çš„ï¼Œç„¶åæˆ‘ä»¬åªéœ€ä½¿ç”¨è¯¥å›è°ƒæ¥æ‹Ÿåˆæ¨¡å‹ï¼š

```python
from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(
    output_dir="marian-finetuned-kde4-en-to-fr", tokenizer=tokenizer
)

model.fit(
    tf_train_dataset,
    validation_data=tf_eval_dataset,
    callbacks=[callback],
    epochs=num_epochs,
)
```

è¯·æ³¨æ„ï¼Œä½ å¯ä»¥ä½¿ç”¨ `hub_model_id` å‚æ•°æŒ‡å®šè¦æ¨é€åˆ°çš„å­˜å‚¨åº“çš„åç§°ï¼ˆå½“ä½ æƒ³æŠŠæ¨¡å‹æ¨é€åˆ°æŒ‡å®šçš„ç»„ç»‡çš„æ—¶å€™ï¼Œä½ ä¹Ÿå¿…é¡»ä½¿ç”¨æ­¤å‚æ•°ï¼‰ã€‚ä¾‹å¦‚ï¼Œå½“æˆ‘ä»¬å°†æ¨¡å‹æ¨é€åˆ° [`huggingface-course` ç»„ç»‡](https://huggingface.co/huggingface-course)(https://huggingface.co/huggingface-course)(https://huggingface.co/huggingface-course) æ—¶ï¼Œæˆ‘ä»¬æ·»åŠ äº† `hub_model_id="huggingface-course/marian-finetuned-kde4-en- to-fr"` åˆ° `Seq2SeqTrainingArguments` ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œä½¿ç”¨çš„å­˜å‚¨åº“å°†åœ¨ä½ çš„è´¦æˆ·ä¸­ï¼Œå¹¶ä»¥ä½ è®¾ç½®çš„è¾“å‡ºç›®å½•å‘½åï¼Œå› æ­¤è¿™é‡Œå°†æ˜¯ `"sgugger/marian-finetuned-kde4-en-to-fr"` ã€‚

<div custom-style="Tip-green">


ğŸ’¡ å¦‚æœä½ æ­£åœ¨ä½¿ç”¨çš„è¾“å‡ºç›®å½•å·²ç»å­˜åœ¨ï¼Œå®ƒéœ€è¦æ˜¯ä½ æƒ³è¦æ¨é€åˆ°çš„ä»“åº“çš„æœ¬åœ°å…‹éš†ã€‚å¦‚æœä¸æ˜¯ï¼Œå½“è°ƒç”¨ `model.fit()` æ—¶ä¼šæ”¶åˆ°é”™è¯¯ï¼Œå¹¶éœ€è¦è®¾ç½®ä¸€ä¸ªæ–°åç§°ã€‚

</div>

æœ€åï¼Œè®©æˆ‘ä»¬çœ‹çœ‹è®­ç»ƒç»“æŸåæˆ‘ä»¬çš„æŒ‡æ ‡æ˜¯ä»€ä¹ˆæ ·çš„ï¼š

```py
print(compute_metrics())
```

```
{'bleu': 57.334066271545865}
```

åœ¨è¿™ä¸ªé˜¶æ®µï¼Œä½ å¯ä»¥ä½¿ç”¨æ¨¡å‹ä¸­å¿ƒä¸Šçš„æ¨ç†å°éƒ¨ä»¶æ¥æµ‹è¯•ä½ çš„æ¨¡å‹å¹¶ä¸ä½ çš„æœ‹å‹åˆ†äº«ã€‚ä½ å·²ç»æˆåŠŸåœ°åœ¨ç¿»è¯‘ä»»åŠ¡ä¸Šå¾®è°ƒäº†ä¸€ä¸ªæ¨¡å‹â€”â€”æ­å–œï¼

{:else}

ä¸€æ—¦å®Œæˆè¿™äº›æ­¥éª¤ï¼Œæˆ‘ä»¬å°±å¯ä»¥å®šä¹‰æˆ‘ä»¬çš„ `Seq2SeqTrainingArguments` ã€‚ä¸ `Trainer` ä¸€æ ·ï¼Œæˆ‘ä»¬ä½¿ç”¨ `TrainingArguments` çš„å­ç±»ï¼Œå…¶ä¸­åŒ…å«æ›´å¤šå¯ä»¥è®¾ç½®çš„å­—æ®µï¼š

```python
from transformers import Seq2SeqTrainingArguments

args = Seq2SeqTrainingArguments(
    f"marian-finetuned-kde4-en-to-fr",
    evaluation_strategy="no",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=64,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=3,
    predict_with_generate=True,
    fp16=True,
    push_to_hub=True,
)
```

é™¤äº†é€šå¸¸çš„è¶…å‚æ•°ï¼ˆå¦‚å­¦ä¹ ç‡ã€è®­ç»ƒè½®æ•°ã€æ‰¹æ¬¡å¤§å°å’Œä¸€äº›æƒé‡è¡°å‡ï¼‰ä¹‹å¤–ï¼Œè¿™é‡Œä¸æˆ‘ä»¬åœ¨å‰é¢ç« èŠ‚çœ‹åˆ°çš„æœ‰ä¸€äº›ä¸åŒï¼š

- æˆ‘ä»¬æ²¡æœ‰è®¾ç½®ä»»ä½•å®šæœŸè¯„ä¼°ï¼Œå› ä¸ºè¯„ä¼°éœ€è¦è€—è´¹ä¸€å®šçš„æ—¶é—´ï¼›æˆ‘ä»¬åªä¼šåœ¨è®­ç»ƒå¼€å§‹ä¹‹å‰å’Œç»“æŸä¹‹åè¯„ä¼°æˆ‘ä»¬çš„æ¨¡å‹ä¸€æ¬¡ã€‚
- æˆ‘ä»¬è®¾ç½® `fp16=True` ï¼Œè¿™å¯ä»¥åŠ å¿«æ”¯æŒ fp16 çš„ GPU ä¸Šçš„è®­ç»ƒé€Ÿåº¦ã€‚
- å’Œä¸Šé¢æˆ‘ä»¬è®¨è®ºçš„é‚£æ ·ï¼Œæˆ‘ä»¬è®¾ç½® `predict_with_generate=True` ã€‚
- æˆ‘ä»¬ç”¨ `push_to_hub=True` åœ¨æ¯ä¸ª epoch ç»“æŸæ—¶å°†æ¨¡å‹ä¸Šä¼ åˆ° Hubã€‚

è¯·æ³¨æ„ï¼Œä½ å¯ä»¥ä½¿ç”¨ `hub_model_id` å‚æ•°æŒ‡å®šè¦æ¨é€åˆ°çš„å­˜å‚¨åº“çš„åç§°ï¼ˆå½“ä½ æƒ³æŠŠæ¨¡å‹æ¨é€åˆ°æŒ‡å®šçš„ç»„ç»‡çš„æ—¶å€™ï¼Œä½ ä¹Ÿå¿…é¡»ä½¿ç”¨æ­¤å‚æ•°ï¼‰ã€‚ä¾‹å¦‚ï¼Œå½“æˆ‘ä»¬å°†æ¨¡å‹æ¨é€åˆ° [`huggingface-course` ç»„ç»‡](https://huggingface.co/huggingface-course)(https://huggingface.co/huggingface-course)(https://huggingface.co/huggingface-course) æ—¶ï¼Œæˆ‘ä»¬æ·»åŠ äº† `hub_model_id="huggingface-course/marian-finetuned-kde4-en- to-fr"` åˆ° `Seq2SeqTrainingArguments` ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œä½¿ç”¨çš„å­˜å‚¨åº“å°†åœ¨ä½ çš„è´¦æˆ·ä¸­ï¼Œå¹¶ä»¥ä½ è®¾ç½®çš„è¾“å‡ºç›®å½•å‘½åï¼Œå› æ­¤è¿™é‡Œå°†æ˜¯ `"sgugger/marian-finetuned-kde4-en-to-fr"` ã€‚

<div custom-style="Tip-green">


ğŸ’¡å¦‚æœä½ ä½¿ç”¨çš„è¾“å‡ºç›®å½•å·²ç»å­˜åœ¨ï¼Œåˆ™å®ƒéœ€è¦æ˜¯ä½ è¦æ¨é€åˆ°çš„å­˜å‚¨åº“çš„æœ¬åœ°å…‹éš†ã€‚å¦‚æœä¸æ˜¯ï¼Œä½ å°†åœ¨å®šä¹‰ä½ çš„ `Seq2SeqTrainer` åç§°æ—¶ä¼šé‡åˆ°é”™è¯¯ï¼Œå¹¶ä¸”éœ€è¦è®¾ç½®ä¸€ä¸ªæ–°åç§°ã€‚

</div>

æœ€åï¼Œæˆ‘ä»¬å°†æ‰€æœ‰å†…å®¹ä¼ é€’ç»™ `Seq2SeqTrainer` ï¼š

```python
from transformers import Seq2SeqTrainer

trainer = Seq2SeqTrainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)
```

åœ¨å¼€å§‹è®­ç»ƒä¹‹å‰ï¼Œæˆ‘ä»¬å…ˆçœ‹ä¸€ä¸‹æˆ‘ä»¬çš„æ¨¡å‹å¾—åˆ°çš„åˆ†æ•°ï¼Œä»¥ç¡®ä¿æˆ‘ä»¬çš„å¾®è°ƒå¹¶æœªä½¿æƒ…å†µå˜å¾—æ›´ç³Ÿã€‚è¿™ä¸ªå‘½ä»¤éœ€è¦ä¸€äº›æ—¶é—´ï¼Œæ‰€ä»¥ä½ å¯ä»¥åœ¨æ‰§è¡ŒæœŸé—´å»å–æ¯å’–å•¡ï¼š

```python
trainer.evaluate(max_length=max_length)
```

```python out
{'eval_loss': 1.6964408159255981,
 'eval_bleu': 39.26865061007616,
 'eval_runtime': 965.8884,
 'eval_samples_per_second': 21.76,
 'eval_steps_per_second': 0.341}
```

BLEU å¾—åˆ†ä¸º 39 å¹¶ä¸ç®—å¤ªå·®ï¼Œè¿™åæ˜ äº†æˆ‘ä»¬çš„æ¨¡å‹å·²ç»æ“…é•¿å°†è‹±è¯­å¥å­ç¿»è¯‘æˆæ³•è¯­å¥å­ã€‚

æ¥ä¸‹æ¥æ˜¯è®­ç»ƒï¼Œè¿™ä¹Ÿéœ€è¦ä¸€äº›æ—¶é—´ï¼š

```python
trainer.train()
```

è¯·æ³¨æ„ï¼Œå½“è®­ç»ƒå‘ç”Ÿæ—¶ï¼Œæ¯æ¬¡ä¿å­˜æ¨¡å‹æ—¶ï¼ˆè¿™é‡Œæ˜¯æ¯ä¸ª epochï¼‰ï¼Œå®ƒéƒ½ä¼šåœ¨åå°ä¸Šä¼ åˆ° Hubã€‚è¿™æ ·ï¼Œå¦‚æœ‰å¿…è¦ï¼Œä½ å°†èƒ½å¤Ÿåœ¨å¦ä¸€å°æœºå™¨ä¸Šç»§ç»­ä½ çš„è®­ç»ƒã€‚

è®­ç»ƒå®Œæˆåï¼Œæˆ‘ä»¬å†æ¬¡è¯„ä¼°æˆ‘ä»¬çš„æ¨¡å‹â€”â€”å¸Œæœ›æˆ‘ä»¬ä¼šçœ‹åˆ° BLEU åˆ†æ•°æœ‰æ‰€æé«˜ï¼

```py
trainer.evaluate(max_length=max_length)
```

```python out
{'eval_loss': 0.8558505773544312,
 'eval_bleu': 52.94161337775576,
 'eval_runtime': 714.2576,
 'eval_samples_per_second': 29.426,
 'eval_steps_per_second': 0.461,
 'epoch': 3.0}
```

è¿™æ˜¯è¿‘ 14 ç‚¹çš„æ”¹è¿›ï¼Œè¿™å¾ˆæ£’ã€‚

æœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨ `push_to_hub()` æ–¹æ³•æ¥ç¡®ä¿æˆ‘ä»¬ä¸Šä¼ æ¨¡å‹çš„æœ€æ–°ç‰ˆæœ¬ã€‚ `Trainer` è¿˜åˆ›å»ºäº†ä¸€å¼ åŒ…å«æ‰€æœ‰è¯„ä¼°ç»“æœçš„æ¨¡å‹å¡å¹¶ä¸Šä¼ ã€‚æ­¤æ¨¡å‹å¡åŒ…å«å¯å¸®åŠ©æ¨¡å‹ä¸­å¿ƒä¸ºæ¨ç†æ¼”ç¤ºé€‰æ‹©å°éƒ¨ä»¶çš„å…ƒæ•°æ®ã€‚é€šå¸¸ä¸éœ€è¦åšé¢å¤–çš„æ›´æ”¹ï¼Œå› ä¸ºå®ƒå¯ä»¥ä»æ¨¡å‹ç±»ä¸­æ¨æ–­å‡ºæ­£ç¡®çš„å°éƒ¨ä»¶ï¼Œä½†åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç›¸åŒçš„æ¨¡å‹ç±»å¯ä»¥ç”¨äºæ‰€æœ‰ç±»å‹çš„åºåˆ—åˆ°åºåˆ—é—®é¢˜ï¼Œæ‰€ä»¥æˆ‘ä»¬æŒ‡å®šå®ƒæ˜¯ä¸€ä¸ªç¿»è¯‘æ¨¡å‹ï¼š

```py
trainer.push_to_hub(tags="translation", commit_message="Training complete")
```

å¦‚æœä½ æƒ³æ£€æŸ¥å‘½ä»¤æ‰§è¡Œçš„ç»“æœï¼Œæ­¤å‘½ä»¤å°†è¿”å›å®ƒåˆšåˆšæ‰§è¡Œçš„æäº¤çš„ URLï¼Œå¯ä»¥æ‰“å¼€ url è¿›è¡Œæ£€æŸ¥ï¼š

```python out
'https://huggingface.co/sgugger/marian-finetuned-kde4-en-to-fr/commit/3601d621e3baae2bc63d3311452535f8f58f6ef3'
```

åœ¨æ­¤é˜¶æ®µï¼Œä½ å¯ä»¥åœ¨ Model Hub ä¸Šä½¿ç”¨æ¨ç†å°éƒ¨ä»¶æ¥æµ‹è¯•ä½ çš„æ¨¡å‹ï¼Œå¹¶ä¸ä½ çš„æœ‹å‹åˆ†äº«ã€‚ä½ å·²ç»æˆåŠŸåœ°åœ¨ç¿»è¯‘ä»»åŠ¡ä¸Šè¿›è¡Œäº†æ¨¡å‹çš„å¾®è°ƒï¼Œæ­å–œä½ ï¼

å¦‚æœä½ æƒ³æ›´æ·±å…¥åœ°äº†è§£è®­ç»ƒå¾ªç¯ï¼Œæˆ‘ä»¬ç°åœ¨å°†å‘ä½ å±•ç¤ºå¦‚ä½•ä½¿ç”¨ Accelerate åšåŒæ ·çš„äº‹æƒ…ã€‚

{/if}

{#if fw === 'pt'}

## è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ 

æˆ‘ä»¬ç°åœ¨æ¥çœ‹ä¸€ä¸‹å®Œæ•´çš„è®­ç»ƒå¾ªç¯ï¼Œè¿™æ ·ä½ å°±å¯ä»¥è½»æ¾å®šåˆ¶ä½ éœ€è¦çš„éƒ¨åˆ†ã€‚å®ƒå°†ä¸æˆ‘ä»¬åœ¨ç¬¬ 2 èŠ‚å’Œç¬¬ 3 èŠ‚ä¸­åšçš„éå¸¸ç›¸ä¼¼ã€‚

### å‡†å¤‡è®­ç»ƒæ‰€éœ€çš„ä¸€åˆ‡ 

ä½ å·²ç»å¤šæ¬¡çœ‹åˆ°æ‰€æœ‰è¿™äº›ï¼Œå› æ­¤è¿™ä¸€å—ä¼šç®€ç•¥è¿›è¡Œã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å°†æ•°æ®é›†è®¾ç½®ä¸º"torch"æ ¼å¼ï¼Œä»¥ä¾¿æˆ‘ä»¬å¾—åˆ° PyTorch å¼ é‡ï¼Œç„¶åä»æˆ‘ä»¬çš„æ•°æ®é›†ä¸­æ„å»º `DataLoader` ï¼š

```py
from torch.utils.data import DataLoader

tokenized_datasets.set_format("torch")
train_dataloader = DataLoader(
    tokenized_datasets["train"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], collate_fn=data_collator, batch_size=8
)
```

æ¥ä¸‹æ¥æˆ‘ä»¬é‡æ–°å®ä¾‹åŒ–æˆ‘ä»¬çš„æ¨¡å‹ï¼Œä»¥ç¡®ä¿æˆ‘ä»¬ä¸ä¼šç»§ç»­ä¸Šä¸€èŠ‚çš„å¾®è°ƒï¼Œè€Œæ˜¯å†æ¬¡ä»é¢„è®­ç»ƒæ¨¡å‹å¼€å§‹é‡æ–°è®­ç»ƒï¼š

```py
model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)
```

ç„¶åæˆ‘ä»¬éœ€è¦ä¸€ä¸ªä¼˜åŒ–å™¨ï¼š

```py
from transformers import AdamW

optimizer = AdamW(model.parameters(), lr=2e-5)
```

ä¸€æ—¦æˆ‘ä»¬æ‹¥æœ‰æ‰€æœ‰è¿™äº›å¯¹è±¡ï¼Œæˆ‘ä»¬å°±å¯ä»¥å°†å®ƒä»¬å‘é€åˆ° `accelerator.prepare()` æ–¹æ³•ã€‚è¯·è®°ä½ï¼Œå¦‚æœä½ æƒ³åœ¨ Colab ç¬”è®°æœ¬è®­ç»ƒä¸­ä½¿ç”¨ TPUï¼Œåˆ™éœ€è¦å°†æ‰€æœ‰è¿™äº›ä»£ç ç§»åŠ¨åˆ°è®­ç»ƒå‡½æ•°ä¸­ï¼Œè¿™æ ·ä¸ä¼šæ‰§è¡Œå®ä¾‹åŒ– `Accelerator` çš„ä»»ä½•å•å…ƒã€‚

```py
from accelerate import Accelerator

accelerator = Accelerator()
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)
```

ç°åœ¨æˆ‘ä»¬å·²ç»å°†æˆ‘ä»¬çš„ `train_dataloader` å‘é€åˆ° `accelerator.prepare()` ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å®ƒçš„é•¿åº¦æ¥è®¡ç®—è®­ç»ƒæ­¥éª¤çš„æ•°é‡ã€‚è¯·è®°ä½ï¼Œæˆ‘ä»¬åº”è¯¥å§‹ç»ˆåœ¨å‡†å¤‡å¥½æ•°æ®åŠ è½½å™¨åæ‰§è¡Œæ­¤æ“ä½œï¼Œå› ä¸ºè¯¥æ–¹æ³•ä¼šæ”¹å˜ `DataLoader` çš„é•¿åº¦ã€‚æˆ‘ä»¬ä½¿ç”¨ä»å­¦ä¹ ç‡è¡°å‡åˆ° 0 çš„ç»å…¸çº¿æ€§å­¦ä¹ ç‡è°ƒåº¦ï¼š

```py
from transformers import get_scheduler

num_train_epochs = 3
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
```

æœ€åï¼Œè¦å°†æˆ‘ä»¬çš„æ¨¡å‹æ¨é€åˆ° Hubï¼Œæˆ‘ä»¬éœ€è¦åœ¨ä¸€ä¸ªå·¥ä½œæ–‡ä»¶å¤¹ä¸­åˆ›å»ºä¸€ä¸ª `Repository` å¯¹è±¡ã€‚å¦‚æœä½ å°šæœªç™»å½•ï¼Œè¯·å…ˆç™»å½• Hugging Faceã€‚æˆ‘ä»¬å°†ä»æˆ‘ä»¬æƒ³è¦ä¸ºæ¨¡å‹æä¾›çš„æ¨¡å‹ ID ä¸­ç¡®å®šå­˜å‚¨åº“åç§°ï¼ˆä½ å¯ä»¥ç”¨ä½ è‡ªå·±çš„é€‰æ‹©æ›¿æ¢ `repo_name` ï¼Œåªéœ€è¦åŒ…å«ä½ çš„ç”¨æˆ·åï¼Œç”¨å‡½æ•° `get_full_repo_name()` å‡½æ•°å¯ä»¥æŸ¥çœ‹å½“å‰çš„ç”¨æˆ·åï¼‰ï¼š

```py
from huggingface_hub import Repository, get_full_repo_name

model_name = "marian-finetuned-kde4-en-to-fr-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name
```

```python out
'sgugger/marian-finetuned-kde4-en-to-fr-accelerate'
```

ç„¶åæˆ‘ä»¬å¯ä»¥åœ¨æœ¬åœ°æ–‡ä»¶å¤¹ä¸­å…‹éš†è¯¥å­˜å‚¨åº“ã€‚å¦‚æœå®ƒå·²ç»å­˜åœ¨ï¼Œè¿™ä¸ªæœ¬åœ°æ–‡ä»¶å¤¹åº”è¯¥æ˜¯æˆ‘ä»¬æ­£åœ¨ä½¿ç”¨çš„å­˜å‚¨åº“çš„å…‹éš†ï¼š

```py
output_dir = "marian-finetuned-kde4-en-to-fr-accelerate"
repo = Repository(output_dir, clone_from=repo_name)
```

ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡è°ƒç”¨ `repo.push_to_hub()` æ–¹æ³•ä¸Šä¼ æˆ‘ä»¬åœ¨ `output_dir` ä¸­ä¿å­˜çš„æ‰€æœ‰æ–‡ä»¶ã€‚è¿™å°†å¸®åŠ©æˆ‘ä»¬åœ¨æ¯ä¸ª epoch ç»“æŸæ—¶ä¸Šä¼ ä¸­é—´æ¨¡å‹ã€‚

### è®­ç»ƒå¾ªç¯ 

æˆ‘ä»¬ç°åœ¨å‡†å¤‡ç¼–å†™å®Œæ•´çš„è®­ç»ƒå¾ªç¯ã€‚ä¸ºäº†ç®€åŒ–å…¶è¯„ä¼°éƒ¨åˆ†ï¼Œæˆ‘ä»¬å®šä¹‰äº†è¿™ä¸ª `postprocess()` å‡½æ•°ï¼Œå®ƒæ¥å—é¢„æµ‹å’Œæ ‡ç­¾ï¼Œå¹¶å°†å®ƒä»¬è½¬æ¢ä¸ºæˆ‘ä»¬çš„ `metric` å¯¹è±¡æ‰€æœŸæœ›çš„å­—ç¬¦ä¸²åˆ—è¡¨ï¼š

```py
def postprocess(predictions, labels):
    predictions = predictions.cpu().numpy()
    labels = labels.cpu().numpy()

    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)

    # æ›¿æ¢æ ‡ç­¾ä¸­çš„ -100ï¼Œå› ä¸ºæˆ‘ä»¬æ— æ³•è§£ç å®ƒä»¬ã€‚
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # ä¸€äº›ç®€å•çš„åå¤„ç†
    decoded_preds = [pred.strip() for pred in decoded_preds]
    decoded_labels = [[label.strip()] for label in decoded_labels]
    return decoded_preds, decoded_labels
```

è®­ç»ƒå¾ªç¯çœ‹èµ·æ¥å’Œæœ¬ç« ç¬¬äºŒèŠ‚ä¸ç¬¬ä¸‰ç« å¾ˆåƒï¼Œåœ¨è¯„ä¼°éƒ¨åˆ†æœ‰ä¸€äº›ä¸åŒ â€”â€” æ‰€ä»¥è®©æˆ‘ä»¬ä¸»è¦å…³æ³¨ä¸€ä¸‹è¿™ä¸€ç‚¹ï¼

é¦–å…ˆè¦æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬ä½¿ç”¨ `generate()` æ–¹æ³•æ¥è®¡ç®—é¢„æµ‹ï¼Œä½†è¿™æ˜¯æˆ‘ä»¬åŸºç¡€æ¨¡å‹ä¸Šçš„ä¸€ä¸ªæ–¹æ³•ï¼Œè€Œä¸æ˜¯Accelerate åœ¨ `prepare()` æ–¹æ³•ä¸­åˆ›å»ºçš„å°è£…æ¨¡å‹ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬é¦–å…ˆ `unwrap_model` ï¼Œç„¶åè°ƒç”¨æ­¤æ–¹æ³•ã€‚

ç¬¬äºŒä¸ªè¦æ³¨æ„çš„æ˜¯ï¼Œå°±åƒ [tokenåˆ†ç±»](https://chat.openai.com/course/chapter7/2)(https://chat.openai.com/course/chapter7/2) ä¸€æ ·ï¼Œä¸¤ä¸ªè¿‡ç¨‹å¯èƒ½ä»¥ä¸åŒçš„å½¢çŠ¶å¯¹è¾“å…¥å’Œæ ‡ç­¾è¿›è¡Œäº†å¡«å……ï¼Œæ‰€ä»¥æˆ‘ä»¬ä½¿ç”¨ `accelerator.pad_across_processes()` æ¥åœ¨è°ƒç”¨ `gather()` æ–¹æ³•ä¹‹å‰ä½¿é¢„æµ‹å’Œæ ‡ç­¾å…·æœ‰ç›¸åŒçš„å½¢çŠ¶ã€‚å¦‚æœæˆ‘ä»¬ä¸è¿™ä¹ˆåšï¼Œè¯„ä¼°å°†å‡ºé”™æˆ–æ°¸è¿œæŒ‚èµ·ã€‚

```py
from tqdm.auto import tqdm
import torch

progress_bar = tqdm(range(num_training_steps))

for epoch in range(num_train_epochs):
    # è®­ç»ƒ
    model.train()
    for batch in train_dataloader:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

    # è¯„ä¼°
    model.eval()
    for batch in tqdm(eval_dataloader):
        with torch.no_grad():
            generated_tokens = accelerator.unwrap_model(model).generate(
                batch["input_ids"],
                attention_mask=batch["attention_mask"],
                max_length=128,
            )
        labels = batch["labels"]

        # éœ€è¦å¡«å……é¢„æµ‹å’Œæ ‡ç­¾æ‰èƒ½è°ƒç”¨gather()
        generated_tokens = accelerator.pad_across_processes(
            generated_tokens, dim=1, pad_index=tokenizer.pad_token_id
        )
        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)

        predictions_gathered = accelerator.gather(generated_tokens)
        labels_gathered = accelerator.gather(labels)

        decoded_preds, decoded_labels = postprocess(predictions_gathered, labels_gathered)
        metric.add_batch(predictions=decoded_preds, references=decoded_labels)

    results = metric.compute()
    print(f"epoch {epoch}, BLEU score: {results['score']:.2f}")

    # ä¿å­˜å’Œä¸Šä¼ 
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
    if accelerator.is_main_process:
        tokenizer.save_pretrained(output_dir)
        repo.push_to_hub(
            commit_message=f"Training in progress epoch {epoch}", blocking=False
        )
```

```python out
epoch 0, BLEU score: 53.47
epoch 1, BLEU score: 54.24
epoch 2, BLEU score: 54.44
```

ä¸€æ—¦å®Œæˆï¼Œä½ åº”è¯¥æœ‰ä¸€ä¸ªæ¨¡å‹ï¼Œå…¶ç»“æœä¸ `Seq2SeqTrainer` è®­ç»ƒçš„æ¨¡å‹éå¸¸ç›¸ä¼¼ã€‚ä½ å¯ä»¥æŸ¥çœ‹æˆ‘ä»¬ä½¿ç”¨æ­¤ä»£ç è®­ç»ƒçš„æ¨¡å‹ [*huggingface-course/marian-finetuned-kde4-en-to-fr-accelerate*](https://huggingface.co/huggingface-course/marian-finetuned-kde4-en-to-fr-accelerate)(https://huggingface.co/huggingface-course/marian-finetuned-kde4-en-to-fr-accelerate) ã€‚å¦‚æœä½ æƒ³æµ‹è¯•å¯¹è®­ç»ƒå¾ªç¯çš„ä»»ä½•è°ƒæ•´ï¼Œä½ å¯ä»¥ç›´æ¥é€šè¿‡ç¼–è¾‘ä¸Šé¢æ˜¾ç¤ºçš„ä»£ç æ¥å®ç°ï¼

{/if}

## ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹ 

æˆ‘ä»¬å·²ç»å‘ä½ å±•ç¤ºäº†å¦‚ä½•åœ¨æ¨¡å‹ Hub ä¸Šä½¿ç”¨æˆ‘ä»¬å¾®è°ƒçš„æ¨¡å‹ã€‚è¦åœ¨æœ¬åœ°çš„ `pipeline` ä¸­ä½¿ç”¨å®ƒï¼Œæˆ‘ä»¬åªéœ€è¦æŒ‡å®šæ­£ç¡®çš„æ¨¡å‹æ ‡è¯†ç¬¦ï¼š

```py
from transformers import pipeline

# å°†å…¶æ›¿æ¢æˆä½ è‡ªå·±çš„checkpoint
model_checkpoint = "huggingface-course/marian-finetuned-kde4-en-to-fr"
translator = pipeline("translation", model=model_checkpoint)
translator("Default to expanded threads")
```

```python out
[{'translation_text': 'Par dÃ©faut, dÃ©velopper les fils de discussion'}]
```

å¦‚é¢„æœŸï¼Œæˆ‘ä»¬çš„é¢„è®­ç»ƒæ¨¡å‹é€‚åº”äº†æˆ‘ä»¬å¾®è°ƒå®ƒçš„è¯­æ–™åº“ï¼Œè€Œä¸æ˜¯ä¿ç•™è‹±è¯­å•è¯"threads"ï¼Œè€Œæ˜¯å°†å®ƒç¿»è¯‘æˆæ³•è¯­çš„å®˜æ–¹ç‰ˆæœ¬ã€‚å¯¹äº"plugin"ä¹Ÿæ˜¯å¦‚æ­¤ï¼š

```py
translator(
    "Unable to import %1 using the OFX importer plugin. This file is not the correct format."
)
```

```python out
[{'translation_text': "Impossible d'importer %1 en utilisant le module externe d'importation OFX. Ce fichier n'est pas le bon format."}]
```

å¦ä¸€ä¸ªé¢†åŸŸé€‚åº”çš„å¥½ä¾‹å­ï¼

<div custom-style="Tip-green">


âœï¸ **è½®åˆ°ä½ äº†ï¼** æ¨¡å‹å¯¹ä½ ä¹‹å‰æ‰¾åˆ°çš„åŒ…å«å•è¯"email"çš„æ ·æœ¬è¿”å›ä»€ä¹ˆç»“æœï¼Ÿ

</div>
