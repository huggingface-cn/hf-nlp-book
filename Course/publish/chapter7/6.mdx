

# ä»å¤´å¼€å§‹è®­ç»ƒå› æœè¯­è¨€æ¨¡å‹ 

{#if fw === 'pt'}



{:else}



{/if}

åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬ä¸»è¦ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¹¶é€šè¿‡é‡ç”¨é¢„è®­ç»ƒçš„æƒé‡æ¥é’ˆå¯¹æ–°ç”¨ä¾‹å¯¹å®ƒä»¬è¿›è¡Œå¾®è°ƒï¼Œä»¥é€‚åº”æ–°çš„åº”ç”¨åœºæ™¯ã€‚æ­£å¦‚æˆ‘ä»¬åœ¨ [ç¬¬ä¸€ç« ](/course/chapter1) , è¿™é€šå¸¸ç§°ä¸º*è¿ç§»å­¦ä¹ *ï¼Œå®ƒæ˜¯å°† Transformer æ¨¡å‹åº”ç”¨åˆ°å¤§éƒ¨åˆ†å®é™…åº”ç”¨ä¸­ï¼ˆæœ‰æ ‡è®°çš„æ•°æ®ç¨€ç–çš„æƒ…å†µï¼‰çš„ä¸€ä¸ªéå¸¸æˆåŠŸçš„ç­–ç•¥ã€‚åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†é‡‡ç”¨ä¸åŒçš„æ–¹æ³•å¹¶ä»å¤´å¼€å§‹è®­ç»ƒä¸€ä¸ªå…¨æ–°çš„æ¨¡å‹ã€‚å¦‚æœæ‚¨æœ‰å¤§é‡æ•°æ®è€Œä¸”è¿™äº›æ•°æ®ä¸å¯ç”¨æ¨¡å‹çš„é¢„è®­ç»ƒæ•°æ®å·®å¼‚å¾ˆå¤§ï¼Œé‚£ä¹ˆè¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œé¢„è®­ç»ƒä¸€ä¸ªè¯­è¨€æ¨¡å‹éœ€è¦æ¯”åªå¾®è°ƒä¸€ä¸ªç°æœ‰æ¨¡å‹æ›´å¤šçš„è®¡ç®—èµ„æºã€‚è®­ç»ƒæ–°æ¨¡å‹çš„ä¾‹å­åŒ…æ‹¬æ•°æ®é›†ç”±éŸ³ä¹ç¬¦å·ã€DNA ç­‰åˆ†å­åºåˆ—æˆ–ç¼–ç¨‹è¯­è¨€ç»„æˆã€‚åè€…æœ€è¿‘å—åˆ°å…³æ³¨ï¼Œè¿™è¦å½’åŠŸäº TabNine å’Œ GitHub çš„ Copilot ç­‰å·¥å…·ï¼Œå®ƒä»¬ç”± OpenAI çš„ Codex æ¨¡å‹æä¾›æ”¯æŒï¼Œå¯ä»¥ç”Ÿæˆé•¿ä»£ç åºåˆ—ã€‚è¿™ç§æ–‡æœ¬ç”Ÿæˆä»»åŠ¡æœ€å¥½ä½¿ç”¨è‡ªå›å½’æˆ–å› æœè¯­è¨€æ¨¡å‹ï¼ˆä¾‹å¦‚ GPT-2ï¼‰æ¥è§£å†³ã€‚

åœ¨è¿™ä¸€èŠ‚ï¼Œæˆ‘ä»¬å°†æ„å»ºä¸€ä¸ªç¼©å°ç‰ˆçš„ä»£ç ç”Ÿæˆæ¨¡å‹ï¼šæˆ‘ä»¬å°†åªå…³æ³¨ä¸€è¡Œä»£ç çš„è¡¥å…¨ï¼Œè€Œä¸æ˜¯å®Œæ•´çš„å‡½æ•°æˆ–ç±»ï¼Œä½¿ç”¨çš„æ˜¯ Python ä»£ç çš„ä¸€ä¸ªå­é›†ã€‚å½“ä½ ä½¿ç”¨ Python å¤„ç†æ•°æ®æ—¶ï¼Œä½ ä¼šé¢‘ç¹åœ°æ¥è§¦åˆ° Python çš„æ•°æ®ç§‘å­¦å †æ ˆï¼ŒåŒ…æ‹¬ `matplotlib` ï¼Œ `seaborn` ï¼Œ `pandas` ï¼Œå’Œ `scikit-learn` è¿™äº›åº“ã€‚å½“ä½¿ç”¨è¿™äº›æ¡†æ¶æ—¶ï¼Œç»å¸¸éœ€è¦æŸ¥æ‰¾ç‰¹å®šçš„å‘½ä»¤ï¼Œå¦‚æœæˆ‘ä»¬èƒ½ç”¨æ¨¡å‹æ¥å¸®åŠ©æˆ‘ä»¬å®Œæˆè¿™äº›è°ƒç”¨å°±å¤ªå¥½äº†ã€‚




åœ¨ [ç¬¬å…­ç« ](/course/chapter6) ä¸­ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªé«˜æ•ˆçš„è¯å…ƒåˆ†æå™¨æ¥å¤„ç† Python æºä»£ç ï¼Œä½†æˆ‘ä»¬è¿˜éœ€è¦ä¸€ä¸ªå¤§è§„æ¨¡çš„æ•°æ®é›†æ¥é¢„è®­ç»ƒæ¨¡å‹ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†æŠŠæˆ‘ä»¬çš„è¯å…ƒåˆ†æå™¨åº”ç”¨åˆ°ä¸€ä¸ªæ¥è‡ª GitHub ä»“åº“çš„ Python ä»£ç è¯­æ–™åº“ä¸Šã€‚ç„¶åï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ Trainer API å’Œ ğŸ¤— Accelerate æ¥è®­ç»ƒæ¨¡å‹ã€‚è®©æˆ‘ä»¬å¼€å§‹å§ï¼



è¿™é‡Œå±•ç¤ºçš„æ˜¯ä¸€ä¸ªå·²ç»è®­ç»ƒå¹¶ä¸Šä¼ åˆ° Hub çš„æ¨¡å‹ï¼Œå®ƒä½¿ç”¨çš„æ˜¯æœ¬èŠ‚ä¸­æ˜¾ç¤ºçš„ä»£ç ã€‚ä½ å¯ä»¥åœ¨ [è¿™é‡Œ](https://huggingface.co/huggingface-course/codeparrot-ds?text=plt.imshow%28) æ‰¾åˆ°å®ƒã€‚æ³¨æ„ï¼Œç”±äºæ–‡æœ¬ç”Ÿæˆè¿‡ç¨‹ä¸­æœ‰ä¸€äº›éšæœºæ€§ï¼Œä½ å¯èƒ½ä¼šå¾—åˆ°ç¨å¾®ä¸åŒçš„ç»“æœã€‚

## æ”¶é›†æ•°æ® 

Python ä»£ç åœ¨è¯¸å¦‚ GitHub è¿™æ ·çš„ä»£ç ä»“åº“ä¸­ä¸°å¯Œå¯å¾—ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡çˆ¬å–æ¯ä¸€ä¸ª Python ä»“åº“æ¥åˆ›å»ºæ•°æ®é›†ã€‚è¿™æ˜¯åœ¨ [Transformersæ•™ç§‘ä¹¦](https://learning.oreilly.com/library/view/natural-language-processing/9781098136789/) ä¸­é¢„è®­ç»ƒä¸€ä¸ªå¤§å‹ GPT-2 æ¨¡å‹çš„æ–¹æ³•ã€‚ä½¿ç”¨ä¸€ä¸ªå¤§çº¦ 180GB çš„ GitHub è½¬å‚¨ï¼ŒåŒ…å«å¤§çº¦ 2000 ä¸‡ä¸ª Python æ–‡ä»¶çš„ `codeparrot` ï¼Œä½œè€…ä»¬æ„å»ºäº†ä¸€ä¸ªæ•°æ®é›†ï¼Œç„¶ååœ¨ [Hugging Face Hub](https://huggingface.co/datasets/transformersbook/codeparrot) ä¸Šåˆ†äº«ã€‚

ç„¶è€Œï¼Œå¯¹å®Œæ•´è¯­æ–™åº“çš„è®­ç»ƒæ—¢è€—æ—¶åˆè´¹åŠ›ï¼Œæˆ‘ä»¬åªéœ€è¦ä¸ Python æ•°æ®ç§‘å­¦å †æ ˆç›¸å…³çš„æ•°æ®é›†å­é›†ã€‚æ‰€ä»¥ï¼Œè®©æˆ‘ä»¬å¼€å§‹è¿‡æ»¤ `codeparrot` åŒ…å«æ­¤å †æ ˆä¸­ä»»ä½•åº“çš„æ‰€æœ‰æ–‡ä»¶çš„æ•°æ®é›†ã€‚ç”±äºæ•°æ®é›†çš„å¤ªå¤§ï¼Œæˆ‘ä»¬å¸Œæœ›é¿å…ä¸‹è½½å®ƒï¼›å› æ­¤åï¼Œæˆ‘ä»¬å°†ä½¿ç”¨æµåŠŸèƒ½æ¥åŠ¨æ€è¿‡æ»¤å®ƒã€‚ä¸ºäº†ä½¿ç”¨å‰é¢æåˆ°çš„åº“è¿‡æ»¤ä»£ç ç¤ºä¾‹ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä»¥ä¸‹å‡½æ•°ï¼š
ç„¶è€Œï¼Œè®­ç»ƒæ•´ä¸ªè¯­æ–™åº“è€—æ—¶ä¸”éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºï¼Œæˆ‘ä»¬åªéœ€è¦æ‰¾åˆ° Python æ•°æ®ç§‘å­¦å †æ ˆç›¸å…³çš„æ•°æ®é›†çš„å­é›†ã€‚æ‰€ä»¥ï¼Œè®©æˆ‘ä»¬å…ˆä»è¿‡æ»¤ `codeparrot` æ•°æ®é›†ä¸­åŒ…å« Python æ•°æ®ç§‘å­¦å †æ ˆç›¸å…³åº“çš„æ‰€æœ‰æ–‡ä»¶å¼€å§‹ã€‚ç”±äºæ•°æ®é›†çš„å¤ªå¤§ï¼Œæˆ‘ä»¬å¸Œæœ›é¿å…ç›´æ¥ä¸‹è½½å®ƒï¼›è€Œæ˜¯ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨æµå¼ç‰¹æ€§æ¥å®æ—¶è¿‡æ»¤ã€‚ä¸ºäº†ä½¿ç”¨å‰é¢æåˆ°çš„åº“è¿‡æ»¤ä»£ç ç¤ºä¾‹ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä»¥ä¸‹å‡½æ•°ï¼š

```py
def any_keyword_in_string(string, keywords):
    for keyword in keywords:
        if keyword in string:
            return True
    return False
```

è®©æˆ‘ä»¬ç”¨ä¸¤ä¸ªä¾‹å­æ¥æµ‹è¯•ä¸€ä¸‹ï¼š

```py
filters = ["pandas", "sklearn", "matplotlib", "seaborn"]
example_1 = "import numpy as np"
example_2 = "import pandas as pd"

print(
    any_keyword_in_string(example_1, filters), any_keyword_in_string(example_2, filters)
)
```

```python out
False True
```

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨å®ƒæ¥åˆ›å»ºä¸€ä¸ªå‡½æ•°æ¥æµå¼ä¼ è¾“æ•°æ®é›†å¹¶è¿‡æ»¤æˆ‘ä»¬æƒ³è¦çš„å…ƒç´ ï¼š

```py
from collections import defaultdict
from tqdm import tqdm
from datasets import Dataset

def filter_streaming_dataset(dataset, filters):
    filtered_dict = defaultdict(list)
    total = 0
    for sample in tqdm(iter(dataset)):
        total += 1
        if any_keyword_in_string(sample["content"], filters):
            for k, v in sample.items():
                filtered_dict[k].append(v)
    print(f"{len(filtered_dict['content'])/total:.2%} of data after filtering.")
    return Dataset.from_dict(filtered_dict)
```

ç„¶åæˆ‘ä»¬å¯ä»¥ç›´æ¥å°†æ­¤å‡½æ•°åº”ç”¨äºæµæ•°æ®é›†ï¼š

```py
# æ‰§è¡Œè¿™ä¸ªä»£ç å—éœ€è¦éå¸¸é•¿çš„æ—¶é—´ï¼Œå› æ­¤ä½ å¯ä»¥è·³è¿‡å®ƒï¼Œç»§ç»­æ‰§è¡Œä¸‹ä¸€ä¸ªï¼
from datasets import load_dataset

split = "train"  # "valid"
filters = ["pandas", "sklearn", "matplotlib", "seaborn"]

data = load_dataset(f"transformersbook/codeparrot-{split}", split=split, streaming=True)
filtered_data = filter_streaming_dataset(data, filters)
```

```python out
3.26% of data after filtering.
```
è¿™å°†æˆ‘ä»¬çš„æ•°æ®é›†å‹ç¼©åˆ°äº†åŸå§‹æ•°æ®é›†çš„å¤§çº¦ 3%ï¼Œä½†è¿™ä»ç„¶æ˜¯ç›¸å½“å¯è§‚çš„å¤§å°â€”â€”æœ€ç»ˆçš„æ•°æ®é›†æ˜¯ 6GBï¼Œç”± 600,000 ä¸ª Python è„šæœ¬ç»„æˆï¼

è¿‡æ»¤å®Œæ•´çš„æ•°æ®é›†å¯èƒ½éœ€è¦ 2-3 å°æ—¶ï¼Œè¿™å–å†³äºä½ çš„æœºå™¨å’Œå¸¦å®½ã€‚å¦‚æœä½ ä¸æƒ³äº²è‡ªç»å†è¿™ä¸ªæ¼«é•¿çš„è¿‡ç¨‹ï¼Œæˆ‘ä»¬åœ¨ Hub ä¸Šæä¾›äº†è¿‡æ»¤åçš„æ•°æ®é›†ä¾›ä½ ä¸‹è½½ï¼š

```py
from datasets import load_dataset, DatasetDict

ds_train = load_dataset("huggingface-course/codeparrot-ds-train", split="train")
ds_valid = load_dataset("huggingface-course/codeparrot-ds-valid", split="validation")

raw_datasets = DatasetDict(
    {
        "train": ds_train,  # .shuffle().select(range(50000)),
        "valid": ds_valid,  # .shuffle().select(range(500))
    }
)

raw_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],
        num_rows: 606720
    })
    valid: Dataset({
        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],
        num_rows: 3322
    })
})
```

<div custom-style="Tip-green">


é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹éœ€è¦ä¸€æ®µæ—¶é—´ã€‚æˆ‘ä»¬å»ºè®®æ‚¨é¦–å…ˆé€šè¿‡æ³¨é‡Šå–æ¶ˆä»¥ä¸Šä¸¤è¡Œï¼Œå¯¹æ•°æ®æ ·æœ¬è¿è¡Œè®­ç»ƒå¾ªç¯ï¼Œå¹¶ç¡®ä¿è®­ç»ƒæˆåŠŸå®Œæˆå¹¶å­˜å‚¨æ¨¡å‹ã€‚æ²¡æœ‰ä»€ä¹ˆæ¯”æœ€åä¸€æ­¥çš„è®­ç»ƒå¤±è´¥æ›´ä»¤äººæ²®ä¸§çš„äº†ï¼Œç‰¹åˆ«æ˜¯å› ä¸ºä½ å¿˜è®°åˆ›å»ºä¸€ä¸ªæ–‡ä»¶å¤¹æˆ–è€…å› ä¸ºä¿å­˜è·¯å¾„åœ¨è®­ç»ƒå¾ªç¯ç»“æŸæ—¶æœ‰ä¸€ä¸ªé”™å­—ï¼

</div>

è®©æˆ‘ä»¬çœ‹ä¸€ä¸ªæ¥è‡ªæ•°æ®é›†çš„ä¾‹å­ã€‚æˆ‘ä»¬å°†åªæ˜¾ç¤ºæ¯ä¸ªå­—æ®µçš„å‰ 200 ä¸ªå­—ç¬¦ï¼š

```py
for key in raw_datasets["train"][0]:
    print(f"{key.upper()}: {raw_datasets['train'][0][key][:200]}")
```

```python out
'REPO_NAME: kmike/scikit-learn'
'PATH: sklearn/utils/__init__.py'
'COPIES: 3'
'SIZE: 10094'
'''CONTENT: """
The :mod:`sklearn.utils` module includes various utilites.
"""

from collections import Sequence

import numpy as np
from scipy.sparse import issparse
import warnings

from .murmurhash import murm
LICENSE: bsd-3-clause'''
```

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œ `content` å­—æ®µåŒ…å«äº†æˆ‘ä»¬å¸Œæœ›æ¨¡å‹è®­ç»ƒçš„ä»£ç ã€‚ç°åœ¨æˆ‘ä»¬æœ‰äº†æ•°æ®é›†ï¼Œæˆ‘ä»¬éœ€è¦å‡†å¤‡æ–‡æœ¬ï¼Œä»¥ä¾¿å®ƒä»¬é€‚åˆäºé¢„è®­ç»ƒã€‚

## å‡†å¤‡æ•°æ®é›† 



é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦å°†æ•°æ®è¿›è¡Œåˆ†è¯å¤„ç†ï¼Œè¿™æ ·æ‰èƒ½è¿›è¡Œè®­ç»ƒã€‚ç”±äºæˆ‘ä»¬çš„ä¸»è¦ç›®æ ‡æ˜¯è‡ªåŠ¨è¡¥å…¨çŸ­å‡½æ•°è°ƒç”¨ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥å°†ä¸Šä¸‹æ–‡å¤§å°è®¾ç½®å¾—ç›¸å¯¹è¾ƒå°ã€‚è¿™æ ·åšçš„å¥½å¤„æ˜¯æˆ‘ä»¬å¯ä»¥æ›´å¿«åœ°è®­ç»ƒæ¨¡å‹ï¼Œè€Œä¸”éœ€è¦çš„å†…å­˜ä¹Ÿå¤§å¤§å‡å°‘ã€‚å¦‚æœä½ çš„åº”ç”¨éœ€è¦æ›´å¤šçš„ä¸Šä¸‹æ–‡ï¼ˆæ¯”å¦‚ï¼Œä½ å¸Œæœ›æ¨¡å‹æ ¹æ®åŒ…å«å‡½æ•°å®šä¹‰çš„æ–‡ä»¶ç¼–å†™å•å…ƒæµ‹è¯•ï¼‰ï¼Œé‚£ä¹ˆåº”è¯¥å¢å¤§è¯¥æ•°å­—ï¼Œä½†æ˜¯ä¹Ÿè¦è®°ä½è¿™ä¼šå¢åŠ  GPU å†…å­˜çš„å ç”¨ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬å°†ä¸Šä¸‹æ–‡å¤§å°å›ºå®šä¸º 128 ä¸ªè¯å…ƒï¼Œè€Œä¸æ˜¯åœ¨ GPT-2 æˆ– GPT-3 ä¸­ä½¿ç”¨çš„ 1,024 æˆ– 2,048 ä¸ªè¯å…ƒã€‚

å¤§å¤šæ•°æ–‡æ¡£éƒ½åŒ…å«è¶…è¿‡ 128 ä¸ªè¯å…ƒï¼Œå› æ­¤ç®€å•åœ°å°†è¾“å…¥æˆªæ–­åˆ°æœ€å¤§é•¿åº¦ä¼šåˆ é™¤æˆ‘ä»¬æ•°æ®é›†çš„å¤§éƒ¨åˆ†å†…å®¹ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ `return_overflowing_tokens` é€‰é¡¹å°†æ•´ä¸ªè¾“å…¥è¿›è¡Œåˆ†è¯å¤„ç†ï¼Œå¹¶å°†å…¶åˆ†å‰²ä¸ºå‡ ä¸ªå—ï¼Œæ­£å¦‚æˆ‘ä»¬åœ¨ [ç¬¬å…­ç« ](/course/chapter6/4) ä¸­æ‰€åšçš„é‚£æ ·ã€‚æˆ‘ä»¬è¿˜å°†ä½¿ç”¨ `return_length` é€‰é¡¹è‡ªåŠ¨è¿”å›æ¯ä¸ªåˆ›å»ºçš„å—çš„é•¿åº¦ã€‚é€šå¸¸ï¼Œæœ€åä¸€ä¸ªå—çš„å¤§å°ä¼šå°äºä¸Šä¸‹æ–‡å¤§å°ï¼Œæˆ‘ä»¬å°†å»æ‰æœ€åä¸€å—ä»¥é¿å…å¡«å……é—®é¢˜ï¼›å› ä¸ºæˆ‘ä»¬å·²ç»æœ‰è¶³å¤Ÿçš„æ•°æ®ï¼Œæ‰€ä»¥å¹¶ä¸çœŸæ­£éœ€è¦å®ƒä»¬ã€‚


![Chunking a large texts in several pieces.](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/chunking_texts.svg "Chunking a large texts in several pieces.")

è®©æˆ‘ä»¬é€šè¿‡æŸ¥çœ‹å‰ä¸¤ä¸ªç¤ºä¾‹æ¥å…·ä½“äº†è§£è¿™æ˜¯å¦‚ä½•å®ç°çš„ï¼š

```py
from transformers import AutoTokenizer

context_length = 128
tokenizer = AutoTokenizer.from_pretrained("huggingface-course/code-search-net-tokenizer")

outputs = tokenizer(
    raw_datasets["train"][:2]["content"],
    truncation=True,
    max_length=context_length,
    return_overflowing_tokens=True,
    return_length=True,
)

print(f"Input IDs length: {len(outputs['input_ids'])}")
print(f"Input chunk lengths: {(outputs['length'])}")
print(f"Chunk mapping: {outputs['overflow_to_sample_mapping']}")
```

```python out
Input IDs length: 34
Input chunk lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 117, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 41]
Chunk mapping: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
```

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œè¿™ä¸¤ä¸ªä¾‹å­æ€»å…±å¾—åˆ°äº† 34 ä¸ªæ®µã€‚æŸ¥çœ‹å—é•¿åº¦ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¸¤ä¸ªæ–‡æ¡£æœ«ç«¯çš„å—å°‘äº 128 ä¸ªè¯å…ƒï¼ˆåˆ†åˆ«ä¸º 117 å’Œ 41ï¼‰ã€‚è¿™äº›åªå æˆ‘ä»¬æ‰€æ‹¥æœ‰çš„æ€»å—æ•°çš„ä¸€å°éƒ¨åˆ†ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥æ”¾å¿ƒåœ°ä¸¢æ‰å®ƒä»¬ã€‚é€šè¿‡ `overflow_to_sample_mapping` å­—æ®µï¼Œæˆ‘ä»¬è¿˜å¯ä»¥é‡æ–°æ„å»ºå“ªäº›å—å±äºå“ªä¸ªè¾“å…¥æ ·æœ¬ã€‚

é€šè¿‡è¿™ä¸ªæ“ä½œï¼Œæˆ‘ä»¬æ­£åœ¨ä½¿ç”¨ğŸ¤— Datasets ä¸­çš„ `Dataset.map()` åŠŸèƒ½çš„ä¸€ä¸ªä¾¿æ·çš„ç‰¹æ€§ï¼Œå³å®ƒå¹¶ä¸éœ€è¦ä¸€å¯¹ä¸€çš„æ˜ å°„ï¼›æ­£å¦‚æˆ‘ä»¬åœ¨ [ç¬¬ä¸‰èŠ‚](/course/chapter7/3) ä¸­çœ‹åˆ°çš„ï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºæ¯”è¾“å…¥ batch_size æ›´å¤šæˆ–æ›´å°‘å…ƒç´ çš„ batchã€‚å½“è¿›è¡Œåƒæ•°æ®å¢å¼ºæˆ–æ•°æ®è¿‡æ»¤è¿™æ ·æ”¹å˜å…ƒç´ æ•°é‡çš„æ“ä½œæ—¶ï¼Œè¿™æ˜¯éå¸¸æœ‰ç”¨çš„ã€‚åœ¨æˆ‘ä»¬çš„æƒ…å†µä¸‹ï¼Œå½“å°†æ¯ä¸ªå…ƒç´ åˆ†è¯æˆæŒ‡å®šä¸Šä¸‹æ–‡å¤§å°çš„å—æ—¶ï¼Œæˆ‘ä»¬ä»æ¯ä¸ªæ–‡æ¡£ä¸­åˆ›å»ºäº†è®¸å¤šæ ·æœ¬ã€‚æˆ‘ä»¬éœ€è¦ç¡®ä¿åˆ é™¤ç°æœ‰çš„åˆ—ï¼Œå› ä¸ºå®ƒä»¬çš„å¤§å°æœ‰å†²çªã€‚å¦‚æœæˆ‘ä»¬æƒ³ä¿ç•™å®ƒä»¬ï¼Œæˆ‘ä»¬å¯ä»¥å¤åˆ¶å¹¶å¡«å……å®ƒä»¬ï¼Œå¹¶åœ¨ Dataset.map() è°ƒç”¨ä¸­è¿”å›å®ƒä»¬ã€‚

```py
def tokenize(element):
    outputs = tokenizer(
        element["content"],
        truncation=True,
        max_length=context_length,
        return_overflowing_tokens=True,
        return_length=True,
    )
    input_batch = []
    for length, input_ids in zip(outputs["length"], outputs["input_ids"]):
        if length == context_length:
            input_batch.append(input_ids)
    return {"input_ids": input_batch}


tokenized_datasets = raw_datasets.map(
    tokenize, batched=True, remove_columns=raw_datasets["train"].column_names
)
tokenized_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['input_ids'],
        num_rows: 16702061
    })
    valid: Dataset({
        features: ['input_ids'],
        num_rows: 93164
    })
})
```

æˆ‘ä»¬ç°åœ¨æœ‰ 1670 ä¸‡ä¸ªç¤ºä¾‹ï¼Œæ¯ä¸ªç¤ºä¾‹æœ‰ 128 ä¸ª tokens æ€»å…±ç›¸å½“äºå¤§çº¦ 21 äº¿ä¸ª tokens ä½œä¸ºå‚è€ƒï¼ŒOpenAI çš„ GPT-3 å’Œ Codex æ¨¡å‹åˆ†åˆ«åœ¨ 300 å’Œ 1000 äº¿ä¸ª tokens ä¸Šè®­ç»ƒï¼Œå…¶ä¸­ Codex æ¨¡å‹ä» GPT-3 checkpoint åˆå§‹åŒ–ã€‚æœ¬èŠ‚çš„ç›®æ ‡ä¸æ˜¯ä¸è¿™äº›èƒ½ç”Ÿæˆé•¿ä¸”è¿è´¯æ–‡æœ¬çš„æ¨¡å‹ç«äº‰ï¼Œè€Œæ˜¯åˆ›å»ºä¸€ä¸ªèƒ½ä¸ºæ•°æ®ç§‘å­¦å®¶æä¾›å¿«é€Ÿè‡ªåŠ¨ä»£ç å®ŒæˆåŠŸèƒ½çš„ç¼©å‡ç‰ˆæœ¬ã€‚

æ—¢ç„¶æˆ‘ä»¬å·²ç»å‡†å¤‡å¥½äº†æ•°æ®é›†ï¼Œé‚£å°±æ¥è®¾ç½®æ¨¡å‹å§ï¼

<div custom-style="Tip-green">


âœï¸ **è¯•ä¸€è¯•ï¼**è¿™é‡Œæˆ‘ä»¬åˆ é™¤äº†æ‰€æœ‰å°äºä¸Šä¸‹æ–‡å¤§å°çš„å—ï¼Œå¹¶ä¸ä¼šé€ æˆå¤§é—®é¢˜ï¼Œå› ä¸ºæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯å°çš„ä¸Šä¸‹æ–‡çª—å£ã€‚éšç€ä½ å¢å¤§ä¸Šä¸‹æ–‡å¤§å°ï¼ˆæˆ–è€…ä½ æ‹¥æœ‰çš„æ˜¯ä¸€ä¸ªçŸ­æ–‡æ¡£çš„è¯­æ–™åº“ï¼‰ï¼Œè¢«æŠ›å¼ƒçš„å—çš„æ¯”ä¾‹ä¹Ÿä¼šå¢åŠ ã€‚å‡†å¤‡æ•°æ®çš„æ›´æœ‰æ•ˆæ–¹æ³•æ˜¯å°†æ‰€æœ‰è¯å…ƒåŒ–åçš„æ ·æœ¬åŠ å…¥ä¸€ä¸ª batch ä¸­ï¼Œæ¯ä¸ªè¯­æ–™ä¹‹é—´æœ‰ä¸€ä¸ª `eos_token_id` è¯å…ƒï¼Œç„¶åå¯¹è¿æ¥åçš„åºåˆ—è¿›è¡Œåˆ‡å—å¤„ç†ã€‚ä½œä¸ºç»ƒä¹ ï¼Œä¿®æ”¹ `tokenize()` å‡½æ•°ä»¥åˆ©ç”¨è¿™ç§æ–¹æ³•ã€‚è¯·æ³¨æ„ï¼Œä½ éœ€è¦è®¾ç½® `truncation=False` ï¼Œå¹¶ä»è¯å…ƒåˆ†æå™¨ä¸­ç§»é™¤å…¶ä»–å‚æ•°ä»¥è·å–å®Œæ•´çš„è¯å…ƒ ID åºåˆ—ã€‚

</div>


## åˆå§‹åŒ–ä¸€ä¸ªæ–°æ¨¡å‹ 

æˆ‘ä»¬çš„ç¬¬ä¸€æ­¥æ˜¯å…¨æ–°åœ°åˆå§‹åŒ–ä¸€ä¸ª GPT-2 æ¨¡å‹ã€‚æˆ‘ä»¬å°†é€šè¿‡åŠ è½½é¢„è®­ç»ƒé…ç½®æ¥ä½¿ç”¨å°å‹ GPT-2 æ¨¡å‹ç›¸åŒçš„é…ç½®ï¼Œç¡®ä¿è¯å…ƒåˆ†æå™¨å¤§å°ä¸æ¨¡å‹è¯æ±‡è¡¨å¤§å°åŒ¹é…å¹¶è®¾ç½® `bos` å’Œ `eos` ï¼ˆåºåˆ—çš„å¼€å§‹å’Œç»“æŸï¼‰è¯å…ƒ IDï¼š


{#if fw === 'pt'}

```py
from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig

config = AutoConfig.from_pretrained(
    "gpt2",
    vocab_size=len(tokenizer),
    n_ctx=context_length,
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
)
```

æœ‰äº†è¿™ä¸ªé…ç½®ï¼Œæˆ‘ä»¬å¯ä»¥åŠ è½½ä¸€ä¸ªæ–°æ¨¡å‹ã€‚æ³¨æ„ï¼Œè¿™æ˜¯æˆ‘ä»¬ç¬¬ä¸€æ¬¡ä¸ä½¿ç”¨ `from_pretrained()` å‡½æ•°ï¼Œå› ä¸ºæˆ‘ä»¬å®é™…ä¸Šæ˜¯è‡ªå·±åˆå§‹åŒ–ä¸€ä¸ªæ¨¡å‹ï¼š

```py
model = GPT2LMHeadModel(config)
model_size = sum(t.numel() for t in model.parameters())
print(f"GPT-2 size: {model_size/1000**2:.1f}M parameters")
```

```python out
GPT-2 size: 124.2M parameters
```

{:else}

```py
from transformers import AutoTokenizer, TFGPT2LMHeadModel, AutoConfig

config = AutoConfig.from_pretrained(
    "gpt2",
    vocab_size=len(tokenizer),
    n_ctx=context_length,
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
)
```

æœ‰äº†è¿™ä¸ªé…ç½®ï¼Œæˆ‘ä»¬å¯ä»¥åŠ è½½ä¸€ä¸ªæ–°æ¨¡å‹ã€‚æ³¨æ„ï¼Œè¿™æ˜¯æˆ‘ä»¬ç¬¬ä¸€æ¬¡ä¸ä½¿ç”¨ `from_pretrained()` å‡½æ•°ï¼Œå› ä¸ºæˆ‘ä»¬å®é™…ä¸Šæ˜¯è‡ªå·±åˆå§‹åŒ–ä¸€ä¸ªæ¨¡å‹ï¼š

```py
model = TFGPT2LMHeadModel(config)
model(model.dummy_inputs)  # æ„å»ºæ¨¡å‹
model.summary()
```

```python out
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
transformer (TFGPT2MainLayer multiple)                  124242432 
=================================================================
Total params: 124,242,432
Trainable params: 124,242,432
Non-trainable params: 0
_________________________________________________________________
```

{/if}

æˆ‘ä»¬çš„æ¨¡å‹æœ‰ 124M ä¸ªå‚æ•°éœ€è¦è®­ç»ƒã€‚åœ¨æˆ‘ä»¬å¼€å§‹è®­ç»ƒä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦è®¾ç½®ä¸€ä¸ªæ•°æ®æ•´ç†å™¨(DataCollator)ï¼Œå®ƒå°†è´Ÿè´£åˆ›å»º Batchã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `DataCollatorForLanguageModeling` ï¼Œå®ƒç‰¹åˆ«è®¾è®¡ç”¨äºè¯­è¨€å»ºæ¨¡ï¼ˆå¦‚å…¶åå­—æ‰€æš—ç¤ºï¼‰ã€‚é™¤äº†å †å å’Œå¡«å……å»º Batchï¼Œå®ƒè¿˜è´Ÿè´£åˆ›å»ºè¯­è¨€æ¨¡å‹æ ‡ç­¾ â€”â€” åœ¨å› æœè¯­è¨€å»ºæ¨¡ä¸­ï¼Œè¾“å…¥ä¹Ÿä½œä¸ºæ ‡ç­¾ï¼ˆåªæ˜¯åç§»ä¸€ä¸ªå…ƒç´ ï¼‰ï¼Œè¿™ä¸ªæ•°æ®æ•´ç†å™¨(DataCollator)åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å®æ—¶åˆ›å»ºå®ƒä»¬ï¼Œæ‰€ä»¥æˆ‘ä»¬ä¸éœ€è¦å¤åˆ¶ `input_ids` ã€‚

æ³¨æ„ï¼Œ `DataCollatorForLanguageModeling` åŒæ—¶æ”¯æŒæ©ç è¯­è¨€å»ºæ¨¡ (MLM) å’Œå› æœè¯­è¨€å»ºæ¨¡ (CLM)ã€‚é»˜è®¤æƒ…å†µä¸‹å®ƒä¸º MLM å‡†å¤‡æ•°æ®ï¼Œä½†æˆ‘ä»¬å¯ä»¥é€šè¿‡è®¾ç½® `mlm=False` å‚æ•°åˆ‡æ¢åˆ° CLM 

{#if fw === 'pt'}

```py
from transformers import DataCollatorForLanguageModeling

tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)
```

{:else}

```py
from transformers import DataCollatorForLanguageModeling

tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False, return_tensors="tf")
```

{/if}

è®©æˆ‘ä»¬çœ‹ä¸€ä¸ªä¾‹å­ï¼š

```py
out = data_collator([tokenized_dataset["train"][i] for i in range(5)])
for key in out:
    print(f"{key} shape: {out[key].shape}")
```

{#if fw === 'pt'}

```python out
input_ids shape: torch.Size([5, 128])
attention_mask shape: torch.Size([5, 128])
labels shape: torch.Size([5, 128])
```

{:else}

```python out
input_ids shape: (5, 128)
attention_mask shape: (5, 128)
labels shape: (5, 128)
```

{/if}

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ç¤ºä¾‹å·²ç»å †å åœ¨ä¸€èµ·ï¼Œå¹¶ä¸”æ‰€æœ‰ tensor éƒ½å…·æœ‰ç›¸åŒçš„å½¢çŠ¶ã€‚

{#if fw === 'tf'}

ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `prepare_tf_dataset()` æ–¹æ³•ï¼Œä½¿ç”¨ä¸Šé¢åˆ›å»ºçš„æ•°æ®æ•´ç†å™¨(DataCollator)å°†æ•°æ®é›†è½¬æ¢ä¸º TensorFlow æ•°æ®é›†ï¼š

```python
tf_train_dataset = model.prepare_tf_dataset(
    tokenized_dataset["train"],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=32,
)
tf_eval_dataset = model.prepare_tf_dataset(
    tokenized_dataset["valid"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=32,
)
```

{/if}

<div custom-style="Tip-yellow">


âš ï¸ è°ƒæ•´è¾“å…¥å’Œæ ‡ç­¾ä»¥å¯¹é½å®ƒä»¬çš„æ“ä½œåœ¨æ¨¡å‹å†…éƒ¨è¿›è¡Œï¼Œæ‰€ä»¥æ•°æ®æ•´ç†å™¨(DataCollator)åªéœ€å¤åˆ¶è¾“å…¥æ¥åˆ›å»ºæ ‡ç­¾ã€‚

</div>

ç°åœ¨æˆ‘ä»¬å·²ç»å‡†å¤‡å¥½äº†æ‰€æœ‰ä¸œè¥¿ï¼Œå¯ä»¥å¼€å§‹è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹äº†â€”â€”å¥½åƒä¹Ÿä¸æ˜¯é‚£ä¹ˆå›°éš¾ï¼åœ¨æˆ‘ä»¬å¼€å§‹è®­ç»ƒä¹‹å‰ï¼Œæˆ‘ä»¬åº”è¯¥ç™»å½•åˆ° Hugging Faceã€‚å¦‚æœä½ æ­£åœ¨ä½¿ç”¨ Notebook è¿›è¡Œå·¥ä½œï¼Œä½ å¯ä»¥ä½¿ç”¨ä¸‹é¢çš„å®ç”¨å‡½æ•°è¿›è¡Œç™»å½•ï¼š

```python
from huggingface_hub import notebook_login

notebook_login()
```

è¿™å°†æ˜¾ç¤ºä¸€ä¸ªå°éƒ¨ä»¶ï¼Œæ‚¨å¯ä»¥åœ¨å…¶ä¸­è¾“å…¥æ‚¨çš„ Hugging Face ç™»å½•å‡­æ®ã€‚

å¦‚æœæ‚¨ä¸æ˜¯åœ¨ Notebook ä¸Šå·¥ä½œï¼Œåªéœ€åœ¨ç»ˆç«¯ä¸­è¾“å…¥ä»¥ä¸‹è¡Œï¼š

```bash
huggingface-cli login
```

{#if fw === 'pt'}

å‰©ä¸‹è¦åšçš„å°±æ˜¯é…ç½®è®­ç»ƒå‚æ•°å¹¶å¯åŠ¨ `Trainer` ï¼æˆ‘ä»¬å°†ä½¿ç”¨ä½™å¼¦å­¦ä¹ ç‡ï¼Œå¹¶è¿›è¡Œä¸€äº› Warmup å’Œæœ‰æ•ˆæ‰¹é‡å¤§å°ä¸º 256 ( `per_device_train_batch_size` * `gradient_accumulation_steps` ï¼‰ã€‚å½“å•ä¸ªæ‰¹æ¬¡ä¸é€‚åˆå†…å­˜æ—¶ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯ï¼Œå¹¶é€šè¿‡å¤šæ¬¡å‘å‰/å‘åä¼ é€’é€æ­¥å»ºç«‹æ¢¯åº¦ã€‚å½“æˆ‘ä»¬ä½¿ç”¨ ğŸ¤— Accelerate åˆ›å»ºè®­ç»ƒå¾ªç¯æ—¶ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°è¿™ä¸€ç‚¹ã€‚

æ¥ä¸‹æ¥æˆ‘ä»¬éœ€è¦åšçš„å°±æ˜¯é…ç½®è®­ç»ƒå‚æ•°ï¼Œç„¶åå¯åŠ¨ `Trainer` ã€‚æˆ‘ä»¬å°†ä½¿ç”¨å¸¦æœ‰ä¸€äº› Warmup å’Œæœ‰æ•ˆ batch å¤§å°ä¸º 256 ( `per_device_train_batch_size` * `gradient_accumulation_steps` )çš„ä½™å¼¦å­¦ä¹ ç‡è°ƒåº¦ã€‚å½“ä¸€ä¸ª batch ä¸é€‚åˆå†…å­˜æ—¶ï¼Œä¼šä½¿ç”¨æ¢¯åº¦ç´¯ç§¯ï¼Œå¹¶é€šè¿‡å¤šæ¬¡å‰å‘/åå‘ä¼ é€’æ¥é€æ­¥å»ºç«‹æ¢¯åº¦ã€‚å½“æˆ‘ä»¬ä½¿ç”¨ ğŸ¤— Accelerate åˆ›å»ºè®­ç»ƒå¾ªç¯æ—¶ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°è¿™ä¸€ç‚¹ã€‚

```py
from transformers import Trainer, TrainingArguments

args = TrainingArguments(
    output_dir="codeparrot-ds",
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    evaluation_strategy="steps",
    eval_steps=5_000,
    logging_steps=5_000,
    gradient_accumulation_steps=8,
    num_train_epochs=1,
    weight_decay=0.1,
    warmup_steps=1_000,
    lr_scheduler_type="cosine",
    learning_rate=5e-4,
    save_steps=5_000,
    fp16=True,
    push_to_hub=True,
)

trainer = Trainer(
    model=model,
    tokenizer=tokenizer,
    args=args,
    data_collator=data_collator,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["valid"],
)
```

ç°åœ¨æˆ‘ä»¬åªéœ€å¯åŠ¨ `Trainer` å¹¶ç­‰å¾…è®­ç»ƒå®Œæˆã€‚æ ¹æ®æ‚¨æ˜¯åœ¨æ•´ä¸ªè®­ç»ƒé›†è¿˜æ˜¯åœ¨è®­ç»ƒé›†çš„ä¸€ä¸ªå­é›†ä¸Šè¿è¡Œå®ƒï¼Œè¿™å°†åˆ†åˆ«éœ€è¦ 20 æˆ– 2 ä¸ªå°æ—¶ï¼Œå› æ­¤è¯·å–æ¯å’–å•¡æˆ–è€…æ‰¾ä¸€æœ¬å¥½ä¹¦æ¥é˜…è¯»ï¼


```py
trainer.train()
```

è®­ç»ƒå®Œæˆåï¼Œæˆ‘ä»¬å¯ä»¥å°†æ¨¡å‹å’Œè¯å…ƒåˆ†æå™¨æ¨é€åˆ° Hubï¼š

```py
trainer.push_to_hub()
```

{:else}

æˆ‘ä»¬éœ€è¦åšçš„å°±æ˜¯é…ç½®è®­ç»ƒè¶…å‚æ•°å¹¶è°ƒç”¨ `compile()` å’Œ `fit()` ã€‚æˆ‘ä»¬å°†ä½¿ç”¨å¸¦æœ‰ä¸€äº›é¢„çƒ­çš„å­¦ä¹ ç‡è°ƒæ•´ç­–ç•¥æ¥æé«˜è®­ç»ƒçš„ç¨³å®šæ€§ï¼š

```py
from transformers import create_optimizer
import tensorflow as tf

num_train_steps = len(tf_train_dataset)
optimizer, schedule = create_optimizer(
    init_lr=5e-5,
    num_warmup_steps=1_000,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)
model.compile(optimizer=optimizer)

# ä½¿ç”¨æ··åˆç²¾åº¦ float16 è¿›è¡Œè®­ç»ƒ
tf.keras.mixed_precision.set_global_policy("mixed_float16")
```

ç°åœ¨æˆ‘ä»¬åªéœ€è°ƒç”¨ `model.fit()ï¼Œ` å¹¶ç­‰å¾…è®­ç»ƒå®Œæˆã€‚æ ¹æ®ä½ æ˜¯å¦åœ¨å®Œæ•´çš„è®­ç»ƒé›†æˆ–è€…è®­ç»ƒé›†çš„å­é›†ä¸Šè¿è¡Œï¼Œè¿™å°†åˆ†åˆ«éœ€è¦ 20 å°æ—¶æˆ–è€… 2 å°æ—¶ï¼Œæ‰€ä»¥æ‹¿ä¸€äº›å’–å•¡å’Œä¸€æœ¬å¥½ä¹¦æ¥é˜…è¯»ï¼è®­ç»ƒå®Œæˆåï¼Œæˆ‘ä»¬å¯ä»¥å°†æ¨¡å‹å’Œè¯å…ƒåˆ†æå™¨æ¨é€åˆ° Hubï¼š

```py
from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(output_dir="codeparrot-ds", tokenizer=tokenizer)

model.fit(tf_train_dataset, validation_data=tf_eval_dataset, callbacks=[callback])
```

{/if}

<div custom-style="Tip-green">


âœï¸ **è¯•è¯•çœ‹ï¼** é™¤äº† `TrainingArguments` ä¹‹å¤–ï¼Œæˆ‘ä»¬åªéœ€è¦å¤§çº¦ 30 è¡Œä»£ç å°±å¯ä»¥ä»åŸå§‹æ–‡æœ¬åˆ°è®­ç»ƒ GPT-2ã€‚ç”¨ä½ è‡ªå·±çš„æ•°æ®é›†è¯•è¯•çœ‹ï¼Œçœ‹çœ‹ä½ èƒ½ä¸èƒ½å¾—åˆ°å¥½çš„ç»“æœï¼

</div>

<div custom-style="Tip-green">


{#if fw === 'pt'}

ğŸ’¡ å¦‚æœä½ èƒ½ä½¿ç”¨å¤š GPU çš„æœºå™¨ï¼Œå°è¯•åœ¨é‚£é‡Œè¿è¡Œä»£ç ã€‚ `Trainer` è‡ªåŠ¨ç®¡ç†å¤šå°æœºå™¨ï¼Œè¿™èƒ½æå¤§åœ°åŠ å¿«è®­ç»ƒé€Ÿåº¦ã€‚

{:else}

ğŸ’¡ å¦‚æœæ‚¨æ­£åœ¨ä½¿ç”¨å…·æœ‰å¤šä¸ª GPU çš„è®¡ç®—æœºï¼Œåˆ™å¯ä»¥å°è¯•ä½¿ç”¨ `MirroredStrategy` ä¸Šä¸‹æ–‡æ¥å¤§å¹…åŠ å¿«è®­ç»ƒé€Ÿåº¦ã€‚æ‚¨éœ€è¦åˆ›å»ºä¸€ä¸ª `tf.distribute.MirroredStrategy` å¯¹è±¡ï¼Œå¹¶ç¡®ä¿æ‰€æœ‰çš„ `to_tf_dataset` æˆ– `prepare_tf_dataset()` æ–¹æ³•ä»¥åŠæ¨¡å‹åˆ›å»ºå’Œå¯¹ `fit()` çš„è°ƒç”¨éƒ½åœ¨å…¶ `scope()` ä¸Šä¸‹æ–‡ä¸­è¿è¡Œã€‚æ‚¨å¯ä»¥åœ¨ [è¿™é‡Œ](https://www.tensorflow.org/guide/distributed_training#use_tfdistributestrategy_with_keras_modelfit) æŸ¥çœ‹æœ‰å…³æ­¤å†…å®¹çš„æ–‡æ¡£ã€‚

{/if}

</div>

## ä½¿ç”¨ pipeline è¿›è¡Œä»£ç ç”Ÿæˆ 

ç°åœ¨æ˜¯çœŸç›¸çš„æ—¶åˆ»ï¼šæˆ‘ä»¬æ¥çœ‹çœ‹è®­ç»ƒå¥½çš„æ¨¡å‹åˆ°åº•è¡¨ç°å¦‚ä½•ï¼æˆ‘ä»¬å¯ä»¥åœ¨æ—¥å¿—ä¸­çœ‹åˆ°æŸå¤±æŒç»­ä¸‹é™ï¼Œä½†è¦æµ‹è¯•æ¨¡å‹çš„æ•ˆæœï¼Œæˆ‘ä»¬å°±çœ‹çœ‹å®ƒå¯¹ä¸€äº›æç¤ºçš„ååº”å¦‚ä½•ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†æ¨¡å‹åŒ…è£…åœ¨ä¸€ä¸ªæ–‡æœ¬ç”Ÿæˆ `pipeline` ä¸­ï¼Œå¹¶å¦‚æœæœ‰ GPU å¯ç”¨ï¼Œæˆ‘ä»¬å°†æŠŠå®ƒæ”¾åœ¨ GPU ä¸Šè¿›è¡Œå¿«é€Ÿç”Ÿæˆï¼š

{#if fw === 'pt'}

```py
import torch
from transformers import pipeline

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
pipe = pipeline(
    "text-generation", model="huggingface-course/codeparrot-ds", device=device
)
```

{:else}

```py
from transformers import pipeline

course_model = TFGPT2LMHeadModel.from_pretrained("huggingface-course/codeparrot-ds")
course_tokenizer = AutoTokenizer.from_pretrained("huggingface-course/codeparrot-ds")
pipe = pipeline(
    "text-generation", model=course_model, tokenizer=course_tokenizer, device=0
)
```

{/if}

è®©æˆ‘ä»¬ä»åˆ›å»ºæ•£ç‚¹å›¾çš„ç®€å•ä»»åŠ¡å¼€å§‹ï¼š

```py
txt = """\
# åˆ›å»ºä¸€äº›æ•°æ®
x = np.random.randn(100)
y = np.random.randn(100)

# ä½¿ç”¨ xï¼Œy åˆ›å»ºæ•£ç‚¹å›¾
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python out
# åˆ›å»ºä¸€äº›æ•°æ®
x = np.random.randn(100)
y = np.random.randn(100)

# ä½¿ç”¨ xï¼Œy åˆ›å»ºæ•£ç‚¹å›¾
plt.scatter(x, y)

# åˆ›å»ºæ•£ç‚¹
```

ç»“æœçœ‹èµ·æ¥æ˜¯æ­£ç¡®çš„ã€‚é‚£ä¹ˆå¯¹äº `pandas` æ“ä½œä¹Ÿå¯ä»¥å—ï¼Ÿè®©æˆ‘ä»¬çœ‹çœ‹æ˜¯å¦èƒ½ä»ä¸¤ä¸ªæ•°ç»„åˆ›å»ºä¸€ä¸ª `DataFrame` ï¼š

```py
txt = """\
# åˆ›å»ºä¸€äº›æ•°æ®
x = np.random.randn(100)
y = np.random.randn(100)

# ä» x å’Œ y åˆ›å»º dataframe
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python out
# åˆ›å»ºä¸€äº›æ•°æ®
x = np.random.randn(100)
y = np.random.randn(100)

# ä» x å’Œ y åˆ›å»º dataframe
df = pd.DataFrame({'x': x, 'y': y})
df.insert(0,'x', x)
for
```

å¾ˆå¥½ï¼Œè¿™æ˜¯æ­£ç¡®çš„ç­”æ¡ˆâ€”â€”å°½ç®¡å®ƒåˆæ’å…¥äº†ä¸€åˆ— `x` ã€‚ç”±äºç”Ÿæˆçš„ token æ•°é‡æœ‰é™ï¼Œæ‰€ä»¥ä¸‹é¢çš„ `for` å¾ªç¯è¢«åˆ‡æ–­äº†ã€‚è®©æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬æ˜¯å¦èƒ½åšäº›æ›´å¤æ‚çš„äº‹æƒ…ï¼Œè®©æ¨¡å‹å¸®åŠ©æˆ‘ä»¬ä½¿ç”¨ `groupby` æ“ä½œï¼š

```py
txt = """\
# æœ‰èŒä¸šï¼Œæ”¶å…¥å’Œåå­—çš„ dataframe
df = pd.DataFrame({'profession': x, 'income':y, 'name': z})

# è®¡ç®—æ¯ä¸ªèŒä¸šçš„å¹³å‡æ”¶å…¥
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python out
# æœ‰èŒä¸šï¼Œæ”¶å…¥å’Œåå­—çš„ dataframe
df = pd.DataFrame({'profession': x, 'income':y, 'name': z})

# è®¡ç®—æ¯ä¸ªèŒä¸šçš„å¹³å‡æ”¶å…¥
profession = df.groupby(['profession']).mean()

# compute the
```

ä¸é”™ï¼›è¿™æ˜¯æ­£ç¡®çš„åšæ³•ã€‚æœ€åï¼Œè®©æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬æ˜¯å¦ä¹Ÿå¯ä»¥å°†å…¶ç”¨äº `scikit-learn` å¹¶å»ºç«‹ä¸€ä¸ªéšæœºæ£®æ—æ¨¡å‹ï¼š

```py
txt = """
# ä» scikit-learn å¯¼å…¥éšæœºæ£®æ—å›å½’å™¨
from sklearn.ensemble import RandomForestRegressor

# ç”¨ X, y æ‹Ÿåˆå¸¦æœ‰ 300 ä¸ªä¼°ç®—å™¨çš„éšæœºæ£®æ—æ¨¡å‹ï¼š
"""
print(pipe(txt, num_return_sequences=1)[0]["generated_text"])
```

```python out
# ä» scikit-learn å¯¼å…¥éšæœºæ£®æ—å›å½’å™¨
from sklearn.ensemble import RandomForestRegressor

# ç”¨ X, y æ‹Ÿåˆå¸¦æœ‰ 300 ä¸ªä¼°ç®—å™¨çš„éšæœºæ£®æ—æ¨¡å‹ï¼š
rf = RandomForestRegressor(n_estimators=300, random_state=random_state, max_depth=3)
rf.fit(X, y)
rf
```

{#if fw === 'tf'}

çœ‹çœ‹è¿™å‡ ä¸ªä¾‹å­ï¼Œä¼¼ä¹æ¨¡å‹å·²ç»å­¦ä¹ äº† Python æ•°æ®ç§‘å­¦å †æ ˆçš„ä¸€äº›è¯­æ³•ã€‚å½“ç„¶ï¼Œåœ¨å°†æ¨¡å‹éƒ¨ç½²åˆ°ç°å®ä¸–ç•Œä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦æ›´å½»åº•åœ°è¯„ä¼°æ¨¡å‹ï¼Œä½†è¿™ä»ç„¶æ˜¯ä¸€ä¸ªä»¤äººå°è±¡æ·±åˆ»çš„åŸå‹ã€‚

{:else}

ä»è¿™å‡ ä¸ªä¾‹å­æ¥çœ‹ï¼Œæ¨¡å‹ä¼¼ä¹å·²ç»å­¦ä¹ äº† Python æ•°æ®ç§‘å­¦å †æ ˆçš„ä¸€äº›è¯­æ³•ï¼ˆå½“ç„¶ï¼Œåœ¨å°†æ¨¡å‹éƒ¨ç½²åˆ°ç°å®ä¸–ç•Œä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å¯¹å…¶è¿›è¡Œæ›´å…¨é¢çš„è¯„ä¼°ï¼‰ã€‚ç„¶è€Œï¼Œæœ‰æ—¶å€™å®ƒéœ€è¦æ›´å¤šçš„æ¨¡å‹è®­ç»ƒå®šåˆ¶æ¥è¾¾åˆ°ç»™å®šç”¨ä¾‹æ‰€éœ€çš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬æƒ³åŠ¨æ€æ›´æ–° batch_size æˆ–æ·»åŠ ä¸€ä¸ªæ¡ä»¶è®­ç»ƒå¾ªç¯æ¥å³ä½¿è·³è¿‡åç¤ºä¾‹æ€ä¹ˆåŠï¼Ÿä¸€ç§é€‰æ‹©æ˜¯ä¿®æ”¹ `Trainer` æ·»åŠ æ–°çš„åŠŸèƒ½ï¼Œä½†æœ‰æ—¶ä»å¤´å¼€å§‹ç¼–å†™è®­ç»ƒå¾ªç¯ä¼šæ›´ç®€å•ã€‚è¿™å°±æ˜¯ğŸ¤— Accelerate çš„ç”¨æ­¦ä¹‹åœ°ã€‚

{/if}

{#if fw === 'pt'}

## ä½¿ç”¨ğŸ¤— Accelerate è¿›è¡Œè®­ç»ƒ 

æˆ‘ä»¬å·²ç»çœ‹åˆ°äº†å¦‚ä½•ä½¿ç”¨ `Trainer` è®­ç»ƒæ¨¡å‹ï¼Œè¿™å¯ä»¥å¯¹è®­ç»ƒè¿‡ç¨‹è¿›è¡Œä¸€äº›å®šåˆ¶ã€‚ç„¶è€Œï¼Œæœ‰æ—¶æˆ‘ä»¬æƒ³è¦å®Œå…¨æ§åˆ¶è®­ç»ƒå¾ªç¯ï¼Œæˆ–è€…æˆ‘ä»¬æƒ³è¦è¿›è¡Œä¸€äº›å¥‡ç‰¹çš„æ›´æ”¹ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ ğŸ¤— Accelerate æ˜¯ä¸€ä¸ªä¸é”™çš„é€‰æ‹©ï¼Œæœ¬èŠ‚æˆ‘ä»¬å°†ä»‹ç»å¦‚ä½•ä½¿ç”¨å®ƒæ¥è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ã€‚ä¸ºäº†è®©äº‹æƒ…å˜å¾—æ›´æœ‰è¶£ï¼Œæˆ‘ä»¬è¿˜å°†åœ¨è®­ç»ƒå¾ªç¯ä¸­æ·»åŠ ä¸€äº›ä¿®æ”¹ã€‚



ç”±äºæˆ‘ä»¬ä¸»è¦å…³æ³¨çš„æ˜¯ä¸ºæ•°æ®ç§‘å­¦åº“æä¾›åˆç†çš„ä»£ç è‡ªåŠ¨è¡¥å……åŠŸèƒ½ï¼Œå› æ­¤å¯¹äºæ›´å¤šä½¿ç”¨è¿™äº›åº“çš„è®­ç»ƒæ ·æœ¬èµ‹äºˆæ›´é«˜çš„æƒé‡æ˜¯æœ‰æ„ä¹‰çš„ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿ç”¨ `plt` ã€ `pd` ã€ `sk` ã€ `fit` å’Œ `predict` ç­‰å…³é”®è¯æ¥è½»æ¾åœ°è¯†åˆ«å‡ºè¿™äº›ä¾‹å­ï¼Œè¿™äº›å…³é”®è¯æ˜¯ `matplotlib.pyplot` ã€ `pandas` å’Œ `sklearn` çš„æœ€å¸¸ç”¨é‡å‘½ååçš„å¯¼å…¥åç§°ï¼Œä»¥åŠåè€…çš„ fit/predict æ¨¡å¼ã€‚å¦‚æœè¿™äº›éƒ½è¡¨ç¤ºä¸ºä¸€ä¸ªå•ä¸€çš„ tokenï¼Œæˆ‘ä»¬å¯ä»¥è½»æ¾åœ°æ£€æŸ¥å®ƒä»¬æ˜¯å¦å‡ºç°åœ¨è¾“å…¥åºåˆ—ä¸­ã€‚Tokens å¯ä»¥æœ‰ç©ºæ ¼å‰ç¼€ï¼Œæ‰€ä»¥æˆ‘ä»¬ä¹Ÿä¼šåœ¨ tokenizer è¯æ±‡è¡¨ä¸­æ£€æŸ¥è¿™äº›ç‰ˆæœ¬ã€‚ä¸ºäº†éªŒè¯å®ƒæ˜¯å¦æœ‰æ•ˆï¼Œæˆ‘ä»¬ä¼šæ·»åŠ ä¸€ä¸ªåº”è¯¥è¢«åˆ†å‰²ä¸ºå¤šä¸ª tokens çš„æµ‹è¯• tokenï¼š

```py
keytoken_ids = []
for keyword in [
    "plt",
    "pd",
    "sk",
    "fit",
    "predict",
    " plt",
    " pd",
    " sk",
    " fit",
    " predict",
    "testtest",
]:
    ids = tokenizer([keyword]).input_ids[0]
    if len(ids) == 1:
        keytoken_ids.append(ids[0])
    else:
        print(f"Keyword has not single token: {keyword}")
```

```python out
'Keyword has not single token: testtest'
```

å¤ªå¥½äº†ï¼Œè¿™ä¸ªæ–¹æ³•ä¼¼ä¹å¾ˆæœ‰æ•ˆï¼æˆ‘ä»¬ç°åœ¨å¯ä»¥ç¼–å†™ä¸€ä¸ªè‡ªå®šä¹‰çš„æŸå¤±å‡½æ•°ï¼Œå®ƒå°†è¾“å…¥åºåˆ—ã€logits å’Œæˆ‘ä»¬åˆšåˆšé€‰æ‹©çš„å…³é”® token ä½œä¸ºè¾“å…¥ã€‚é¦–å…ˆæˆ‘ä»¬éœ€è¦å¯¹é½ logits å’Œ inputsï¼šå³ç§»ä¸€ä¸ªå•ä½çš„è¾“å…¥åºåˆ—å½¢æˆäº†æ ‡ç­¾ï¼Œå› ä¸ºä¸‹ä¸€ä¸ª token å°±æ˜¯å½“å‰ token çš„æ ‡ç­¾ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡ä»è¾“å…¥åºåˆ—çš„ç¬¬äºŒä¸ª token å¼€å§‹è®¾ç½®æ ‡ç­¾ï¼Œå› ä¸ºæ¨¡å‹ä¸ä¼šä¸ºç¬¬ä¸€ä¸ª token åšå‡ºé¢„æµ‹ã€‚ç„¶åæˆ‘ä»¬æˆªæ–­æœ€åä¸€ä¸ª logitï¼Œå› ä¸ºæˆ‘ä»¬æ²¡æœ‰æ ‡ç­¾æ¥è·Ÿè¸ªå®Œæ•´çš„è¾“å…¥åºåˆ—ã€‚æœ‰äº†è¿™ä¸ªï¼Œæˆ‘ä»¬å°±å¯ä»¥è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„æŸå¤±ï¼Œå¹¶è®¡ç®—æ¯ä¸ªæ ·æœ¬ä¸­æ‰€æœ‰å…³é”®è¯çš„å‡ºç°æ¬¡æ•°ã€‚æœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨å‡ºç°æ¬¡æ•°ä½œä¸ºæƒé‡ï¼Œè®¡ç®—æ‰€æœ‰æ ·æœ¬çš„åŠ æƒå¹³å‡å€¼ã€‚ç”±äºæˆ‘ä»¬ä¸æƒ³æŠ›å¼ƒæ‰€æœ‰æ²¡æœ‰å…³é”®è¯çš„æ ·æœ¬ï¼Œæ‰€ä»¥æˆ‘ä»¬å°† s æ‰€æœ‰çš„æƒé‡åŠ  1ï¼š

```py
from torch.nn import CrossEntropyLoss
import torch


def keytoken_weighted_loss(inputs, logits, keytoken_ids, alpha=1.0):
    # å·¦ç§» tokens < n é¢„æµ‹ n
    shift_labels = inputs[..., 1:].contiguous()
    shift_logits = logits[..., :-1, :].contiguous()
    # è®¡ç®—æ¯ä¸€ä¸ªtokençš„loss
    loss_fct = CrossEntropyLoss(reduce=False)
    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
    # å¯¹äºæ¯ä¸ªæ ·æœ¬é‡æ–°è°ƒæ•´å¤§å°å¹¶å¹³å‡
    loss_per_sample = loss.view(shift_logits.size(0), shift_logits.size(1)).mean(axis=1)
    # è®¡ç®—å¹¶ç¼©æ”¾æƒé‡
    weights = torch.stack([(inputs == kt).float() for kt in keytoken_ids]).sum(
        axis=[0, 2]
    )
    weights = alpha * (1.0 + weights)
    # è®¡ç®—è¯„ä»·æƒé‡
    weighted_loss = (loss_per_sample * weights).mean()
    return weighted_loss
```

åœ¨æˆ‘ä»¬å¼€å§‹ä½¿ç”¨è¿™ä¸ªç²¾å¦™çš„æ–°æŸå¤±å‡½æ•°è¿›è¡Œè®­ç»ƒä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å‡†å¤‡ä¸€äº›äº‹æƒ…ï¼š

- æˆ‘ä»¬éœ€è¦æ•°æ®åŠ è½½å™¨æ¥æ‰¹é‡åŠ è½½æ•°æ®ã€‚
- æˆ‘ä»¬éœ€è¦è®¾ç½®æƒé‡è¡°å‡å‚æ•°ã€‚
- æœ‰æ—¶æˆ‘ä»¬å¸Œæœ›è¿›è¡Œè¯„ä¼°ï¼Œæ‰€ä»¥å°†è¯„ä¼°ä»£ç åŒ…è£…åœ¨ä¸€ä¸ªå‡½æ•°ä¸­æ˜¯æœ‰æ„ä¹‰çš„ã€‚

è®©æˆ‘ä»¬ä»æ•°æ®åŠ è½½å™¨å¼€å§‹ã€‚æˆ‘ä»¬åªéœ€è¦å°†æ•°æ®é›†çš„æ ¼å¼è®¾ç½®ä¸º `"torch"` ï¼Œç„¶åæˆ‘ä»¬å°±å¯ä»¥å°†å®ƒä¼ é€’ç»™ä¸€ä¸ªå…·æœ‰é€‚å½“æ‰¹é‡å¤§å°çš„ PyTorch çš„ `DataLoader` ï¼š

```py
from torch.utils.data.dataloader import DataLoader

tokenized_dataset.set_format("torch")
train_dataloader = DataLoader(tokenized_dataset["train"], batch_size=32, shuffle=True)
eval_dataloader = DataLoader(tokenized_dataset["valid"], batch_size=32)
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†å‚æ•°åˆ†ç»„ï¼Œä»¥ä¾¿ä¼˜åŒ–å™¨çŸ¥é“å“ªäº›å‚æ•°éœ€è¦è¿›è¡Œé¢å¤–çš„æƒé‡è¡°å‡ã€‚é€šå¸¸ï¼Œæ‰€æœ‰çš„åç½®å’Œ LayerNorm æƒé‡é¡¹éƒ½ä¸éœ€è¦ï¼›æˆ‘ä»¬å¯ä»¥è¿™æ ·åšï¼š

```py
weight_decay = 0.1


def get_grouped_params(model, no_decay=["bias", "LayerNorm.weight"]):
    params_with_wd, params_without_wd = [], []
    for n, p in model.named_parameters():
        if any(nd in n for nd in no_decay):
            params_without_wd.append(p)
        else:
            params_with_wd.append(p)
    return [
        {"params": params_with_wd, "weight_decay": weight_decay},
        {"params": params_without_wd, "weight_decay": 0.0},
    ]
```

æˆ‘ä»¬å¸Œæœ›åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å®šæœŸåœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹ï¼Œè®©æˆ‘ä»¬ä¸ºæ­¤ç¼–å†™ä¸€ä¸ªå‡½æ•°ã€‚å®ƒåªéœ€éå†è¯„ä¼°æ•°æ®åŠ è½½å™¨ï¼Œå¹¶æ”¶é›†æ‰€æœ‰è¿›ç¨‹ä¸­çš„æŸå¤±å€¼ï¼š

```py
def evaluate():
    model.eval()
    losses = []
    for step, batch in enumerate(eval_dataloader):
        with torch.no_grad():
            outputs = model(batch["input_ids"], labels=batch["input_ids"])

        losses.append(accelerator.gather(outputs.loss))
    loss = torch.mean(torch.cat(losses))
    try:
        perplexity = torch.exp(loss)
    except OverflowError:
        perplexity = float("inf")
    return loss.item(), perplexity.item()
```

é€šè¿‡ `evaluate()` å‡½æ•°æˆ‘ä»¬å®šæœŸå¯ä»¥è·å–æŸå¤±å€¼å’Œ [å›°æƒ‘åº¦(perplexity)](/course/chapter7/3) ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬é‡æ–°å®šä¹‰æˆ‘ä»¬çš„æ¨¡å‹ä»¥ç¡®ä¿æˆ‘ä»¬å†æ¬¡ä»å¤´å¼€å§‹è®­ç»ƒï¼š

```py
model = GPT2LMHeadModel(config)
```

ç„¶åæˆ‘ä»¬å¯ä»¥å®šä¹‰æˆ‘ä»¬çš„ä¼˜åŒ–å™¨ï¼Œä½¿ç”¨ä¹‹å‰çš„å‡½æ•°æ¥åˆ†å‰²æƒé‡è¡°å‡çš„å‚æ•°ï¼š

```py
from torch.optim import AdamW

optimizer = AdamW(get_grouped_params(model), lr=5e-4)
```

ç°åœ¨è®©æˆ‘ä»¬å‡†å¤‡æ¨¡å‹ã€ä¼˜åŒ–å™¨å’Œæ•°æ®åŠ è½½å™¨ï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥å¼€å§‹è®­ç»ƒï¼š

```py
from accelerate import Accelerator

accelerator = Accelerator(fp16=True)

model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)
```

<div custom-style="Tip-red">


ğŸš¨ å¦‚æœä½ åœ¨ TPU ä¸Šè®­ç»ƒï¼Œä½ éœ€è¦å°†ä¸Šè¿°å•å…ƒæ ¼å¼€å§‹çš„æ‰€æœ‰ä»£ç ç§»åˆ°ä¸€ä¸ªä¸“é—¨çš„è®­ç»ƒå‡½æ•°ä¸­ã€‚æ›´å¤šè¯¦æƒ…è¯·å‚é˜… [ç¬¬3ç« ](/course/chapter3) ã€‚

</div>

ç°åœ¨æˆ‘ä»¬å·²ç»å°†æˆ‘ä»¬çš„ `train_dataloader` ä¼ é€’ç»™äº† `accelerator.prepare()` ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `len()` æ¥è®¡ç®—è®­ç»ƒæ­¥éª¤çš„æ•°é‡ã€‚è¯·è®°ä½ï¼Œæˆ‘ä»¬åº”è¯¥åœ¨å‡†å¤‡å¥½ `dataloader` åå†ä½¿ç”¨ `len()` ï¼Œå› ä¸ºæ”¹åŠ¨ `dataloader` ä¼šæ”¹å˜å…¶é•¿åº¦ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªä»å­¦ä¹ ç‡åˆ° 0 çš„ç»å…¸çº¿æ€§å­¦ä¹ ç‡è°ƒåº¦ï¼š


```py
num_train_epochs = 1
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    name="linear",
    optimizer=optimizer,
    num_warmup_steps=1_000,
    num_training_steps=num_training_steps,
)
```

æœ€åï¼Œä¸ºäº†å°†æˆ‘ä»¬çš„æ¨¡å‹æ¨é€åˆ° Hubï¼Œæˆ‘ä»¬éœ€è¦åœ¨ä¸€ä¸ªå·¥ä½œæ–‡ä»¶å¤¹ä¸­åˆ›å»ºä¸€ä¸ª `Repository` å¯¹è±¡ã€‚å¦‚æœä½ è¿˜æ²¡æœ‰ç™»å½•çš„è¯ï¼Œé¦–å…ˆéœ€è¦ç™»å½•åˆ° Hugging Faceï¼Œæˆ‘ä»¬å°†æ ¹æ®æˆ‘ä»¬æƒ³ç»™æˆ‘ä»¬çš„æ¨¡å‹çš„æ¨¡å‹ ID æ¥ç¡®å®šä»“åº“åç§°ï¼ˆä½ å¯ä»¥ä½¿ç”¨ä½ å–œæ¬¢çš„åå­—æ›¿æ¢ `repo_name` ï¼›å®ƒåªéœ€è¦åŒ…å«ä½ çš„ç”¨æˆ·åï¼Œå¯ä»¥ä½¿ç”¨ `get_full_repo_name()` å‡½æ•°çš„æŸ¥çœ‹ç›®å‰çš„ repo_nameï¼‰ï¼š

```py
from huggingface_hub import Repository, get_full_repo_name

model_name = "codeparrot-ds-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name
```

```python out
'sgugger/codeparrot-ds-accelerate'
```

ç„¶åæˆ‘ä»¬å¯ä»¥å°†è¯¥ä»“åº“å…‹éš†åˆ°æœ¬åœ°æ–‡ä»¶å¤¹ä¸­ã€‚å¦‚æœå®ƒå·²ç»å­˜åœ¨ï¼Œè¿™ä¸ªæœ¬åœ°æ–‡ä»¶å¤¹åº”è¯¥æ˜¯æˆ‘ä»¬æ­£åœ¨ä½¿ç”¨çš„ä»“åº“çš„ç°æœ‰å…‹éš†ï¼š
```py
output_dir = "codeparrot-ds-accelerate"
repo = Repository(output_dir, clone_from=repo_name)
```

æˆ‘ä»¬ç°åœ¨å¯ä»¥é€šè¿‡è°ƒç”¨ `repo.push_to_hub()` æ–¹æ³•ä¸Šä¼ ä¿å­˜åœ¨ `output_dir` ä¸­çš„æ‰€æœ‰å†…å®¹ã€‚è¿™å°†å¸®åŠ©æˆ‘ä»¬åœ¨æ¯ä¸ªè®­ç»ƒå‘¨æœŸç»“æŸæ—¶ä¸Šä¼ ä¸­é—´æ¨¡å‹ã€‚

```py
evaluate()
```

```python out
(10.934126853942871, 56057.14453125)
```

ç›®å‰çš„æŸå¤±å’Œå›°æƒ‘åº¦éƒ½æ˜¯éå¸¸é«˜çš„å€¼ï¼Œä½†è¿™å¹¶ä¸å¥‡æ€ªï¼Œå› ä¸ºæˆ‘ä»¬è¿˜æ²¡æœ‰è®­ç»ƒæ¨¡å‹ã€‚æœ‰äº†è¿™äº›ï¼Œæˆ‘ä»¬å·²ç»ä¸ºç¼–å†™è®­ç»ƒè„šæœ¬çš„æ ¸å¿ƒéƒ¨åˆ†ï¼šè®­ç»ƒå¾ªç¯åšå¥½äº†å‡†å¤‡ã€‚åœ¨è®­ç»ƒå¾ªç¯ä¸­ï¼Œæˆ‘ä»¬è¿­ä»£éå†æ•°æ®åŠ è½½å™¨å¹¶å°†æ‰¹æ¬¡ä¼ é€’ç»™æ¨¡å‹ã€‚æœ‰äº† logitsï¼Œæˆ‘ä»¬å°±å¯ä»¥è¯„ä¼°æˆ‘ä»¬çš„è‡ªå®šä¹‰æŸå¤±å‡½æ•°ã€‚æˆ‘ä»¬å°†æŸå¤±ä¹˜ä»¥æ¢¯åº¦ç§¯ç´¯æ­¥æ•°ï¼Œä»¥ä¾¿åœ¨èšåˆæ›´å¤šæ­¥éª¤æ—¶ä¸ä¼šäº§ç”Ÿæ›´å¤§çš„æŸå¤±ã€‚åœ¨æˆ‘ä»¬ä¼˜åŒ–ä¹‹å‰ï¼Œæˆ‘ä»¬ä¹Ÿä¼šå‰ªè£æ¢¯åº¦ä»¥è·å¾—æ›´å¥½çš„æ”¶æ•›ã€‚æœ€åï¼Œæ¯éš”ä¸€æ®µæ­¥æ•°ï¼Œæˆ‘ä»¬ç”¨æˆ‘ä»¬æ–°çš„ `evaluate()` å‡½æ•°åœ¨è¯„ä¼°é›†ä¸Šè¯„ä¼°æ¨¡å‹ï¼š

```py
from tqdm.notebook import tqdm

gradient_accumulation_steps = 8
eval_steps = 5_000

model.train()
completed_steps = 0
for epoch in range(num_train_epochs):
    for step, batch in tqdm(
        enumerate(train_dataloader, start=1), total=num_training_steps
    ):
        logits = model(batch["input_ids"]).logits
        loss = keytoken_weighted_loss(batch["input_ids"], logits, keytoken_ids)
        if step % 100 == 0:
            accelerator.print(
                {
                    "lr": get_lr(),
                    "samples": step * samples_per_step,
                    "steps": completed_steps,
                    "loss/train": loss.item() * gradient_accumulation_steps,
                }
            )
        loss = loss / gradient_accumulation_steps
        accelerator.backward(loss)
        if step % gradient_accumulation_steps == 0:
            accelerator.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            lr_scheduler.step()
            optimizer.zero_grad()
            completed_steps += 1
        if (step % (eval_steps * gradient_accumulation_steps)) == 0:
            eval_loss, perplexity = evaluate()
            accelerator.print({"loss/eval": eval_loss, "perplexity": perplexity})
            model.train()
            accelerator.wait_for_everyone()
            unwrapped_model = accelerator.unwrap_model(model)
            unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
            if accelerator.is_main_process:
                tokenizer.save_pretrained(output_dir)
                repo.push_to_hub(
                    commit_message=f"Training in progress step {step}", blocking=False
                )
```

å°±æ˜¯è¿™æ · - æ‚¨ç°åœ¨æ‹¥æœ‰è‡ªå·±çš„å› æœè¯­è¨€æ¨¡å‹ï¼ˆä¾‹å¦‚ GPT-2ï¼‰çš„è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ï¼Œæ‚¨å¯ä»¥æ ¹æ®è‡ªå·±çš„éœ€è¦è¿›ä¸€æ­¥å®šåˆ¶ã€‚

<div custom-style="Tip-green">


âœï¸ **è¯•è¯•çœ‹ï¼** åˆ›å»ºé€‚åˆæ‚¨çš„ç”¨ä¾‹çš„è‡ªå®šä¹‰æŸå¤±å‡½æ•°ï¼Œæˆ–åœ¨è®­ç»ƒå¾ªç¯ä¸­æ·»åŠ å¦ä¸€ä¸ªè‡ªå®šä¹‰æ­¥éª¤ã€‚

</div>

<div custom-style="Tip-green">


âœï¸ **è¯•è¯•çœ‹ï¼**  å½“è¿è¡Œé•¿æ—¶é—´çš„è®­ç»ƒå®éªŒæ—¶ï¼Œä½¿ç”¨ TensorBoard æˆ– Weights & Biases ç­‰å·¥å…·è®°å½•é‡è¦æŒ‡æ ‡æ˜¯ä¸ªå¥½ä¸»æ„ã€‚å‘è®­ç»ƒå¾ªç¯ä¸­æ·»åŠ é€‚å½“çš„æ—¥å¿—è®°å½•ï¼Œè¿™æ ·ä½ å¯ä»¥éšæ—¶æ£€æŸ¥è®­ç»ƒè¿›åº¦ã€‚

</div>

{/if}