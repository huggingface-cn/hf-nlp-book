# ç¬¬å…­ç«  Datasetsåº“

æˆ‘ä»¬åœ¨ç¬¬å››ç« ç¬¬ä¸€æ¬¡ä½“éªŒäº†Datasets åº“ï¼Œäº†è§£åˆ°å¾®è°ƒæ¨¡å‹ä¸»è¦æœ‰ä¸‰ä¸ªæ­¥éª¤ï¼š

1. ä» Hugging Face Hub åŠ è½½æ•°æ®é›†ã€‚
2. ä½¿ç”¨ `Dataset.map()` é¢„å¤„ç†æ•°æ®ã€‚
3. åŠ è½½å’Œè®¡ç®—æŒ‡æ ‡ï¼ˆç‰¹å¾ï¼‰ã€‚

ä½†è¿™ä»…ä»…è§¦åŠäº†Datasets åº“èƒ½åšçš„äº‹æƒ…çš„å†°å±±ä¸€è§’ï¼åœ¨æœ¬ç« ï¼Œæˆ‘ä»¬å°†æ·±å…¥æ¢ç´¢è¿™ä¸ªåº“ã€‚ä¸€è·¯ä¸Šï¼Œæˆ‘ä»¬ä¼šæ‰¾åˆ°ä»¥ä¸‹é—®é¢˜çš„ç­”æ¡ˆï¼š

* å½“ä½ çš„æ•°æ®é›†ä¸åœ¨ Hub ä¸Šæ—¶ï¼Œä½ åº”è¯¥æ€ä¹ˆåšï¼Ÿ
* ä½ å¦‚ä½•åˆ‡åˆ†å’Œæ“ä½œæ•°æ®é›†ï¼Ÿï¼ˆå¦‚æœä½ éå¸¸éœ€è¦ä½¿ç”¨ Pandasï¼Œè¯¥å¦‚ä½•å¤„ç†ï¼Ÿï¼‰
* å½“ä½ çš„æ•°æ®é›†éå¸¸å¤§ï¼Œä¼šæ’‘çˆ†ä½ ç¬”è®°æœ¬ç”µè„‘çš„ RAM æ—¶ï¼Œä½ åº”è¯¥æ€ä¹ˆåŠï¼Ÿ
* ä»€ä¹ˆæ˜¯â€œå†…å­˜æ˜ å°„â€å’Œ â€œApache Arrowâ€ï¼Ÿ
* å¦‚ä½•åˆ›å»ºè‡ªå·±çš„æ•°æ®é›†å¹¶å°†å…¶æ¨é€åˆ°ä¸­å¿ƒï¼Ÿ

ä½ åœ¨è¿™é‡Œå­¦åˆ°çš„æŠ€æœ¯å°†ä¸ºä½ åœ¨ç¬¬ä¸ƒç« å’Œç¬¬å…«ç« ä¸­çš„é«˜çº§ tokenization å’Œå¾®è°ƒä»»åŠ¡åšå¥½å‡†å¤‡â€”â€”æ‰€ä»¥ï¼Œæ¥æ¯å’–å•¡ï¼Œè®©æˆ‘ä»¬å¼€å§‹å§ï¼

## 6.1 å¦‚æœæˆ‘çš„æ•°æ®é›†ä¸åœ¨ Hub ä¸Šæ€ä¹ˆåŠï¼Ÿ

ä½ ä»¥åŠçŸ¥é“å¦‚ä½•ä½¿ç”¨ [Hugging Face Hub](https://huggingface.co/datasets)(https://huggingface.co/datasets) ä¸­çš„æ•°æ®é›†ï¼Œä½†ä½ å¾€å¾€ä¼šå‘ç°è‡ªå·±éœ€è¦å¤„ç†åœ¨è‡ªå·±çš„ç¬”è®°æœ¬ç”µè„‘æˆ–è€…ç½‘ç»œä¸Šçš„æ•°æ®é›†ã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†å±•ç¤ºå¦‚ä½•ä½¿ç”¨Datasets åŠ è½½ä¸åœ¨ Hugging Face Hub ä¸­çš„æ•°æ®é›†ã€‚

### ä½¿ç”¨æœ¬åœ°å’Œè¿œç¨‹æ•°æ®é›† 

Datasets æä¾›äº†åŠ è½½æœ¬åœ°å’Œè¿œç¨‹æ•°æ®é›†çš„æ–¹æ³•ã€‚å®ƒæ”¯æŒå‡ ç§å¸¸è§çš„æ•°æ®æ ¼å¼ï¼Œä¾‹å¦‚ï¼š

|       æ•°æ®æ ¼å¼      |    ç±»å‹å‚æ•°    |                         åŠ è½½çš„æŒ‡ä»¤                            |
| :----------------: | :------------: | :-----------------------------------------------------: |
|     CSV & TSV      | `csv` | `load_dataset("csv", data_files="my_file.csv")` |
|     Text files     | `text` | `load_dataset("text", data_files="my_file.txt")` |
| JSON & JSON Lines  | `json` | `load_dataset("json", data_files="my_file.jsonl")` |
| Pickled DataFrames | `pandas` | `load_dataset("pandas", data_files="my_dataframe.pkl")` |

å¦‚è¡¨æ‰€ç¤ºï¼Œå¯¹äºæ¯ç§æ•°æ®æ ¼å¼ï¼Œæˆ‘ä»¬åªéœ€è¦åœ¨ `load_dataset()` å‡½æ•°ä¸­æŒ‡å®šæ•°æ®çš„ç±»å‹ï¼Œå¹¶ä½¿ç”¨ `data_files` æŒ‡å®šä¸€ä¸ªæˆ–å¤šä¸ªæ–‡ä»¶çš„è·¯å¾„çš„å‚æ•°ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä»åŠ è½½æœ¬åœ°æ–‡ä»¶çš„æ•°æ®é›†å¼€å§‹ï¼›ç¨åï¼Œæˆ‘ä»¬å°†çœ‹åˆ°å¦‚ä½•ä½¿ç”¨è¿œç¨‹æ–‡ä»¶åšåŒæ ·çš„äº‹æƒ…ã€‚

### åŠ è½½æœ¬åœ°æ•°æ®é›† 

åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ [SQuAD-it æ•°æ®é›†](https://github.com/crux82/squad-it/)(https://github.com/crux82/squad-it/) ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºæ„å¤§åˆ©è¯­é—®ç­”çš„å¤§è§„æ¨¡æ•°æ®é›†ã€‚

è®­ç»ƒé›†å’Œæµ‹è¯•é›†éƒ½æ‰˜ç®¡åœ¨ GitHub ä¸Šï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥é€šè¿‡ `wget` å‘½ä»¤éå¸¸ç®€å•åœ°ä¸‹è½½å®ƒä»¬ï¼š

```python
!wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-train.json.gz
!wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-test.json.gz
```

è¿™å°†ä¸‹è½½ä¸¤ä¸ªåä¸º `SQuAD_it-train.json.gz` å’Œ `SQuAD_it-test.json.gz` çš„å‹ç¼©æ–‡ä»¶ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ Linux çš„ `gzip` å‘½ä»¤è§£å‹ä»–ä»¬ï¼š

```python
!gzip -dkv SQuAD_it-*.json.gz
```

```python
SQuAD_it-test.json.gz:	   87.4% -- replaced with SQuAD_it-test.json
SQuAD_it-train.json.gz:	   82.2% -- replaced with SQuAD_it-train.json
```

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°å‹ç¼©æ–‡ä»¶å·²ç»è¢«æ›¿æ¢ä¸º `SQuAD_it-train.json` å’Œ `SQuAD_it-test.json` ï¼Œå¹¶ä¸”æ•°æ®ä»¥ JSON æ ¼å¼å­˜å‚¨ã€‚

<div custom-style="Tip-green">

âœï¸ å¦‚æœä½ æƒ³çŸ¥é“ä¸ºä»€ä¹ˆä¸Šé¢çš„ shell å‘½ä»¤ä¸­æœ‰ä¸€ä¸ª `!` ï¼Œé‚£æ˜¯å› ä¸ºæˆ‘ä»¬æ˜¯åœ¨ Jupyter notebook ä¸­è¿è¡Œå®ƒä»¬ã€‚å¦‚æœä½ æƒ³åœ¨å‘½ä»¤è¡Œä¸­ä¸‹è½½å’Œè§£å‹ç¼©æ•°æ®é›†ï¼Œåªéœ€åˆ é™¤å‰ç¼€ `!` å³å¯ã€‚

</div>

å½“æˆ‘ä»¬ä½¿ç”¨ `load_dataset()` å‡½æ•°æ¥åŠ è½½ JSON æ–‡ä»¶æ—¶ï¼Œæˆ‘ä»¬éœ€è¦çŸ¥é“æˆ‘ä»¬æ˜¯åœ¨å¤„ç†æ™®é€šçš„ JSONï¼ˆç±»ä¼¼äºåµŒå¥—å­—å…¸ï¼‰è¿˜æ˜¯ JSON Linesï¼ˆæ¯ä¸€è¡Œéƒ½æ˜¯ä¸€ä¸ª JSONï¼‰ã€‚åƒè®¸å¤šé—®ç­”æ•°æ®é›†ä¸€æ ·ï¼ŒSQuAD-it ä½¿ç”¨çš„æ˜¯åµŒå¥—æ ¼å¼ï¼Œæ‰€æœ‰æ–‡æœ¬éƒ½å­˜å‚¨åœ¨ `data` å­—æ®µä¸­ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿ç”¨å‚æ•° `field` æ¥åŠ è½½æ•°æ®é›†ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```python
from datasets import load_dataset

squad_it_dataset = load_dataset("json", data_files="SQuAD_it-train.json", field="data")
```

é»˜è®¤æƒ…å†µä¸‹ï¼ŒåŠ è½½æœ¬åœ°æ–‡ä»¶ä¼šåˆ›å»ºä¸€ä¸ªå¸¦æœ‰ `train` æ ‡ç­¾çš„ `DatasetDict` å¯¹è±¡ã€‚æˆ‘ä»¬å¯ä»¥åœ¨è¿™é‡ŒæŸ¥çœ‹ä¸€ä¸‹ `squad_it_dataset` å¯¹è±¡ï¼š

```python
squad_it_dataset
```

```python
DatasetDict({
    train: Dataset({
        features: ['title', 'paragraphs'],
        num_rows: 442
    })
})
```

è¾“å‡ºäº†ä¸è®­ç»ƒé›†çš„è¡Œæ•°å’Œåˆ—åã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `train` æ ‡ç­¾æ¥æŸ¥çœ‹ä¸€ä¸ªç¤ºä¾‹ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```python
squad_it_dataset["train"][0]
```

```python
{
    "title": "Terremoto del Sichuan del 2008",
    "paragraphs": [
        {
            "context": "Il terremoto del Sichuan del 2008 o il terremoto...",
            "qas": [
                {
                    "answers": [{"answer_start": 29, "text": "2008"}],
                    "id": "56cdca7862d2951400fa6826",
                    "question": "In quale anno si Ã¨ verificato il terremoto nel Sichuan?",
                },
                ...
            ],
        },
        ...
    ],
}
```

å¾ˆå¥½ï¼Œæˆ‘ä»¬å·²ç»åŠ è½½äº†æˆ‘ä»¬çš„ç¬¬ä¸€ä¸ªæœ¬åœ°æ•°æ®é›†ï¼ä½†æ˜¯ï¼Œä¹Ÿä»…ä»…åŠ è½½äº†è®­ç»ƒé›†ï¼Œæˆ‘ä»¬çœŸæ­£æƒ³è¦çš„æ˜¯åŒ…å« `train` å’Œ `test` çš„ `DatasetDict` å¯¹è±¡ã€‚è¿™æ ·çš„è¯å°±å¯ä»¥ä½¿ç”¨ `Dataset.map()` å‡½æ•°åŒæ—¶å¤„ç†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å‘ `data_files` å‚æ•°è¾“å…¥ä¸€ä¸ªå­—å…¸ï¼Œå°†æ•°æ®é›†çš„æ ‡ç­¾åæ˜ å°„åˆ°ç›¸å…³è”çš„æ–‡ä»¶ï¼š

```python
data_files = {"train": "SQuAD_it-train.json", "test": "SQuAD_it-test.json"}
squad_it_dataset = load_dataset("json", data_files=data_files, field="data")
squad_it_dataset
```

```python
DatasetDict({
    train: Dataset({
        features: ['title', 'paragraphs'],
        num_rows: 442
    })
    test: Dataset({
        features: ['title', 'paragraphs'],
        num_rows: 48
    })
})
```

è¿™æ­£æ˜¯æˆ‘ä»¬æƒ³è¦çš„ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥åº”ç”¨å„ç§é¢„å¤„ç†æŠ€æœ¯æ¥æ¸…æ´—æ•°æ®ã€tokenize è¯„è®ºç­‰ç­‰ã€‚

<div custom-style="Tip-green">`load_dataset()` å‡½æ•°çš„ `data_files` å‚æ•°éå¸¸çµæ´»ï¼šå¯ä»¥æ˜¯å•ä¸ªæ–‡ä»¶è·¯å¾„ã€æ–‡ä»¶è·¯å¾„åˆ—è¡¨æˆ–è€…æ˜¯æ ‡ç­¾æ˜ å°„åˆ°æ–‡ä»¶è·¯å¾„çš„å­—å…¸ã€‚ä½ è¿˜å¯ä»¥æ ¹æ® Unix shell çš„è§„åˆ™ï¼Œå¯¹ç¬¦åˆæŒ‡å®šæ¨¡å¼çš„æ–‡ä»¶è¿›è¡Œé€šé…ï¼ˆä¾‹å¦‚ï¼Œä½ å¯ä»¥é€šè¿‡è®¾ç½® `data_files="*.JSON"` åŒ¹é…ç›®å½•ä¸­æ‰€æœ‰çš„ JSON æ–‡ä»¶ï¼‰ã€‚æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… [Datasets æ–‡æ¡£](https://huggingface.co/docs/datasets/v2.12.0/en/loading#local-and-remote-files)(https://huggingface.co/docs/datasets/v2.12.0/en/loading#local-and-remote-files) ã€‚

</div>

Datasets å®é™…ä¸Šæ”¯æŒè‡ªåŠ¨è§£å‹è¾“å…¥æ–‡ä»¶ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥è·³è¿‡ä½¿ç”¨ `gzip` ï¼Œç›´æ¥å°† `data_files` å‚æ•°è®¾ç½®ä¸ºå‹ç¼©æ–‡ä»¶ï¼š

```python
data_files = {"train": "SQuAD_it-train.json.gz", "test": "SQuAD_it-test.json.gz"}
squad_it_dataset = load_dataset("json", data_files=data_files, field="data")
```

å¦‚æœä½ ä¸æƒ³æ‰‹åŠ¨è§£å‹ç¼©è®¸å¤š GZIP æ–‡ä»¶ï¼Œè¿™ä¼šå¾ˆæœ‰ç”¨ã€‚è‡ªåŠ¨è§£å‹ä¹Ÿæ”¯æŒäºå…¶ä»–å¸¸è§æ ¼å¼ï¼Œå¦‚ ZIP å’Œ TARï¼Œå› æ­¤ä½ åªéœ€å°† `data_files` è®¾ç½®ä¸ºå‹ç¼©æ–‡ä»¶æ‰€åœ¨çš„è·¯å¾„ï¼Œæ¥ä¸‹æ¥å°±äº¤ç»™Datasets å§ï¼

ç°åœ¨ä½ çŸ¥é“å¦‚ä½•åœ¨ç¬”è®°æœ¬ç”µè„‘æˆ–å°å¼æœºä¸ŠåŠ è½½æœ¬åœ°æ–‡ä»¶ï¼Œè®©æˆ‘ä»¬æ¥çœ‹çœ‹åŠ è½½è¿œç¨‹æ–‡ä»¶ã€‚

### åŠ è½½è¿œç¨‹æ•°æ®é›† 

å¦‚æœä½ åœ¨å…¬å¸æ‹…ä»»æ•°æ®ç ”ç©¶å‘˜æˆ–ç¼–ç å‘˜ï¼Œé‚£ä¹ˆä½ è¦åˆ†æçš„æ•°æ®é›†å¾ˆæœ‰å¯èƒ½å­˜å‚¨åœ¨æŸä¸ªè¿œç¨‹æœåŠ¡å™¨ä¸Šã€‚å¹¸è¿çš„æ˜¯ï¼ŒåŠ è½½è¿œç¨‹æ–‡ä»¶å°±åƒåŠ è½½æœ¬åœ°æ–‡ä»¶ä¸€æ ·ç®€å•ï¼æˆ‘ä»¬åªéœ€è¦å°† `load_dataset()` çš„ `data_files` å‚æ•°æŒ‡å‘å­˜å‚¨è¿œç¨‹æ–‡ä»¶çš„ä¸€ä¸ªæˆ–å¤šä¸ª URLã€‚ä¾‹å¦‚ï¼Œå¯¹äºæ‰˜ç®¡åœ¨ GitHub ä¸Šçš„ SQuAD-it æ•°æ®é›†ï¼Œæˆ‘ä»¬å¯ä»¥å°† `data_files` è®¾ç½®ä¸ºæŒ‡å‘ `SQuAD_it-*.json.gz` çš„ç½‘å€ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```python
url = "https://github.com/crux82/squad-it/raw/master/"
data_files = {
    "train": url + "SQuAD_it-train.json.gz",
    "test": url + "SQuAD_it-test.json.gz",
}
squad_it_dataset = load_dataset("json", data_files=data_files, field="data")
```

è¿™å°†è¿”å›å’Œä¸Šé¢çš„æœ¬åœ°ä¾‹å­ç›¸åŒçš„ `DatasetDict` å¯¹è±¡ï¼Œä½†çœå»äº†æˆ‘ä»¬æ‰‹åŠ¨ä¸‹è½½å’Œè§£å‹ `SQuAD_it-*.json.gz` æ–‡ä»¶çš„æ­¥éª¤ã€‚è¿™æ˜¯æˆ‘ä»¬å¯¹åŠ è½½æœªæ‰˜ç®¡åœ¨ Hugging Face Hub çš„æ•°æ®é›†çš„å„ç§æ–¹æ³•çš„æ€»ç»“ã€‚æ—¢ç„¶æˆ‘ä»¬å·²ç»æœ‰äº†ä¸€ä¸ªå¯ä»¥ä½¿ç”¨çš„æ•°æ®é›†ï¼Œè®©æˆ‘ä»¬å¼€å§‹å¤§å±•èº«æ‰‹å§ï¼

<div custom-style="Tip-green">

âœï¸ **è¯•è¯•çœ‹ï¼** é€‰æ‹©æ‰˜ç®¡åœ¨ GitHub æˆ– [UCI æœºå™¨å­¦ä¹ ä»“åº“](https://archive.ics.uci.edu/ml/index.php)(https://archive.ics.uci.edu/ml/index.php) ä¸Šçš„å¦ä¸€ä¸ªæ•°æ®é›†å¹¶å°è¯•ä½¿ç”¨ä¸Šè¿°æŠ€æœ¯åœ¨æœ¬åœ°å’Œè¿œç¨‹åŠ è½½å®ƒã€‚å¦å¤–ï¼Œå¯ä»¥å°è¯•åŠ è½½ CSV æˆ–è€…æ–‡æœ¬æ ¼å¼å­˜å‚¨çš„æ•°æ®é›†ï¼ˆæœ‰å…³è¿™äº›æ ¼å¼çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜… [æ–‡æ¡£](https://huggingface.co/docs/datasets/loading.html#local-and-remote-files)(https://huggingface.co/docs/datasets/loading.html#local-and-remote-files) ï¼‰ã€‚

</div>


## 6.2 åˆ†å‰²å’Œæ•´ç†æ•°æ® 

å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œä½ å¤„ç†çš„æ•°æ®å¹¶ä¸èƒ½ç›´æ¥ç”¨äºè®­ç»ƒæ¨¡å‹ã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æ¢ç´¢Datasets æä¾›çš„å„ç§åŠŸèƒ½ï¼Œç”¨äºæ¸…æ´—ä½ çš„æ•°æ®é›†ã€‚

### åˆ†å‰²å’Œæ•´ç†æˆ‘ä»¬çš„æ•°æ® 

ä¸ Pandas ç±»ä¼¼ï¼ŒDatasets æä¾›äº†å¤šä¸ªå‡½æ•°æ¥æ“ä½œ `Dataset` å’Œ `DatasetDict` å¯¹è±¡ã€‚æˆ‘ä»¬åœ¨ç¬¬å››ç« å·²ç»é‡åˆ°äº† `Dataset.map()` æ–¹æ³•ï¼Œåœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æ¢ç´¢ä¸€äº›å…¶ä»–å¯ç”¨çš„å‡½æ•°ã€‚

åœ¨æœ¬ä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨æ‰˜ç®¡åœ¨ [åŠ å·å¤§å­¦æ¬§æ–‡åˆ†æ ¡æœºå™¨å­¦ä¹ å­˜å‚¨åº“](https://archive.ics.uci.edu/ml/index.php)(https://archive.ics.uci.edu/ml/index.php) çš„ [è¯ç‰©å®¡æŸ¥æ•°æ®é›†](https://archive.ics.uci.edu/ml/datasets/Drug+Review+Dataset+%28Drugs.com%29)(https://archive.ics.uci.edu/ml/datasets/Drug+Review+Dataset+%28Drugs.com%29) ï¼Œå…¶ä¸­åŒ…å«æ‚£è€…å¯¹å„ç§è¯ç‰©çš„è¯„è®ºï¼Œä»¥åŠæ­£åœ¨æ²»ç–—çš„ç—…æƒ…å’Œæ‚£è€…æ»¡æ„åº¦çš„ 10 æ˜Ÿè¯„ä»·ã€‚

é¦–å…ˆæˆ‘ä»¬éœ€è¦ä¸‹è½½å¹¶è§£å‹æ•°æ®ï¼Œå¯ä»¥é€šè¿‡ `wget` å’Œ `unzip` å‘½ä»¤ï¼š

```python
!wget "https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip"
!unzip drugsCom_raw.zip
```

ç”±äº TSV ä»…ä»…æ˜¯ CSV çš„ä¸€ä¸ªå˜ä½“ï¼Œå®ƒä½¿ç”¨åˆ¶è¡¨ç¬¦è€Œä¸æ˜¯é€—å·ä½œä¸ºåˆ†éš”ç¬¦ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨åŠ è½½ `csv` æ–‡ä»¶çš„ `load_dataset()` å‡½æ•°å¹¶æŒ‡å®šåˆ†éš”ç¬¦ï¼Œæ¥åŠ è½½è¿™äº›æ–‡ä»¶ï¼š

```python
from datasets import load_dataset

data_files = {"train": "drugsComTrain_raw.tsv", "test": "drugsComTest_raw.tsv"}
## \t åœ¨pythonä¸­æ˜¯åˆ¶è¡¨ç¬¦çš„æ„æ€
drug_dataset = load_dataset("csv", data_files=data_files, delimiter="\t")
```

åœ¨è¿›è¡Œæ•°æ®åˆ†ææ—¶ï¼Œè·å–ä¸€ä¸ªå°çš„éšæœºæ ·æœ¬ä»¥å¿«é€Ÿäº†è§£ä½ æ­£åœ¨å¤„ç†çš„æ•°æ®ç‰¹ç‚¹æ˜¯ä¸€ç§å¥½çš„å®è·µã€‚åœ¨æ•°æ®é›†ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡é“¾æ¥ `Dataset.shuffle()` å’Œ `Dataset.select()` å‡½æ•°åˆ›å»ºä¸€ä¸ªéšæœºçš„æ ·æœ¬ï¼š

```python
drug_sample = drug_dataset["train"].shuffle(seed=42).select(range(1000))
## ç»†çœ‹å‰å‡ ä¸ªä¾‹å­
drug_sample[:3]
```

```python
{'Unnamed: 0': [87571, 178045, 80482],
 'drugName': ['Naproxen', 'Duloxetine', 'Mobic'],
 'condition': ['Gout, Acute', 'ibromyalgia', 'Inflammatory Conditions'],
 'review': ['"like the previous person mention, I&#039;m a strong believer of aleve, it works faster for my gout than the prescription meds I take. No more going to the doctor for refills.....Aleve works!"',
  '"I have taken Cymbalta for about a year and a half for fibromyalgia pain. It is great\r\nas a pain reducer and an anti-depressant, however, the side effects outweighed \r\nany benefit I got from it. I had trouble with restlessness, being tired constantly,\r\ndizziness, dry mouth, numbness and tingling in my feet, and horrible sweating. I am\r\nbeing weaned off of it now. Went from 60 mg to 30mg and now to 15 mg. I will be\r\noff completely in about a week. The fibro pain is coming back, but I would rather deal with it than the side effects."',
  '"I have been taking Mobic for over a year with no side effects other than an elevated blood pressure.  I had severe knee and ankle pain which completely went away after taking Mobic.  I attempted to stop the medication however pain returned after a few days."'],
 'rating': [9.0, 3.0, 10.0],
 'date': ['September 2, 2015', 'November 7, 2011', 'June 5, 2013'],
 'usefulCount': [36, 13, 128]}
```

è¯·æ³¨æ„ï¼Œå‡ºäºå¯ä»¥å¤ç°çš„ç›®çš„ï¼Œæˆ‘ä»¬å·²å°†åœ¨ `Dataset.shuffle()` è®¾å®šäº†å›ºå®šçš„éšæœºæ•°ç§å­ã€‚ `Dataset.select()` éœ€è¦ä¸€ä¸ªå¯è¿­ä»£çš„ç´¢å¼•ï¼Œæ‰€ä»¥æˆ‘ä»¬ä¼ é€’äº† `range(1000)` ä»éšæœºæ‰“ä¹±çš„æ•°æ®é›†ä¸­æŠ½å–å‰ 1,000 ä¸ªç¤ºä¾‹ã€‚ä»æŠ½å–çš„æ•°æ®ä¸­ï¼Œæˆ‘ä»¬å·²ç»å¯ä»¥çœ‹åˆ°æˆ‘ä»¬æ•°æ®é›†ä¸­æœ‰ä¸€äº›ç‰¹æ®Šçš„åœ°æ–¹ï¼š

* `Unnamed: 0` è¿™åˆ—çœ‹èµ·æ¥å¾ˆåƒæ¯ä¸ªæ‚£è€…çš„åŒ¿å IDã€‚
* `condition` åˆ—åŒ…å«äº†å¤§å°å†™æ··åˆçš„æ ‡ç­¾ã€‚
* è¯„è®ºé•¿çŸ­ä¸ä¸€ï¼Œæ··åˆæœ‰ Python è¡Œåˆ†éš”ç¬¦ ï¼ˆ `\r\n` ï¼‰ ä»¥åŠ HTML å­—ç¬¦ä»£ç ï¼Œå¦‚ `&\#039;` ã€‚
  

è®©æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬å¦‚ä½•ä½¿ç”¨ Datasets æ¥å¤„ç†è¿™äº›é—®é¢˜ã€‚ä¸ºäº†éªŒè¯ `Unnamed: 0` åˆ—å­˜å‚¨çš„æ˜¯æ‚£è€… ID çš„çŒœæƒ³ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `Dataset.unique()` å‡½æ•°æ¥éªŒè¯åŒ¿å ID çš„æ•°é‡æ˜¯å¦ä¸åˆ†å‰²åæ¯ä¸ªåˆ†ç»„ä¸­çš„è¡Œæ•°åŒ¹é…ï¼š

```python
for split in drug_dataset.keys():
    assert len(drug_dataset[split]) == len(drug_dataset[split].unique("Unnamed: 0"))
```

è¿™ä¼¼ä¹è¯å®äº†æˆ‘ä»¬çš„å‡è®¾ï¼Œæ‰€ä»¥è®©æˆ‘ä»¬æŠŠ `Unnamed: 0` åˆ—é‡å‘½åä¸ºæ‚£è€…çš„ idã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `DatasetDict.rename_column()` å‡½æ•°æ¥ä¸€æ¬¡æ€§é‡å‘½åä¸¤ä¸ªåˆ†ç»„ï¼š

```python
drug_dataset = drug_dataset.rename_column(
    original_column_name="Unnamed: 0", new_column_name="patient_id"
)
drug_dataset
```

```python
DatasetDict({
    train: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],
        num_rows: 161297
    })
    test: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount'],
        num_rows: 53766
    })
})
```

<div custom-style="Tip-green">

âœï¸  **è¯•è¯•çœ‹ï¼** ä½¿ç”¨ `Dataset.unique()` å‡½æ•°æŸ¥æ‰¾è®­ç»ƒå’Œæµ‹è¯•é›†ä¸­çš„ç‰¹å®šè¯ç‰©å’Œç—…ç—‡çš„æ•°é‡ã€‚

</div>

æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬ä½¿ç”¨ `Dataset.map()` æ¥è§„èŒƒæ‰€æœ‰çš„ `condition` æ ‡ç­¾ã€‚æ­£å¦‚æˆ‘ä»¬åœ¨ç¬¬å››ç« ä¸­å¤„ç† tokenizer ä¸€æ ·ï¼Œæˆ‘ä»¬å¯ä»¥å®šä¹‰ä¸€ä¸ªç®€å•çš„å‡½æ•°ï¼Œå¯ä»¥å°†è¯¥å‡½æ•°åº”ç”¨äº `drug_dataset` æ¯ä¸ªåˆ†ç»„çš„æ‰€æœ‰è¡Œï¼š

```python
def lowercase_condition(example):
    return {"condition": example["condition"].lower()}

drug_dataset.map(lowercase_condition)
```

```python
AttributeError: 'NoneType' object has no attribute 'lower'
```

å“¦ä¸ï¼Œæˆ‘ä»¬çš„ map åŠŸèƒ½é‡åˆ°äº†é—®é¢˜ï¼ä»é”™è¯¯ä¸­æˆ‘ä»¬å¯ä»¥æ¨æ–­å‡º `condition` åˆ—å­˜åœ¨ `None` ï¼Œä¸èƒ½è½¬æ¢ä¸ºå°å†™ï¼Œå› ä¸ºå®ƒä»¬ä¸æ˜¯å­—ç¬¦ä¸²ã€‚è®©æˆ‘ä»¬ä½¿ç”¨ `Dataset.filter()` åˆ é™¤è¿™äº›è¡Œ å…¶å·¥ä½œæ–¹å¼ç±»ä¼¼äº `Dataset.map()` ã€‚ä¾‹å¦‚ï¼š

```python
def filter_nones(x):
    return x["condition"] is not None
```

ç„¶åè¿è¡Œ `drug_dataset.filter(filter_nones)` ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ lambda å‡½æ•°åœ¨ä¸€è¡Œä»£ç å®Œæˆè¿™ä¸ªä»»åŠ¡ã€‚åœ¨ Pyhton ä¸­ï¼Œlambda å‡½æ•°æ˜¯ä½ æ— éœ€æ˜ç¡®å‘½åå³å¯ä½¿ç”¨çš„å¾®å‡½æ•°ï¼ˆåŒ¿åå‡½æ•°ï¼‰ã€‚å®ƒä»¬ä¸€èˆ¬é‡‡ç”¨å¦‚ä¸‹å½¢å¼ï¼š

```python
lambda <arguments> : <expression>
```

å…¶ä¸­ `lambda` æ˜¯ Python çš„ç‰¹æ®Š [å…³é”®å­—](https://docs.python.org/3/reference/lexical_analysis.html#keywords)(https://docs.python.org/3/reference/lexical_analysis.html#keywords) ä¹‹ä¸€ï¼Œ `arguments` æ˜¯ä»¥é€—å·è¿›è¡Œåˆ†éš”çš„å‡½æ•°å‚æ•°çš„åˆ—è¡¨/é›†åˆï¼Œ `expression` ä»£è¡¨ä½ å¸Œæœ›æ‰§è¡Œçš„æ“ä½œã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥å®šä¹‰ä¸€ä¸ªç®€å•çš„ lambda å‡½æ•°æ¥å¯¹ä¸€ä¸ªæ•°å­—è¿›è¡Œå¹³æ–¹ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```python
lambda x : x * x
```

æˆ‘ä»¬éœ€è¦å°†è¦è¾“å…¥æ”¾åœ¨æ‹¬å·ä¸­ï¼š

```python
(lambda x: x * x)(3)
```

```python
9
```

åŒæ ·ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿ç”¨é€—å·åˆ†éš”æ¥å®šä¹‰å¸¦æœ‰å¤šä¸ªå‚æ•°çš„ lambda å‡½æ•°ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥æŒ‰å¦‚ä¸‹æ–¹å¼è®¡ç®—ä¸‰è§’å½¢çš„é¢ç§¯ï¼š

```python
(lambda base, height: 0.5 * base * height)(4, 8)
```

```python
16.0
```

å½“ä½ æƒ³å®šä¹‰å°å‹ã€ä¸€æ¬¡æ€§ä½¿ç”¨çš„å‡½æ•°æ—¶ï¼Œlambda å‡½æ•°éå¸¸æ–¹ä¾¿ï¼ˆæœ‰å…³å®ƒä»¬çš„æ›´å¤šä¿¡æ¯ï¼Œæˆ‘ä»¬å»ºè®®é˜…è¯» Andre Burgaud å†™çš„ [çœŸæ­£çš„Pythonæ•™ç¨‹](https://realpython.com/python-lambda/)(https://realpython.com/python-lambda/) ï¼‰ã€‚åœ¨Datasets ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ lambda å‡½æ•°æ¥å®šä¹‰ç®€å•çš„æ˜ å°„å’Œè¿‡æ»¤æ“ä½œï¼Œæ‰€ä»¥è®©æˆ‘ä»¬ä½¿ç”¨è¿™ä¸ªæŠ€å·§æ¥åˆ é™¤æˆ‘ä»¬æ•°æ®é›†ä¸­çš„ `None` æ¡ç›®ï¼š

```python
drug_dataset = drug_dataset.filter(lambda x: x["condition"] is not None)
``` 

`None` æ¡ç›®åˆ é™¤ä¹‹å,æˆ‘ä»¬å¯ä»¥è§„èŒƒæˆ‘ä»¬çš„ `condition` åˆ—:

```python
drug_dataset = drug_dataset.map(lowercase_condition)
## æ£€æŸ¥ä¸€ä¸‹è½¬æ¢åçš„ç»“æœ
drug_dataset["train"]["condition"][:3]
```

```python
['left ventricular dysfunction', 'adhd', 'birth control']
```

æœ‰ç”¨ï¼ç°åœ¨æˆ‘ä»¬å·²ç»æ¸…ç†äº†æ ‡ç­¾ï¼Œè®©æˆ‘ä»¬æ¥çœ‹çœ‹æ¸…æ´—åçš„è¯„è®ºæ–‡æœ¬ã€‚

### åˆ›å»ºæ–°çš„åˆ— 

æ¯å½“æˆ‘ä»¬å¤„ç†å®¢æˆ·è¯„è®ºæ—¶ï¼Œä¸€ä¸ªå¥½çš„ä¹ æƒ¯æ˜¯æ£€æŸ¥æ¯ä¸€æ¡è¯„è®ºçš„å­—æ•°ã€‚è¯„è®ºå¯èƒ½åªæ˜¯ä¸€ä¸ªè¯ï¼Œæ¯”å¦‚â€œå¤ªæ£’äº†ï¼â€æˆ–åŒ…å«æ•°åƒå­—çš„å®Œæ•´æ–‡ç« ã€‚åœ¨ä¸åŒçš„ä½¿ç”¨åœºæ™¯ï¼Œä½ éœ€è¦ä»¥ä¸åŒçš„æ–¹å¼å¤„ç†è¿™äº›æç«¯æƒ…å†µã€‚ä¸ºäº†è®¡ç®—æ¯æ¡è¯„è®ºä¸­çš„å•è¯æ•°ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ç©ºæ ¼åˆ†å‰²æ¯ä¸ªæ–‡æœ¬è¿›è¡Œç²—ç•¥ç»Ÿè®¡ã€‚

è®©æˆ‘ä»¬å®šä¹‰ä¸€ä¸ªç®€å•çš„å‡½æ•°æ¥è®¡ç®—ï¼Œè®¡ç®—æ¯æ¡è¯„è®ºçš„å­—æ•°ï¼š

```python
def compute_review_length(example):
    return {"review_length": len(example["review"].split())}
```

ä¸åŒäºæˆ‘ä»¬çš„ `lowercase_condition()` å‡½æ•°ï¼Œ `compute_review_length()` è¿”å›ä¸€ä¸ªå­—å…¸ï¼Œå…¶é”®å¹¶ä¸å¯¹åº”æ•°æ®é›†ä¸­çš„æŸä¸€åˆ—åç§°ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå½“ `compute_review_length()` ä¼ é€’ç»™ `Dataset.map()` æ—¶ï¼Œå®ƒå°†åº”ç”¨äºæ•°æ®é›†ä¸­çš„æ‰€æœ‰è¡Œï¼Œæœ€åä¼šåˆ›å»ºæ–°çš„ `review_length` åˆ—ï¼š

```python
drug_dataset = drug_dataset.map(compute_review_length)
## æ£€æŸ¥ç¬¬ä¸€ä¸ªè®­ç»ƒæ ·ä¾‹
drug_dataset["train"][0]
```

```python
{'patient_id': 206461,
 'drugName': 'Valsartan',
 'condition': 'left ventricular dysfunction',
 'review': '"It has no side effect, I take it in combination of Bystolic 5 Mg and Fish Oil"',
 'rating': 9.0,
 'date': 'May 20, 2012',
 'usefulCount': 27,
 'review_length': 17}
```

æ­£å¦‚é¢„æœŸçš„é‚£æ ·ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¸€ä¸ª `review_length` åˆ—å·²æ·»åŠ åˆ°æˆ‘ä»¬çš„è®­ç»ƒé›†ä¸­ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `Dataset.sort()` å¯¹è¿™ä¸ªæ–°åˆ—è¿›è¡Œæ’åºï¼Œç„¶åæŸ¥çœ‹ä¸€ä¸‹æç«¯é•¿åº¦çš„è¯„è®ºæ˜¯ä»€ä¹ˆæ ·çš„ï¼š

```python
drug_dataset["train"].sort("review_length")[:3]
```

```python
{'patient_id': [103488, 23627, 20558],
 'drugName': ['Loestrin 21 1 / 20', 'Chlorzoxazone', 'Nucynta'],
 'condition': ['birth control', 'muscle spasm', 'pain'],
 'review': ['"Excellent."', '"useless"', '"ok"'],
 'rating': [10.0, 1.0, 6.0],
 'date': ['November 4, 2008', 'March 24, 2017', 'August 20, 2016'],
 'usefulCount': [5, 2, 10],
 'review_length': [1, 1, 1]}
```

æ­£å¦‚æˆ‘ä»¬æ‰€çŒœæƒ³çš„é‚£æ ·ï¼Œæœ‰äº›è¯„è®ºåªåŒ…å«ä¸€ä¸ªè¯ï¼Œè™½ç„¶è¿™å¯¹äºæƒ…æ„Ÿåˆ†ææ¥è¯´è¿˜å¯ä»¥æ¥å—ï¼Œä½†å¦‚æœæˆ‘ä»¬æƒ³è¦é¢„æµ‹ç—…æƒ…ï¼Œé‚£ä¹ˆå®ƒæ‰€æä¾›çš„ä¿¡æ¯å°±ä¸å¤Ÿä¸°å¯Œäº†ã€‚

<div custom-style="Tip-green">

ğŸ™‹å‘æ•°æ®é›†æ·»åŠ æ–°åˆ—çš„å¦ä¸€ç§æ–¹æ³•æ˜¯ä½¿ç”¨å‡½æ•° `Dataset.add_column()` ï¼Œåœ¨ä½¿ç”¨å®ƒæ—¶ä½ å¯ä»¥é€šè¿‡ Python åˆ—è¡¨æˆ– NumPy æ•°ç»„çš„æ–¹å¼æä¾›æ•°æ®ï¼Œåœ¨ä¸é€‚åˆä½¿ç”¨ `Dataset.map()` æƒ…å†µä¸‹å¯ä»¥å¾ˆæ–¹ä¾¿ã€‚

</div>

è®©æˆ‘ä»¬ä½¿ç”¨ `Dataset.filter()` åŠŸèƒ½æ¥åˆ é™¤åŒ…å«å°‘äº 30 ä¸ªå•è¯çš„è¯„è®ºã€‚è¿™ä¸æˆ‘ä»¬è¿‡æ»¤ `condition` åˆ—çš„å¤„ç†æ–¹å¼ç›¸ä¼¼ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡è®¾å®šè¯„è®ºé•¿åº¦çš„æœ€å°é˜ˆå€¼ï¼Œç­›é€‰å‡ºè¿‡çŸ­çš„è¯„è®ºï¼š

```python
drug_dataset = drug_dataset.filter(lambda x: x["review_length"] > 30)
print(drug_dataset.num_rows)
```

```python
{'train': 138514, 'test': 46108}
```

å¦‚ä½ æ‰€è§ï¼Œè¿™å·²ç»ä»æˆ‘ä»¬çš„åŸå§‹è®­ç»ƒå’Œæµ‹è¯•é›†ä¸­åˆ é™¤äº†å¤§çº¦ 15ï¼… çš„è¯„è®ºã€‚

<div custom-style="Tip-green">

âœï¸ **è¯•è¯•çœ‹ï¼**ä½¿ç”¨ `Dataset.sort()` å‡½æ•°æŸ¥çœ‹å•è¯æ•°æœ€å¤šçš„è¯„è®ºã€‚ä½ å¯ä»¥å‚é˜… [æ–‡æ¡£](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.sort)(https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.sort) äº†è§£å¦‚ä½•æŒ‰ç…§è¯„è®ºçš„é•¿åº¦é™åºæ’åºã€‚

</div>

æˆ‘ä»¬éœ€è¦å¤„ç†çš„æœ€åä¸€ä»¶äº‹æ˜¯å¤„ç†è¯„è®ºä¸­çš„ HTML å­—ç¬¦ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ Python çš„ `html` æ¨¡å—æ¥è§£ç è¿™äº›å­—ç¬¦ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```python
import html

text = "I&#039;m a transformer called BERT"
html.unescape(text)
```

```python
"I'm a transformer called BERT"
```

æˆ‘ä»¬å°†ä½¿ç”¨ `Dataset.map()` å¯¹æˆ‘ä»¬è¯­æ–™åº“ä¸­çš„æ‰€æœ‰ HTML å­—ç¬¦è¿›è¡Œè§£ç ï¼š

```python
drug_dataset = drug_dataset.map(lambda x: {"review": html.unescape(x["review"])})
```

å¦‚ä½ æ‰€è§ï¼Œ `Dataset.map()` æ–¹æ³•å¯¹äºå¤„ç†æ•°æ®éå¸¸æœ‰ç”¨ï¼Œå³ä½¿æˆ‘ä»¬è¿˜æ²¡æœ‰å®Œå…¨äº†è§£å®ƒçš„æ‰€æœ‰åŠŸèƒ½ï¼

### `map()` æ–¹æ³•çš„è¶…çº§åŠ é€Ÿ 

`Dataset.map()` æ–¹æ³•æœ‰ä¸€ä¸ª `batched` å‚æ•°ï¼Œå¦‚æœè®¾ç½®ä¸º `True` ï¼Œmap å‡½æ•°å°†ä¼šåˆ†æ‰¹æ‰§è¡Œæ‰€éœ€è¦è¿›è¡Œçš„æ“ä½œï¼ˆæ‰¹é‡å¤§å°æ˜¯å¯é…ç½®çš„ï¼Œä½†é»˜è®¤ä¸º 1,000ï¼‰ã€‚ä¾‹å¦‚ï¼Œä¹‹å‰å¯¹æ‰€æœ‰ HTML è¿›è¡Œè§£ç çš„ map å‡½æ•°è¿è¡Œéœ€è¦ä¸€äº›æ—¶é—´ï¼ˆä½ å¯ä»¥ä»è¿›åº¦æ¡ä¸­çœ‹åˆ°æ‰€éœ€çš„æ—¶é—´ï¼‰ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿ç”¨åˆ—è¡¨æ¨å¯¼åŒæ—¶å¤„ç†å¤šä¸ªå…ƒç´ æ¥åŠ é€Ÿã€‚

å½“ä½ åœ¨ä½¿ç”¨ `Dataset.map()` å‡½æ•°æ—¶è®¾å®š `batched=True` ã€‚è¯¥å‡½æ•°éœ€è¦æ¥æ”¶ä¸€ä¸ªåŒ…å«æ•°æ®é›†å­—æ®µçš„å­—å…¸ï¼Œå­—å…¸çš„å€¼æ˜¯ä¸€ä¸ªåˆ—è¡¨ã€‚ä¾‹å¦‚ï¼Œè¿™æ˜¯ä½¿ç”¨ `batched=True` å¯¹æ‰€æœ‰ HTML å­—ç¬¦è¿›è¡Œè§£ç çš„æ–¹æ³• 

```python
new_drug_dataset = drug_dataset.map(
    lambda x: {"review": [html.unescape(o) for o in x["review"]]}, batched=True
)
```

å¦‚æœä½ åœ¨ç¬”è®°æœ¬ä¸­è¿è¡Œæ­¤ä»£ç ï¼Œä½ ä¼šçœ‹åˆ°æ­¤å‘½ä»¤çš„æ‰§è¡Œé€Ÿåº¦æ¯”å‰ä¸€ä¸ªå‘½ä»¤å¿«å¾—å¤šã€‚è¿™ä¸æ˜¯å› ä¸ºæˆ‘ä»¬çš„è¯„è®ºå·²ç»æ˜¯å¤„ç†è¿‡çš„â€”â€”å¦‚æœä½ é‡æ–°æ‰§è¡Œä¸Šä¸€èŠ‚çš„æŒ‡ä»¤ï¼ˆæ²¡æœ‰ `batched=True` ï¼‰ï¼Œå®ƒå°†èŠ±è´¹ä¸ä¹‹å‰ç›¸åŒçš„æ—¶é—´ã€‚è¿™æ˜¯å› ä¸ºåˆ—è¡¨æ¨å¯¼å¼é€šå¸¸æ¯”åœ¨åŒä¸€ä»£ç ä¸­ç”¨ `for` å¾ªç¯æ‰§è¡Œç›¸åŒçš„ä»£ç æ›´å¿«ï¼Œå¹¶ä¸”æˆ‘ä»¬è¿˜é€šè¿‡åŒæ—¶è®¿é—®å¤šä¸ªå…ƒç´ è€Œä¸æ˜¯ä¸€ä¸ªä¸€ä¸ªæ¥å¤„ç†æ¥æé«˜å¤„ç†çš„é€Ÿåº¦ã€‚

åœ¨ç¬¬ä¸ƒç« æˆ‘ä»¬å°†é‡åˆ°çš„â€œå¿«é€Ÿâ€ tokenizer å®ƒå¯ä»¥å¿«é€Ÿå¯¹é•¿æ–‡æœ¬åˆ—è¡¨è¿›è¡Œ tokenizationã€‚ä½¿ç”¨ `Dataset.map()` æ­é… `batched=True` å‚æ•°æ˜¯åŠ é€Ÿçš„å…³é”®ã€‚ä¾‹å¦‚ï¼Œè¦ä½¿ç”¨å¿«é€Ÿ tokenizer å¯¹æ‰€æœ‰è¯ç‰©è¯„è®º tokenizationï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å¦‚ä¸‹çš„å‡½æ•°ï¼š

```python
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

def tokenize_function(examples):
    return tokenizer(examples["review"], truncation=True)
```

æ­£å¦‚æˆ‘ä»¬åœ¨ç¬¬å››ç« æ‰€çœ‹åˆ°çš„ï¼Œæˆ‘ä»¬åŸæœ¬å°±å¯ä»¥å°†ä¸€ä¸ªæˆ–å¤šä¸ªç¤ºä¾‹ä¼ é€’ç»™ tokenizer å› æ­¤åœ¨ `batched=True` æ˜¯ä¸€ä¸ªéå¿…é¡»çš„é€‰é¡¹ã€‚è®©æˆ‘ä»¬å€Ÿæ­¤æœºä¼šæ¯”è¾ƒä¸åŒé€‰é¡¹çš„æ€§èƒ½ã€‚åœ¨ notebook ä¸­ï¼Œä½ å¯ä»¥åœ¨ä½ è¦æµ‹é‡çš„ä»£ç è¡Œä¹‹å‰æ·»åŠ  `%time` æ¥è®°å½•è¯¥è¡Œè¿è¡Œæ‰€æ¶ˆè€—çš„æ—¶é—´ï¼š

```python
%time tokenized_dataset = drug_dataset.map(tokenize_function, batched=True)
```

ä½ ä¹Ÿå¯ä»¥å°† `%%time` æ”¾ç½®åœ¨å•å…ƒæ ¼å¼€å¤´æ¥ç»Ÿè®¡æ•´ä¸ªå•å…ƒæ ¼çš„æ‰§è¡Œæ—¶é—´ã€‚åœ¨æˆ‘ä»¬çš„ç¡¬ä»¶ä¸Šï¼Œè¯¥æŒ‡ä»¤æ˜¾ç¤º 10.8 ç§’ï¼ˆè¿™å°±æ˜¯çœŸæ­£ï¼ˆWall timeï¼‰çš„æ‰§è¡Œæ—¶é—´ï¼‰ã€‚

<div custom-style="Tip-green">

âœï¸ **è¯•è¯•çœ‹ï¼** åœ¨æœ‰å’Œæ—  `batched=True` çš„æƒ…å†µä¸‹æ‰§è¡Œç›¸åŒçš„æŒ‡ä»¤ï¼Œç„¶åè¯•è¯•æ…¢é€Ÿ tokenizer ï¼ˆåœ¨ `AutoTokenizer.from_pretrained()` æ–¹æ³•ä¸­æ·»åŠ  `use_fast=False` ï¼‰ï¼Œè¿™æ ·ä½ å°±å¯ä»¥çœ‹çœ‹åœ¨ä½ çš„ç”µè„‘ä¸Šå®ƒéœ€è¦å¤šé•¿çš„æ—¶é—´ã€‚

</div>

ä»¥ä¸‹æ˜¯æˆ‘ä»¬åœ¨ä½¿ç”¨å’Œä¸ä½¿ç”¨æ‰¹å¤„ç†æ—¶ä½¿ç”¨å¿«é€Ÿå’Œæ…¢é€Ÿ tokenizer è·å¾—çš„ç»“æœï¼š

é€‰é¡¹         | å¿«é€Ÿ tokenizer | æ…¢é€Ÿ tokenizer
:--------------:|:--------------:|:-------------:
`batched=True` | 10.8s          | 4min41s 
`batched=False` | 59.2s          | 5min3s

è¿™æ„å‘³ç€ä½¿ç”¨å¿«é€Ÿ tokenizer é…åˆ `batched=True` é€‰é¡¹æ¯”æ²¡æœ‰æ‰¹å¤„ç†çš„æ…¢é€Ÿç‰ˆæœ¬å¿« 30 å€â€”â€”è¿™çœŸçš„å¤ª Amazing äº†ï¼è¿™å°±æ˜¯ä¸ºä»€ä¹ˆåœ¨ä½¿ç”¨ `AutoTokenizer` æ—¶ï¼Œå°†ä¼šé»˜è®¤ä½¿ç”¨ `use_fast=True` çš„ä¸»è¦åŸå›  ï¼ˆä»¥åŠä¸ºä»€ä¹ˆå®ƒä»¬è¢«ç§°ä¸ºâ€œå¿«é€Ÿâ€çš„åŸå› ï¼‰ã€‚ä»–ä»¬èƒ½å¤Ÿå®ç°è¿™æ ·çš„åŠ é€Ÿï¼Œå› ä¸ºåœ¨åº•å±‚çš„ tokenization ä»£ç æ˜¯åœ¨ Rust ä¸­æ‰§è¡Œçš„ï¼ŒRust æ˜¯ä¸€ç§å¯ä»¥æ˜“äºå¹¶è¡ŒåŒ–æ‰§è¡Œçš„è¯­è¨€ã€‚

å¹¶è¡ŒåŒ–ä¹Ÿæ˜¯å¿«é€Ÿ tokenizer é€šè¿‡æ‰¹å¤„ç†å®ç°è¿‘ 6 å€åŠ é€Ÿçš„åŸå› ï¼šå•ä¸ª tokenization æ“ä½œæ˜¯ä¸èƒ½å¹¶è¡Œçš„ï¼Œä½†æ˜¯å½“ä½ æƒ³åŒæ—¶å¯¹å¤§é‡æ–‡æœ¬è¿›è¡Œ tokenization æ—¶ï¼Œä½ å¯ä»¥å°†æ‰§è¡Œè¿‡ç¨‹æ‹†åˆ†ä¸ºå¤šä¸ªè¿›ç¨‹ï¼Œæ¯ä¸ªè¿›ç¨‹è´Ÿè´£å¤„ç†è‡ªå·±çš„æ–‡æœ¬ã€‚ `Dataset.map()` ä¹Ÿæœ‰ä¸€äº›è‡ªå·±çš„å¹¶è¡ŒåŒ–èƒ½åŠ›ã€‚å°½ç®¡å®ƒä»¬æ²¡æœ‰ Rust æä¾›æ”¯æŒï¼Œä½†å®ƒä»¬ä»ç„¶å¯ä»¥å¸®åŠ©æ…¢é€Ÿ tokenizer åŠ é€Ÿï¼ˆå°¤å…¶æ˜¯å½“ä½ ä½¿ç”¨çš„ tokenizer æ²¡æœ‰å¿«é€Ÿç‰ˆæœ¬æ—¶ï¼‰ã€‚è¦å¯ç”¨å¤šè¿›ç¨‹å¤„ç†ï¼Œè¯·åœ¨è°ƒç”¨ `Dataset.map()` æ—¶ä½¿ç”¨ `num_proc` å‚æ•°å¹¶æŒ‡å®šè¦åœ¨è°ƒç”¨ä¸­ä½¿ç”¨çš„è¿›ç¨‹æ•° 

```python
slow_tokenizer = AutoTokenizer.from_pretrained("bert-base-cased", use_fast=False)

def slow_tokenize_function(examples):
    return slow_tokenizer(examples["review"], truncation=True)

tokenized_dataset = drug_dataset.map(slow_tokenize_function, batched=True, num_proc=8)
```

ä½ å¯ä»¥å¯¹å¤„ç†è¿›è¡Œä¸€äº›è®¡æ—¶çš„è¯•éªŒï¼Œä»¥ç¡®å®šæœ€ä½³è¿›ç¨‹æ•°ï¼›åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œ8 ä¼¼ä¹äº§ç”Ÿäº†æœ€å¥½çš„é€Ÿåº¦å¢ç›Šã€‚ä»¥ä¸‹æ˜¯æˆ‘ä»¬åœ¨æœ‰æ— å¤šè¿›ç¨‹å¤„ç†çš„æƒ…å†µä¸‹ï¼Œå¾—åˆ°çš„ç»“æœï¼š

é€‰é¡¹         | å¿«é€Ÿ tokenizer  | æ…¢é€Ÿ tokenizer 
:--------------:|:--------------:|:-------------: 
`batched=True` | 10.8s          | 4min41s
`batched=False` | 59.2s          | 5min3s 
`batched=True` , `num_proc=8` | 6.52s          | 41.3s
`batched=False` , `num_proc=8` | 9.49s          | 45.2s

è¿™ä¸ªç»“æœå¯¹äºæ…¢é€Ÿåˆ†è¯å™¨æ¥è¯´æ˜¯æ›´åŠ å‹å¥½äº†ï¼Œä½†å¿«é€Ÿåˆ†è¯å™¨çš„æ€§èƒ½ä¹Ÿå¾—åˆ°äº†æ˜¾è‘—æå‡ã€‚ä½†æ˜¯è¯·æ³¨æ„ï¼Œæƒ…å†µå¹¶éæ€»æ˜¯å¦‚æ­¤â€”å¯¹äº `num_proc` çš„å…¶ä»–å€¼ï¼Œåœ¨æˆ‘ä»¬çš„æµ‹è¯•ä¸­ï¼Œä½¿ç”¨ `batched=True` è€Œä¸å¸¦æœ‰ `num_proc` å‚æ•°çš„é€‰é¡¹å¤„ç†èµ·æ¥æ›´å¿«ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬å¹¶ä¸æ¨èåœ¨å¿«é€Ÿ tokenizer å’Œ `batched=True` çš„æƒ…å†µä¸‹ä½¿ç”¨ Python çš„å¤šè¿›ç¨‹å¤„ç†ã€‚

<div custom-style="Tip-green">

é€šå¸¸æ¥è¯´ï¼Œä½¿ç”¨ `num_proc` ä»¥åŠ å¿«å¤„ç†é€Ÿåº¦é€šå¸¸æ˜¯ä¸€ä¸ªå¥½ä¸»æ„ï¼Œåªè¦ä½ ä½¿ç”¨çš„å‡½æ•°æœ¬èº«æ²¡æœ‰è¿›è¡ŒæŸç§ç±»å‹çš„å¤šè¿›ç¨‹å¤„ç†ã€‚

</div>

å°†æ‰€æœ‰è¿™äº›åŠŸèƒ½æµ“ç¼©åˆ°ä¸€ä¸ªæ–¹æ³•ä¸­å·²ç»éå¸¸äº†ä¸èµ·ï¼Œä½†æ˜¯è¿˜æœ‰æ›´å¤šï¼ä½¿ç”¨ `Dataset.map()` å’Œ `batched=True` ä½ å¯ä»¥æ›´æ”¹æ•°æ®é›†ä¸­çš„å…ƒç´ æ•°é‡ã€‚å½“ä½ æƒ³ä»ä¸€ä¸ªæ ·æœ¬ä¸­åˆ›å»ºå‡ ä¸ªè®­ç»ƒç‰¹å¾æ—¶ï¼Œè¿™æ˜¯éå¸¸æœ‰ç”¨çš„ã€‚æˆ‘ä»¬å°†åœ¨ç¬¬å…«ç« ä¸­å‡ ä¸ª NLP ä»»åŠ¡çš„é¢„å¤„ç†ä¸­ä½¿ç”¨åˆ°è¿™ä¸ªåŠŸèƒ½ï¼Œå®ƒéå¸¸ä¾¿æ·ã€‚

<div custom-style="Tip-green">

ğŸ’¡åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œä¸€ä¸ªæ ·æœ¬é€šå¸¸å¯ä»¥ä¸ºæˆ‘ä»¬çš„æ¨¡å‹æä¾›ä¸€ç»„ç‰¹å¾ã€‚åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œè¿™ç»„ç‰¹å¾ä¼šå‚¨å­˜åœ¨æ•°æ®é›†çš„å‡ ä¸ªåˆ—ï¼Œä½†åœ¨æŸäº›æƒ…å†µä¸‹ï¼ˆä¾‹å¦‚æ­¤å¤„çš„ä¾‹å­å’Œç”¨äºé—®ç­”çš„æ•°æ®ï¼‰ï¼Œå¯ä»¥ä»å•ä¸ªæ ·æœ¬çš„é‚£ä¸€åˆ—ä¸­æå–å¤šä¸ªç‰¹å¾

</div>

è®©æˆ‘ä»¬æ¥çœ‹çœ‹å®ƒæ˜¯å¦‚ä½•å®ç°çš„ï¼åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†å¯¹æˆ‘ä»¬çš„æ ·æœ¬è¿›è¡Œ tokenization å¹¶å°†æœ€å¤§æˆªæ–­é•¿åº¦è®¾ç½®ä¸º 128ï¼Œä½†æˆ‘ä»¬å°†è¦æ±‚ tokenizer è¿”å›å…¨éƒ¨æ–‡æœ¬å—ï¼Œè€Œä¸ä»…ä»…æ˜¯ç¬¬ä¸€ä¸ªã€‚è¿™å¯ä»¥é€šè¿‡è®¾ç½® `return_overflowing_tokens=True` æ¥å®ç°ï¼š

```python
def tokenize_and_split(examples):
    return tokenizer(
        examples["review"],
        truncation=True,
        max_length=128,
        return_overflowing_tokens=True,
    )
```

åœ¨ä½¿ç”¨ `Dataset.map()` æ­£å¼å¼€å§‹å¤„ç†æ•´ä¸ªæ•°æ®é›†ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å…ˆåœ¨ä¸€ä¸ªæ ·æœ¬ä¸Šæµ‹è¯•ä¸€ä¸‹ï¼š

```python
result = tokenize_and_split(drug_dataset["train"][0])
[len(inp) for inp in result["input_ids"]]
```

```python
[128, 49]
```

ç§ï¼æˆ‘ä»¬åœ¨è®­ç»ƒé›†ä¸­çš„ç¬¬ä¸€ä¸ªæ ·æœ¬å˜æˆäº†ä¸¤ä¸ªç‰¹å¾ï¼Œå› ä¸ºå®ƒè¶…è¿‡äº†æˆ‘ä»¬æŒ‡å®šçš„æœ€å¤§æˆªæ–­é•¿åº¦ï¼Œå› æ­¤ç»“æœè¢«æˆªæˆäº†ä¸¤æ®µï¼šç¬¬ä¸€æ®µé•¿åº¦ä¸º 128 ç¬¬äºŒæ®µé•¿åº¦ä¸º 49 ç°åœ¨è®©æˆ‘ä»¬å¯¹æ•°æ®é›†çš„æ‰€æœ‰æ ·æœ¬æ‰§è¡Œæ­¤æ“ä½œï¼

```python
tokenized_dataset = drug_dataset.map(tokenize_and_split, batched=True)
```

```python
ArrowInvalid: Column 1 named condition expected length 1463 but got length 1000
```

ä¸å¥½äº†ï¼è¿™å¹¶æ²¡æœ‰æˆåŠŸï¼ä¸ºä»€ä¹ˆå‘¢ï¼ŸæŸ¥çœ‹é”™è¯¯æ¶ˆæ¯ä¼šç»™æˆ‘ä»¬ä¸€ä¸ªçº¿ç´¢ï¼šåˆ—çš„é•¿åº¦ä¸åŒ¹é…ï¼Œä¸€åˆ—é•¿åº¦ä¸º 1,463ï¼Œå¦ä¸€åˆ—é•¿åº¦ä¸º 1,000ã€‚1,000 è¡Œçš„â€œreviewâ€ç”Ÿæˆäº† 1,463 è¡Œçš„æ–°ç‰¹å¾ï¼Œå¯¼è‡´å’ŒåŸæœ¬çš„ 1000 è¡Œçš„é•¿åº¦ä¸åŒ¹é…ã€‚

é—®é¢˜å‡ºåœ¨æˆ‘ä»¬è¯•å›¾æ··åˆä¸¤ä¸ªé•¿åº¦ä¸åŒçš„æ•°æ®é›†ï¼š `drug_dataset` åˆ—å°†æœ‰ 1000 ä¸ªæ ·æœ¬ï¼Œä½†æ˜¯æˆ‘ä»¬æ­£åœ¨æ„å»º `tokenized_dataset` åˆ—å°†æœ‰ 1,463 ä¸ªæ ·æœ¬ï¼ˆå› ä¸ºæˆ‘ä»¬ä½¿ç”¨ `return_overflowing_tokens=True` å°†é•¿è¯„è®ºåˆ†è¯æˆäº†å¤šä¸ªæ ·æœ¬ï¼‰ã€‚è¿™å¯¹ `Dataset` æ¥è¯´ä¸å¯è¡Œï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦è¦ä¹ˆåˆ é™¤æ—§æ•°æ®é›†çš„åˆ—ï¼Œè¦ä¹ˆä½¿å®ƒä»¬ä¸æ–°æ•°æ®é›†ä¸­çš„å°ºå¯¸ç›¸åŒã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `remove_columns` å‚æ•°æ¥å®ç°å‰è€…ï¼š

```python
tokenized_dataset = drug_dataset.map(
    tokenize_and_split, batched=True, remove_columns=drug_dataset["train"].column_names
)
```

ç°åœ¨è¿™ä¸ªè¿‡ç¨‹æ²¡æœ‰é”™è¯¯ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡æ¯”è¾ƒé•¿åº¦æ¥æ£€æŸ¥æˆ‘ä»¬çš„æ–°æ•°æ®é›†æ˜¯å¦æ¯”åŸå§‹æ•°æ®é›†æœ‰æ›´å¤šçš„å…ƒç´ ï¼š

```python
len(tokenized_dataset["train"]), len(drug_dataset["train"])
```

```python
(206772, 138514)
```

æˆ‘ä»¬ä¹Ÿå¯ä»¥é€šè¿‡ä½¿æ—§åˆ—ä¸æ–°åˆ—ä¿æŒç›¸åŒå¤§å°æ¥å¤„ç†ä¸åŒ¹é…é•¿åº¦çš„é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œå½“æˆ‘ä»¬è®¾ç½® `return_overflowing_tokens=True` æ—¶ï¼Œå¯ä»¥ä½¿ç”¨ `overflow_to_sample_mapping` å­—æ®µã€‚å®ƒç»™å‡ºäº†æ–°ç‰¹å¾ç´¢å¼•åˆ°å®ƒæºè‡ªçš„æ ·æœ¬ç´¢å¼•çš„æ˜ å°„ã€‚ä½¿ç”¨è¿™ä¸ªï¼Œæˆ‘ä»¬å¯ä»¥å°†åŸå§‹æ•°æ®é›†ä¸­çš„æ¯ä¸ªé”®å…³è”åˆ°ä¸€ä¸ªåˆé€‚å¤§å°çš„å€¼åˆ—è¡¨ä¸­ï¼Œé€šè¿‡éå†æ‰€æœ‰çš„æ•°æ®æ¥ç”Ÿæˆæ–°ç‰¹æ€§ï¼š

```python
def tokenize_and_split(examples):
    result = tokenizer(
        examples["review"],
        truncation=True,
        max_length=128,
        return_overflowing_tokens=True,
    )
    # æå–æ–°æ—§ç´¢å¼•ä¹‹é—´çš„æ˜ å°„
    sample_map = result.pop("overflow_to_sample_mapping")
    for key, values in examples.items():
        result[key] = [values[i] for i in sample_map]
    return result
```

å¯ä»¥çœ‹åˆ°å®ƒå¯ä»¥ä¸ `Dataset.map()` ä¸€èµ·å·¥ä½œï¼Œæ— éœ€æˆ‘ä»¬åˆ é™¤æ—§åˆ—ï¼š

```python
tokenized_dataset = drug_dataset.map(tokenize_and_split, batched=True)
tokenized_dataset
```

```python
DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'condition', 'date', 'drugName', 'input_ids', 'patient_id', 'rating', 'review', 'review_length', 'token_type_ids', 'usefulCount'],
        num_rows: 206772
    })
    test: Dataset({
        features: ['attention_mask', 'condition', 'date', 'drugName', 'input_ids', 'patient_id', 'rating', 'review', 'review_length', 'token_type_ids', 'usefulCount'],
        num_rows: 68876
    })
})
```

æˆ‘ä»¬è·å¾—äº†ä¸ä¹‹å‰æ•°é‡ç›¸åŒçš„è®­ç»ƒç‰¹å¾ï¼Œä½†åœ¨è¿™é‡Œæˆ‘ä»¬ä¿ç•™äº†æ‰€æœ‰æ—§å­—æ®µã€‚å¦‚æœä½ åœ¨ä½¿ç”¨æ¨¡å‹è®¡ç®—ä¹‹åéœ€è¦å®ƒä»¬è¿›è¡Œä¸€äº›åç»­å¤„ç†ï¼Œä½ å¯èƒ½éœ€è¦ä½¿ç”¨è¿™ç§æ–¹æ³•ã€‚

ä½ ç°åœ¨å·²ç»äº†è§£äº†å¦‚ä½•ä½¿ç”¨ Datasets ä»¥å„ç§æ–¹å¼ç”¨äºé¢„å¤„ç†æ•°æ®é›†ã€‚è™½ç„¶Datasets çš„å¤„ç†åŠŸèƒ½ä¼šè¦†ç›–ä½ å¤§éƒ¨åˆ†çš„æ¨¡å‹è®­ç»ƒéœ€æ±‚ï¼Œæœ‰æ—¶ä½ å¯èƒ½éœ€è¦åˆ‡æ¢åˆ° Pandas ä»¥ä½¿ç”¨æ›´å¼ºå¤§çš„åŠŸèƒ½ï¼Œä¾‹å¦‚ `DataFrame.groupby()` æˆ–ç”¨äºå¯è§†åŒ–çš„é«˜çº§ APIã€‚å¹¸è¿çš„æ˜¯ï¼ŒDatasets è®¾è®¡å®—æ—¨å°±æ˜¯ä¸ Pandasã€NumPyã€PyTorchã€TensorFlow å’Œ JAX ç­‰åº“å¯ä»¥ç›¸äº’è½¬æ¢ã€‚è®©æˆ‘ä»¬æ¥çœ‹çœ‹è¿™æ˜¯å¦‚ä½•å®ç°çš„ã€‚

### Datasets å’Œ DataFrames çš„ç›¸äº’è½¬æ¢ 

ä¸ºäº†å®ç°å„ç§ç¬¬ä¸‰æ–¹åº“ä¹‹é—´çš„è½¬æ¢ï¼ŒDatasets æä¾›äº†ä¸€ä¸ª `Dataset.set_format()` å‡½æ•°ã€‚æ­¤å‡½æ•°å¯ä»¥é€šè¿‡ä»…æ›´æ”¹è¾“å‡ºæ ¼å¼çš„ï¼Œè½»æ¾åˆ‡æ¢åˆ°å¦ä¸€ç§æ ¼å¼ï¼Œè€Œä¸ä¼šå½±å“åº•å±‚æ•°æ®æ ¼å¼ï¼ˆä»¥ Apache Arrow æ–¹å¼è¿›è¡Œå­˜å‚¨ï¼‰ã€‚ä¸ºäº†æ¼”ç¤ºï¼Œè®©æˆ‘ä»¬æŠŠæ•°æ®é›†è½¬æ¢ä¸º Pandasï¼š

```python
drug_dataset.set_format("pandas")
```

ç°åœ¨ï¼Œå½“æˆ‘ä»¬è®¿é—®æ•°æ®é›†çš„å…ƒç´ æ—¶ï¼Œæˆ‘ä»¬ä¼šå¾—åˆ°ä¸€ä¸ª `pandas.DataFrame` è€Œä¸æ˜¯å­—å…¸ï¼š

```python
drug_dataset["train"][:3]
```

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>patient_id</th>
      <th>drugName</th>
      <th>condition</th>
      <th>review</th>
      <th>rating</th>
      <th>date</th>
      <th>usefulCount</th>
      <th>review_length</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>95260</td>
      <td>Guanfacine</td>
      <td>adhd</td>
      <td>"My son is halfway through his fourth week of Intuniv."</td>
      <td>8.0</td>
      <td>April 27, 2010</td>
      <td>192</td>
      <td>141</td>
    </tr>
    <tr>
      <th>1</th>
      <td>92703</td>
      <td>Lybrel</td>
      <td>birth control</td>
      <td>"I used to take another oral contraceptive, which had 21 pill cycle, and was very happy- very light periods, max 5 days, no other side effects."</td>
      <td>5.0</td>
      <td>December 14, 2009</td>
      <td>17</td>
      <td>134</td>
    </tr>
    <tr>
      <th>2</th>
      <td>138000</td>
      <td>Ortho Evra</td>
      <td>birth control</td>
      <td>"This is my first time using any form of birth control."</td>
      <td>8.0</td>
      <td>November 3, 2015</td>
      <td>10</td>
      <td>89</td>
    </tr>
  </tbody>
</table>

æ¥ä¸‹æ¥æˆ‘ä»¬ä»æ•°æ®é›†ä¸­é€‰æ‹© `drug_dataset[train]` çš„æ‰€æœ‰æ•°æ®æ¥å¾—åˆ°è®­ç»ƒé›†æ•°æ®ï¼š

```python
train_df = drug_dataset["train"][:]
```

<div custom-style="Tip-red">

ğŸš¨ å®é™…ä¸Šï¼Œ `Dataset.set_format()` æ”¹å˜äº†æ•°æ®é›†çš„ `__getitem__()` æ–¹æ³•çš„è¿”å›æ ¼å¼ã€‚è¿™æ„å‘³ç€å½“æˆ‘ä»¬æƒ³ä» `"pandas"` æ ¼å¼çš„ `Dataset` ä¸­åˆ›å»ºåƒ `train_df` è¿™æ ·çš„æ–°å¯¹è±¡æ—¶ï¼Œæˆ‘ä»¬éœ€è¦å¯¹æ•´ä¸ªæ•°æ®é›†è¿›è¡Œåˆ‡ç‰‡ï¼ˆ[:]ï¼‰ä»¥è·å¾— `pandas.DataFrame` ã€‚æ— è®ºè¾“å‡ºæ ¼å¼å¦‚ä½•ï¼Œä½ éƒ½å¯ä»¥è‡ªå·±éªŒè¯ `drug_dataset["train"]` çš„ç±»å‹ä¾ç„¶è¿˜æ˜¯ `Dataset` ã€‚

</div>

æœ‰äº†è¿™ä¸ªåŸºç¡€ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æˆ‘ä»¬æƒ³è¦çš„æ‰€æœ‰ Pandas åŠŸèƒ½ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥å·§å¦™åœ°é“¾å¼æ“ä½œï¼Œæ¥è®¡ç®— `condition` åˆ—ä¸­ä¸åŒç±»åˆ«çš„åˆ†å¸ƒ 

```python
frequencies = (
    train_df["condition"]
    .value_counts()
    .to_frame()
    .reset_index()
    .rename(columns={"index": "condition", "condition": "frequency"})
)
frequencies.head()
```

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>condition</th>
      <th>frequency</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>birth control</td>
      <td>27655</td>
    </tr>
    <tr>
      <th>1</th>
      <td>depression</td>
      <td>8023</td>
    </tr>
    <tr>
      <th>2</th>
      <td>acne</td>
      <td>5209</td>
    </tr>
    <tr>
      <th>3</th>
      <td>anxiety</td>
      <td>4991</td>
    </tr>
    <tr>
      <th>4</th>
      <td>pain</td>
      <td>4744</td>
    </tr>
  </tbody>
</table>

å½“æˆ‘ä»¬å®Œæˆäº† Pandas åˆ†æä¹‹åï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å¯¹è±¡ `Dataset.from_pandas()` æ–¹æ³•å¯ä»¥åˆ›å»ºä¸€ä¸ªæ–°çš„ `Dataset` å¯¹è±¡ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```python
from datasets import Dataset

freq_dataset = Dataset.from_pandas(frequencies)
freq_dataset
```

```python
Dataset({
    features: ['condition', 'frequency'],
    num_rows: 819
})
```

<div custom-style="Tip-green">

âœï¸**è¯•è¯•çœ‹ï¼**è®¡ç®—æ¯ç§è¯ç‰©çš„å¹³å‡è¯„åˆ†å¹¶å°†ç»“æœå­˜å‚¨åœ¨ä¸€ä¸ªæ–°çš„ Dataset ä¸­ã€‚

</div>

åˆ°æ­¤ä¸ºæ­¢ï¼Œæˆ‘ä»¬å¯¹Datasets ä¸­å¯ç”¨çš„å„ç§é¢„å¤„ç†æŠ€æœ¯çš„ä»‹ç»å°±ç»“æŸäº†ã€‚åœ¨æœ¬èŠ‚çš„æœ€åä¸€éƒ¨åˆ†ï¼Œè®©æˆ‘ä»¬ä¸ºè®­ç»ƒåˆ†ç±»å™¨åˆ›å»ºä¸€ä¸ªéªŒè¯é›†ã€‚åœ¨æ­¤ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å°†è¾“å‡ºæ ¼å¼ `drug_dataset` ä» `pandas` é‡ç½®åˆ° `arrow` ï¼š

```python
drug_dataset.reset_format()
```

### åˆ›å»ºéªŒè¯é›† 

å°½ç®¡æˆ‘ä»¬æœ‰ä¸€ä¸ªå¯ä»¥ç”¨äºè¯„ä¼°çš„æµ‹è¯•é›†ï¼Œä½†åœ¨å¼€å‘è¿‡ç¨‹ä¸­ä¿æŒæµ‹è¯•é›†ä¸å˜å¹¶åˆ›å»ºä¸€ä¸ªå•ç‹¬çš„éªŒè¯é›†æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„åšæ³•ã€‚ä¸€æ—¦ä½ å¯¹æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„è¡¨ç°æ„Ÿåˆ°æ»¡æ„ï¼Œä½ å°±å¯ä»¥ä½¿ç”¨éªŒè¯é›†è¿›è¡Œæœ€ç»ˆçš„æ£€æŸ¥ã€‚æ­¤è¿‡ç¨‹æœ‰åŠ©äºé™ä½ä½ è¿‡æ‹Ÿåˆæµ‹è¯•é›†å’Œéƒ¨ç½²åœ¨ç°å®ä¸–ç•Œæ•°æ®ä¸Šå¤±è´¥çš„æ¨¡å‹çš„é£é™©ã€‚

Datasets æä¾›äº†ä¸€ä¸ªåŸºäº `scikit-learn` çš„ç»å…¸æ–¹æ³•ï¼š `Dataset.train_test_split()` ã€‚è®©æˆ‘ä»¬ç”¨å®ƒæŠŠæˆ‘ä»¬çš„è®­ç»ƒé›†åˆ†æˆ `train` å’Œ `validation` ï¼ˆä¸ºäº†å¯ä»¥å¤ç°ï¼Œæˆ‘ä»¬å°†è®¾ç½® `seed` çš„å€¼ä¸ºä¸€ä¸ªå¸¸é‡ï¼‰ï¼š

```python
drug_dataset_clean = drug_dataset["train"].train_test_split(train_size=0.8, seed=42)
## å°†é»˜è®¤çš„ "test" éƒ¨åˆ†é‡å‘½åä¸º"validation"
drug_dataset_clean["validation"] = drug_dataset_clean.pop("test")
## å°† "test" é›†æ·»åŠ åˆ°æˆ‘ä»¬çš„`DatasetDict`ä¸­
drug_dataset_clean["test"] = drug_dataset["test"]
drug_dataset_clean
```

```python
DatasetDict({
    train: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length', 'review_clean'],
        num_rows: 110811
    })
    validation: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length', 'review_clean'],
        num_rows: 27703
    })
    test: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length', 'review_clean'],
        num_rows: 46108
    })
})
```

å¤ªå¥½äº†ï¼Œæˆ‘ä»¬ç°åœ¨å·²ç»å‡†å¤‡å¥½äº†ä¸€ä¸ªé€‚åˆè®­ç»ƒæ¨¡å‹çš„æ•°æ®é›†äº†ï¼åœ¨ç¬¬äº”èŠ‚æˆ‘ä»¬å°†å‘ä½ å±•ç¤ºå¦‚ä½•å°†æ•°æ®é›†ä¸Šä¼ åˆ° Hugging Face Hubï¼Œç°åœ¨è®©æˆ‘ä»¬å…ˆç»“æŸæˆ‘ä»¬çš„åˆ†æï¼Œçœ‹ä¸€çœ‹åœ¨æœ¬åœ°è®¡ç®—æœºä¸Šä¿å­˜æ•°æ®é›†çš„å‡ ç§æ–¹æ³•ã€‚

### ä¿å­˜æ•°æ®é›† 

è™½ç„¶ Datasets ä¼šç¼“å­˜æ¯ä¸ªä¸‹è½½çš„æ•°æ®é›†å’Œå¯¹å®ƒæ‰§è¡Œçš„æ“ä½œï¼Œä½†æœ‰æ—¶ä½ ä¼šæƒ³è¦å°†æ•°æ®é›†ä¿å­˜åˆ°ç£ç›˜ï¼ˆæ¯”å¦‚ï¼Œä»¥é˜²ç¼“å­˜è¢«åˆ é™¤ï¼‰ã€‚å¦‚ä¸‹è¡¨æ‰€ç¤ºï¼ŒDatasets æä¾›äº†ä¸‰ä¸ªä¸»è¦å‡½æ•°æ¥ä»¥ä¸åŒçš„æ ¼å¼ä¿å­˜ä½ çš„æ•°æ®é›†ï¼š

| æ•°æ®æ ¼å¼    |        å¯¹åº”çš„æ–¹æ³•        |
| :---------: | :--------------------: |
|    Arrow    | `Dataset.save_to_disk()` |
|     CSV     | `Dataset.to_csv()` |
|    JSON     | `Dataset.to_json()` |

ä¾‹å¦‚ï¼Œè®©æˆ‘ä»¬ä»¥ Arrow æ ¼å¼ä¿å­˜æˆ‘ä»¬æ¸…æ´—è¿‡çš„æ•°æ®é›†ï¼š

```python
drug_dataset_clean.save_to_disk("drug-reviews")
```

è¿™å°†åˆ›å»ºä¸€ä¸ªå…·æœ‰ä»¥ä¸‹ç»“æ„çš„ç›®å½•ï¼š

```python
drug-reviews/
â”œâ”€â”€ dataset_dict.json
â”œâ”€â”€ test
â”‚   â”œâ”€â”€ dataset.arrow
â”‚   â”œâ”€â”€ dataset_info.json
â”‚   â””â”€â”€ state.json
â”œâ”€â”€ train
â”‚   â”œâ”€â”€ dataset.arrow
â”‚   â”œâ”€â”€ dataset_info.json
â”‚   â”œâ”€â”€ indices.arrow
â”‚   â””â”€â”€ state.json
â””â”€â”€ validation
    â”œâ”€â”€ dataset.arrow
    â”œâ”€â”€ dataset_info.json
    â”œâ”€â”€ indices.arrow
    â””â”€â”€ state.json
```

å…¶ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œæ¯ä¸ªéƒ¨åˆ†éƒ½æœ‰ `dataset.arrow` è¡¨ï¼Œä»¥åŠä¿å­˜å…ƒæ•°æ®çš„ `dataset_info.json` å’Œ `state.json` ã€‚ä½ å¯ä»¥å°† Arrow æ ¼å¼è§†ä¸ºä¸€ä¸ªä¼˜åŒ–çš„åˆ—å’Œè¡Œçš„ç²¾ç¾è¡¨æ ¼ï¼Œå®ƒé’ˆå¯¹æ„å»ºå¤„ç†å’Œä¼ è¾“å¤§å‹æ•°æ®é›†çš„é«˜æ€§èƒ½åº”ç”¨ç¨‹åºè¿›è¡Œäº†ä¼˜åŒ–ã€‚

ä¿å­˜æ•°æ®é›†åï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `load_from_disk()` åŠŸèƒ½ä»ç£ç›˜è¯»å–æ•°æ®ï¼š

```python
from datasets import load_from_disk

drug_dataset_reloaded = load_from_disk("drug-reviews")
drug_dataset_reloaded
```

```python
DatasetDict({
    train: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],
        num_rows: 110811
    })
    validation: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],
        num_rows: 27703
    })
    test: Dataset({
        features: ['patient_id', 'drugName', 'condition', 'review', 'rating', 'date', 'usefulCount', 'review_length'],
        num_rows: 46108
    })
})
```

å¯¹äº CSV å’Œ JSON æ ¼å¼ï¼Œæˆ‘ä»¬å¿…é¡»å°†æ¯ä¸ªéƒ¨åˆ†å­˜å‚¨ä¸ºå•ç‹¬çš„æ–‡ä»¶ã€‚ä¸€ç§æ–¹æ³•æ˜¯éå† `DatasetDict` ä¸­çš„é”®å’Œå€¼ 

```python
for split, dataset in drug_dataset_clean.items():
    dataset.to_json(f"drug-reviews-{split}.jsonl")
```

è¿™å°†æŠŠæ¯ä¸ªéƒ¨åˆ†ä¿å­˜ä¸º [JSON Linesæ ¼å¼](https://jsonlines.org)(https://jsonlines.org) ï¼Œå…¶ä¸­æ•°æ®é›†ä¸­çš„æ¯ä¸€è¡Œéƒ½å­˜å‚¨ä¸ºä¸€è¡Œ JSONã€‚ä¸‹é¢æ˜¯ç¬¬ä¸€ä¸ªä¾‹å­çš„æ ·å­ï¼š

```python
!head -n 1 drug-reviews-train.jsonl
```

```python
{"patient_id":141780,"drugName":"Escitalopram","condition":"depression","review":"\"I seemed to experience the regular side effects of LEXAPRO, insomnia, low sex drive, sleepiness during the day. I am taking it at night because my doctor said if it made me tired to take it at night. I assumed it would and started out taking it at night. Strange dreams, some pleasant. I was diagnosed with fibromyalgia. Seems to be helping with the pain. Have had anxiety and depression in my family, and have tried quite a few other medications that haven't worked. Only have been on it for two weeks but feel more positive in my mind, want to accomplish more in my life. Hopefully the side effects will dwindle away, worth it to stick with it from hearing others responses. Great medication.\"","rating":9.0,"date":"May 29, 2011","usefulCount":10,"review_length":125}
```

ç„¶åæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ç¬¬äºŒèŠ‚ä¸­çš„æŠ€å·§ï¼ŒæŒ‰å¦‚ä¸‹æ‰€ç¤ºåŠ è½½ JSON æ–‡ä»¶

```python
data_files = {
    "train": "drug-reviews-train.jsonl",
    "validation": "drug-reviews-validation.jsonl",
    "test": "drug-reviews-test.jsonl",
}
drug_dataset_reloaded = load_dataset("json", data_files=data_files)
```

è‡³æ­¤ï¼Œæˆ‘ä»¬å¯¹ä½¿ç”¨Datasets è¿›è¡Œæ•°æ®æ•´ç†çš„æ¢ç´¢å°±æ­¤ç»“æŸï¼ç°åœ¨æˆ‘ä»¬æœ‰äº†ä¸€ä¸ªæ¸…æ´—è¿‡çš„æ•°æ®é›†ï¼Œä»¥ä¸‹æ˜¯ä½ å¯ä»¥å°è¯•çš„ä¸€äº›æƒ³æ³•ï¼š

1. ä½¿ç”¨ç¬¬å››ç« çš„æŠ€æœ¯æ¥è®­ç»ƒä¸€ä¸ªåˆ†ç±»å™¨ï¼Œå®ƒèƒ½å¤ŸåŸºäºè¯å“è¯„ä»·é¢„æµ‹æ‚£è€…çš„ç—…æƒ…ã€‚
2. ä½¿ç”¨ç¬¬äºŒç« ä¸­çš„ `summarization` ç®¡é“ç”Ÿæˆè¯„è®ºçš„æ‘˜è¦ã€‚

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†çœ‹çœ‹ Datasets å¦‚ä½•ä½¿ä½ èƒ½å¤Ÿåœ¨ä¸æ’‘çˆ†ç¬”è®°æœ¬ç”µè„‘å†…å­˜çš„æƒ…å†µä¸‹å¤„ç†åºå¤§çš„æ•°æ®é›†ï¼

## 6.3 å¤§æ•°æ®ï¼ŸDatasets åº”å¯¹æœ‰æ–¹ï¼


å¦‚ä»Šï¼Œå¤„ç† GB çº§åˆ«çš„æ•°æ®é›†å·²ä¸å†ç½•è§ï¼Œç‰¹åˆ«æ˜¯å¦‚æœä½ æ‰“ç®—ä»å¤´å¼€å§‹é¢„è®­ç»ƒåƒ BERT æˆ–è€… GPT-2 è¿™æ ·çš„ Transormer æ¨¡å‹ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç”šè‡³ `åŠ è½½(load)` æ•°æ®é›†éƒ½å¯èƒ½æˆä¸ºæŒ‘æˆ˜ã€‚ä¾‹å¦‚ï¼Œç”¨äºé¢„è®­ç»ƒ GPT-2 çš„ WebText è¯­æ–™åº“åŒ…å«è¶…è¿‡ 800 ä¸‡ä¸ªæ–‡æ¡£å’Œ 40 GB çš„æ–‡æœ¬ â€”â€” å°†å…¶åŠ è½½åˆ°ç¬”è®°æœ¬ç”µè„‘çš„ RAM ä¸­éƒ½å¯èƒ½ä¼šè®©äººæŠ“ç‹‚ï¼

å¹¸è¿çš„æ˜¯ï¼ŒDatasets çš„è®¾è®¡æ—¨åœ¨å…‹æœè¿™äº›é™åˆ¶ã€‚å®ƒé€šè¿‡å°†æ•°æ®é›†ä½œä¸º `å†…å­˜æ˜ å°„(memory-mapped)` æ–‡ä»¶æ¥å¤„ç†ï¼Œè§£æ”¾å†…å­˜ç®¡ç†é—®é¢˜ï¼›å¹¶é€šè¿‡ `æµå¼å¤„ç†(streaming)` æ¥æ‘†è„±ç¡¬ç›˜é™åˆ¶ã€‚

åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªåºå¤§çš„ 825 GB è¯­æ–™åº“â€”â€”è¢«ç§°ä¸º [the Pile](https://pile.eleuther.ai)(https://pile.eleuther.ai) çš„æ•°æ®é›†ï¼Œæ¥æ¢ç´¢Datasets çš„è¿™äº›åŠŸèƒ½ã€‚è®©æˆ‘ä»¬å¼€å§‹å§ï¼

### ä»€ä¹ˆæ˜¯ the Pileï¼Ÿ

The Pile æ˜¯ç”± [EleutherAI](https://www.eleuther.ai)(https://www.eleuther.ai) åˆ›å»ºçš„ä¸€ä¸ªç”¨äºè®­ç»ƒå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„è‹±è¯­æ–‡æœ¬è¯­æ–™åº“ã€‚å®ƒåŒ…å«å„ç§å„æ ·çš„æ•°æ®é›†ï¼Œæ¶µç›–ç§‘å­¦æ–‡ç« ï¼ŒGitHub ä»£ç åº“ä»¥åŠè¿‡æ»¤åçš„ Web æ–‡æœ¬ã€‚è®­ç»ƒè¯­æ–™åº“ä»¥ [14 GB çš„æ–‡ä»¶å—](https://the-eye.eu/public/AI/pile/)(https://the-eye.eu/public/AI/pile/) æä¾›ï¼Œå¹¶ä¸”ä½ ä¹Ÿå¯ä»¥ä¸‹è½½å‡ ä¸ª [å•ç‹¬çš„ç»„ä»¶](https://the-eye.eu/public/AI/pile_preliminary_components/)(https://the-eye.eu/public/AI/pile_preliminary_components/) ã€‚è®©æˆ‘ä»¬å…ˆæ¥çœ‹çœ‹ PubMed Abstracts éƒ¨åˆ†ï¼Œå®ƒæ˜¯ [PubMed](https://pubmed.ncbi.nlm.nih.gov/)(https://pubmed.ncbi.nlm.nih.gov/) ä¸Šçš„ 1500 ä¸‡ç¯‡ç”Ÿç‰©åŒ»å­¦å‡ºç‰ˆç‰©çš„æ‘˜è¦çš„è¯­æ–™åº“ã€‚æ•°æ®é›†é‡‡ç”¨ [JSON Linesæ ¼å¼](https://jsonlines.org)(https://jsonlines.org) å¹¶ä½¿ç”¨ `zstandard` åº“è¿›è¡Œå‹ç¼©ï¼Œæ‰€ä»¥æˆ‘ä»¬é¦–å…ˆéœ€è¦å…ˆå®‰è£… `zstandard` åº“ï¼š

```python
!pip install zstandard
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ç¬¬äºŒèŠ‚ä¸­æ‰€å­¦çš„åŠ è½½è¿œç¨‹æ•°æ®é›†çš„æ–¹æ³•åŠ è½½æ•°æ®é›†ï¼š

```python
from datasets import load_dataset

## è¿™éœ€è¦å‡ åˆ†é’Ÿæ‰èƒ½è¿è¡Œ,æ‰€ä»¥åœ¨ä½ ç­‰å¾…çš„æ—¶å€™å»å–æ¯èŒ¶æˆ–å’–å•¡ :)
data_files = "https://the-eye.eu/public/AI/pile_preliminary_components/PUBMED_title_abstracts_2019_baseline.jsonl.zst"
pubmed_dataset = load_dataset("json", data_files=data_files, split="train")
pubmed_dataset
```

```python
Dataset({
    features: ['meta', 'text'],
    num_rows: 15518009
})
```

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°æˆ‘ä»¬çš„æ•°æ®é›†ä¸­æœ‰ 15,518,009 è¡Œå’Œ 2 åˆ— â€”â€” å¦‚æ­¤åºå¤§ï¼

<div custom-style="Tip-green">

âœï¸ é»˜è®¤æƒ…å†µä¸‹ï¼ŒDatasets ä¼šè‡ªåŠ¨è§£å‹åŠ è½½æ•°æ®é›†æ‰€éœ€çš„æ–‡ä»¶ã€‚å¦‚æœä½ æƒ³ä¿ç•™ç¡¬ç›˜ç©ºé—´ï¼Œä½ å¯ä»¥æŠŠ `DownloadConfig(delete_extracted=True)` ä¼ é€’ç»™ `load_dataset()` çš„ `download_config` å‚æ•°ã€‚æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… [æ–‡æ¡£](https://huggingface.co/docs/datasets/package_reference/builder_classes.html?#datasets.utils.DownloadConfig)(https://huggingface.co/docs/datasets/package_reference/builder_classes.html?#datasets.utils.DownloadConfig) ã€‚

</div>

è®©æˆ‘ä»¬çœ‹çœ‹æ•°æ®é›†çš„ç¬¬ä¸€ä¸ªå…ƒç´ çš„å†…å®¹ï¼š

```python
pubmed_dataset[0]
```

```python
{'meta': {'pmid': 11409574, 'language': 'eng'},
 'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection.\nTo determine the prevalence of hypoxaemia in children aged under 5 years suffering acute lower respiratory infections (ALRI), the risk factors for hypoxaemia in children under 5 years of age with ALRI, and the association of hypoxaemia with an increased risk of dying in children of the same age ...'}
```

å¯ä»¥çœ‹åˆ°ï¼Œè¿™çœ‹èµ·æ¥åƒæ˜¯åŒ»å­¦æ–‡ç« çš„æ‘˜è¦ã€‚ç°åœ¨ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹åŠ è½½æ•°æ®é›†æ‰€ä½¿ç”¨çš„ RAMï¼

### å†…å­˜æ˜ å°„çš„é­”åŠ› 

æµ‹é‡ Python å†…å­˜ä½¿ç”¨çš„ç®€å•æ–¹å¼æ˜¯ä½¿ç”¨ [`psutil`](https://psutil.readthedocs.io/en/latest/)(https://psutil.readthedocs.io/en/latest/) åº“ï¼Œå¯ä»¥é€šè¿‡å¦‚ä¸‹æ–¹å¼å®‰è£…ï¼š

```python
!pip install psutil
```

å®ƒæä¾›äº†ä¸€ä¸ª `Process` ç±»ï¼Œè®©æˆ‘ä»¬å¯ä»¥æ£€æŸ¥å½“å‰è¿›ç¨‹çš„å†…å­˜ä½¿ç”¨æƒ…å†µï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```python
import psutil

## Process.memory_infoæ˜¯ä»¥å­—èŠ‚ä¸ºå•ä½çš„,æ‰€ä»¥è½¬æ¢ä¸ºå…†å­—èŠ‚
print(f"ä½¿ç”¨çš„RAM:{psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB")
```

```python
RAM used: 5678.33 MB
```

è¿™é‡Œçš„ `rss` å±æ€§æ˜¯æŒ‡ `å¸¸é©»é›†ï¼ˆresident set sizeï¼‰` çš„å¤§å°ï¼Œå®ƒæ˜¯è¿›ç¨‹åœ¨ RAM ä¸­å ç”¨çš„å†…å­˜çš„éƒ¨åˆ†ã€‚è¿™ä¸ªæµ‹é‡ç»“æœä¹ŸåŒ…æ‹¬äº† Python è§£é‡Šå™¨å’Œæˆ‘ä»¬åŠ è½½çš„åº“æ‰€ä½¿ç”¨çš„å†…å­˜ï¼Œæ‰€ä»¥å®é™…ä¸Šç”¨äºåŠ è½½æ•°æ®é›†çš„å†…å­˜ä¼šæ›´å°ä¸€äº›ã€‚ä½œä¸ºæ¯”è¾ƒï¼Œè®©æˆ‘ä»¬ä½¿ç”¨ `dataset_size` å±æ€§çœ‹çœ‹æ•°æ®é›†åœ¨ç£ç›˜ä¸Šä¸Šçš„å¤§å°ã€‚ç”±äºç»“æœåƒä¹‹å‰ä¸€æ ·ä»¥å­—èŠ‚ä¸ºå•ä½ï¼Œæˆ‘ä»¬éœ€è¦æ‰‹åŠ¨å°†å…¶è½¬æ¢ä¸º GBï¼š

```python
print(f"æ•°æ®é›†ä¸­æ–‡ä»¶çš„æ•°é‡ : {pubmed_dataset.dataset_size}")
size_gb = pubmed_dataset.dataset_size / (1024**3)
print(f"æ•°æ®é›†å¤§å° (ç¼“å­˜æ–‡ä»¶) : {size_gb:.2f} GB")
```

```python
æ•°æ®é›†ä¸­æ–‡ä»¶çš„æ•°é‡ : 20979437051
æ•°æ®é›†å¤§å° (ç¼“å­˜æ–‡ä»¶) : 19.54 GB
```

ä»¤äººæ¬£å–œçš„æ˜¯â€”â€”å°½ç®¡å®ƒå°†è¿‘ 20GB ä¹‹å¤§ï¼Œæˆ‘ä»¬å´èƒ½ç”¨è¿œå°äºæ­¤çš„ RAM åŠ è½½å’Œè®¿é—®æ•°æ®é›†ï¼

<div custom-style="Tip-green">

âœï¸ **è¯•è¯•çœ‹ï¼** ä» Pile é€‰æ‹©ä¸€ä¸ªæ¯”ä½ çš„ç¬”è®°æœ¬ç”µè„‘æˆ–å°å¼æœºçš„ RAM æ›´å¤§çš„ [å­é›†](https://the-eye.eu/public/AI/pile_preliminary_components/)(https://the-eye.eu/public/AI/pile_preliminary_components/) ï¼Œç”¨ Datasets åŠ è½½è¿™ä¸ªæ•°æ®é›†ï¼Œå¹¶ä¸”æµ‹é‡ RAM çš„ä½¿ç”¨é‡ã€‚è¯·æ³¨æ„ï¼Œä¸ºäº†è·å¾—å‡†ç¡®çš„æµ‹é‡ç»“æœï¼Œä½ éœ€è¦æ–°å¼€ä¸€ä¸ªè¿›ç¨‹æ‰§è¡Œè¿™ä¸ªæ“ä½œã€‚ä½ å¯ä»¥åœ¨ [the Pile paper](https://arxiv.org/abs/2101.00027)(https://arxiv.org/abs/2101.00027) çš„è¡¨ 1 ä¸­æ‰¾åˆ°æ¯ä¸ªå­é›†è§£å‹åçš„å¤§å°ã€‚

</div>

å¦‚æœä½ ç†Ÿæ‚‰ Pandasï¼Œè¿™ä¸ªç»“æœå¯èƒ½ä¼šè®©äººæ„Ÿåˆ°å¾ˆæƒŠå¥‡ã€‚å› ä¸ºæ ¹æ® Wes Kinney çš„è‘—åçš„ [ç»éªŒæ³•åˆ™](https://wesmckinney.com/blog/apache-arrow-pandas-internals/)(https://wesmckinney.com/blog/apache-arrow-pandas-internals/) ï¼Œä½ é€šå¸¸éœ€è¦ 5 åˆ° 10 å€äºä½ æ•°æ®é›†å¤§å°çš„ RAMã€‚é‚£ä¹ˆ Datasets æ˜¯å¦‚ä½•è§£å†³è¿™ä¸ªå†…å­˜ç®¡ç†é—®é¢˜çš„å‘¢ï¼ŸDatasets å°†æ¯ä¸€ä¸ªæ•°æ®é›†çœ‹ä½œä¸€ä¸ª [å†…å­˜æ˜ å°„æ–‡ä»¶](https://en.wikipedia.org/wiki/Memory-mapped_file)(https://en.wikipedia.org/wiki/Memory-mapped_file) ï¼Œå®ƒæä¾›äº† RAM å’Œæ–‡ä»¶ç³»ç»Ÿå­˜å‚¨ä¹‹é—´çš„æ˜ å°„ï¼Œè¯¥æ˜ å°„å…è®¸ Datasets åº“æ— éœ€å°†å…¶å®Œå…¨åŠ è½½åˆ°å†…å­˜ä¸­å³å¯è®¿é—®å’Œæ“ä½œæ•°æ®é›†çš„å…ƒç´ ã€‚

å†…å­˜æ˜ å°„æ–‡ä»¶ä¹Ÿä¸€ä¸ªåœ¨å¤šä¸ªè¿›ç¨‹ä¹‹é—´å…±äº«ï¼Œè¿™ä½¿å¾—åƒ `Dataset.map()` ä¹‹ç±»çš„æ–¹æ³•å¯ä»¥åœ¨æ— éœ€ç§»åŠ¨æˆ–è€…å¤åˆ¶æ•°æ®é›†çš„æƒ…å†µä¸‹å®ç°å¹¶è¡ŒåŒ–ã€‚åœ¨åº•å±‚ï¼Œè¿™äº›åŠŸèƒ½éƒ½æ˜¯ç”± [Apache Arrow](https://arrow.apache.org)(https://arrow.apache.org) å†…å­˜æ ¼å¼å’Œ [`pyarrow`](https://arrow.apache.org/docs/python/index.html)(https://arrow.apache.org/docs/python/index.html) åº“å®ç°çš„ï¼Œè¿™ä½¿å¾—æ•°æ®åŠ è½½å’Œå¤„ç†é€Ÿåº¦å¿«å¦‚é—ªç”µã€‚ï¼ˆæ›´å¤šæœ‰å…³ Apache Arrow çš„è¯¦ç»†ä¿¡æ¯ä»¥åŠä¸ Pandas çš„æ¯”è¾ƒï¼Œè¯·æŸ¥çœ‹Dejan Simicçš„åšå®¢æ–‡ç« ã€‚ï¼‰ ä¸ºäº†æ›´æ¸…æ™°åœ°çœ‹åˆ°è¿™ä¸ªè¿‡ç¨‹ï¼Œè®©æˆ‘ä»¬é€šè¿‡éå† PubMed æ‘˜è¦æ•°æ®é›†ä¸­çš„æ‰€æœ‰å…ƒç´ ï¼Œè¿è¡Œä¸€ä¸ªå°é€Ÿåº¦æµ‹è¯•ï¼š

```python
import timeit

code_snippet = """batch_size = 1000

for idx in range(0, len(pubmed_dataset), batch_size):
    _ = pubmed_dataset[idx:idx + batch_size]
"""

time = timeit.timeit(stmt=code_snippet, number=1, globals=globals())
print(
    f"åœ¨ {time:.1f}s å†…éå†äº† {len(pubmed_dataset)}ä¸ªç¤ºä¾‹(çº¦ {size_gb:.1f} GB),å³ {size_gb/time:.3f} GB/s"
)
```

```python
'åœ¨64.2så†…éå†äº†15518009ä¸ªç¤ºä¾‹(çº¦19.5 GB),å³0.304 GB/s'
```

è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨äº† Python çš„ `timeit` æ¨¡å—æ¥æµ‹é‡æ‰§è¡Œ `code_snippet` æ‰€è€—çš„æ—¶é—´ã€‚ä½ é€šå¸¸èƒ½ä»¥ååˆ†ä¹‹å‡  GB/s åˆ°å‡  GB/s çš„é€Ÿåº¦éå†ä¸€ä¸ªæ•°æ®é›†ã€‚é€šè¿‡ä¸Šè¿°çš„æ–¹æ³•å°±å·²ç»èƒ½å¤Ÿè§£å†³å¤§å¤šæ•°å¤§æ•°æ®é›†åŠ è½½çš„é™åˆ¶ï¼Œä½†æ˜¯æœ‰æ—¶å€™ä½ ä¸å¾—ä¸ä½¿ç”¨ä¸€ä¸ªå¾ˆå¤§çš„æ•°æ®é›†ï¼Œå®ƒç”šè‡³éƒ½ä¸èƒ½å­˜å‚¨åœ¨ç¬”è®°æœ¬ç”µè„‘çš„ç¡¬ç›˜ä¸Šã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬å°è¯•ä¸‹è½½æ•´ä¸ª Pileï¼Œæˆ‘ä»¬éœ€è¦ 825GB çš„å¯ç”¨ç£ç›˜ç©ºé—´ï¼ä¸ºäº†å¤„ç†è¿™ç§æƒ…å†µï¼ŒDatasets æä¾›äº†ä¸€ä¸ªæµå¼åŠŸèƒ½ï¼Œè¿™ä¸ªåŠŸèƒ½å…è®¸æˆ‘ä»¬åŠ¨æ€ä¸‹è½½å’Œè®¿é—®å…ƒç´ ï¼Œå¹¶ä¸”ä¸éœ€è¦ä¸‹è½½æ•´ä¸ªæ•°æ®é›†ã€‚è®©æˆ‘ä»¬æ¥çœ‹çœ‹è¿™ä¸ªåŠŸèƒ½æ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚

<div custom-style="Tip-green">

ğŸ’¡åœ¨ Jupyter ç¬”è®°ä¸­ä½ è¿˜å¯ä»¥ä½¿ç”¨ [`%%timeit` é­”æœ¯å‡½æ•°](https://ipython.readthedocs.io/en/stable/interactive/magics.html#magic-timeit)(https://ipython.readthedocs.io/en/stable/interactive/magics.html#magic-timeit) ä¸ºæ•´ä¸ªå•å…ƒæ ¼è®¡æ—¶ã€‚

</div>

### æµå¼æ•°æ®é›† 

è¦ä½¿ç”¨æ•°æ®é›†æµï¼Œä½ åªéœ€è¦å°† `streaming=True` å‚æ•°ä¼ é€’ç»™ `load_dataset()` å‡½æ•°ã€‚æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬ä»¥æµæ¨¡å¼åŠ è½½ PubMed æ‘˜è¦æ•°æ®é›†ï¼š

```python
pubmed_dataset_streamed = load_dataset(
    "json", data_files=data_files, split="train", streaming=True
)
```

ä¸åŒäºæˆ‘ä»¬åœ¨è¿™ä¸€ç« å…¶å®ƒåœ°æ–¹é‡åˆ°çš„ç†Ÿæ‚‰çš„ `Dataset` ï¼Œ `streaming=True` è¿”å›çš„å¯¹è±¡æ˜¯ä¸€ä¸ª `IterableDataset` ã€‚é¡¾åæ€ä¹‰ï¼Œè¦è®¿é—® `IterableDataset` ï¼Œæˆ‘ä»¬éœ€è¦è¿­ä»£å®ƒã€‚æˆ‘ä»¬å¯ä»¥æŒ‰ç…§å¦‚ä¸‹æ–¹å¼è®¿é—®æµå¼æ•°æ®é›†çš„ç¬¬ä¸€ä¸ªå…ƒç´ ï¼š

```python
next(iter(pubmed_dataset_streamed))
```

```python
{'meta': {'pmid': 11409574, 'language': 'eng'},
 'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection.\nTo determine the prevalence of hypoxaemia in children aged under 5 years suffering acute lower respiratory infections (ALRI), the risk factors for hypoxaemia in children under 5 years of age with ALRI, and the association of hypoxaemia with an increased risk of dying in children of the same age ...'}
```

å¦‚æœä½ éœ€è¦åœ¨è®­ç»ƒæœŸé—´å¯¹æµå¼æ•°æ®é›†ä¸­çš„å…ƒç´  tokenizeï¼Œå¯ä»¥ä½¿ç”¨ `IterableDataset.map()` è¿›è¡Œåœ¨çº¿å¤„ç†ï¼Œè€Œä¸éœ€è¦ç­‰å¾…æ•°æ®é›†å…¨éƒ¨åŠ è½½å®Œæ¯•ã€‚è¯¥è¿‡ç¨‹ä¸æˆ‘ä»¬åœ¨ç¬¬å››ç« ä¸­å¯¹æ•°æ®é›† tokenize çš„è¿‡ç¨‹å®Œå…¨ç›¸åŒï¼Œå”¯ä¸€çš„åŒºåˆ«æ˜¯è¾“å‡ºæ˜¯é€ä¸ªè¿”å›çš„ï¼š

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
tokenized_dataset = pubmed_dataset_streamed.map(lambda x: tokenizer(x["text"]))
next(iter(tokenized_dataset))
```

```python
{'input_ids': [101, 4958, 5178, 4328, 6779, ...], 'attention_mask': [1, 1, 1, 1, 1, ...]}
```

<div custom-style="Tip-green">

ğŸ’¡ ä¸ºäº†åŠ é€Ÿæµå¼çš„ tokenizeï¼Œä½ å¯ä»¥ä¼ é€’ `batched=True` ï¼Œå°±åƒæˆ‘ä»¬åœ¨ä¸Šä¸€èŠ‚çœ‹åˆ°çš„é‚£æ ·ã€‚å®ƒä¼šæ‰¹é‡å¤„ç†ç¤ºä¾‹ï¼›é»˜è®¤çš„æ‰¹å¤§å°æ˜¯ 1000ï¼Œå¯ä»¥é€šè¿‡ `batch_size` å‚æ•°æŒ‡å®šæ‰¹é‡å¤§å°ã€‚

</div>

ä½ è¿˜å¯ä»¥ä½¿ç”¨ `IterableDataset.shuffle()` æ‰“ä¹±æµå¼æ•°æ®é›†ï¼Œä½†ä¸ `Dataset.shuffle()` ä¸åŒçš„æ˜¯è¿™åªä¼šæ‰“ä¹±é¢„å®šä¹‰ `buffer_size` ä¸­çš„å…ƒç´ ï¼š

```python
shuffled_dataset = pubmed_dataset_streamed.shuffle(buffer_size=10_000, seed=42)
next(iter(shuffled_dataset))
```

```python
{'meta': {'pmid': 11410799, 'language': 'eng'},
 'text': 'Randomized study of dose or schedule modification of granulocyte colony-stimulating factor in platinum-based chemotherapy for elderly patients with lung cancer ...'}
```

åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ä»ç¼“å†²åŒºçš„å‰ 10,000 ä¸ªç¤ºä¾‹ä¸­éšæœºé€‰æ‹©äº†ä¸€ä¸ªç¤ºä¾‹ã€‚ä¸€æ—¦è®¿é—®äº†ä¸€ä¸ªç¤ºä¾‹ï¼Œå®ƒåœ¨ç¼“å†²åŒºä¸­çš„ä½ç½®å°±ä¼šè¢«è¯­æ–™åº“ä¸­çš„ä¸‹ä¸€ä¸ªç¤ºä¾‹å¡«å…… ï¼ˆå³ï¼Œä¸Šè¿°æ¡ˆä¾‹ä¸­çš„ç¬¬ 10,001 ä¸ªç¤ºä¾‹ï¼‰ã€‚ä½ è¿˜å¯ä»¥ä½¿ç”¨ `IterableDataset.take()` å’Œ `IterableDataset.skip()` å‡½æ•°ä»æµå¼æ•°æ®é›†ä¸­é€‰æ‹©å…ƒç´ ï¼Œå®ƒçš„ä½œç”¨ç±»ä¼¼äº `Dataset.select()` ã€‚ä¾‹å¦‚ï¼Œè¦é€‰æ‹© PubMed Abstracts æ•°æ®é›†çš„å‰ 5 ä¸ªç¤ºä¾‹ï¼Œæˆ‘ä»¬å¯ä»¥æ‰§è¡Œä»¥ä¸‹ä»£ç ï¼š

```python
dataset_head = pubmed_dataset_streamed.take(5)
list(dataset_head)
```

```python
[{'meta': {'pmid': 11409574, 'language': 'eng'},
  'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection ...'},
 {'meta': {'pmid': 11409575, 'language': 'eng'},
  'text': 'Clinical signs of hypoxaemia in children with acute lower respiratory infection: indicators of oxygen therapy ...'},
 {'meta': {'pmid': 11409576, 'language': 'eng'},
  'text': "Hypoxaemia in children with severe pneumonia in Papua New Guinea ..."},
 {'meta': {'pmid': 11409577, 'language': 'eng'},
  'text': 'Oxygen concentrators and cylinders ...'},
 {'meta': {'pmid': 11409578, 'language': 'eng'},
  'text': 'Oxygen supply in rural africa: a personal experience ...'}]
```

åŒæ ·ï¼Œä½ å¯ä»¥ä½¿ç”¨ `IterableDataset.skip()` å‡½æ•°ä»æ‰“ä¹±çš„æ•°æ®é›†ä¸­åˆ›å»ºè®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```python
## è·³è¿‡å‰1,000ä¸ªç¤ºä¾‹,å°†å…¶ä½™éƒ¨åˆ†åˆ›å»ºä¸ºè®­ç»ƒé›†
train_dataset = shuffled_dataset.skip(1000)
## å°†å‰1,000ä¸ªç¤ºä¾‹ç”¨äºéªŒè¯é›†
validation_dataset = shuffled_dataset.take(1000)
```

è®©æˆ‘ä»¬ç”¨ä¸€ä¸ªå¸¸è§çš„ä»»åŠ¡æ¥è¿›è¡Œæˆ‘ä»¬å¯¹æ•°æ®é›†æµçš„æœ€åæ¢ç´¢ï¼šå°†å¤šä¸ªæ•°æ®é›†ç»„åˆåœ¨ä¸€èµ·åˆ›å»ºä¸€ä¸ªæ–°çš„è¯­æ–™åº“ã€‚Datasets æä¾›äº†ä¸€ä¸ª `interleave_datasets()` å‡½æ•°ï¼Œå®ƒå°†ä¸€ä¸ª `IterableDataset` å¯¹è±¡åˆ—è¡¨ç»„åˆä¸ºå•ä¸ªçš„ `IterableDataset` ï¼Œå…¶ä¸­æ–°æ•°æ®é›†çš„å…ƒç´ æ˜¯äº¤æ›¿æŠ½å–åˆ—è¡¨ä¸­çš„æ•°æ®é›†è·å¾—çš„ã€‚å½“ä½ è¯•å›¾ç»„åˆå¤§å‹æ•°æ®é›†æ—¶ï¼Œè¿™ä¸ªå‡½æ•°ç‰¹åˆ«æœ‰ç”¨ï¼Œè®©æˆ‘ä»¬é€šè¿‡ä¸‹é¢è¿™ä¸ªä¾‹å­æ¥è¯•ç€ç»„åˆ Pile çš„ FreeLaw æ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«ç¾å›½æ³•é™¢æ³•å¾‹æ„è§çš„ 51 GB æ•°æ®é›†ï¼š

```python
law_dataset_streamed = load_dataset(
    "json",
    data_files="https://the-eye.eu/public/AI/pile_preliminary_components/FreeLaw_Opinions.jsonl.zst",
    split="train",
    streaming=True,
)
next(iter(law_dataset_streamed))
```

```python
{'meta': {'case_ID': '110921.json',
  'case_jurisdiction': 'scotus.tar.gz',
  'date_created': '2010-04-28T17:12:49Z'},
 'text': '\n461 U.S. 238 (1983)\nOLIM ET AL.\nv.\nWAKINEKONA\nNo. 81-1581.\nSupreme Court of United States.\nArgued January 19, 1983.\nDecided April 26, 1983.\nCERTIORARI TO THE UNITED STATES COURT OF APPEALS FOR THE NINTH CIRCUIT\n*239 Michael A. Lilly, First Deputy Attorney General of Hawaii, argued the cause for petitioners. With him on the brief was James H. Dannenberg, Deputy Attorney General...'}
```

è¿™ä¸ªæ•°æ®é›†è¶³å¤Ÿå¤§ï¼Œå¯ä»¥å¯¹å¤§å¤šæ•°ç¬”è®°æœ¬ç”µè„‘çš„ RAM æœ‰è¶³å¤Ÿçš„å‹åŠ›ï¼Œä½†æ˜¯æˆ‘ä»¬å·²ç»èƒ½å¤Ÿæ¯«ä¸è´¹åŠ›åœ°åŠ è½½å’Œè®¿é—®å®ƒï¼ç°åœ¨æˆ‘ä»¬ä½¿ç”¨ `interleave_datasets()` å‡½æ•°å°† FreeLaw å’Œ PubMed Abstracts æ•°æ®é›†çš„æ ·æœ¬æ•´åˆåœ¨ä¸€èµ·ï¼š

```python
from itertools import islice
from datasets import interleave_datasets

combined_dataset = interleave_datasets([pubmed_dataset_streamed, law_dataset_streamed])
list(islice(combined_dataset, 2))
```

```python
[{'meta': {'pmid': 11409574, 'language': 'eng'},
  'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection ...'},
 {'meta': {'case_ID': '110921.json',
   'case_jurisdiction': 'scotus.tar.gz',
   'date_created': '2010-04-28T17:12:49Z'},
  'text': '\n461 U.S. 238 (1983)\nOLIM ET AL.\nv.\nWAKINEKONA\nNo. 81-1581.\nSupreme Court of United States.\nArgued January 19, 1983.\nDecided April 26, 1983.\nCERTIORARI TO THE UNITED STATES COURT OF APPEALS FOR THE NINTH CIRCUIT\n*239 Michael A. Lilly, First Deputy Attorney General of Hawaii, argued the cause for petitioners. With him on the brief was James H. Dannenberg, Deputy Attorney General...'}]
```

è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨äº†æ¥è‡ª Python çš„ `itertools` æ¨¡å—çš„ `islice()` å‡½æ•°ä»åˆå¹¶çš„æ•°æ®é›†ä¸­é€‰æ‹©å‰ä¸¤ä¸ªç¤ºä¾‹ï¼Œå¹¶ä¸”æˆ‘ä»¬å¯ä»¥çœ‹åˆ°å®ƒä»¬å®é™…ä¸Šå°±æ˜¯ä¸¤ä¸ªæºæ•°æ®é›†ä¸­çš„å‰ä¸¤ä¸ªç¤ºä¾‹æ‹¼åœ¨ä¸€èµ·å½¢æˆçš„ï¼š

æœ€åï¼Œå¦‚æœä½ æƒ³æµå¼ä¼ è¾“æ•´ä¸ª 825GB çš„ Pileï¼Œä½ å¯ä»¥æŒ‰ç…§å¦‚ä¸‹æ–¹å¼è·å–æ‰€æœ‰çš„é¢„å¤„ç†æ–‡ä»¶ï¼š

```python
base_url = "https://the-eye.eu/public/AI/pile/"
data_files = {
    "train": [base_url + "train/" + f"{idx:02d}.jsonl.zst" for idx in range(30)],
    "validation": base_url + "val.jsonl.zst",
    "test": base_url + "test.jsonl.zst",
}
pile_dataset = load_dataset("json", data_files=data_files, streaming=True)
next(iter(pile_dataset["train"]))
```

```python
{'meta': {'pile_set_name': 'Pile-CC'},
 'text': 'It is done, and submitted. You can play â€œSurvival of the Tastiestâ€ on Android, and on the web...'}
```

<div custom-style="Tip-green">

âœï¸ **è¯•è¯•çœ‹ï¼** ä½¿ç”¨åƒ [`mc4`](https://huggingface.co/datasets/mc4)(https://huggingface.co/datasets/mc4) æˆ–è€… [`oscar`](https://huggingface.co/datasets/oscar)(https://huggingface.co/datasets/oscar) è¿™æ ·çš„å¤§å‹ Common Crawl è¯­æ–™åº“æ¥åˆ›å»ºä¸€ä¸ªæµå¼å¤šè¯­è¨€æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†ä»£è¡¨ä½ é€‰æ‹©çš„å›½å®¶/åœ°åŒºè¯­è¨€çš„å£è¯­æ¯”ä¾‹ã€‚ä¾‹å¦‚ï¼Œç‘å£«çš„å››ç§æ°‘æ—è¯­è¨€åˆ†åˆ«æ˜¯å¾·è¯­ã€æ³•è¯­ã€æ„å¤§åˆ©è¯­å’Œç½—æ›¼ä»€è¯­ï¼Œå› æ­¤ä½ å¯ä»¥å°è¯•æ ¹æ®æ ¹æ®å£è¯­æ¯”ä¾‹å¯¹ Oscar å­é›†è¿›è¡ŒæŠ½æ ·æ¥åˆ›å»ºä¸€ä¸ªç‘å£«è¯­æ–™åº“ã€‚

</div>

ä½ ç°åœ¨æ‹¥æœ‰åŠ è½½å’Œå¤„ç†å„ç§ç±»å‹å’Œå¤§å°çš„æ•°æ®é›†çš„æ‰€éœ€çš„æ‰€æœ‰å·¥å…· â€”â€” ä½†æ˜¯é™¤éä½ éå¸¸å¹¸è¿ï¼Œå¦åˆ™åœ¨ä½ çš„ NLP ä¹‹æ—…ä¸­ä¼šæœ‰ä¸€ä¸ªéš¾é¢˜ï¼Œä½ å°†ä¸å¾—ä¸äº²è‡ªåˆ›å»ºä¸€ä¸ªæ•°æ®é›†æ¥è§£å†³æ‰‹å¤´çš„é—®é¢˜ã€‚è¿™å°±æ˜¯æˆ‘ä»¬æ¥ä¸‹æ¥è¦è®¨è®ºçš„ä¸»é¢˜ï¼

## 6.4 åˆ›å»ºè‡ªå·±çš„æ•°æ®é›† 

æœ‰æ—¶ï¼Œä¸å­˜åœ¨ç°æœ‰çš„åˆé€‚çš„æ•°æ®é›†é€‚ç”¨äºä½ æ„å»º NLP åº”ç”¨ï¼Œå› æ­¤ä½ éœ€è¦è‡ªå·±åˆ›å»ºã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†å‘ä½ å±•ç¤ºå¦‚ä½•åˆ›å»ºä¸€ä¸ªç”± [GitHub issues](https://github.com/features/issues/)(https://github.com/features/issues/) ç»„æˆçš„çš„è¯­æ–™åº“ï¼Œè¿™äº› issues é€šå¸¸ç”¨äºè·Ÿè¸ª GitHub ä»“åº“ä¸­çš„é”™è¯¯æˆ–åŠŸèƒ½ã€‚è¯¥è¯­æ–™åº“å¯ç”¨äºå„ç§ç›®çš„ï¼ŒåŒ…æ‹¬ï¼š
* æ¢ç´¢å…³é—­æœªè§£å†³çš„ issue æˆ– pull è¯·æ±‚éœ€è¦å¤šé•¿æ—¶é—´
* è®­ç»ƒä¸€ä¸ª `å¤šæ ‡ç­¾åˆ†ç±»å™¨ï¼ˆmultilabel classifierï¼‰` å¯ä»¥æ ¹æ® issue çš„æè¿°ä¸º issue æ ‡ä¸Šå…ƒæ•°æ®æ ‡ç­¾ï¼ˆä¾‹å¦‚ï¼Œâ€œbugâ€ã€â€œenhancementï¼ˆå¢å¼ºåŠŸèƒ½ï¼‰â€æˆ–â€œquestionâ€ï¼‰
* åˆ›å»ºè¯­ä¹‰æœç´¢å¼•æ“ä»¥æŸ¥æ‰¾ä¸ç”¨æˆ·æŸ¥è¯¢åŒ¹é…çš„ issue

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†å…³æ³¨å¦‚ä½•åˆ›å»ºè¯­æ–™åº“ï¼Œåœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æ¢ç´¢è¯­ä¹‰æœç´¢ã€‚åŸæ±¤åŒ–åŸé£Ÿï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸ä¸€ä¸ªæµè¡Œçš„å¼€æºé¡¹ç›®å…³è”çš„ GitHub issueï¼šDatasetsï¼æ¥ä¸‹æ¥è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•è·å–æ•°æ®å¹¶æ¢ç´¢è¿™äº› issue ä¸­åŒ…å«çš„ä¿¡æ¯ã€‚

### è·å–æ•°æ® 

ä½ å¯ä»¥ç‚¹å‡» [Issues é€‰é¡¹å¡](https://github.com/huggingface/datasets/issues)(https://github.com/huggingface/datasets/issues)(https://github.com/huggingface/datasets/issues) æµè§ˆ Datasets ä¸­çš„æ‰€æœ‰ issusã€‚å¦‚ä»¥ä¸‹å±å¹•æˆªå›¾æ‰€ç¤ºï¼Œåœ¨æ’°å†™æœ¬æ–‡æ—¶ï¼Œæœ‰ 331 ä¸ªæœªè§£å†³çš„ issue å’Œ 668 ä¸ªå·²å…³é—­çš„ issueã€‚

![ä¸ Datasets åº“ç›¸å…³çš„ GitHub issue](./assets/datasets-issues.png "The GitHub issues associated with Datasets.")

å¦‚æœä½ å•å‡»å…¶ä¸­ä¸€ä¸ª issueï¼Œä½ ä¼šå‘ç°å®ƒåŒ…å«ä¸€ä¸ªæ ‡é¢˜ã€ä¸€ä¸ªæè¿°å’Œä¸€ç»„æ ‡ç­¾ï¼Œè¿™äº›æ ‡ç­¾æ˜¯å¯¹ issue çš„æè¿°ã€‚ä¸‹é¢çš„å±å¹•æˆªå›¾æ˜¾ç¤ºäº†ä¸€ä¸ªç¤ºä¾‹ï¼š

![Datasets ä»“åº“ä¸­çš„å…¸å‹ GitHub issue](./assets/datasets-issues-single.png "A typical GitHub issue in the Datasets repository.")

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ [GitHub REST API](https://docs.github.com/en/rest)(https://docs.github.com/en/rest) éå†Issues èŠ‚ç‚¹(endpoint)ä¸‹è½½æ‰€æœ‰ä»“åº“çš„ issueã€‚èŠ‚ç‚¹ï¼ˆendpointï¼‰å°†è¿”å›ä¸€ä¸ª JSON å¯¹è±¡åˆ—è¡¨ï¼Œæ¯ä¸ªå¯¹è±¡åŒ…å«å¤§é‡å­—æ®µï¼Œå…¶ä¸­åŒ…æ‹¬æ ‡é¢˜å’Œæè¿°ä»¥åŠæœ‰å…³ issue çŠ¶æ€çš„å…ƒæ•°æ®ç­‰ã€‚

æˆ‘ä»¬å°†ä½¿ç”¨ `requests` åº“æ¥ä¸‹è½½ï¼Œè¿™æ˜¯ç”¨ Python ä¸­å‘å‡º HTTP è¯·æ±‚çš„æ ‡å‡†æ–¹å¼ã€‚ä½ å¯ä»¥é€šè¿‡è¿è¡Œä»¥ä¸‹çš„ä»£ç æ¥å®‰è£…åº“ï¼š

```python
!pip install requests
```

å®‰è£…åº“åï¼Œä½ é€šè¿‡è°ƒç”¨ `requests.get()` åŠŸèƒ½æ¥è·å– `Issues` èŠ‚ç‚¹ï¼ˆendpointï¼‰ã€‚ä¾‹å¦‚ï¼Œä½ å¯ä»¥è¿è¡Œä»¥ä¸‹å‘½ä»¤æ¥è·å–ç¬¬ä¸€é¡µä¸Šçš„ç¬¬ä¸€ä¸ª Issuesï¼š

```python
import requests

url = "https://api.github.com/repos/huggingface/datasets/issues?page=1&per_page=1"
response = requests.get(url)
```

`response` å¯¹è±¡åŒ…å«å¾ˆå¤šå…³äºè¯·æ±‚çš„æœ‰ç”¨ä¿¡æ¯,åŒ…æ‹¬ HTTP çŠ¶æ€ç :

```python
response.status_code
```

```python
200
```

å…¶ä¸­ï¼Œ `200` çŠ¶æ€è¡¨ç¤ºè¯·æ±‚æˆåŠŸï¼ˆä½ å¯ä»¥åœ¨ [è¿™é‡Œ](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)(https://en.wikipedia.org/wiki/List_of_HTTP_status_codes) æ‰¾åˆ°æ‰€æœ‰å¯èƒ½çš„ HTTP çŠ¶æ€ä»£ç åˆ—è¡¨ï¼‰ã€‚ä¸è¿‡ï¼Œæˆ‘ä»¬çœŸæ­£æ„Ÿå…´è¶£çš„æ˜¯æ¶ˆæ¯ä½“ä¸­çš„æœ‰æ•ˆä¿¡æ¯ï¼Œç”±äºæˆ‘ä»¬çŸ¥é“æˆ‘ä»¬çš„ issues æ˜¯ JSON æ ¼å¼ï¼Œè®©æˆ‘ä»¬æŒ‰å¦‚ä¸‹æ–¹å¼æŸ¥çœ‹æ¶ˆæ¯ä½“çš„ä¿¡æ¯ï¼š

```python
response.json()
```

```python
[{'url': 'https://api.github.com/repos/huggingface/datasets/issues/2792',
  'repository_url': 'https://api.github.com/repos/huggingface/datasets',
  'labels_url': 'https://api.github.com/repos/huggingface/datasets/issues/2792/labels{/name}',
  'comments_url': 'https://api.github.com/repos/huggingface/datasets/issues/2792/comments',
  'events_url': 'https://api.github.com/repos/huggingface/datasets/issues/2792/events',
  'html_url': 'https://github.com/huggingface/datasets/pull/2792',
  'id': 968650274,
  'node_id': 'MDExOlB1bGxSZXF1ZXN0NzEwNzUyMjc0',
  'number': 2792,
  'title': 'Update GooAQ',
  'user': {'login': 'bhavitvyamalik',
   'id': 19718818,
   'node_id': 'MDQ6VXNlcjE5NzE4ODE4',
   'avatar_url': 'https://avatars.githubusercontent.com/u/19718818?v=4',
   'gravatar_id': '',
   'url': 'https://api.github.com/users/bhavitvyamalik',
   'html_url': 'https://github.com/bhavitvyamalik',
   'followers_url': 'https://api.github.com/users/bhavitvyamalik/followers',
   'following_url': 'https://api.github.com/users/bhavitvyamalik/following{/other_user}',
   'gists_url': 'https://api.github.com/users/bhavitvyamalik/gists{/gist_id}',
   'starred_url': 'https://api.github.com/users/bhavitvyamalik/starred{/owner}{/repo}',
   'subscriptions_url': 'https://api.github.com/users/bhavitvyamalik/subscriptions',
   'organizations_url': 'https://api.github.com/users/bhavitvyamalik/orgs',
   'repos_url': 'https://api.github.com/users/bhavitvyamalik/repos',
   'events_url': 'https://api.github.com/users/bhavitvyamalik/events{/privacy}',
   'received_events_url': 'https://api.github.com/users/bhavitvyamalik/received_events',
   'type': 'User',
   'site_admin': False},
  'labels': [],
  'state': 'open',
  'locked': False,
  'assignee': None,
  'assignees': [],
  'milestone': None,
  'comments': 1,
  'created_at': '2021-08-12T11:40:18Z',
  'updated_at': '2021-08-12T12:31:17Z',
  'closed_at': None,
  'author_association': 'CONTRIBUTOR',
  'active_lock_reason': None,
  'pull_request': {'url': 'https://api.github.com/repos/huggingface/datasets/pulls/2792',
   'html_url': 'https://github.com/huggingface/datasets/pull/2792',
   'diff_url': 'https://github.com/huggingface/datasets/pull/2792.diff',
   'patch_url': 'https://github.com/huggingface/datasets/pull/2792.patch'},
  'body': '[GooAQ](https://github.com/allenai/gooaq)(https://github.com/allenai/gooaq) dataset was recently updated after splits were added for the same. This PR contains new updated GooAQ with train/val/test splits and updated README as well.',
  'performed_via_github_app': None}]
```

å“‡ï¼Œå¥½å¤§é‡çš„ä¿¡æ¯ï¼æˆ‘ä»¬å¯ä»¥çœ‹åˆ°æœ‰ç”¨çš„å­—æ®µï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¯¸å¦‚ `title` ã€ `body` å’Œ `number` ç­‰æè¿° issue çš„æœ‰ç”¨å­—æ®µï¼Œä»¥åŠå…³äºåˆ›å»º issue çš„ GitHub ç”¨æˆ·çš„ä¿¡æ¯ã€‚

<div custom-style="Tip-green">

âœï¸ **è¯•è¯•çœ‹ï¼**æ‰“å¼€ä¸Šé¢ JSON ä¸­çš„ä¸€äº› URLï¼Œä»¥äº†è§£æ¯ä¸ª GitHub issue ä¸­ url æ‰€é“¾æ¥çš„ä¿¡æ¯ç±»å‹ã€‚
</div>

å¦‚ GitHub [æ–‡æ¡£](https://docs.github.com/en/rest/overview/resources-in-the-rest-api#rate-limiting)(https://docs.github.com/en/rest/overview/resources-in-the-rest-api#rate-limiting) ä¸­æ‰€è¿°ï¼Œæœªç»èº«ä»½éªŒè¯çš„è¯·æ±‚é™åˆ¶ä¸ºæ¯å°æ—¶ 60 ä¸ªè¯·æ±‚ã€‚è™½ç„¶ä½ å¯ä»¥å¢åŠ  `per_page` æŸ¥è¯¢å‚æ•°ä»¥å‡å°‘ä½ å‘å‡ºçš„è¯·æ±‚æ¬¡æ•°ï¼Œä½†ä½ ä»ä¼šåœ¨ä»»ä½•æœ‰å‡ åƒä¸ªä»¥ä¸Š issue çš„ä»“åº“ä¸Šè§¦å‘é€Ÿç‡é™åˆ¶ã€‚å› æ­¤ï¼Œä½ åº”è¯¥æŒ‰ç…§ GitHub çš„ [è¯´æ˜](https://docs.github.com/en/github/authenticating-to-github/creating-a-personal-access-token)(https://docs.github.com/en/github/authenticating-to-github/creating-a-personal-access-token) ï¼Œåˆ›å»ºä¸€ä¸ª `ä¸ªäººè®¿é—®ä»¤ç‰Œï¼ˆpersonal access tokenï¼‰` è¿™æ ·ä½ å°±å¯ä»¥å°†é€Ÿç‡é™åˆ¶æé«˜åˆ°æ¯å°æ—¶ 5,000 ä¸ªè¯·æ±‚ã€‚è·å¾—ä»¤ç‰Œåï¼Œä½ å¯ä»¥å°†å…¶æ”¾åœ¨è¯·æ±‚æ ‡å¤´ä¸­ï¼š

```python
GITHUB_TOKEN = xxx  #  å°†ä½ çš„GitHubä»¤ç‰Œå¤åˆ¶åˆ°æ­¤å¤„
headers = {"Authorization": f"token {GITHUB_TOKEN}"}
```

<div custom-style="Tip-yellow">

âš ï¸ ä¸è¦ä¸é™Œç”Ÿäººå…±äº«å­˜åœ¨ `GITHUBä»¤ç‰Œ` çš„ç¬”è®°æœ¬ã€‚æˆ‘ä»¬å»ºè®®ä½ åœ¨ä½¿ç”¨å®Œåå°† `GITHUBä»¤ç‰Œ` åˆ é™¤ï¼Œä»¥é¿å…æ„å¤–æ³„æ¼ã€‚ä¸€ä¸ªæ›´å¥½çš„åšæ³•æ˜¯ï¼Œå°†ä»¤ç‰Œå­˜å‚¨åœ¨ï¼env æ–‡ä»¶ä¸­ï¼Œå¹¶ä½¿ç”¨ [`python-dotenv`åº“](https://github.com/theskumar/python-dotenv)(https://github.com/theskumar/python-dotenv) è‡ªåŠ¨åŠ è½½ç¯å¢ƒå˜é‡ã€‚

</div>

ç°åœ¨æˆ‘ä»¬æœ‰äº†è®¿é—®ä»¤ç‰Œï¼Œè®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå¯ä»¥ä» GitHub ä»“åº“ä¸‹è½½æ‰€æœ‰ issue çš„å‡½æ•°ï¼š

```python
import time
import math
from pathlib import Path
import pandas as pd
from tqdm.notebook import tqdm

def fetch_issues(
    owner="huggingface",
    repo="datasets",
    num_issues=10_000,
    rate_limit=5_000,
    issues_path=Path("."),
):
    if not issues_path.is_dir():
        issues_path.mkdir(exist_ok=True)

    batch = []
    all_issues = []
    per_page = 100  ## æ¯é¡µè¿”å›çš„ issue çš„æ•°é‡
    num_pages = math.ceil(num_issues / per_page)
    base_url = "https://api.github.com/repos"

    for page in tqdm(range(num_pages)):
        # ä½¿ç”¨ state=all è¿›è¡ŒæŸ¥è¯¢æ¥è·å– open å’Œ closed çš„issue
        query = f"issues?page={page}&per_page={per_page}&state=all"
        issues = requests.get(f"{base_url}/{owner}/{repo}/{query}", headers=headers)
        batch.extend(issues.json())

        if len(batch) > rate_limit and len(all_issues) < num_issues:
            all_issues.extend(batch)
            batch = []  # é‡ç½®batch
            print(f"Reached GitHub rate limit. Sleeping for one hour ...")
            time.sleep(60 * 60 + 1)

    all_issues.extend(batch)
    df = pd.DataFrame.from_records(all_issues)
    df.to_json(f"{issues_path}/{repo}-issues.jsonl", orient="records", lines=True)
    print(
        f"Downloaded all the issues for {repo}! Dataset stored at {issues_path}/{repo}-issues.jsonl"
    )
```

ç°åœ¨æˆ‘ä»¬å¯ä»¥è°ƒç”¨ `fetch_issues()` å®ƒå°†æŒ‰æ‰¹æ¬¡ä¸‹è½½æ‰€æœ‰ issueï¼Œä»¥é¿å…è¶…è¿‡ GitHub æ¯å°æ—¶è¯·æ±‚æ¬¡æ•°çš„é™åˆ¶ï¼›ç»“æœå°†å­˜å‚¨åœ¨ `repository_name-issues.jsonl` æ–‡ä»¶ï¼Œå…¶ä¸­æ¯ä¸€è¡Œéƒ½æ˜¯ä¸€ä¸ª JSON å¯¹è±¡ï¼Œä»£è¡¨ä¸€ä¸ª issueã€‚è®©æˆ‘ä»¬ä½¿ç”¨è¿™ä¸ªå‡½æ•°ä» Datasets ä¸­æŠ“å–æ‰€æœ‰ issueï¼š

```python
## å–å†³äºä½ çš„ç½‘ç»œè¿æ¥,è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿçš„æ—¶é—´æ¥è¿è¡Œ...
fetch_issues()
```

ä¸‹è½½ issue åï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æˆ‘ä»¬åœ¨ç¬¬äºŒèŠ‚æ–°å­¦ä¼šçš„æ–¹æ³•åœ¨æœ¬åœ°åŠ è½½å®ƒä»¬ï¼š

```python
issues_dataset = load_dataset("json", data_files="datasets-issues.jsonl", split="train")
issues_dataset
```

```python
Dataset({
    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'timeline_url', 'performed_via_github_app'],
    num_rows: 3019
})
```

å¤ªå¥½äº†ï¼Œæˆ‘ä»¬å·²ç»ä»å¤´å¼€å§‹åˆ›å»ºäº†æˆ‘ä»¬çš„ç¬¬ä¸€ä¸ªæ•°æ®é›†ï¼ä½†æ˜¯ä¸ºä»€ä¹ˆä¼šæœ‰å‡ åƒä¸ª issueï¼Œè€ŒDatasets ä»“åº“ä¸­çš„ [Issues é€‰é¡¹å¡](https://github.com/huggingface/datasets/issues)(https://github.com/huggingface/datasets/issues)(https://github.com/huggingface/datasets/issues) æ€»å…±å´åªæ˜¾ç¤ºäº†å¤§çº¦ 1,000 ä¸ª issueğŸ¤”ï¼Ÿå¦‚ GitHub [æ–‡æ¡£](https://docs.github.com/en/rest/reference/issues#list-issues-assigned-to-the-authenticated-user)(https://docs.github.com/en/rest/reference/issues#list-issues-assigned-to-the-authenticated-user) ä¸­æ‰€è¿°ï¼Œé‚£æ˜¯å› ä¸ºæˆ‘ä»¬ä¸‹è½½äº†æ‰€æœ‰çš„ pull è¯·æ±‚ï¼š

>Git Hub çš„ REST API v3 è®¤ä¸ºæ¯ä¸ª pull è¯·æ±‚éƒ½æ˜¯ä¸€ä¸ª issueï¼Œä½†å¹¶ä¸æ˜¯æ¯ä¸ª issue éƒ½æ˜¯ä¸€ä¸ª pull è¯·æ±‚ã€‚å› æ­¤ï¼Œâ€œIssuesâ€èŠ‚ç‚¹å¯èƒ½åŒæ—¶è¿”å›äº† issue å’Œ pull è¯·æ±‚ã€‚ä½ å¯ä»¥é€šè¿‡ `pull_request` çš„ key æ¥è¾¨åˆ« pull è¯·æ±‚ã€‚è¯·æ³¨æ„ï¼Œä»â€œIssuesâ€èŠ‚ç‚¹è¿”å›çš„ pull è¯·æ±‚çš„ id å°†æ˜¯ä¸€ä¸ª issue idã€‚

ç”±äº issue å’Œ pull request çš„å†…å®¹æœ‰å¾ˆå¤§çš„ä¸åŒï¼Œæˆ‘ä»¬å…ˆåšä¸€äº›å°çš„é¢„å¤„ç†ï¼Œè®©æˆ‘ä»¬èƒ½å¤ŸåŒºåˆ†å®ƒä»¬ã€‚

### æ¸…æ´—æ•°æ® 

ä¸Šé¢çš„ GitHub æ–‡æ¡£å‘Šè¯‰æˆ‘ä»¬ï¼Œ `pull_request` åˆ—å¯ç”¨äºåŒºåˆ† issue å’Œ pull è¯·æ±‚ã€‚è®©æˆ‘ä»¬éšæœºæŒ‘é€‰ä¸€äº›æ ·æœ¬ï¼Œçœ‹çœ‹æœ‰ä»€ä¹ˆä¸åŒã€‚æˆ‘ä»¬å°†ä½¿ç”¨åœ¨ç¬¬ä¸‰èŠ‚ï¼Œå­¦ä¹ çš„æ–¹æ³•ï¼Œä½¿ç”¨ `Dataset.shuffle()` å’Œ `Dataset.select()` æŠ½å–ä¸€ä¸ªéšæœºæ ·æœ¬ï¼Œç„¶åå°† `html_url` å’Œ `pull_request` åˆ—ä½¿ç”¨ zip å‡½æ•°ç»„åˆèµ·æ¥ï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥æ¯”è¾ƒå„ç§ URLï¼š

```python
sample = issues_dataset.shuffle(seed=666).select(range(3))

## æ‰“å°å‡º URL å’Œ pull è¯·æ±‚
for url, pr in zip(sample["html_url"], sample["pull_request"]):
    print(f">> URL: {url}")
    print(f">> Pull request: {pr}\n")
```

```python
>> URL: https://github.com/huggingface/datasets/pull/850
>> Pull request: {'url': 'https://api.github.com/repos/huggingface/datasets/pulls/850', 'html_url': 'https://github.com/huggingface/datasets/pull/850', 'diff_url': 'https://github.com/huggingface/datasets/pull/850.diff', 'patch_url': 'https://github.com/huggingface/datasets/pull/850.patch'}

>> URL: https://github.com/huggingface/datasets/issues/2773
>> Pull request: None

>> URL: https://github.com/huggingface/datasets/pull/783
>> Pull request: {'url': 'https://api.github.com/repos/huggingface/datasets/pulls/783', 'html_url': 'https://github.com/huggingface/datasets/pull/783', 'diff_url': 'https://github.com/huggingface/datasets/pull/783.diff', 'patch_url': 'https://github.com/huggingface/datasets/pull/783.patch'}
```

è¿™é‡Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œæ¯ä¸ª pull è¯·æ±‚éƒ½ä¸å„ç§ url ç›¸å…³è”ï¼Œè€Œæ™®é€š issue åªæœ‰ä¸€ä¸ª `None` æ¡ç›®ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¿™ä¸€ç‚¹ä¸åŒæ¥åˆ›å»ºä¸€ä¸ªæ–°çš„ `is_pull_request` åˆ—ï¼Œé€šè¿‡æ£€æŸ¥ `pull_request` å­—æ®µæ˜¯å¦ä¸º `None` æ¥åŒºåˆ†å®ƒä»¬ï¼š

```python
issues_dataset = issues_dataset.map(
    lambda x: {"is_pull_request": False if x["pull_request"] is None else True}
)
```

<div custom-style="Tip-green">

âœï¸ **è¯•è¯•çœ‹ï¼**è®¡ç®—åœ¨ Datasets ä¸­è§£å†³ issue æ‰€éœ€çš„å¹³å‡æ—¶é—´ã€‚ä½ å¯èƒ½ä¼šå‘ç° `Dataset.filter()` å‡½æ•°å¯¹äºè¿‡æ»¤ pull è¯·æ±‚å’Œæœªè§£å†³çš„ issue å¾ˆæœ‰ç”¨ï¼Œå¹¶ä¸”ä½ å¯ä»¥ä½¿ç”¨ `Dataset.set_format()` å‡½æ•°å°†æ•°æ®é›†è½¬æ¢ä¸º `DataFrame` ï¼Œä»¥ä¾¿ä½ å¯ä»¥è½»æ¾åœ°æŒ‰ç…§éœ€æ±‚ä¿®æ”¹ `åˆ›å»º(created_at)` å’Œ `å…³é—­(closed_at)` çš„æ—¶é—´çš„æ ¼å¼ï¼ˆä»¥æ—¶é—´æˆ³æ ¼å¼ï¼‰ã€‚

</div>

å°½ç®¡æˆ‘ä»¬å¯ä»¥é€šè¿‡åˆ é™¤æˆ–é‡å‘½åæŸäº›åˆ—æ¥è¿›ä¸€æ­¥æ¸…ç†æ•°æ®é›†ï¼Œä½†åœ¨æ­¤é˜¶æ®µå°½å¯èƒ½ä¿æŒæ•°æ®é›†â€œåŸå§‹â€çŠ¶æ€é€šå¸¸æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„åšæ³•ï¼Œä»¥ä¾¿å®ƒå¯ä»¥åœ¨å¤šä¸ªä¸åŒçš„é¡¹ç›®ä¸­è½»æ¾ä½¿ç”¨ã€‚åœ¨æˆ‘ä»¬å°†æ•°æ®é›†æ¨é€åˆ° Hugging Face Hub ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å†æ·»åŠ ä¸€äº›ç¼ºå°‘çš„æ•°æ®ï¼šä¸æ¯ä¸ª issue å’Œ pull è¯·æ±‚ç›¸å…³çš„è¯„è®ºã€‚æˆ‘ä»¬æ¥ä¸‹æ¥å°†æ·»åŠ å®ƒä»¬â€”â€”ä½ çŒœå¯¹äº†â€”â€”æˆ‘ä»¬å°†ä¾ç„¶ä½¿ç”¨ GitHub REST APIï¼

### æ‰©å……æ•°æ®é›† 

å¦‚ä»¥ä¸‹æˆªå›¾æ‰€ç¤ºï¼Œissue æˆ– pull è¯·æ±‚ç›¸å…³çš„è¯„è®ºæä¾›äº†ä¸°å¯Œçš„ä¿¡æ¯ï¼Œç‰¹åˆ«æ˜¯å¦‚æœæˆ‘ä»¬æœ‰å…´è¶£æ„å»ºæœç´¢å¼•æ“æ¥å›ç­”ç”¨æˆ·å¯¹è¿™ä¸ªä»“åº“çš„ç–‘é—®æ—¶ã€‚

![ä¸ Datasets ä»“åº“ issue ç›¸å…³çš„è¯„è®º](./assets/datasets-issues-comment.png "Comments associated with an issue about Datasets.")

GitHub REST API æä¾›äº†ä¸€ä¸ª`Comments(è¯„è®º)`èŠ‚ç‚¹è¿”å›ä¸ issue ç¼–å·ç›¸å…³çš„æ‰€æœ‰è¯„è®ºã€‚è®©æˆ‘ä»¬æµ‹è¯•ä¸€ä¸‹è¯¥èŠ‚ç‚¹è¿”å›çš„å†…å®¹ï¼š

```python
issue_number = 2792
url = f"https://api.github.com/repos/huggingface/datasets/issues/{issue_number}/comments"
response = requests.get(url, headers=headers)
response.json()
```

```python
[{'url': 'https://api.github.com/repos/huggingface/datasets/issues/comments/897594128',
  'html_url': 'https://github.com/huggingface/datasets/pull/2792#issuecomment-897594128',
  'issue_url': 'https://api.github.com/repos/huggingface/datasets/issues/2792',
  'id': 897594128,
  'node_id': 'IC_kwDODunzps41gDMQ',
  'user': {'login': 'bhavitvyamalik',
   'id': 19718818,
   'node_id': 'MDQ6VXNlcjE5NzE4ODE4',
   'avatar_url': 'https://avatars.githubusercontent.com/u/19718818?v=4',
   'gravatar_id': '',
   'url': 'https://api.github.com/users/bhavitvyamalik',
   'html_url': 'https://github.com/bhavitvyamalik',
   'followers_url': 'https://api.github.com/users/bhavitvyamalik/followers',
   'following_url': 'https://api.github.com/users/bhavitvyamalik/following{/other_user}',
   'gists_url': 'https://api.github.com/users/bhavitvyamalik/gists{/gist_id}',
   'starred_url': 'https://api.github.com/users/bhavitvyamalik/starred{/owner}{/repo}',
   'subscriptions_url': 'https://api.github.com/users/bhavitvyamalik/subscriptions',
   'organizations_url': 'https://api.github.com/users/bhavitvyamalik/orgs',
   'repos_url': 'https://api.github.com/users/bhavitvyamalik/repos',
   'events_url': 'https://api.github.com/users/bhavitvyamalik/events{/privacy}',
   'received_events_url': 'https://api.github.com/users/bhavitvyamalik/received_events',
   'type': 'User',
   'site_admin': False},
  'created_at': '2021-08-12T12:21:52Z',
  'updated_at': '2021-08-12T12:31:17Z',
  'author_association': 'CONTRIBUTOR',
  'body': "@albertvillanova my tests are failing here:\r\n```\r\ndataset_name = 'gooaq'\r\n\r\n    def test_load_dataset(self, dataset_name):r\n        configs = self.dataset_tester.load_all_configs(dataset_name, is_local=True)[:1]\r\n>       self.dataset_tester.check_load_dataset(dataset_name, configs, is_local=True, use_local_dummy_data=True)\r\n\r\ntests/test_dataset_common.py:234: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\ntests/test_dataset_common.py:187: in check_load_dataset\r\n    self.parent.assertTrue(len(dataset[split]) > 0)\r\nE   AssertionError: False is not true\r\n```python
  'performed_via_github_app': None}]
```

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¯„è®ºå­˜å‚¨åœ¨ `body` å­—æ®µä¸­ï¼Œå› æ­¤è®©æˆ‘ä»¬ç¼–å†™ä¸€ä¸ªç®€å•çš„å‡½æ•°ï¼Œé€šè¿‡æŒ‘é€‰å‡º `response.json()` ä¸­æ¯ä¸ªå…ƒç´ çš„ `body` å†…å®¹ï¼Œè¿”å›ä¸æŸä¸ª issue ç›¸å…³çš„æ‰€æœ‰è¯„è®ºï¼š

```python
def get_comments(issue_number):
    url = f"https://api.github.com/repos/huggingface/datasets/issues/{issue_number}/comments"
    response = requests.get(url, headers=headers)
    return [r["body"] for r in response.json()]

## æµ‹è¯•æˆ‘ä»¬çš„å‡½æ•°æ˜¯å¦æŒ‰é¢„æœŸå·¥ä½œ
get_comments(2792)
```

```python
["@albertvillanova my tests are failing here:\r\n```\r\ndataset_name = 'gooaq'\r\n\r\n    def test_load_dataset(self, dataset_name):r\n        configs = self.dataset_tester.load_all_configs(dataset_name, is_local=True)[:1]\r\n>       self.dataset_tester.check_load_dataset(dataset_name, configs, is_local=True, use_local_dummy_data=True)\r\n\r\ntests/test_dataset_common.py:234: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\ntests/test_dataset_common.py:187: in check_load_dataset\r\n    self.parent.assertTrue(len(dataset[split]) > 0)\r\nE   AssertionError: False is not true\r\n```python
```

è¿™çœ‹èµ·æ¥ä¸é”™ï¼Œæ‰€ä»¥è®©æˆ‘ä»¬ä½¿ç”¨ `Dataset.map()` æ–¹æ³•ï¼Œä¸ºæˆ‘ä»¬æ•°æ®é›†ä¸­æ¯ä¸ª issue çš„æ·»åŠ ä¸€ä¸ª `comments` åˆ—ï¼š

```python
## å–å†³äºä½ çš„ç½‘ç»œè¿æ¥,è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´...
issues_with_comments_dataset = issues_dataset.map(
    lambda x: {"comments": get_comments(x["number"])}
)
```

æœ€åä¸€æ­¥æ˜¯å°†æˆ‘ä»¬çš„æ•°æ®é›†æ¨é€åˆ° Hubï¼Œè®©æˆ‘ä»¬ä¸€èµ·çœ‹çœ‹è¯¥æ€ä¹ˆæ¨é€ï¼š

### å°†æ•°æ®é›†ä¸Šä¼ åˆ° Hugging Face Hub 

ç°åœ¨æˆ‘ä»¬æœ‰äº†æ‰©å……åçš„æ•°æ®é›†ï¼Œæ˜¯æ—¶å€™å°†å®ƒæ¨é€åˆ° Hub å¹¶ä¸ç¤¾åŒºå…±äº«äº†ï¼ä¸Šä¼ æ•°æ®é›†éå¸¸ç®€å•ï¼šå°±åƒ Transformers ä¸­çš„æ¨¡å‹å’Œ tokenizer ä¸€æ ·ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `push_to_hub()` æ–¹æ³•æ¥æ¨é€æ•°æ®é›†ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªèº«ä»½éªŒè¯ä»¤ç‰Œï¼Œå®ƒå¯ä»¥é€šè¿‡é¦–å…ˆä½¿ç”¨ `notebook_login()` å‡½æ•°ç™»å½•åˆ° Hugging Face Hub æ¥è·å¾—ï¼š

```python
from huggingface_hub import notebook_login

notebook_login()
```
è¿™å°†åˆ›å»ºä¸€ä¸ªå°éƒ¨ä»¶ï¼Œä½ å¯ä»¥åœ¨å…¶ä¸­è¾“å…¥ä½ çš„ç”¨æˆ·åå’Œå¯†ç ï¼ŒAPI ä»¤ç‰Œå°†ä¿å­˜åœ¨ `~/.huggingface/token` ä¸­ã€‚å¦‚æœä½ åœ¨ç»ˆç«¯ä¸­è¿è¡Œä»£ç ï¼Œåˆ™å¯ä»¥æ”¹ä¸ºä½¿ç”¨å‘½ä»¤è¡Œç™»å½•ï¼š

```python
huggingface-cli login
```

å®Œæˆæ­¤æ“ä½œåï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡è¿è¡Œä¸‹é¢çš„ä»£ç ä¸Šä¼ æˆ‘ä»¬çš„æ•°æ®é›†ï¼š

```python
issues_with_comments_dataset.push_to_hub("github-issues")
```

ä¹‹åï¼Œä»»ä½•äººéƒ½å¯ä»¥é€šè¿‡ä¾¿æ·åœ°ä½¿ç”¨é™„å¸¦ä»“åº“ ID ä½œä¸º `path` å‚æ•°çš„ `load_dataset()` å‡½æ•° æ¥ä¸‹è½½æ•°æ®é›†ï¼š

```python
remote_dataset = load_dataset("lewtun/github-issues", split="train")
remote_dataset
```

```python
Dataset({
    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'performed_via_github_app', 'is_pull_request'],
    num_rows: 2855
})
```

å¾ˆé…·ï¼Œæˆ‘ä»¬å·²ç»å°†æˆ‘ä»¬çš„æ•°æ®é›†æ¨é€åˆ° Hubï¼Œå…¶ä»–äººå¯ä»¥ä½¿ç”¨å®ƒï¼åªå‰©ä¸‹ä¸€ä»¶é‡è¦çš„äº‹æƒ…è¦åšï¼šæ·»åŠ ä¸€ä¸ªæ•°æ®å¡ç‰‡ï¼Œè§£é‡Šè¯­æ–™åº“æ˜¯å¦‚ä½•åˆ›å»ºçš„ï¼Œå¹¶ä¸ºä½¿ç”¨æ•°æ®é›†çš„å…¶ä»–æä¾›ä¸€äº›å…¶ä»–æœ‰ç”¨çš„ä¿¡æ¯ã€‚

<div custom-style="Tip-green">

ğŸ’¡ ä½ è¿˜å¯ä»¥ä½¿ç”¨ä¸€äº› Git æŠ€å·§å’Œ `huggingface-cli` ç›´æ¥ä»ç»ˆç«¯å°†æ•°æ®é›†ä¸Šä¼ åˆ° Hugging Face Hubã€‚æœ‰å…³å¦‚ä½•æ‰§è¡Œæ­¤æ“ä½œçš„è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… [Datasets æŒ‡å—](https://huggingface.co/docs/datasets/share.html#add-a-community-dataset)(https://huggingface.co/docs/datasets/share.html#add-a-community-dataset) æŒ‡å—ã€‚

</div>

### åˆ›å»ºæ•°æ®é›†å¡ç‰‡ 

æœ‰æ®å¯æŸ¥çš„æ•°æ®é›†æ›´æœ‰å¯èƒ½å¯¹å…¶ä»–äººï¼ˆåŒ…æ‹¬ä½ æœªæ¥çš„è‡ªå·±ï¼ï¼‰æœ‰ç”¨ï¼Œå› ä¸ºå®ƒä»¬æä¾›äº†æ•°æ®é›†ç›¸å…³çš„ä¿¡æ¯ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿå†³å®šæ•°æ®é›†æ˜¯å¦ä¸ä»–ä»¬çš„ä»»åŠ¡ç›¸å…³ï¼Œå¹¶è¯„ä¼°ä»»ä½•æ½œåœ¨çš„åè§æˆ–ä¸ä½¿ç”¨ç›¸å…³çš„é£é™©ã€‚

åœ¨ Hugging Face Hub ä¸Šï¼Œæ­¤ä¿¡æ¯å­˜å‚¨åœ¨æ¯ä¸ªæ•°æ®é›†ä»“åº“çš„è‡ªè¿°æ–‡ä»¶æ–‡ä»¶ã€‚åœ¨åˆ›å»ºæ­¤æ–‡ä»¶ä¹‹å‰ï¼Œä½ åº”è¯¥é‡‡å–ä¸¤ä¸ªä¸»è¦æ­¥éª¤ï¼š

1. ä½¿ç”¨ [`datasets-tagging`åº”ç”¨ç¨‹åº](https://huggingface.co/datasets/tagging/)(https://huggingface.co/datasets/tagging/) åˆ›å»º YAML æ ¼å¼çš„å…ƒæ•°æ®æ ‡ç­¾ã€‚è¿™äº›æ ‡ç­¾ç”¨äº Hugging Face Hub ä¸Šçš„å„ç§æœç´¢åŠŸèƒ½ï¼Œå¹¶ç¡®ä¿ä½ çš„æ•°æ®é›†å¯ä»¥å¾ˆå®¹æ˜“åœ°è¢«ç¤¾åŒºæˆå‘˜æ‰¾åˆ°ã€‚ç”±äºæˆ‘ä»¬å·²ç»åœ¨è¿™é‡Œåˆ›å»ºäº†ä¸€ä¸ªè‡ªå®šä¹‰æ•°æ®é›†ï¼Œæ‰€ä»¥ä½ éœ€è¦å…‹éš†æ•°æ®é›†æ ‡ç­¾ `datasets-tagging` ä»“åº“å¹¶åœ¨æœ¬åœ°è¿è¡Œåº”ç”¨ç¨‹åºã€‚å®ƒçš„ç•Œé¢æ˜¯è¿™æ ·çš„ï¼š

![`datasets-tagger` æ¥å£](./assets/datasets-tagger.png "`datasets-tagger` æ¥å£")

2ï¼é˜…è¯» [Datasets æŒ‡å—](https://github.com/huggingface/datasets/blob/master/templates/README_guide.md)(https://github.com/huggingface/datasets/blob/master/templates/README_guide.md)(https://github.com/huggingface/datasets/blob/master/templates/README_guide.md) ä¸­å…³äºåˆ›å»ºå®Œå–„çš„æ•°æ®é›†å¡ç‰‡çš„æŒ‡å—ï¼Œå¹¶å°†å…¶ä½œä¸ºæ¨¡æ¿ä½¿ç”¨ã€‚

ä½ å¯ä»¥ç›´æ¥åœ¨ Hub ä¸Šåˆ›å»º README.md æ–‡ä»¶ï¼Œä½ å¯ä»¥åœ¨ `lewtun/github-issues` æ•°æ®é›†ä»“åº“ä¸­æ‰¾åˆ°ä¸€ä¸ªæ¨¡æ¿æ•°æ®é›†å¡ç‰‡ã€‚ä¸‹é¢æ˜¾ç¤ºäº†å¡«å†™å¥½çš„æ•°æ®é›†å¡ç‰‡çš„æˆªå›¾ã€‚

![æ•°æ®é›†å¡ç‰‡](./assets/dataset-card.png "A dataset card.")

<div custom-style="Tip-green">

âœï¸**è¯•è¯•çœ‹ï¼**ä½¿ç”¨ `dataset-tagging` åº”ç”¨ç¨‹åºå’Œ [Datasets æŒ‡å—](https://github.com/huggingface/datasets/blob/master/templates/README_guide.md)(https://github.com/huggingface/datasets/blob/master/templates/README_guide.md)(https://github.com/huggingface/datasets/blob/master/templates/README_guide.md) æŒ‡å—æ¥å®Œæˆ GitHub issue æ•°æ®é›†çš„ README.md æ–‡ä»¶ã€‚

</div>

å¾ˆå¥½ï¼æˆ‘ä»¬åœ¨æœ¬èŠ‚ä¸­å¯ä»¥çœ‹åˆ°ï¼Œåˆ›å»ºä¸€ä¸ªå¥½çš„æ•°æ®é›†å¯èƒ½æ¶‰åŠç›¸å½“å¤šçš„å·¥ä½œï¼Œä½†å¹¸è¿çš„æ˜¯ï¼Œå°†å…¶ä¸Šä¼ å¹¶ä¸ç¤¾åŒºå…±äº«ä¼šå¾ˆå®¹æ˜“å®ç°ã€‚åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨æˆ‘ä»¬çš„æ–°æ•°æ®é›†åˆ›å»ºä¸€ä¸ª Datasets çš„è¯­ä¹‰æœç´¢å¼•æ“ï¼Œè¯¥å¼•æ“å¯ä»¥å°†è¾“å…¥åŒ¹é…åˆ°æœ€ç›¸å…³çš„ issue å’Œè¯„è®ºã€‚

<div custom-style="Tip-green">

âœï¸ **è¯•è¯•çœ‹ï¼**æŒ‰ç…§æˆ‘ä»¬åœ¨æœ¬èŠ‚ä¸­é‡‡å–çš„æ­¥éª¤ä¸ºä½ æœ€å–œæ¬¢çš„å¼€æºåº“åˆ›å»ºä¸€ä¸ª GitHub issue æ•°æ®é›†ï¼ˆå½“ç„¶æ˜¯é™¤äº† Datasetsï¼‰ã€‚è¿›é˜¶çš„æŒ‘æˆ˜ï¼šå¾®è°ƒå¤šæ ‡ç­¾åˆ†ç±»å™¨ä»¥é¢„æµ‹åœ¨ `labels` å­—æ®µä¸­å‡ºç°çš„æ ‡ç­¾ã€‚
</div>


## 6.5 ä½¿ç”¨ FAISS è¿›è¡Œè¯­ä¹‰æœç´¢ 

åœ¨ç¬¬5å°èŠ‚ï¼Œä»¬åˆ›å»ºäº†ä¸€ä¸ªæ¥è‡ª Datasets ä»“åº“çš„ GitHub issues å’Œè¯„è®ºçš„æ•°æ®é›†ã€‚åœ¨æœ¬èŠ‚ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨è¿™äº›ä¿¡æ¯æ„å»ºä¸€ä¸ªæœç´¢å¼•æ“ï¼Œå¸®åŠ©æˆ‘ä»¬æ‰¾åˆ°å…³äºè¯¥åº“çš„æœ€ç´§è¿«çš„ issue çš„ç­”æ¡ˆï¼

### ä½¿ç”¨æ–‡æœ¬åµŒå…¥è¿›è¡Œè¯­ä¹‰æœç´¢ 

æ­£å¦‚æˆ‘ä»¬åœ¨ç¬¬äºŒç« ï¼Œå­¦ä¹ çš„ï¼ŒåŸºäº Transformer çš„è¯­è¨€æ¨¡å‹ä¼šå°†æ–‡æœ¬ä¸­çš„æ¯ä¸ª token è½¬æ¢ä¸ºåµŒå…¥å‘é‡ã€‚äº‹å®è¯æ˜ï¼Œæˆ‘ä»¬å¯ä»¥ â€œæ± åŒ–ï¼ˆpoolï¼‰â€ åµŒå…¥å‘é‡ä»¥åˆ›å»ºæ•´ä¸ªå¥å­ã€æ®µè½æˆ–ï¼ˆåœ¨æŸäº›æƒ…å†µä¸‹ï¼‰æ–‡æ¡£çš„å‘é‡è¡¨ç¤ºã€‚ç„¶åï¼Œé€šè¿‡è®¡ç®—æ¯ä¸ªåµŒå…¥ä¹‹é—´çš„ç‚¹ç§¯ç›¸ä¼¼åº¦ï¼ˆæˆ–å…¶ä»–ä¸€äº›ç›¸ä¼¼åº¦åº¦é‡ï¼‰å¹¶è¿”å›ç›¸ä¼¼åº¦æœ€å¤§çš„æ–‡æ¡£ï¼Œè¿™äº›åµŒå…¥å¯ç”¨äºåœ¨è¯­æ–™åº“ä¸­æ‰¾åˆ°ç›¸ä¼¼çš„æ–‡æ¡£ã€‚

åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨æ–‡æœ¬åµŒå…¥å‘é‡æ¥å¼€å‘è¯­ä¹‰æœç´¢å¼•æ“ã€‚ä¸åŸºäºå°†æŸ¥è¯¢ä¸­çš„å…³é”®å­—çš„ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œè¿™äº›æœç´¢å¼•æ“å…·æœ‰å¤šç§ä¼˜åŠ¿ã€‚

![è¯­ä¹‰æœç´¢](./assets/semantic-search.png "Semantic search.")

### åŠ è½½å’Œå‡†å¤‡æ•°æ®é›† 

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦ä¸‹è½½æˆ‘ä»¬çš„ GitHub issues æ•°æ®é›†ï¼Œæ‰€ä»¥è®©æˆ‘ä»¬åƒå¾€å¸¸é‚£æ ·ä½¿ç”¨ `load_dataset()` å‡½æ•°ï¼š

```python
from datasets import load_dataset

issues_dataset = load_dataset("lewtun/github-issues", split="train")

issues_dataset
```

```python
Dataset({
    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'performed_via_github_app', 'is_pull_request'],
    num_rows: 2855
})
```

åœ¨æ­¤ï¼Œæˆ‘ä»¬åœ¨ `load_dataset()` ä¸­æŒ‡å®šäº†é»˜è®¤çš„ `trainï¼ˆè®­ç»ƒé›†ï¼‰` éƒ¨åˆ†ï¼Œå› æ­¤å®ƒè¿”å›ä¸€ä¸ª `Dataset` è€Œä¸æ˜¯ `DatasetDict` ã€‚é¦–è¦ä»»åŠ¡æ˜¯æ’é™¤æ‰æ‹‰å–è¯·æ±‚ï¼Œå› ä¸ºè¿™äº›è¯·æ±‚å¾€å¾€å¾ˆå°‘ç”¨äºå›ç­”æå‡ºçš„ issueï¼Œä¼šä¸ºæˆ‘ä»¬çš„æœç´¢å¼•æ“å¼•å…¥å™ªéŸ³ã€‚æ­£å¦‚æˆ‘ä»¬ç°åœ¨ç†Ÿæ‚‰çš„é‚£æ ·ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `Dataset.filter()` å‡½æ•°æ¥æ’é™¤æ•°æ®é›†ä¸­çš„è¿™äº›è¡Œã€‚å½“æˆ‘ä»¬è¿™æ ·åšçš„æ—¶å€™ï¼Œè®©æˆ‘ä»¬ä¹Ÿç­›é€‰æ‰æ²¡æœ‰è¯„è®ºçš„è¡Œï¼Œå› ä¸ºè¿™äº›è¡Œæ²¡æœ‰ä¸ºç”¨æˆ·æé—®æä¾›å›ç­”ï¼š

```python
issues_dataset = issues_dataset.filter(
    lambda x: (x["is_pull_request"] == False and len(x["comments"]) > 0)
)
issues_dataset
```

```python
Dataset({
    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'performed_via_github_app', 'is_pull_request'],
    num_rows: 771
})
```

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œæˆ‘ä»¬çš„æ•°æ®é›†ä¸­æœ‰å¾ˆå¤šåˆ—ï¼Œå…¶ä¸­å¤§éƒ¨åˆ†åœ¨æ„å»ºæˆ‘ä»¬çš„æœç´¢å¼•æ“éƒ½ä¸ä¼šä½¿ç”¨ã€‚ä»æœç´¢çš„è§’åº¦æ¥çœ‹ï¼Œä¿¡æ¯é‡æœ€å¤§çš„åˆ—æ˜¯ `title` ï¼Œ `body` ï¼Œå’Œ `comments` ï¼Œè€Œ `html_url` ä¸ºæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªå›åˆ°åŸ issue çš„é“¾æ¥ã€‚è®©æˆ‘ä»¬ä½¿ç”¨ `Dataset.remove_columns()` åˆ é™¤å…¶ä½™çš„åˆ—ï¼š

```python
columns = issues_dataset.column_names
columns_to_keep = ["title", "body", "html_url", "comments"]
columns_to_remove = set(columns_to_keep).symmetric_difference(columns)
issues_dataset = issues_dataset.remove_columns(columns_to_remove)
issues_dataset
```

```python
Dataset({
    features: ['html_url', 'title', 'comments', 'body'],
    num_rows: 771
})
```

ä¸ºäº†åˆ›å»ºæˆ‘ä»¬çš„æ–‡æœ¬åµŒå…¥æ•°æ®é›†ï¼Œæˆ‘ä»¬å°†ç”¨ issue çš„æ ‡é¢˜å’Œæ­£æ–‡æ¥æ‰©å……æ¯æ¡è¯„è®ºï¼Œå› ä¸ºè¿™äº›å­—æ®µé€šå¸¸åŒ…å«æœ‰ç”¨çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚å› ä¸ºæˆ‘ä»¬çš„ `comments` åˆ—å½“å‰æ˜¯æ¯ä¸ª issue çš„è¯„è®ºåˆ—è¡¨ï¼Œæˆ‘ä»¬éœ€è¦â€œé‡æ–°ç»„åˆâ€åˆ—ï¼Œä½¿å¾—æ¯ä¸€è¡Œéƒ½æ˜¯ç”±ä¸€ä¸ª `(html_url, title, body, comment)` å…ƒç»„ç»„æˆã€‚åœ¨ Pandas ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ [DataFrame.explode() å‡½æ•°](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.explode.html)(https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.explode.html) å®Œæˆè¿™ä¸ªæ“ä½œ å®ƒä¸ºç±»ä¼¼åˆ—è¡¨çš„åˆ—ä¸­çš„æ¯ä¸ªå…ƒç´ åˆ›å»ºä¸€ä¸ªæ–°è¡Œï¼ŒåŒæ—¶å¤åˆ¶æ‰€æœ‰å…¶ä»–åˆ—å€¼ã€‚è®©æˆ‘ä»¬é¦–å…ˆåˆ‡æ¢åˆ° Pandas çš„ `DataFrame` æ ¼å¼ï¼š

```python
issues_dataset.set_format("pandas")
df = issues_dataset[:]
```

å¦‚æœæˆ‘ä»¬æ£€æŸ¥è¿™ä¸ª `DataFrame` çš„ç¬¬ä¸€è¡Œï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¿™ä¸ª issue æœ‰å››ä¸ªç›¸å…³è¯„è®ºï¼š

```python
df["comments"][0].tolist()
```

```python
['the bug code locate in :\r\n    if data_args.task_name is not None:\r\n        # Downloading and loading a dataset from the hub.\r\n        datasets = load_dataset("glue", data_args.task_name, cache_dir=model_args.cache_dir)',
 'Hi @jinec,\r\n\r\nFrom time to time we get this kind of `ConnectionError` coming from the github.com website: https://raw.githubusercontent.com\r\n\r\nNormally, it should work if you wait a little and then retry.\r\n\r\nCould you please confirm if the problem persists?',
 'cannot connect,even by Web browser,please check that  there is some  problemsã€‚',
 'I can access https://raw.githubusercontent.com/huggingface/datasets/1.7.0/datasets/glue/glue.py without problem...']
```

æˆ‘ä»¬å¸Œæœ›ä½¿ç”¨ `explode()` å°†è¿™äº›è¯„è®ºä¸­çš„æ¯ä¸€æ¡éƒ½å±•å¼€æˆä¸ºä¸€è¡Œã€‚è®©æˆ‘ä»¬çœ‹çœ‹æ˜¯å¦å¯ä»¥åšåˆ°ï¼š

```python
comments_df = df.explode("comments", ignore_index=True)
comments_df.head(4)
```

<table border="1" class="dataframe" style="table-layout: fixed; word-wrap:break-word; width: 100%;">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>html_url</th>
      <th>title</th>
      <th>comments</th>
      <th>body</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>https://github.com/huggingface/datasets/issues/2787</td>
      <td>ConnectionError: Couldn't reach https://raw.githubusercontent.com</td>
      <td>the bug code locate in ï¼š\r\n    if data_args.task_name is not None...</td>
      <td>Hello,\r\nI am trying to run run_glue.py and it gives me this error...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>https://github.com/huggingface/datasets/issues/2787</td>
      <td>ConnectionError: Couldn't reach https://raw.githubusercontent.com</td>
      <td>Hi @jinec,\r\n\r\nFrom time to time we get this kind of `ConnectionError` coming from the github.com website: https://raw.githubusercontent.com...</td>
      <td>Hello,\r\nI am trying to run run_glue.py and it gives me this error...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>https://github.com/huggingface/datasets/issues/2787</td>
      <td>ConnectionError: Couldn't reach https://raw.githubusercontent.com</td>
      <td>cannot connectï¼Œeven by Web browserï¼Œplease check that  there is some  problemsã€‚</td>
      <td>Hello,\r\nI am trying to run run_glue.py and it gives me this error...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>https://github.com/huggingface/datasets/issues/2787</td>
      <td>ConnectionError: Couldn't reach https://raw.githubusercontent.com</td>
      <td>I can access https://raw.githubusercontent.com/huggingface/datasets/1.7.0/datasets/glue/glue.py without problem...</td>
      <td>Hello,\r\nI am trying to run run_glue.py and it gives me this error...</td>
    </tr>
  </tbody>
</table>

éå¸¸å¥½ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å…¶ä»–ä¸‰åˆ—å·²ç»è¢«å¤åˆ¶äº†ï¼Œ `comments` åˆ—é‡Œå­˜æ”¾ç€å•ç‹¬çš„è¯„è®ºï¼ç°åœ¨æˆ‘ä»¬å·²ç»å®Œæˆäº† Pandas è¦å®Œæˆçš„éƒ¨åˆ†åŠŸèƒ½ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡åŠ è½½å†…å­˜ä¸­çš„ `DataFrame` å¿«é€Ÿåˆ‡æ¢å› `Dataset` ï¼š

```python
from datasets import Dataset

comments_dataset = Dataset.from_pandas(comments_df)
comments_dataset
```

```python
Dataset({
    features: ['html_url', 'title', 'comments', 'body'],
    num_rows: 2842
})
```

å¤ªå¥½äº†ï¼Œæˆ‘ä»¬è·å–åˆ°äº†å‡ åƒæ¡çš„è¯„è®ºï¼

<div custom-style="Tip-green">

âœï¸ **è¯•ä¸€è¯•ï¼** çœ‹çœ‹ä½ æ˜¯å¦å¯ä»¥ä½¿ç”¨ `Dataset.map()` å±•å¼€ `issues_dataset` çš„ `comments` åˆ—ï¼Œè¿™æœ‰ç‚¹æ£˜æ‰‹ï¼›ä½ å¯èƒ½ä¼šå‘ç° Datasets æ–‡æ¡£çš„ ["æ‰¹å¤„ç†æ˜ å°„(Batch mapping)"](https://huggingface.co/docs/datasets/v1.12.1/about_map_batch.html?batch-mapping#batch-mapping)(https://huggingface.co/docs/datasets/v1.12.1/about_map_batch.html?batch-mapping#batch-mapping) å¯¹è¿™ä¸ªä»»åŠ¡å¾ˆæœ‰ç”¨ã€‚

</div>

æ—¢ç„¶æˆ‘ä»¬æ¯è¡Œæœ‰ä¸€ä¸ªè¯„è®ºï¼Œè®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªæ–°çš„ `comments_length` åˆ—æ¥å­˜æ”¾æ¯æ¡è¯„è®ºçš„å­—æ•°ï¼š

```python
comments_dataset = comments_dataset.map(
    lambda x: {"comment_length": len(x["comments"].split())}
)
```

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¿™ä¸ªæ–°åˆ—æ¥è¿‡æ»¤æ‰ç®€çŸ­çš„è¯„è®ºï¼Œå…¶ä¸­é€šå¸¸åŒ…æ‹¬â€œcc @lewtunâ€æˆ–â€œè°¢è°¢ï¼â€ä¹‹ç±»ä¸æˆ‘ä»¬çš„æœç´¢å¼•æ“æ— å…³çš„å†…å®¹ã€‚ç­›é€‰çš„ç²¾ç¡®æ•°å­—æ²¡æœ‰ç¡¬æ€§è§„å®šï¼Œä½†å¤§çº¦å¤§äº 15 ä¸ªå•è¯ä¼¼ä¹æ˜¯ä¸€ä¸ªä¸é”™çš„é€‰æ‹©ï¼š

```python
comments_dataset = comments_dataset.filter(lambda x: x["comment_length"] > 15)
comments_dataset
```

```python
Dataset({
    features: ['html_url', 'title', 'comments', 'body', 'comment_length'],
    num_rows: 2098
})
```

ç¨å¾®æ¸…ç†äº†æˆ‘ä»¬çš„æ•°æ®é›†åï¼Œè®©æˆ‘ä»¬å°† issue æ ‡é¢˜ã€æè¿°å’Œè¯„è®ºè¿æ¥åˆ°ä¸€ä¸ªæ–°çš„ `text` åˆ—ã€‚åƒå¾€å¸¸ä¸€æ ·ï¼Œæˆ‘ä»¬å¯ä»¥ç¼–å†™ä¸€ä¸ªç®€å•çš„å‡½æ•°ï¼Œå¹¶å°†å…¶ä¼ é€’ç»™ `Dataset.map()` æ¥å®Œæˆè¿™äº›æ“ä½œ 

```python
def concatenate_text(examples):
    return {
        "text": examples["title"]
        + " \n "
        + examples["body"]
        + " \n "
        + examples["comments"]
    }

comments_dataset = comments_dataset.map(concatenate_text)
```

æˆ‘ä»¬ç»ˆäºå‡†å¤‡å¥½åˆ›å»ºä¸€äº›æ–‡æœ¬åµŒå…¥äº†ï¼è®©æˆ‘ä»¬ç»§ç»­ã€‚

### åˆ›å»ºæ–‡æœ¬åµŒå…¥ 

æˆ‘ä»¬åœ¨ç¬¬ä¸‰ç« å­¦è¿‡ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿ç”¨ `AutoModel` ç±»æ¥å®Œæˆæ–‡æœ¬åµŒå…¥ã€‚æˆ‘ä»¬éœ€è¦åšçš„å°±æ˜¯é€‰æ‹©ä¸€ä¸ªåˆé€‚çš„ Checkpoint æ¥åŠ è½½æ¨¡å‹ã€‚å¹¸è¿çš„æ˜¯ï¼Œæœ‰ä¸€ä¸ªåä¸º `sentence-transformers` çš„åº“ä¸“é—¨ç”¨äºåˆ›å»ºæ–‡æœ¬åµŒå…¥ã€‚å¦‚åº“ä¸­çš„ [æ–‡æ¡£](https://www.sbert.net/examples/applications/semantic-search/README.html#symmetric-vs-asymmetric-semantic-search)(https://www.sbert.net/examples/applications/semantic-search/README.html#symmetric-vs-asymmetric-semantic-search) æ‰€è¿°çš„ï¼Œæˆ‘ä»¬è¿™æ¬¡è¦å®ç°çš„æ˜¯éå¯¹ç§°è¯­ä¹‰æœç´¢ï¼ˆasymmetric semantic searchï¼‰ï¼Œå› ä¸ºæˆ‘ä»¬æœ‰ä¸€ä¸ªç®€çŸ­çš„æŸ¥è¯¢ï¼Œæˆ‘ä»¬å¸Œæœ›åœ¨æ¯”å¦‚ issue è¯„è®ºç­‰æ›´é•¿çš„æ–‡æ¡£ä¸­æ‰¾åˆ°å…¶ç­”æ¡ˆã€‚é€šè¿‡æŸ¥çœ‹ [æ¨¡å‹æ¦‚è§ˆè¡¨](https://www.sbert.net/docs/pretrained_models.html#model-overview)(https://www.sbert.net/docs/pretrained_models.html#model-overview) æˆ‘ä»¬å¯ä»¥å‘ç° `multi-qa-mpnet-base-dot-v1` Checkpoint åœ¨è¯­ä¹‰æœç´¢æ–¹é¢å…·æœ‰æœ€ä½³æ€§èƒ½ï¼Œå› æ­¤æˆ‘ä»¬å°†åœ¨æˆ‘ä»¬çš„åº”ç”¨ç¨‹åºä¸­ä½¿ç”¨å®ƒã€‚æˆ‘ä»¬è¿˜å°†ä½¿ç”¨ç›¸åŒçš„ Checkpoint åŠ è½½äº†å¯¹åº”çš„ tokenizer ï¼š

{#if fw === 'pt'}

```python
from transformers import AutoTokenizer, AutoModel

model_ckpt = "sentence-transformers/multi-qa-mpnet-base-dot-v1"
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
model = AutoModel.from_pretrained(model_ckpt)
```

å°†æ¨¡å‹å’Œè¾“å…¥æ”¾åœ¨ä¸€ä¸ª GPU è®¾å¤‡ä¸Šä¼šåŠ é€ŸåµŒå…¥è¿‡ç¨‹ï¼Œæ‰€ä»¥è®©æˆ‘ä»¬ç°åœ¨å°±è¿™ä¹ˆåšï¼š

```python
import torch

device = torch.device("cuda")
model.to(device)
```

{:else}

```python
from transformers import AutoTokenizer, TFAutoModel

model_ckpt = "sentence-transformers/multi-qa-mpnet-base-dot-v1"
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
model = TFAutoModel.from_pretrained(model_ckpt, from_pt=True)
```

è¯·æ³¨æ„ï¼Œæˆ‘ä»¬åœ¨è°ƒç”¨ `from_pretrained()` æ–¹æ³•çš„æ—¶å€™æ·»åŠ äº† `from_pt=True` å‚æ•°ã€‚è¿™æ˜¯å› ä¸º `multi-qa-mpnet-base-dot-v1` Checkpoint åªæœ‰ PyTorch æƒé‡ï¼Œå› æ­¤è®¾ç½® `from_pt=True` ä¼šè‡ªåŠ¨å°†å®ƒä»¬è½¬æ¢ä¸º TensorFlow æ ¼å¼ã€‚å¦‚ä½ æ‰€è§ï¼Œåœ¨Transformers ä¸­åˆ‡æ¢æ¡†æ¶éå¸¸ç®€å•ï¼

{/if}

æ ¹æ®æˆ‘ä»¬ä¹‹å‰çš„æƒ³æ³•ï¼Œæˆ‘ä»¬å¸Œæœ›å°†æˆ‘ä»¬çš„ GitHub é—®é¢˜åº“ä¸­çš„æ¯ä¸€æ¡è®°å½•è½¬åŒ–ä¸ºä¸€ä¸ªå•ä¸€çš„å‘é‡ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦ä»¥æŸç§æ–¹å¼â€œæ± åŒ–ï¼ˆpoolï¼‰â€æˆ–å¹³å‡æˆ‘ä»¬çš„è¯åµŒå…¥å‘é‡ã€‚ä¸€ç§æµè¡Œçš„æ–¹æ³•æ˜¯åœ¨æˆ‘ä»¬æ¨¡å‹çš„è¾“å‡ºä¸Šæ‰§è¡Œ `CLS æ± åŒ–` ï¼Œæˆ‘ä»¬åªéœ€è¦æ”¶é›† `[CLS]` token çš„çš„æœ€åä¸€ä¸ªéšè—çŠ¶æ€ã€‚ä»¥ä¸‹å‡½æ•°å®ç°äº†è¿™ä¸ªåŠŸèƒ½ï¼š

```python
def cls_pooling(model_output):
    return model_output.last_hidden_state[:, 0]
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªè¾…åŠ©å‡½æ•°ï¼Œå®ƒå°†å¯¹ä¸€ç»„æ–‡æ¡£è¿›è¡Œ tokenizeï¼Œå°†å¼ é‡æ”¾åœ¨ GPU ä¸Šï¼Œå°†å®ƒä»¬å–‚ç»™æ¨¡å‹ï¼Œæœ€åå¯¹è¾“å‡ºè¿›è¡Œ CLS æ± åŒ–ï¼š
{#if fw === 'pt'}

```python
def get_embeddings(text_list):
    encoded_input = tokenizer(
        text_list, padding=True, truncation=True, return_tensors="pt"
    )
    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}
    model_output = model(**encoded_input)
    return cls_pooling(model_output)
```

æˆ‘ä»¬å¯ä»¥å°†ç¬¬ä¸€ä¸ªæ–‡æœ¬æ¡ç›®å–‚ç»™å®ƒå¹¶æ£€æŸ¥è¾“å‡ºçš„å½¢çŠ¶æ¥æµ‹è¯•è¿™ä¸ªå‡½æ•°æ˜¯å¦æ­£å¸¸å·¥ä½œï¼š

```python
embedding = get_embeddings(comments_dataset["text"][0])
embedding.shape
```

```python
torch.Size([1, 768])
```

å¤ªå¥½äº†ï¼Œæˆ‘ä»¬å·²ç»å°†è¯­æ–™åº“ä¸­çš„ç¬¬ä¸€ä¸ªæ¡ç›®è½¬æ¢ä¸ºäº†ä¸€ä¸ª 768 ç»´å‘é‡ï¼æˆ‘ä»¬å¯ä»¥ç”¨ `Dataset.map()` å°†æˆ‘ä»¬çš„ `get_embeddings()` å‡½æ•°åº”ç”¨åˆ°æˆ‘ä»¬è¯­æ–™åº“ä¸­çš„æ¯ä¸€è¡Œï¼Œç„¶ååˆ›å»ºä¸€ä¸ªæ–°çš„ `embeddings` åˆ—ï¼š

```python
embeddings_dataset = comments_dataset.map(
    lambda x: {"embeddings": get_embeddings(x["text"]).detach().cpu().numpy()[0]}
)
```

{:else}

```python
def get_embeddings(text_list):
    encoded_input = tokenizer(
        text_list, padding=True, truncation=True, return_tensors="tf"
    )
    encoded_input = {k: v for k, v in encoded_input.items()}
    model_output = model(**encoded_input)
    return cls_pooling(model_output)
```

æˆ‘ä»¬å¯ä»¥å°†ç¬¬ä¸€ä¸ªæ–‡æœ¬æ¡ç›®å–‚ç»™å®ƒå¹¶æ£€æŸ¥è¾“å‡ºçš„å½¢çŠ¶æ¥æµ‹è¯•è¿™ä¸ªå‡½æ•°æ˜¯å¦æ­£å¸¸å·¥ä½œï¼š

```python
embedding = get_embeddings(comments_dataset["text"][0])
embedding.shape
```

```python
TensorShape([1, 768])
```

å¤ªå¥½äº†ï¼Œæˆ‘ä»¬å·²ç»å°†è¯­æ–™åº“ä¸­çš„ç¬¬ä¸€ä¸ªæ¡ç›®è½¬æ¢ä¸ºäº†ä¸€ä¸ª 768 ç»´å‘é‡ï¼æˆ‘ä»¬å¯ä»¥ç”¨ `Dataset.map()` å°†æˆ‘ä»¬çš„ `get_embeddings()` å‡½æ•°åº”ç”¨åˆ°æˆ‘ä»¬è¯­æ–™åº“ä¸­çš„æ¯ä¸€è¡Œï¼Œç„¶ååˆ›å»ºä¸€ä¸ªæ–°çš„ `embeddings` åˆ—ï¼š

```python
embeddings_dataset = comments_dataset.map(
    lambda x: {"embeddings": get_embeddings(x["text"]).numpy()[0]}
)
```

{/if}

è¯·æ³¨æ„ï¼Œæˆ‘ä»¬å·²ç»å°†æ–‡æœ¬åµŒå…¥è½¬æ¢ä¸º NumPy æ•°ç»„â€”â€”è¿™æ˜¯å› ä¸ºå½“æˆ‘ä»¬å°è¯•ä½¿ç”¨ FAISS æœç´¢å®ƒä»¬æ—¶ï¼ŒDatasets éœ€è¦è¿™ç§æ ¼å¼ï¼Œè®©æˆ‘ä»¬ç»§ç»­å§ã€‚

### ä½¿ç”¨ FAISS è¿›è¡Œé«˜æ•ˆçš„ç›¸ä¼¼æ€§æœç´¢ 

ç°åœ¨æˆ‘ä»¬æœ‰äº†ä¸€ä¸ªæ–‡æœ¬åµŒå…¥æ•°æ®é›†ï¼Œæˆ‘ä»¬éœ€è¦ä¸€äº›æ–¹æ³•æ¥æœç´¢å®ƒä»¬ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨Datasets ä¸­ä¸€ç§ç‰¹æ®Šçš„æ•°æ®ç»“æ„ï¼Œç§°ä¸º FAISS æŒ‡æ•°ã€‚ [FAISS](https://faiss.ai/)(https://faiss.ai/) ï¼ˆFacebook AI Similarity Search çš„ç¼©å†™ï¼‰æ˜¯ä¸€ä¸ªåº“ï¼Œæä¾›äº†ç”¨äºå¿«é€Ÿæœç´¢å’Œèšç±»åµŒå…¥å‘é‡çš„é«˜æ•ˆç®—æ³•ã€‚

FAISS èƒŒåçš„åŸºæœ¬æ€æƒ³æ˜¯åˆ›å»ºä¸€ä¸ªç‰¹æ®Šçš„æ•°æ®ç»“æ„ï¼Œç§°ä¸º `indexï¼ˆç´¢å¼•ï¼‰` å®ƒå¯ä»¥æ‰¾åˆ°å“ªäº›åµŒå…¥ä¸è¾“å…¥åµŒå…¥ç›¸ä¼¼ã€‚åœ¨ Datasets ä¸­åˆ›å»ºä¸€ä¸ª FAISS indexï¼ˆç´¢å¼•ï¼‰å¾ˆç®€å•â€”â€”æˆ‘ä»¬ä½¿ç”¨ `Dataset.add_faiss_index()` å‡½æ•°å¹¶æŒ‡å®šæˆ‘ä»¬è¦ç´¢å¼•çš„æ•°æ®é›†çš„å“ªä¸€åˆ—ï¼š

```python
embeddings_dataset.add_faiss_index(column="embeddings")
```

ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `Dataset.get_nearest_examples()` å‡½æ•°è¿›è¡Œæœ€è¿‘é‚»å±…æŸ¥æ‰¾ã€‚è®©æˆ‘ä»¬é€šè¿‡é¦–å…ˆåµŒå…¥ä¸€ä¸ª issue æ¥æµ‹è¯•è¿™ä¸€ç‚¹ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

{#if fw === 'pt'}

```python
question = "How can I load a dataset offline?"
question_embedding = get_embeddings([question]).cpu().detach().numpy()
question_embedding.shape
```

```python
torch.Size([1, 768])
```

{:else}

```python
question = "How can I load a dataset offline?"
question_embedding = get_embeddings([question]).numpy()
question_embedding.shape
```

```python
(1, 768)
```

{/if}

å°±åƒæ–‡æ¡£ä¸€æ ·ï¼Œæˆ‘ä»¬ç°åœ¨æœ‰ä¸€ä¸ªè¡¨ç¤ºæŸ¥è¯¢çš„ 768 ç»´å‘é‡ï¼Œæˆ‘ä»¬å¯ä»¥å°†å…¶ä¸æ•´ä¸ªè¯­æ–™åº“è¿›è¡Œæ¯”è¾ƒä»¥æ‰¾åˆ°æœ€ç›¸ä¼¼çš„åµŒå…¥ï¼š

```python
scores, samples = embeddings_dataset.get_nearest_examples(
    "embeddings", question_embedding, k=5
)
```

`Dataset.get_nearest_examples()` å‡½æ•°è¿”å›ä¸€ä¸ªå…ƒç»„ï¼ŒåŒ…æ‹¬è¯„åˆ†ï¼ˆè¯„ä»·æŸ¥è¯¢å’Œæ–‡æ¡£ä¹‹é—´çš„ç›¸ä¼¼ç¨‹åº¦ï¼‰å’Œå¯¹åº”çš„æ ·æœ¬ï¼ˆè¿™é‡Œæ˜¯ 5 ä¸ªæœ€ä½³åŒ¹é…ï¼‰ã€‚è®©æˆ‘ä»¬æŠŠè¿™äº›æ”¶é›†åˆ°ä¸€ä¸ª `pandas.DataFrame` ï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥è½»æ¾åœ°å¯¹å®ƒä»¬è¿›è¡Œæ’åºï¼š

```python
import pandas as pd

samples_df = pd.DataFrame.from_dict(samples)
samples_df["scores"] = scores
samples_df.sort_values("scores", ascending=False, inplace=True)
```

ç°åœ¨æˆ‘ä»¬å¯ä»¥éå†å‰å‡ è¡Œæ¥æŸ¥çœ‹æˆ‘ä»¬çš„æŸ¥è¯¢ä¸è¯„è®ºçš„åŒ¹é…ç¨‹åº¦å¦‚ä½•ï¼š

```python
for _, row in samples_df.iterrows():
    print(f"COMMENT: {row.comments}")
    print(f"SCORE: {row.scores}")
    print(f"TITLE: {row.title}")
    print(f"URL: {row.html_url}")
    print("=" * 50)
    print()
```

```python
"""
COMMENT: Requiring online connection is a deal breaker in some cases unfortunately so it'd be great if offline mode is added similar to how `transformers` loads models offline fine.

@mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?
SCORE: 25.505046844482422
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: The local dataset builders (csv, text , json and pandas) are now part of the `datasets` package since #1726 :)
You can now use them offline
\`\`\`python
datasets = load_dataset("text", data_files=data_files)
\`\`\`

We'll do a new release soon
SCORE: 24.555509567260742
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: I opened a PR that allows to reload modules that have already been loaded once even if there's no internet.

Let me know if you know other ways that can make the offline mode experience better. I'd be happy to add them :)

I already note the "freeze" modules option, to prevent local modules updates. It would be a cool feature.

----------

> @mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?

Indeed `load_dataset` allows to load remote dataset script (squad, glue, etc.) but also you own local ones.
For example if you have a dataset script at `./my_dataset/my_dataset.py` then you can do
\`\`\`python
load_dataset("./my_dataset")
\`\`\`
and the dataset script will generate your dataset once and for all.

----------

About I'm looking into having `csv`, `json`, `text`, `pandas` dataset builders already included in the `datasets` package, so that they are available offline by default, as opposed to the other datasets that require the script to be downloaded.
cf #1724
SCORE: 24.14896583557129
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: > here is my way to load a dataset offline, but it **requires** an online machine
>
> 1. (online machine)
>
> ```
>
> import datasets
>
> data = datasets.load_dataset(.)
>
> data.save_to_disk(/YOUR/DATASET/DIR)
>
> ```python
>
> 2. copy the dir from online to the offline machine
>
> 3. (offline machine)
>
> ```
>
> import datasets
>
> data = datasets.load_from_disk(/SAVED/DATA/DIR)
>
> ```python
>
>
>
> HTH.

SCORE: 22.893993377685547
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================

COMMENT: here is my way to load a dataset offline, but it **requires** an online machine
1. (online machine)
\`\`\`
import datasets
data = datasets.load_dataset(...)
data.save_to_disk(/YOUR/DATASET/DIR)
\`\`\`
2. copy the dir from online to the offline machine
3. (offline machine)
\`\`\`
import datasets
data = datasets.load_from_disk(/SAVED/DATA/DIR)
\`\`\`

HTH.
SCORE: 22.406635284423828
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================
"""
```

ä¸é”™ï¼æˆ‘ä»¬çš„è¾“å‡ºçš„ç¬¬ 2 ä¸ªç»“æœä¼¼ä¹ä¸æŸ¥è¯¢åŒ¹é…ã€‚

<div custom-style="Tip-green">

âœï¸  è¯•è¯•çœ‹ï¼åˆ›å»ºä½ è‡ªå·±çš„æŸ¥è¯¢å¹¶æŸ¥çœ‹ä½ æ˜¯å¦å¯ä»¥åœ¨æ£€ç´¢åˆ°çš„æ–‡æ¡£ä¸­æ‰¾åˆ°ç­”æ¡ˆã€‚ä½ å¯èƒ½éœ€è¦åœ¨ `Dataset.get_nearest_examples()` å¢åŠ å‚æ•° `k` ä»¥æ‰©å¤§æœç´¢èŒƒå›´ã€‚

</div>

## 6.6 ç« æœ«æ€»ç»“åŠæµ‹è¯•

è¿™æ˜¯å¯¹ Datasets åº“çš„ä¸€æ¬¡å®Œæ•´çš„æ¢ç´¢â€”â€”ç¥è´ºä½ èµ°åˆ°è¿™ä¸€æ­¥ï¼å‡­å€Ÿä»æœ¬ç« ä¸­è·å¾—çš„çŸ¥è¯†ï¼Œä½ åº”è¯¥èƒ½å¤Ÿï¼š

- ä»ä»»ä½•åœ°æ–¹åŠ è½½æ•°æ®é›†ï¼Œæ— è®ºæ˜¯ Hugging Face Hubã€ä½ çš„ç¬”è®°æœ¬ç”µè„‘è¿˜æ˜¯ä½ å…¬å¸çš„è¿œç¨‹æœåŠ¡å™¨ã€‚
- æ··åˆä½¿ç”¨ `Dataset.map()` å’Œ `Dataset.filter()` å‡½æ•°æ¥æ•´ç†æ•°æ®ã€‚
- ä½¿ç”¨ `Dataset.set_format()` åœ¨ Pandas å’Œ NumPy ç­‰æ•°æ®æ ¼å¼ä¹‹é—´å¿«é€Ÿåˆ‡æ¢ã€‚
- åˆ›å»ºä½ è‡ªå·±çš„æ•°æ®é›†å¹¶å°†å…¶æ¨é€åˆ° Hugging Face Hubã€‚
- ä½¿ç”¨ Transformer æ¨¡å‹ä¸ºä½ çš„æ–‡æ¡£åˆ›å»ºæ–‡æœ¬åµŒå…¥ï¼Œå¹¶ä½¿ç”¨ FAISS æ„å»ºè¯­ä¹‰æœç´¢å¼•æ“ã€‚

åœ¨ç¬¬å…«ç« ï¼Œæˆ‘ä»¬å°†æŠŠæ‰€æœ‰è¿™äº›ç”¨äºæ·±å…¥ç ”ç©¶ Transformer æ¨¡å‹æ“…é•¿çš„æ ¸å¿ƒ NLP ä»»åŠ¡ã€‚ä¸è¿‡ï¼Œåœ¨è·³åˆ°ä¸‹ä¸€æ­¥ä¹‹å‰ï¼Œå…ˆç”¨ä¸€æ¬¡å¿«é€Ÿçš„å°æµ‹éªŒæ¥æ£€éªŒä½ å¯¹ Datasets çš„äº†è§£ï¼

### ç« æœ«æµ‹è¯• 

####  1.Datasets ä¸­çš„ `load_dataset ()` å‡½æ•°å…è®¸ä½ ä»ä¸‹åˆ—å“ªä¸ªä½ç½®åŠ è½½æ•°æ®é›†ï¼Ÿ

1. åœ¨æœ¬åœ°ï¼Œä¾‹å¦‚åœ¨ä½ çš„ç¬”è®°æœ¬ç”µè„‘ä¸Š
2. Hugging Face Hub
3. è¿œç¨‹æœåŠ¡å™¨

####  2ï¼å‡è®¾ä½ æŒ‰ç…§ä»¥ä¸‹æ–¹å¼åŠ è½½äº†ä¸€ä¸ª GLUE ä»»åŠ¡ï¼š

```python
from datasets import load_dataset

dataset = load_dataset("glue", "mrpc", split="train")
```

ä»¥ä¸‹å“ªä¸ªå‘½ä»¤å¯ä»¥ä» `dataset` ä¸­ç”Ÿæˆ 50 ä¸ªå…ƒç´ çš„éšæœºæ ·æœ¬ï¼Ÿ
1.  `dataset.sample (50)` 
2.  `dataset.shuffle().select(range(50))` 
3.  `dataset.select(range(50)).shuffle()` 

####  3ï¼å‡è®¾ä½ æœ‰ä¸€ä¸ªå…³äºå®¶åº­å® ç‰©çš„æ•°æ®é›† `pets_dataset` ï¼Œå®ƒæœ‰ä¸€ä¸ª `name` åˆ—è¡¨ç¤ºæ¯ä¸ªå® ç‰©çš„åå­—ã€‚ä»¥ä¸‹å“ªç§æ–¹æ³•å¯ä»¥ç­›é€‰å‡ºæ‰€æœ‰åå­—ä»¥ "L" å¼€å¤´çš„å® ç‰©æ•°æ®ï¼Ÿ

1.  `pets_dataset.filter(lambda x: x ['name'].startswith ('L'))` 
2.  `pets_dataset.filter(lambda x ['name'].startswith ('L'))` 
3. åˆ›å»ºä¸€ä¸ªç±»ä¼¼äº `def filter_names (x) : return x['name'].startswith ('L')` çš„å‡½æ•°ï¼Œç„¶åè¿è¡Œ `pets_dataset.filter(filter_names)` ã€‚

####  4ï¼ä»€ä¹ˆæ˜¯å†…å­˜æ˜ å°„ï¼Ÿ

1. CPU ä¸ GPU RAM ä¹‹é—´çš„æ˜ å°„
2. RAM å’Œæ–‡ä»¶ç³»ç»Ÿå­˜å‚¨ä¹‹é—´çš„æ˜ å°„
3. Datasets ç¼“å­˜ä¸­ä¸¤ä¸ªæ–‡ä»¶ä¹‹é—´çš„æ˜ å°„

####  5ï¼ä¸‹åˆ—å“ªä¸€é¡¹æ˜¯å†…å­˜æ˜ å°„çš„ä¸»è¦å¥½å¤„ï¼Ÿ

1. è®¿é—®å†…å­˜æ˜ å°„æ–‡ä»¶æ¯”ä»ç£ç›˜è¯»å–æˆ–å†™å…¥ç£ç›˜æ›´å¿«ã€‚
2. åº”ç”¨ç¨‹åºå¯ä»¥è®¿é—®ä¸€ä¸ªéå¸¸å¤§çš„æ–‡ä»¶ä¸­çš„æ•°æ®æ®µï¼Œè€Œä¸å¿…é¦–å…ˆå°†æ•´ä¸ªæ–‡ä»¶è¯»å…¥ RAMã€‚
3. å®ƒæ¶ˆè€—æ›´å°‘çš„èƒ½é‡ï¼Œæ‰€ä»¥ä½ çš„ç”µæ± æŒç»­æ—¶é—´æ›´é•¿ã€‚

####  6ï¼ä¸ºä»€ä¹ˆä¸‹é¢çš„ä»£ç æ˜¯é”™è¯¯çš„ï¼Ÿ

```python
from datasets import load_dataset

dataset = load_dataset("allocine", streaming=True, split="train")
dataset[0]
```
1. å®ƒè¯•å›¾å¯¹ä¸€ä¸ªå¤ªå¤§è€Œæ— æ³•æ”¾å…¥ RAM çš„æ•°æ®é›†è¿›è¡Œæµå¼å¤„ç†ã€‚
2. å®ƒå°è¯•è®¿é—® `IterableDataset` ã€‚
3. æ•°æ®é›† `allocine` æ²¡æœ‰ `train` éƒ¨åˆ†ã€‚

####  7ï¼åˆ›å»ºæ•°æ®é›†å¡ç‰‡çš„ä¸»è¦å¥½å¤„æ˜¯ä»€ä¹ˆï¼Ÿ

1. å®ƒæä¾›äº†å…³äºæ•°æ®é›†çš„é¢„æœŸç”¨é€”å’Œæ”¯æŒçš„ä»»åŠ¡çš„ä¿¡æ¯ï¼Œä»¥ä¾¿ç¤¾åŒºä¸­çš„å…¶ä»–äººå¯ä»¥å¯¹ä½¿ç”¨å®ƒåšå‡ºæ˜æ™ºçš„å†³å®šã€‚
2. å®ƒæœ‰åŠ©äºå¼•èµ·äººä»¬å¯¹è¯­æ–™åº“ä¸­å­˜åœ¨çš„åè§çš„å…³æ³¨ã€‚
3. å®ƒæé«˜äº†ç¤¾åŒºä¸­å…¶ä»–äººä½¿ç”¨æˆ‘çš„æ•°æ®é›†çš„å¯èƒ½æ€§ã€‚

####  8ï¼ä»€ä¹ˆæ˜¯è¯­ä¹‰æœç´¢ï¼Ÿ

1. ä¸€ç§åœ¨æŸ¥è¯¢ä¸­çš„å•è¯å’Œè¯­æ–™åº“ä¸­çš„æ–‡æ¡£ä¹‹é—´ç²¾ç¡®åŒ¹é…çš„æ–¹æ³•
2. ä¸€ç§é€šè¿‡ç†è§£æŸ¥è¯¢çš„ä¸Šä¸‹æ–‡å«ä¹‰æ¥æœç´¢åŒ¹é…æ–‡æ¡£çš„æ–¹æ³•
3. ä¸€ç§æé«˜æœç´¢å‡†ç¡®æ€§çš„æ–¹æ³•

####  9ï¼å¯¹äºéå¯¹ç§°è¯­ä¹‰æœç´¢ï¼Œé€šå¸¸æœ‰ï¼š

1. ä¸€ä¸ªç®€çŸ­çš„æŸ¥è¯¢å’Œä¸€ä¸ªè¾ƒé•¿çš„æ®µè½å›ç­”æŸ¥è¯¢
2. ç›¸åŒé•¿åº¦çš„æŸ¥è¯¢å’Œæ®µè½
3. ä¸€ä¸ªè¾ƒé•¿çš„æŸ¥è¯¢å’Œä¸€ä¸ªè¾ƒçŸ­çš„æ®µè½æ¥å›ç­”æŸ¥è¯¢

####  10ï¼æˆ‘å¯ä»¥ä½¿ç”¨Datasets æ¥åŠ è½½ç”¨äºå…¶ä»–é¢†åŸŸï¼ˆå¦‚è¯­éŸ³å¤„ç†ï¼‰çš„æ•°æ®å—ï¼Ÿ

1. ä¸èƒ½
2. èƒ½

### è§£æ

####  1.Datasets ä¸­çš„ `load_dataset ()` å‡½æ•°å…è®¸ä½ ä»ä¸‹åˆ—å“ªä¸ªä½ç½®åŠ è½½æ•°æ®é›†ï¼Ÿ

æ­£ç¡®é€‰é¡¹: 1. åœ¨æœ¬åœ°ï¼Œä¾‹å¦‚åœ¨ä½ çš„ç¬”è®°æœ¬ç”µè„‘ä¸Š

æ­£ç¡®é€‰é¡¹: 2. Hugging Face Hub

æ­£ç¡®é€‰é¡¹: 3. è¿œç¨‹æœåŠ¡å™¨

1. åœ¨æœ¬åœ°ï¼Œä¾‹å¦‚åœ¨ä½ çš„ç¬”è®°æœ¬ç”µè„‘ä¸Š    
è§£æ: æ­£ç¡®ï¼ä½ å¯ä»¥å°†æœ¬åœ°æ–‡ä»¶çš„è·¯å¾„ä¼ é€’ç»™ `load_dataset()` å‡½æ•°çš„ `data_files` å‚æ•°æ¥åŠ è½½æœ¬åœ°æ•°æ®é›†ã€‚
2. Hugging Face Hub    
è§£æ: æ­£ç¡®ï¼ä½ å¯ä»¥é€šè¿‡æä¾›æ•°æ®é›† ID æ¥åŠ è½½ Hub ä¸Šçš„æ•°æ®é›†ï¼Œä¾‹å¦‚ `load_dataset('emotion')` ã€‚
3. è¿œç¨‹æœåŠ¡å™¨    
è§£æ: æ­£ç¡®ï¼ä½ å¯ä»¥å°† URL ä¼ é€’ç»™ `load_dataset()` å‡½æ•°çš„ `data_files` å‚æ•°æ¥åŠ è½½è¿œç¨‹æ–‡ä»¶ã€‚

####  2ï¼å‡è®¾ä½ æŒ‰ç…§ä»¥ä¸‹æ–¹å¼åŠ è½½äº†ä¸€ä¸ª GLUE ä»»åŠ¡ï¼š

```python
from datasets import load_dataset

dataset = load_dataset("glue", "mrpc", split="train")
```

ä»¥ä¸‹å“ªä¸ªå‘½ä»¤å¯ä»¥ä» `dataset` ä¸­ç”Ÿæˆ 50 ä¸ªå…ƒç´ çš„éšæœºæ ·æœ¬ï¼Ÿ
æ­£ç¡®é€‰é¡¹: 2.  `dataset.shuffle().select(range(50))` 

1.  `dataset.sample (50)`     
è§£æ: ä¸æ­£ç¡® â€”â€” æ²¡æœ‰ `Dataset.sample()` æ–¹æ³•ã€‚
2.  `dataset.shuffle().select(range(50))`     
è§£æ: æ­£ç¡®ï¼æ­£å¦‚ä½ åœ¨è¿™ä¸€ç« ä¸­æ‰€å­¦åˆ°çš„ï¼Œä½ é¦–å…ˆæ‰“ä¹±äº†æ•°æ®é›†ï¼Œç„¶åä»ä¸­é€‰æ‹©æ ·æœ¬ã€‚
3.  `dataset.select(range(50)).shuffle()`     
è§£æ: è¿™æ˜¯ä¸æ­£ç¡®çš„â€”â€”å°½ç®¡ä»£ç ä¼šè¿è¡Œï¼Œä½†å®ƒåªä¼šé€‰å–æ•°æ®é›†çš„å‰ 50 ä¸ªå…ƒç´ ç„¶åæ‰“ä¹±å®ƒä»¬ã€‚

####  3ï¼å‡è®¾ä½ æœ‰ä¸€ä¸ªå…³äºå®¶åº­å® ç‰©çš„æ•°æ®é›† `pets_dataset` ï¼Œå®ƒæœ‰ä¸€ä¸ª `name` åˆ—è¡¨ç¤ºæ¯ä¸ªå® ç‰©çš„åå­—ã€‚ä»¥ä¸‹å“ªç§æ–¹æ³•å¯ä»¥ç­›é€‰å‡ºæ‰€æœ‰åå­—ä»¥ "L" å¼€å¤´çš„å® ç‰©æ•°æ®ï¼Ÿ

æ­£ç¡®é€‰é¡¹: 1.  `pets_dataset.filter(lambda x: x ['name'].startswith ('L'))` 

æ­£ç¡®é€‰é¡¹: 3. åˆ›å»ºä¸€ä¸ªç±»ä¼¼äº `def filter_names (x) : return x['name'].startswith ('L')` çš„å‡½æ•°ï¼Œç„¶åè¿è¡Œ `pets_dataset.filter(filter_names)` ã€‚

1.  `pets_dataset.filter(lambda x: x ['name'].startswith ('L'))`     
è§£æ: æ­£ç¡®ï¼ä½¿ç”¨ Python çš„ lambda å‡½æ•°æ¥å¿«é€Ÿè¿‡æ»¤æ˜¯ä¸€ä¸ªå¥½ä¸»æ„ã€‚ä½ èƒ½æƒ³å‡ºå¦ä¸€ç§è§£å†³æ–¹æ¡ˆå—ï¼Ÿ
2.  `pets_dataset.filter(lambda x ['name'].startswith ('L'))`     
è§£æ: è¿™æ˜¯ä¸æ­£ç¡®çš„â€”â€”lambda å‡½æ•°é€šå¸¸çš„æ ¼å¼ä¸ºï¼š `lambda *arguments*:*expression*`
3. åˆ›å»ºä¸€ä¸ªç±»ä¼¼äº `def filter_names (x) : return x['name'].startswith ('L')` çš„å‡½æ•°ï¼Œç„¶åè¿è¡Œ `pets_dataset.filter(filter_names)` ã€‚    
è§£æ: æ­£ç¡®ï¼å°±åƒä½¿ç”¨ `Dataset.map ()` ä¸€æ ·ï¼Œä½ å¯ä»¥å°†å‡½æ•°ä¼ é€’ç»™ `Dataset.filter ()` ã€‚å½“ä½ æœ‰ä¸€äº›ä¸é€‚åˆ lambda å‡½æ•°çš„å¤æ‚é€»è¾‘æ—¶ï¼Œè¿™å¾ˆæœ‰ç”¨ã€‚å…¶ä»–è§£å†³æ–¹æ¡ˆä¸­è¿˜æœ‰å“ªä¸€ä¸ªå¯è¡Œï¼Ÿ

####  4ï¼ä»€ä¹ˆæ˜¯å†…å­˜æ˜ å°„ï¼Ÿ

æ­£ç¡®é€‰é¡¹: 2. RAM å’Œæ–‡ä»¶ç³»ç»Ÿå­˜å‚¨ä¹‹é—´çš„æ˜ å°„

1. CPU ä¸ GPU RAM ä¹‹é—´çš„æ˜ å°„    
è§£æ: ä¸æ˜¯è¿™æ ·çš„â€”â€”è¯·å†è¯•ä¸€æ¬¡ï¼
2. RAM å’Œæ–‡ä»¶ç³»ç»Ÿå­˜å‚¨ä¹‹é—´çš„æ˜ å°„    
è§£æ: æ­£ç¡®ï¼Datasets å°†æ¯ä¸ªæ•°æ®é›†è§†ä¸ºä¸€ä¸ªå†…å­˜æ˜ å°„æ–‡ä»¶ã€‚è¿™ä½¿å¾—Datasets åº“è®¿é—®å’Œæ“ä½œæ•°æ®é›†çš„å…ƒç´ æ—¶ï¼Œæ— éœ€å°†å…¶å®Œå…¨åŠ è½½åˆ°å†…å­˜ä¸­ã€‚
3. Datasets ç¼“å­˜ä¸­ä¸¤ä¸ªæ–‡ä»¶ä¹‹é—´çš„æ˜ å°„    
è§£æ: è¿™æ˜¯ä¸æ­£ç¡®çš„â€”â€”è¯·å†è¯•ä¸€æ¬¡ï¼

####  5ï¼ä¸‹åˆ—å“ªä¸€é¡¹æ˜¯å†…å­˜æ˜ å°„çš„ä¸»è¦å¥½å¤„ï¼Ÿ

æ­£ç¡®é€‰é¡¹: 1. è®¿é—®å†…å­˜æ˜ å°„æ–‡ä»¶æ¯”ä»ç£ç›˜è¯»å–æˆ–å†™å…¥ç£ç›˜æ›´å¿«ã€‚

æ­£ç¡®é€‰é¡¹: 2. åº”ç”¨ç¨‹åºå¯ä»¥è®¿é—®ä¸€ä¸ªéå¸¸å¤§çš„æ–‡ä»¶ä¸­çš„æ•°æ®æ®µï¼Œè€Œä¸å¿…é¦–å…ˆå°†æ•´ä¸ªæ–‡ä»¶è¯»å…¥ RAMã€‚

1. è®¿é—®å†…å­˜æ˜ å°„æ–‡ä»¶æ¯”ä»ç£ç›˜è¯»å–æˆ–å†™å…¥ç£ç›˜æ›´å¿«ã€‚    
è§£æ: æ­£ç¡®ï¼è¿™ä½¿å¾— Datasets éå¸¸å¿«ã€‚ä½†è¿™å¹¶ä¸æ˜¯å”¯ä¸€çš„å¥½å¤„ã€‚
2. åº”ç”¨ç¨‹åºå¯ä»¥è®¿é—®ä¸€ä¸ªéå¸¸å¤§çš„æ–‡ä»¶ä¸­çš„æ•°æ®æ®µï¼Œè€Œä¸å¿…é¦–å…ˆå°†æ•´ä¸ªæ–‡ä»¶è¯»å…¥ RAMã€‚    
è§£æ: æ­£ç¡®ï¼â€”â€”è¿™å…è®¸Datasets åœ¨ä½ çš„ç¬”è®°æœ¬ç”µè„‘ä¸ŠåŠ è½½æ•° GB çš„æ•°æ®é›†ï¼Œè€Œä¸ä¼šè®©ä½ çš„ CPU å´©æºƒã€‚å†…å­˜æ˜ å°„è¿˜æä¾›äº†ä»€ä¹ˆå…¶ä»–ä¼˜åŠ¿ï¼Ÿ
3. å®ƒæ¶ˆè€—æ›´å°‘çš„èƒ½é‡ï¼Œæ‰€ä»¥ä½ çš„ç”µæ± æŒç»­æ—¶é—´æ›´é•¿ã€‚    
è§£æ: è¿™æ˜¯ä¸æ­£ç¡®çš„â€”â€”è¯·å†è¯•ä¸€æ¬¡ï¼

####  6ï¼ä¸ºä»€ä¹ˆä¸‹é¢çš„ä»£ç æ˜¯é”™è¯¯çš„ï¼Ÿ

```python
from datasets import load_dataset

dataset = load_dataset("allocine", streaming=True, split="train")
dataset[0]
```
æ­£ç¡®é€‰é¡¹: 2. å®ƒå°è¯•è®¿é—® `IterableDataset` ã€‚

1. å®ƒè¯•å›¾å¯¹ä¸€ä¸ªå¤ªå¤§è€Œæ— æ³•æ”¾å…¥ RAM çš„æ•°æ®é›†è¿›è¡Œæµå¼å¤„ç†ã€‚    
è§£æ: è¿™æ˜¯ä¸æ­£ç¡®çš„â€”â€” æµå¼æ•°æ®é›†æ˜¯åŠ¨æ€è§£å‹çš„ï¼Œä½ å¯ä»¥ç”¨éå¸¸å°çš„ RAM å¤„ç† TB é‡çº§çš„æ•°æ®é›†ï¼
2. å®ƒå°è¯•è®¿é—® `IterableDataset` ã€‚    
è§£æ: æ­£ç¡®ï¼ `IterableDataset` æ˜¯ä¸€ä¸ªç”Ÿæˆå™¨ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªå®¹å™¨ï¼Œå› æ­¤ä½ åº”è¯¥ä½¿ç”¨ `next(iter(dataset))` æ¥è®¿é—®å®ƒçš„å…ƒç´ ã€‚
3. æ•°æ®é›† `allocine` æ²¡æœ‰ `train` éƒ¨åˆ†ã€‚    
è§£æ: è¿™æ˜¯ä¸æ­£ç¡®çš„â€”â€” æŸ¥çœ‹ Hub ä¸Šçš„ [`allocine`æ•°æ®å¡ç‰‡](https://huggingface.co/datasets/allocine)(https://huggingface.co/datasets/allocine)

####  7ï¼åˆ›å»ºæ•°æ®é›†å¡ç‰‡çš„ä¸»è¦å¥½å¤„æ˜¯ä»€ä¹ˆï¼Ÿ

æ­£ç¡®é€‰é¡¹: 1. å®ƒæä¾›äº†å…³äºæ•°æ®é›†çš„é¢„æœŸç”¨é€”å’Œæ”¯æŒçš„ä»»åŠ¡çš„ä¿¡æ¯ï¼Œä»¥ä¾¿ç¤¾åŒºä¸­çš„å…¶ä»–äººå¯ä»¥å¯¹ä½¿ç”¨å®ƒåšå‡ºæ˜æ™ºçš„å†³å®šã€‚

æ­£ç¡®é€‰é¡¹: 2. å®ƒæœ‰åŠ©äºå¼•èµ·äººä»¬å¯¹è¯­æ–™åº“ä¸­å­˜åœ¨çš„åè§çš„å…³æ³¨ã€‚

æ­£ç¡®é€‰é¡¹: 3. å®ƒæé«˜äº†ç¤¾åŒºä¸­å…¶ä»–äººä½¿ç”¨æˆ‘çš„æ•°æ®é›†çš„å¯èƒ½æ€§ã€‚

1. å®ƒæä¾›äº†å…³äºæ•°æ®é›†çš„é¢„æœŸç”¨é€”å’Œæ”¯æŒçš„ä»»åŠ¡çš„ä¿¡æ¯ï¼Œä»¥ä¾¿ç¤¾åŒºä¸­çš„å…¶ä»–äººå¯ä»¥å¯¹ä½¿ç”¨å®ƒåšå‡ºæ˜æ™ºçš„å†³å®šã€‚    
è§£æ: æ­£ç¡®ï¼æœªè®°å½•çš„æ•°æ®é›†å¯ç”¨äºè®­ç»ƒå¯èƒ½æ— æ³•åæ˜ æ•°æ®é›†åˆ›å»ºè€…æ„å›¾çš„æ¨¡å‹ï¼Œæˆ–è€…å¦‚æœå¯¹è¿åéšç§æˆ–è®¸å¯é™åˆ¶çš„æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œåˆ™å¯èƒ½ä¼šäº§ç”Ÿæ³•å¾‹çŠ¶æ€æ¨¡ç³Šçš„æ¨¡å‹ã€‚ä¸è¿‡ï¼Œè¿™å¹¶ä¸æ˜¯å”¯ä¸€çš„å¥½å¤„ï¼
2. å®ƒæœ‰åŠ©äºå¼•èµ·äººä»¬å¯¹è¯­æ–™åº“ä¸­å­˜åœ¨çš„åè§çš„å…³æ³¨ã€‚    
è§£æ: æ­£ç¡®ï¼å‡ ä¹æ‰€æœ‰æ•°æ®é›†éƒ½å­˜åœ¨æŸç§å½¢å¼çš„åè§ï¼Œè¿™å¯èƒ½ä¼šäº§ç”Ÿè´Ÿé¢çš„åæœã€‚äº†è§£å®ƒä»¬æœ‰åŠ©äºæ¨¡å‹æ„å»ºè€…äº†è§£å¦‚ä½•è§£å†³å›ºæœ‰çš„åè§ã€‚æ•°æ®é›†å¡è¿˜æœ‰ä»€ä¹ˆå¸®åŠ©ï¼Ÿ
3. å®ƒæé«˜äº†ç¤¾åŒºä¸­å…¶ä»–äººä½¿ç”¨æˆ‘çš„æ•°æ®é›†çš„å¯èƒ½æ€§ã€‚    
è§£æ: æ­£ç¡®ï¼ä¸€ä¸ªå†™å¾—å¥½çš„æ•°æ®é›†å¡é€šå¸¸ä¼šä¿ƒä½¿ä½ çš„æ•°æ®é›†çš„ä½¿ç”¨ç‡æ›´é«˜ã€‚å®ƒè¿˜æœ‰ä»€ä¹ˆå¥½å¤„å‘¢ï¼Ÿ

####  8ï¼ä»€ä¹ˆæ˜¯è¯­ä¹‰æœç´¢ï¼Ÿ

æ­£ç¡®é€‰é¡¹: 2. ä¸€ç§é€šè¿‡ç†è§£æŸ¥è¯¢çš„ä¸Šä¸‹æ–‡å«ä¹‰æ¥æœç´¢åŒ¹é…æ–‡æ¡£çš„æ–¹æ³•

æ­£ç¡®é€‰é¡¹: 3. ä¸€ç§æé«˜æœç´¢å‡†ç¡®æ€§çš„æ–¹æ³•

1. ä¸€ç§åœ¨æŸ¥è¯¢ä¸­çš„å•è¯å’Œè¯­æ–™åº“ä¸­çš„æ–‡æ¡£ä¹‹é—´ç²¾ç¡®åŒ¹é…çš„æ–¹æ³•    
è§£æ: è¿™æ˜¯ä¸æ­£ç¡®çš„â€”â€”è¿™ç§ç±»å‹çš„æœç´¢è¢«ç§°ä¸ºâ€œè¯æ±‡æœç´¢ï¼ˆlexical searchï¼‰â€ï¼Œè¿™æ˜¯ä½ åœ¨ä¼ ç»Ÿæœç´¢å¼•æ“ä¸­é€šå¸¸çœ‹åˆ°çš„ã€‚
2. ä¸€ç§é€šè¿‡ç†è§£æŸ¥è¯¢çš„ä¸Šä¸‹æ–‡å«ä¹‰æ¥æœç´¢åŒ¹é…æ–‡æ¡£çš„æ–¹æ³•    
è§£æ: æ­£ç¡®ï¼è¯­ä¹‰æœç´¢ä½¿ç”¨åµŒå…¥å‘é‡æ¥è¡¨ç¤ºæŸ¥è¯¢å’Œæ–‡æ¡£ï¼Œå¹¶ä½¿ç”¨ç›¸ä¼¼åº¦åº¦é‡æ¥è¡¡é‡å®ƒä»¬ä¹‹é—´çš„ç›¸ä¼¼ç¨‹åº¦ã€‚ä½ è¿˜ä¼šæ€ä¹ˆæè¿°å®ƒå‘¢ï¼Ÿ
3. ä¸€ç§æé«˜æœç´¢å‡†ç¡®æ€§çš„æ–¹æ³•    
è§£æ: æ­£ç¡®ï¼è¯­ä¹‰æœç´¢å¼•æ“èƒ½å¤Ÿæ¯”å…³é”®å­—åŒ¹é…æ›´å¥½åœ°æ•è·æŸ¥è¯¢æ„å›¾ï¼Œå¹¶ä¸”é€šå¸¸èƒ½å¤Ÿä»¥æ›´é«˜çš„ç²¾åº¦æ£€ç´¢æ–‡æ¡£ã€‚ä½†è¿™ä¸æ˜¯å”¯ä¸€æ­£ç¡®çš„ç­”æ¡ˆâ€”â€”è¯­ä¹‰æœç´¢è¿˜æä¾›äº†å“ªäº›åŠŸèƒ½ï¼Ÿ

####  9ï¼å¯¹äºéå¯¹ç§°è¯­ä¹‰æœç´¢ï¼Œé€šå¸¸æœ‰ï¼š

æ­£ç¡®é€‰é¡¹: 1. ä¸€ä¸ªç®€çŸ­çš„æŸ¥è¯¢å’Œä¸€ä¸ªè¾ƒé•¿çš„æ®µè½å›ç­”æŸ¥è¯¢

1. ä¸€ä¸ªç®€çŸ­çš„æŸ¥è¯¢å’Œä¸€ä¸ªè¾ƒé•¿çš„æ®µè½å›ç­”æŸ¥è¯¢    
è§£æ: æ­£ç¡®ï¼
2. ç›¸åŒé•¿åº¦çš„æŸ¥è¯¢å’Œæ®µè½    
è§£æ: è¿™å®é™…ä¸Šæ˜¯ä¸€ä¸ªå¯¹ç§°è¯­ä¹‰æœç´¢çš„ä¾‹å­â€”â€”è¯·å†è¯•ä¸€æ¬¡ï¼
3. ä¸€ä¸ªè¾ƒé•¿çš„æŸ¥è¯¢å’Œä¸€ä¸ªè¾ƒçŸ­çš„æ®µè½æ¥å›ç­”æŸ¥è¯¢    
è§£æ: è¿™æ˜¯é”™è¯¯çš„â€”â€” å†è¯•ä¸€æ¬¡ï¼

####  10ï¼æˆ‘å¯ä»¥ä½¿ç”¨Datasets æ¥åŠ è½½ç”¨äºå…¶ä»–é¢†åŸŸï¼ˆå¦‚è¯­éŸ³å¤„ç†ï¼‰çš„æ•°æ®å—ï¼Ÿ

æ­£ç¡®é€‰é¡¹: 2. èƒ½

1. ä¸èƒ½    
è§£æ: è¿™æ˜¯ä¸æ­£ç¡®çš„ -- Datasets ç›®å‰æ”¯æŒè¡¨æ ¼æ•°æ®ã€éŸ³é¢‘å’Œè®¡ç®—æœºè§†è§‰ã€‚ä½ å¯ä»¥åœ¨ Hub ä¸ŠæŸ¥çœ‹è®¡ç®—æœºè§†è§‰çš„ [MNIST]( https://huggingface.co/datasets/allocine )( https://huggingface.co/datasets/allocine ) ä¾‹å­ã€‚
2. èƒ½    
è§£æ: æ­£ç¡®ï¼ä½ å¯ä»¥æŸ¥çœ‹Transformers åº“ä¸­å…³äºè¯­éŸ³å’Œè§†è§‰çš„å‘å±•ï¼Œçœ‹çœ‹Datasets æ˜¯å¦‚ä½•åº”ç”¨äºè¿™äº›é¢†åŸŸä¸­çš„ã€‚

