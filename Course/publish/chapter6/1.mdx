
## 本章简介 

在第四章中，我们研究了如何在特定任务上微调模型。当我们需要微调模型时，我们需要使用与模型预训练相同的 tokenizer ——但是当我们想从头开始训练模型时该怎么办？使用在来自其他领域或语言的语料库上预训练的 tokenizer 通常不是最理想的。例如，在英语语料库上训练的 tokenizer 在日语文本语料库上效果会大打折扣，因为两种语言在空格和标点的使用上有着显著的差异。

在本章中，你将学习如何在一份文本语料库上训练一个全新的 tokenizer，然后将其用于预训练语言模型。这一切都将在 [Tokenizers](https://github.com/huggingface/tokenizers)(https://github.com/huggingface/tokenizers) 库的帮助下完成，该库提供了 [Transformers](https://github.com/huggingface/transformers)(https://github.com/huggingface/transformers) 库中的“快速” tokenizer 我们将深入探讨这个库所提供的功能，并研究“快速” tokenizer 与“慢速”版本的区别。

本章将涵盖以下主题：

* 如何在新的文本语料库上训练一个类似于给定 Checkpoint 所使用的新 tokenizer
* 快速 tokenizer 的特殊功能
* 目前 NLP 中使用的三种主要子词 tokenization 算法之间的差异
* 如何使用Tokenizers 库从头开始构建 tokenizer 并在一些数据上进行训练

本章介绍的技术将使你为第八章中的部分做好准备，在那部分中，我们着眼于为 Python 源代码创建语言模型。让我们首先看一下什么是“训练” tokenizer 