
##  7.9 章末总结及测试

恭喜你完成了这一章！

在深入研究 tokenizer 之后，你应该：

- 能够使用旧的 tokenizer 作为模板来训练新的 tokenizer 
- 了解如何使用偏移量将 tokens 的位置映射到其原始文本范围
- 了解 BPE、WordPiece 和 Unigram 之间的区别
- 能够混合使用 Tokenizers 库提供的块来构建你自己的 tokenizer 
- 能够在 Transformers 库中使用该 tokenizer 

### 章末测试 

####  1．你应该什么时候训练一个新的 tokenizer 

1. 当你的数据集与现有预训练模型使用的数据集相似，且你想预训练一个新模型时
2. 当你的数据集与现有预训练模型使用的数据集相似，且你想对此预训练模型进行微调时
3. 当你的数据集与现有预训练模型使用的数据集不同，且你想预训练一个新模型时
4. 当你的数据集与现有预训练模型所使用的数据集不同时，但是你对此预训练模型对新模型进行微调

####  2．当使用 `train_new_from_iterator()` 时，使用文本列表生成器与文本列表相比有什么优点？

1. 文本列表生成器是`train_new_from_iterator()`方法唯一接受的输入类型。
2. 你可以避免一次性将整个数据集加载到内存中。
3. 这将允许Tokenizers 库使用多进程。
4. 你训练的 Tokenizer 将生成效果更好的输出。

####  3．使用“快速” tokenizer 有什么优势？

1. 当你批处理大量的输入时，它可以比慢速的 tokenizer 更快地处理输入。
2. 快速的 tokenizer 总是比慢速的 tokenizer 快。
3. 它可以填充和截断文本。
4. 它有一些额外的功能，允许你将 tokens 映射到生成它们的文本范围。

####  4. `token-classification` 管道如何处理跨越多个 tokens 的实体？

1. 具有相同标签的实体被合并为一个实体。
2. 使用一个标签代表实体的开始，另一个标签代表实体的继续。
3. 在给定的单词中，只要第一个 tokens 具有实体的标签，整个单词都被认为带有该实体的标签。
4. 当一个 tokens 具有特定实体的标签时，任何其他后续的带有相同标签的 tokens 都被视为同一实体的一部分，除非它被预测为新实体的开始。

####  5. `question-answering` 管道如何处理长上下文？

1. 它其实并未处理，因为它会将超过模型接受的最大长度的长上下文截断。
2. 它将上下文分成若干部分，并对所得结果进行平均。
3. 它将上下文拆分为若干部分(有重叠部分) 并在每个部分中查找一个答案的最大分数。
4. 它将上下文分成若干部分(不重叠，以提高效率) 并在每个部分中找到一个答案的最大得分。

####  6．什么是标准化？

1. 这是 tokenizer 在初始阶段对文本进行的清洗。
2. 这是一种数据增强技术，包括通过删除稀有单词使文本更加标准。
3. 在最后的后处理步骤中，tokenizer 添加特殊 tokens 。
4. 这是通过减去平均值并除以标准差，使嵌入具有 0 的均值和 1 的标准差的过程。

####  7．什么是 tokenizer 的预分词？

1. 这是 tokenization 之前的步骤，对数据进行增强(如随机掩码)。
2. 这是 tokenization 之前的步骤，在这个步骤中，对文本进行清洗。
3. 这是应用 tokenizer 模型之前的步骤，将输入拆分为单词。
4. 这是应用 tokenizer 模型之前的步骤，将输入切分为 tokens 。

####  8．选择描述 BPE 算法最准确的句子。

1. BPE 是一个 tokenization 算法，从小词汇表开始，学习合并规则。
2. BPE 是一种 tokenization 算法，从大词汇表开始，逐步从中删除 tokens 。
3. BPE tokenizer 通过合并最常见的一对 token 来学习合并规则。
4. BPE（字节对编码） tokenizer 通过合并一对 tokens 来学习合并规则，该规则会优先选择经常一起出现以及不经常独立出现的 tokens 对。
5. BPE 通过将单词分割成字符，然后使用合并规则将单词分解成子单词。
6. BPE 通过从词汇表的开头找到最长的子词，然后在文本的其余部分重复这个过程，将单词转化为子词。

####  9．选择描述 WordPiece 算法最准确的句子。

1. WordPiece 是一个 tokenization 算法，它从一个小词汇表开始，学习合并规则。
2. WordPiece 是一种 tokenization 算法，从大词汇表开始，逐步从中删除 tokens。
3. WordPiece tokenizer 通过合并最常见的两个 tokens 来学习合并规则。
4. WordPiece tokenizer 通过合并一对 tokens 来学习合并规则，该规则会优先选择经常一起出现以及不经常独立出现的 tokens 对。
5. WordPiece 通过模型找到最可能的分割方式，将单词分词为子词。
6. WordPiece 通过从词汇表的开头找到最长的子词，然后在文本的其余部分重复这个过程，将单词转化为子词。

####  10．选择描述 Unigram 算法最准确的句子。

1. Unigram 是一个 tokenization 算法，它从一个很小的词汇表开始学习合并规则。
2. Unigram 是一种 tokenization 算法，它从大词汇表开始，逐步从中删除 tokens。
3. Unigram 通过最小化在整个语料库中的损失来调整词汇量。
4. Unigram 通过保留出现最频繁的子词来调整它的词汇量。
5. Unigram 通过模型找到最可能的分割位置来将词转化为子词。
6. Unigram 通过将单词分割成字符，然后使用合并规则将单词分解成子词。

### 解析

####  1．你应该什么时候训练一个新的 tokenizer 

正确选项: 3. 当你的数据集与现有预训练模型使用的数据集不同，且你想预训练一个新模型时

1. 当你的数据集与现有预训练模型使用的数据集相似，且你想预训练一个新模型时    
解析: 在这种情况下，为了节省时间和计算资源，一个更好的选择是使用与预训练模型相同的 tokenizer ，并对该 tokenizer 进行微调。
2. 当你的数据集与现有预训练模型使用的数据集相似，且你想对此预训练模型进行微调时    
解析: 从预训练模型进行微调，你应该使用与预训练模型相同的 tokenizer 。
3. 当你的数据集与现有预训练模型使用的数据集不同，且你想预训练一个新模型时    
解析: 正确！在这种情况下，使用现有预训练模型 tokenizer 将没有任何优势。
4. 当你的数据集与现有预训练模型所使用的数据集不同时，但是你对此预训练模型对新模型进行微调    
解析: 从预训练模型进行微调，你应该使用与预训练模型相同的 tokenizer 。

####  2．当使用 `train_new_from_iterator()` 时，使用文本列表生成器与文本列表相比有什么优点？

正确选项: 2. 你可以避免一次性将整个数据集加载到内存中。

1. 文本列表生成器是`train_new_from_iterator()`方法唯一接受的输入类型。    
解析: 文本列表是一种特殊的文本列表生成器，因此该方法也会接受这种方法。再试一次！
2. 你可以避免一次性将整个数据集加载到内存中。    
解析: 正确！每个 batch 的文本在迭代时都将从内存中释放，如果你使用Datasets 存储你的文本，你会看到特别明显的收益。
3. 这将允许Tokenizers 库使用多进程。    
解析: 不，无论如何它都会使用多进程。
4. 你训练的 Tokenizer 将生成效果更好的输出。    
解析: Tokenizer 不生成文本——你是否将其与语言模型混淆了？

####  3．使用“快速” tokenizer 有什么优势？

正确选项: 1. 当你批处理大量的输入时，它可以比慢速的 tokenizer 更快地处理输入。

正确选项: 4. 它有一些额外的功能，允许你将 tokens 映射到生成它们的文本范围。

1. 当你批处理大量的输入时，它可以比慢速的 tokenizer 更快地处理输入。    
解析: 正确！由于 Rust 中实现的并行性，它将在批量处理输入上更快。你还能想到其他什么好处吗？
2. 快速的 tokenizer 总是比慢速的 tokenizer 快。    
解析: 当你只给它一个或很少的文本时，快速 tokenizer 实际上可能更慢，因为它不能并行。
3. 它可以填充和截断文本。    
解析: 是的，但是慢速的 tokenizer 也可以做到这一点。
4. 它有一些额外的功能，允许你将 tokens 映射到生成它们的文本范围。    
解析: 确实——这些被称为偏移映射。但这不是唯一的优势。

####  4. `token-classification` 管道如何处理跨越多个 tokens 的实体？

正确选项: 2. 使用一个标签代表实体的开始，另一个标签代表实体的继续。

正确选项: 3. 在给定的单词中，只要第一个 tokens 具有实体的标签，整个单词都被认为带有该实体的标签。

正确选项: 4. 当一个 tokens 具有特定实体的标签时，任何其他后续的带有相同标签的 tokens 都被视为同一实体的一部分，除非它被预测为新实体的开始。

1. 具有相同标签的实体被合并为一个实体。    
解析: 这样处理有点过于简单了，再试一次！
2. 使用一个标签代表实体的开始，另一个标签代表实体的继续。    
解析: 正确！
3. 在给定的单词中，只要第一个 tokens 具有实体的标签，整个单词都被认为带有该实体的标签。    
解析: 这是处理实体的其中一种策略。这里还有什么其他的答案？
4. 当一个 tokens 具有特定实体的标签时，任何其他后续的带有相同标签的 tokens 都被视为同一实体的一部分，除非它被预测为新实体的开始。    
解析: 这是最常见的将实体组合在一起的方法——但这并不是唯一的正确答案。

####  5. `question-answering` 管道如何处理长上下文？

正确选项: 3. 它将上下文拆分为若干部分(有重叠部分) 并在每个部分中查找一个答案的最大分数。

1. 它其实并未处理，因为它会将超过模型接受的最大长度的长上下文截断。    
解析: 有一个技巧你可以用来处理很长的上下文，你还记得是什么吗？
2. 它将上下文分成若干部分，并对所得结果进行平均。    
解析: 不，对所有的部分进行平均是没有意义的，因为上下文的某些部分不包括答案。
3. 它将上下文拆分为若干部分(有重叠部分) 并在每个部分中查找一个答案的最大分数。    
解析: 这就是正确答案！
4. 它将上下文分成若干部分(不重叠，以提高效率) 并在每个部分中找到一个答案的最大得分。    
解析: 不，它包括部分之间的一些重叠，以避免出现答案将分成两部分的情况。

####  6．什么是标准化？

正确选项: 1. 这是 tokenizer 在初始阶段对文本进行的清洗。

1. 这是 tokenizer 在初始阶段对文本进行的清洗。    
解析: 这是正确的——例如，它可能涉及删除重音符号或空格，或输入转化为小写。
3. 在最后的后处理步骤中，tokenizer 添加特殊 tokens 。    
解析: 这个阶段简单地称为后期处理。
4. 这是通过减去平均值并除以标准差，使嵌入具有 0 的均值和 1 的标准差的过程。    
解析: 在计算机视觉中，对像素值进行这种处理通常被称为标准化，但在自然语言处理中，标准化的含义并非如此。

####  7．什么是 tokenizer 的预分词？

正确选项: 3. 这是应用 tokenizer 模型之前的步骤，将输入拆分为单词。

1. 这是 tokenization 之前的步骤，对数据进行增强(如随机掩码)。    
解析: 不，这一步是预处理的一部分，但是不是预分词。
2. 这是 tokenization 之前的步骤，在这个步骤中，对文本进行清洗。    
解析: 不，这是标准化步骤。
3. 这是应用 tokenizer 模型之前的步骤，将输入拆分为单词。    
解析: 这就是正确答案！
4. 这是应用 tokenizer 模型之前的步骤，将输入切分为 tokens 。    
解析: 不，切分为 tokens 是 tokenizer 子词分词算法模型的工作。

####  8．选择描述 BPE 算法最准确的句子。

正确选项: 1. BPE 是一个 tokenization 算法，从小词汇表开始，学习合并规则。

正确选项: 3. BPE tokenizer 通过合并最常见的一对 token 来学习合并规则。

正确选项: 5. BPE 通过将单词分割成字符，然后使用合并规则将单词分解成子单词。

####  9．选择描述 WordPiece 算法最准确的句子。

正确选项: 1. WordPiece 是一个 tokenization 算法，它从一个小词汇表开始，学习合并规则。

正确选项: 4. WordPiece tokenizer 通过合并一对 tokens 来学习合并规则，该规则会优先选择经常一起出现以及不经常独立出现的 tokens 对。

正确选项: 6. WordPiece 通过从词汇表的开头找到最长的子词，然后在文本的其余部分重复这个过程，将单词转化为子词。

####  10．选择描述 Unigram 算法最准确的句子。

正确选项: 2. Unigram 是一种 tokenization 算法，它从大词汇表开始，逐步从中删除 tokens。

正确选项: 3. Unigram 通过最小化在整个语料库中的损失来调整词汇量。

正确选项: 5. Unigram 通过模型找到最可能的分割位置来将词转化为子词。

