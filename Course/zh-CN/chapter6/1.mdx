# 本章简介 [[本章简介]]

<CourseFloatingBanner
    chapter={6}
    classNames="absolute z-10 right-0 top-0"
/>

在 [第三章] (/course/chapter3) 中，我们研究了如何在特定任务上微调模型。 当我们这样做时，我们需要使用与模型预训练相同的词元分析器——但是当我们想从头开始训练模型时该怎么办？ 不过，使用在来自其他领域或语言的语料库上预训练的词元分析器通常不是最理想的。 例如，在英语语料库上训练的词元分析器在日语文本语料库上效果会大打折扣，因为两种语言在空格和标点的使用上有着显著的差异。

在本章中，你将学习如何在一份文本语料库上训练一个全新的词元分析器，然后将其用于预训练语言模型。 这一切都将在 [🤗 Tokenizers](https://github.com/huggingface/tokenizers) 库的帮助下完成，该库提供了 [🤗 Transformers](https://github.com/huggingface/transformers) 库中的"快速"分词器。 我们将深入探讨这个库所提供的功能，并研究快速分词器与"慢速"版本的区别。

本章将涵盖以下主题：

* 如何在新的文本语料库上训练一个类似于给定检查点所使用的新词元分析器
* 快速词元分析器的特殊功能
* 目前 NLP 中使用的三种主要子词标记化算法之间的差异
* 如何使用🤗 Tokenizers 库从头开始构建词元分析器，并在一些数据上进行训练

本章介绍的技术将使您为 [第 7 章](/course/chapter7/6) 中的部分做好准备，在那部分中，我们着眼于为 Python 源代码创建语言模型。 让我们首先看一下什么是“训练”词元分析器？