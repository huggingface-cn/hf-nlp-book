# æ¨¡å—åŒ–æ„å»ºè¯å…ƒåˆ†æå™¨ [[æ¨¡å—åŒ–æ„å»ºè¯å…ƒåˆ†æå™¨]]

<CourseFloatingBanner chapter={6}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/zh-CN/chapter6/section8.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/zh-CN/chapter6/section8.ipynb"},
]} />

æ­£å¦‚æˆ‘ä»¬åœ¨å‰å‡ èŠ‚ä¸­çœ‹åˆ°çš„ï¼Œè¯å…ƒåŒ–åŒ…æ‹¬å‡ ä¸ªæ­¥éª¤ï¼š

- æ ‡å‡†åŒ–ï¼ˆä»»ä½•è®¤ä¸ºå¿…è¦çš„æ–‡æœ¬æ¸…ç†ï¼Œä¾‹å¦‚åˆ é™¤ç©ºæ ¼æˆ–é‡éŸ³ç¬¦å·ã€Unicode è§„èŒƒåŒ–ç­‰ï¼‰ 
- é¢„åˆ†è¯ï¼ˆå°†è¾“å…¥æ‹†åˆ†ä¸ºå•è¯ï¼‰ 
- é€šè¿‡æ¨¡å‹å¤„ç†è¾“å…¥ï¼ˆä½¿ç”¨é¢„å…ˆæ‹†åˆ†çš„è¯æ¥ç”Ÿæˆä¸€ç³»åˆ—è¯å…ƒï¼‰ 
- åå¤„ç†ï¼ˆæ·»åŠ è¯å…ƒåˆ†æå™¨çš„ç‰¹æ®Šè¯å…ƒï¼Œç”Ÿæˆæ³¨æ„åŠ›æ©ç å’Œè¯å…ƒç±»å‹ IDï¼‰ 

ä½œä¸ºæé†’ï¼Œè¿™é‡Œå†çœ‹ä¸€éæ•´ä¸ªè¿‡ç¨‹ï¼š

<div class="flex justify-center">
<img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline.svg" alt="The tokenization pipeline.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/tokenization_pipeline-dark.svg" alt="The tokenization pipeline.">
</div>

ğŸ¤— Tokenizers åº“æ—¨åœ¨ä¸ºæ¯ä¸ªæ­¥éª¤æä¾›å¤šä¸ªé€‰é¡¹ï¼Œä½ å¯ä»¥ä»»æ„æ­é…è¿™äº›é€‰é¡¹ã€‚åœ¨è¿™ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°å¦‚ä½•ä»é›¶å¼€å§‹æ„å»ºè¯å…ƒåˆ†æå™¨ï¼Œè€Œä¸æ˜¯åƒæˆ‘ä»¬åœ¨ [ç¬¬äºŒèŠ‚](/course/chapter6/2) ä¸­é‚£æ ·ä»æ—§çš„åˆ†è¯å™¨è®­ç»ƒæ–°çš„åˆ†è¯å™¨ã€‚ç„¶åï¼Œä½ å°†èƒ½å¤Ÿæ„å»ºä»»ä½•ä½ èƒ½æƒ³åˆ°çš„ç±»å‹çš„åˆ†è¯å™¨ï¼

<Youtube id="MR8tZm5ViWU"/>

æ›´ç²¾ç¡®åœ°è¯´ï¼Œè¿™ä¸ªåº“å›´ç»•ä¸€ä¸ªä¸­å¿ƒçš„ `Tokenizer` ç±»æ„å»ºï¼ŒåŒ…å«äº†å„ä¸ªå­æ¨¡å—çš„æ„å»ºæ¨¡å—ï¼š

- `normalizers` åŒ…å«æ‰€æœ‰å¯èƒ½ä½¿ç”¨çš„ `Normalizer` ç±»å‹ï¼ˆå®Œæ•´åˆ—è¡¨ [åœ¨è¿™é‡Œ](https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.normalizers) ï¼‰ã€‚
- `pre_tokenizesr` åŒ…å«æ‰€æœ‰å¯èƒ½ä½¿ç”¨çš„ `PreTokenizer` ç±»å‹ï¼ˆå®Œæ•´åˆ—è¡¨ [åœ¨è¿™é‡Œ](https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.pre_tokenizers) ï¼‰ã€‚
- `models` åŒ…å«äº†ä½ å¯ä»¥ä½¿ç”¨çš„å„ç§ `Model` ç±»å‹ï¼Œå¦‚ `BPE` ã€ `WordPiece` å’Œ `Unigram` ï¼ˆå®Œæ•´åˆ—è¡¨ [åœ¨è¿™é‡Œ](https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.models) ï¼‰ã€‚
- `trainers` åŒ…å«æ‰€æœ‰ä¸åŒç±»å‹çš„ `trainer` ï¼Œä½ å¯ä»¥ä½¿ç”¨å®ƒä»¬åœ¨è¯­æ–™åº“ä¸Šè®­ç»ƒä½ çš„æ¨¡å‹ï¼ˆæ¯ç§æ¨¡å‹ä¸€ä¸ªï¼›å®Œæ•´åˆ—è¡¨ [åœ¨è¿™é‡Œ](https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.trainers) ï¼‰ã€‚
- `post_processors` åŒ…å«ä½ å¯ä»¥ä½¿ç”¨çš„å„ç§ç±»å‹çš„ `PostProcessor` ï¼ˆå®Œæ•´åˆ—è¡¨ [åœ¨è¿™é‡Œ](https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.processors) ï¼‰ã€‚
- `decoders` åŒ…å«å„ç§ç±»å‹çš„ `Decoder` ï¼Œå¯ä»¥ç”¨æ¥è§£ç è¯å…ƒåŒ–åçš„è¾“å‡ºï¼ˆå®Œæ•´åˆ—è¡¨ [åœ¨è¿™é‡Œ](https://huggingface.co/docs/tokenizers/python/latest/components.html#decoders) ï¼‰ã€‚

æ‚¨å¯ä»¥ [åœ¨è¿™é‡Œ](https://huggingface.co/docs/tokenizers/python/latest/components.html) æ‰¾åˆ°å®Œæ•´çš„æ¨¡å—åˆ—è¡¨ã€‚

## è·å–è¯­â€‹â€‹æ–™åº“ [[è·å–è¯­â€‹â€‹æ–™åº“]]

ä¸ºäº†è®­ç»ƒæ–°çš„è¯å…ƒåˆ†æå™¨ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸€å°éƒ¨åˆ†æ–‡æœ¬è¯­æ–™åº“ï¼ˆè¿™æ ·ç¤ºä¾‹è¿è¡Œå¾—æ›´å¿«ï¼‰ã€‚è·å–è¯­â€‹â€‹æ–™åº“çš„æ­¥éª¤ä¸æˆ‘ä»¬åœ¨ [åœ¨è¿™ç« çš„å¼€å¤´](/course/chapter6/2) é‡‡å–çš„æ­¥éª¤ç±»ä¼¼ï¼Œä½†è¿™æ¬¡æˆ‘ä»¬å°†ä½¿ç”¨ [WikiText-2](https://huggingface.co/datasets/wikitext) æ•°æ®é›†ï¼š

```python
from datasets import load_dataset

dataset = load_dataset("wikitext", name="wikitext-2-raw-v1", split="train")


def get_training_corpus():
    for i in range(0, len(dataset), 1000):
        yield dataset[i : i + 1000]["text"]
```

**get_training_corpus()** å‡½æ•°æ˜¯ä¸€ä¸ªç”Ÿæˆå™¨ï¼Œæ¯æ¬¡è°ƒç”¨çš„æ—¶å€™å°†äº§ç”Ÿ 1,000 ä¸ªæ–‡æœ¬ï¼Œæˆ‘ä»¬å°†ç”¨å®ƒæ¥è®­ç»ƒè¯å…ƒåˆ†æå™¨ã€‚

ğŸ¤— Tokenizers ä¹Ÿå¯ä»¥ç›´æ¥åœ¨æ–‡æœ¬æ–‡ä»¶ä¸Šè¿›è¡Œè®­ç»ƒã€‚ä»¥ä¸‹æ˜¯æˆ‘ä»¬å¦‚ä½•ç”Ÿæˆä¸€ä¸ªåŒ…å« WikiText-2 æ‰€æœ‰æ–‡æœ¬/è¾“å…¥çš„æ–‡æœ¬æ–‡ä»¶ï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥åœ¨æœ¬åœ°ç¦»çº¿ä½¿ç”¨ï¼š

```python
with open("wikitext-2.txt", "w", encoding="utf-8") as f:
    for i in range(len(dataset)):
        f.write(dataset[i]["text"] + "\n")
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†å±•ç¤ºå¦‚ä½•æ¨¡å—åŒ–åœ°æ„å»ºä½ è‡ªå·±çš„ BERTã€GPT-2 å’Œ XLNet è¯å…ƒåˆ†æå™¨ã€‚è¿™å°†åŒ…å«ä¸»è¦çš„åˆ†è¯ç®—æ³•ï¼šWordPieceã€BPE å’Œ Unigram çš„ä¾‹å­ã€‚è®©æˆ‘ä»¬ä» BERT å¼€å§‹å§ï¼

## ä»å¤´å¼€å§‹æ„å»º WordPiece è¯å…ƒåˆ†æå™¨ [[ä»å¤´å¼€å§‹æ„å»º WordPiece è¯å…ƒåˆ†æå™¨]]

è¦ç”¨ğŸ¤— Tokenizers åº“æ„å»ºä¸€ä¸ªåˆ†è¯å™¨ï¼Œæˆ‘ä»¬é¦–å…ˆå®ä¾‹åŒ–ä¸€ä¸ªå¸¦æœ‰ `model` çš„ `Tokenizer` å¯¹è±¡ï¼Œç„¶åå°†å…¶ `normalizer` ï¼Œ `pre_tokenizer` ï¼Œ `post_processor` å’Œ `decoder` å±æ€§è®¾ç½®ä¸ºæˆ‘ä»¬æƒ³è¦çš„å€¼ã€‚

ä»¥è¿™ä¸ªä¾‹å­æ¥è¯´ï¼Œæˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªä½¿ç”¨ WordPiece æ¨¡å‹çš„ `Tokenizer` ï¼š
```python
from tokenizers import (
    decoders,
    models,
    normalizers,
    pre_tokenizers,
    processors,
    trainers,
    Tokenizer,
)

tokenizer = Tokenizer(models.WordPiece(unk_token="[UNK]"))
```

æˆ‘ä»¬å¿…é¡»æŒ‡å®š `unk_token` ï¼Œè¿™æ ·å½“æ¨¡å‹é‡åˆ°å®ƒä»æœªè§è¿‡çš„å­—ç¬¦æ—¶ï¼Œå®ƒçŸ¥é“åº”è¯¥è¿”å›ä»€ä¹ˆã€‚æˆ‘ä»¬åœ¨è¿™é‡Œå¯ä»¥è®¾ç½®çš„å…¶ä»–å‚æ•°åŒ…æ‹¬æˆ‘ä»¬æ¨¡å‹çš„ `vocab` ï¼ˆæˆ‘ä»¬è¦è®­ç»ƒæ¨¡å‹ï¼Œæ‰€ä»¥æˆ‘ä»¬ä¸éœ€è¦è®¾ç½®è¿™ä¸ªï¼‰å’Œ `max_input_chars_per_word` ï¼Œå®ƒæŒ‡å®šäº†æ¯ä¸ªè¯çš„æœ€å¤§é•¿åº¦ï¼ˆæ¯” `max_input_chars_per_word` é•¿çš„è¯å°†è¢«æ‹†åˆ†ï¼‰ã€‚

åˆ†è¯çš„ç¬¬ä¸€æ­¥æ˜¯æ ‡å‡†åŒ–ï¼Œæ‰€ä»¥æˆ‘ä»¬ä»è¿™é‡Œå¼€å§‹ã€‚ç”±äº BERT è¢«å¹¿æ³›ä½¿ç”¨ï¼Œæ‰€ä»¥æœ‰ä¸€ä¸ª `BertNormalizer` ï¼Œæˆ‘ä»¬å¯ä»¥ä¸º BERT è®¾ç½®ç»å…¸é€‰é¡¹ï¼š `lowercaseï¼ˆå°å†™ï¼‰` å’Œ `strip_accentsï¼ˆå»é™¤é‡éŸ³çš„å­—ç¬¦ï¼‰` ï¼Œ `clean_text` ç”¨äºåˆ é™¤æ‰€æœ‰æ§åˆ¶å­—ç¬¦å¹¶å°†é‡å¤çš„ç©ºæ ¼æ›¿æ¢ä¸ºä¸€ä¸ªï¼›ä»¥åŠ `handle_chinese_chars` ï¼Œå®ƒå°†åœ¨ä¸­æ–‡å­—ç¬¦å‘¨å›´æ·»åŠ ç©ºæ ¼ã€‚è¦å¤ç° `bert-base-uncased` åˆ†è¯å™¨ï¼Œæˆ‘ä»¬å¯ä»¥è¿™æ ·è®¾ç½® `normalizer` ï¼š

```python
tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)
```

ç„¶è€Œï¼Œé€šå¸¸æ¥è¯´ï¼Œå½“ä½ æ„å»ºä¸€ä¸ªæ–°çš„åˆ†è¯å™¨æ—¶ï¼Œä½ ä¸ä¼šæœ‰ğŸ¤— Tokenizers åº“ä¸­å·²ç»å®ç°çš„ `normalizer` â€”â€” æ‰€ä»¥æˆ‘ä»¬æ¥çœ‹çœ‹å¦‚ä½•æ‰‹åŠ¨åˆ›å»º `BERT normalizer` ã€‚ğŸ¤— Tokenizers åº“æä¾›äº†ä¸€ä¸ª `Lowercase normalizer` å’Œä¸€ä¸ª `StripAccents normalizer` ï¼Œå¹¶ä¸”ä½ å¯ä»¥ä½¿ç”¨ Sequence æ¥ç»„åˆå¤šä¸ªæ ‡å‡†åŒ–å™¨ï¼š

```python
tokenizer.normalizer = normalizers.Sequence(
    [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()]
)
```

æˆ‘ä»¬è¿˜ä½¿ç”¨äº†ä¸€ä¸ª `NFD Unicode normalizer` ï¼Œå¦åˆ™ï¼Œå¦åˆ™ `StripAccents normalizer` å°†æ— æ³•æ­£ç¡®è¯†åˆ«å¸¦æœ‰é‡éŸ³çš„å­—ç¬¦ï¼Œå› æ­¤æ²¡åŠæ³•å»é™¤å®ƒä»¬ã€‚

æ­£å¦‚æˆ‘ä»¬ä¹‹å‰çœ‹åˆ°çš„ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `normalizer` çš„ `normalize_str()` æ–¹æ³•æ¥å¯¹å®ƒè¿›è¡Œæµ‹è¯•ï¼š

```python
print(tokenizer.normalizer.normalize_str("HÃ©llÃ² hÃ´w are Ã¼?"))
```

```python out
hello how are u?
```

<Tip>

**æ›´è¿›ä¸€æ­¥**å¦‚æœæ‚¨åœ¨åŒ…å« unicode å­—ç¬¦çš„å­—ç¬¦ä¸²ä¸Šæµ‹è¯•å…ˆå‰ normalizers çš„ä¸¤ä¸ªç‰ˆæœ¬ï¼Œæ‚¨è‚¯å®šä¼šæ³¨æ„åˆ°è¿™ä¸¤ä¸ª normalizers å¹¶ä¸å®Œå…¨ç­‰æ•ˆã€‚
ä¸ºäº†é¿å… `normalizers.Sequence` è¿‡äºå¤æ‚ï¼Œæˆ‘ä»¬çš„å®ç°æ²¡æœ‰åŒ…å«å½“ `clean_text` å‚æ•°è®¾ç½®ä¸º `True` æ—¶ `BertNormalizer` éœ€è¦çš„æ­£åˆ™è¡¨è¾¾å¼æ›¿æ¢ â€”â€” è¿™æ˜¯ `BertNormalizer` é»˜è®¤çš„ä¼šå®ç°çš„ã€‚ä½†ä¸è¦æ‹…å¿ƒï¼šé€šè¿‡åœ¨ normalizer åºåˆ—ä¸­æ·»åŠ ä¸¤ä¸ª `normalizers.Replace` å¯ä»¥åœ¨ä¸ä½¿ç”¨æ–¹ä¾¿çš„ `BertNormalizer` çš„æƒ…å†µä¸‹è·å¾—å®Œå…¨ç›¸åŒçš„æ ‡å‡†åŒ–ã€‚

</Tip>

ä¸‹ä¸€æ­¥æ˜¯é¢„åˆ†è¯ã€‚åŒæ ·ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨é¢„æ„å»ºçš„ `BertPreTokenizer` ï¼š

```python
tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()
```

æˆ–è€…æˆ‘ä»¬å¯ä»¥ä»å¤´å¼€å§‹æ„å»ºå®ƒï¼š

```python
tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()
```

æ³¨æ„ï¼Œ `Whitespace` é¢„åˆ†è¯å™¨æ ¹æ®ç©ºç™½å’Œæ‰€æœ‰ä¸æ˜¯å­—æ¯ã€æ•°å­—æˆ–ä¸‹åˆ’çº¿çš„å­—ç¬¦è¿›è¡Œåˆ†å‰²ï¼Œå› æ­¤åœ¨æœ¬æ¬¡çš„ä¾‹å­ä¸­ä¸Šä¼šæ ¹æ®ç©ºæ ¼å’Œæ ‡ç‚¹ç¬¦å·è¿›è¡Œåˆ†å‰²ï¼š

```python
tokenizer.pre_tokenizer.pre_tokenize_str("Let's test my pre-tokenizer.")
```

```python out
[('Let', (0, 3)), ("'", (3, 4)), ('s', (4, 5)), ('test', (6, 10)), ('my', (11, 13)), ('pre', (14, 17)),
 ('-', (17, 18)), ('tokenizer', (18, 27)), ('.', (27, 28))]
```

å¦‚æœä½ åªæƒ³åœ¨ç©ºç™½ä¸Šåˆ†å‰²ï¼Œåˆ™åº”è¯¥ä½¿ç”¨ `WhitespaceSplit` é¢„åˆ†è¯å™¨ï¼š

```python
pre_tokenizer = pre_tokenizers.WhitespaceSplit()
pre_tokenizer.pre_tokenize_str("Let's test my pre-tokenizer.")
```

```python out
[("Let's", (0, 5)), ('test', (6, 10)), ('my', (11, 13)), ('pre-tokenizer.', (14, 28))]
```

å°±åƒæ ‡å‡†åŒ–å™¨ä¸€æ ·ï¼Œä½ å¯ä»¥ä½¿ç”¨ `Sequence` æ¥ç»„åˆå‡ ä¸ªé¢„åˆ†è¯å™¨ï¼š

```python
pre_tokenizer = pre_tokenizers.Sequence(
    [pre_tokenizers.WhitespaceSplit(), pre_tokenizers.Punctuation()]
)
pre_tokenizer.pre_tokenize_str("Let's test my pre-tokenizer.")
```

```python out
[('Let', (0, 3)), ("'", (3, 4)), ('s', (4, 5)), ('test', (6, 10)), ('my', (11, 13)), ('pre', (14, 17)),
 ('-', (17, 18)), ('tokenizer', (18, 27)), ('.', (27, 28))]
```

è¯å…ƒåŒ–æµç¨‹çš„ä¸‹ä¸€æ­¥æ˜¯å°†è¾“å…¥æ•°æ®ä¼ é€’ç»™æ¨¡å‹ã€‚æˆ‘ä»¬å·²ç»åœ¨åˆå§‹åŒ–æ—¶æŒ‡å®šäº†æˆ‘ä»¬çš„æ¨¡å‹ï¼Œä½†æ˜¯æˆ‘ä»¬è¿˜éœ€è¦å¯¹å…¶è¿›è¡Œè®­ç»ƒï¼Œè¿™å°±éœ€è¦ä¸€ä¸ª `WordPieceTrainer` ã€‚åœ¨å®ä¾‹åŒ–ä¸€ä¸ªğŸ¤— Tokenizers ä¸­çš„è®­ç»ƒå™¨æ—¶ï¼Œä¸€ä»¶å¾ˆé‡è¦çš„äº‹æƒ…æ˜¯ï¼Œä½ éœ€è¦å°†ä½ æ‰“ç®—ä½¿ç”¨çš„æ‰€æœ‰ç‰¹æ®Šè¯å…ƒéƒ½ä¼ é€’ç»™å®ƒâ€”â€”å¦åˆ™ï¼Œç”±äºå®ƒä»¬ä¸åœ¨è®­ç»ƒè¯­æ–™åº“ä¸­ï¼Œè®­ç»ƒå™¨å°±ä¸ä¼šå°†å®ƒä»¬æ·»åŠ åˆ°è¯æ±‡è¡¨ä¸­ï¼š

```python
special_tokens = ["[UNK]", "[PAD]", "[CLS]", "[SEP]", "[MASK]"]
trainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens)
```

é™¤äº†æŒ‡å®š `vocab_size` å’Œ `special_tokens` ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥è®¾ç½® `min_frequency` ï¼ˆä¸€ä¸ªè¯å…ƒå¿…é¡»è¾¾åˆ°çš„æœ€å°çš„å‡ºç°çš„æ¬¡æ•°æ‰èƒ½è¢«åŒ…å«åœ¨è¯æ±‡è¡¨ä¸­ï¼‰æˆ–æ›´æ”¹ `continuing_subword_prefix` ï¼ˆå¦‚æœæˆ‘ä»¬æƒ³ä½¿ç”¨å…¶ä»–çš„å­—ç¬¦æ¥æ›¿ä»£ `##` ï¼‰ã€‚

è¦ä½¿ç”¨æˆ‘ä»¬ä¹‹å‰å®šä¹‰çš„è¿­ä»£å™¨è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ï¼Œæˆ‘ä»¬åªéœ€è¦æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

```python
tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)
```

æˆ‘ä»¬è¿˜å¯ä»¥ä½¿ç”¨æ–‡æœ¬æ–‡ä»¶æ¥è®­ç»ƒæˆ‘ä»¬çš„è¯å…ƒåˆ†æå™¨ï¼Œå®ƒçœ‹èµ·æ¥åƒè¿™æ ·ï¼ˆæˆ‘ä»¬éœ€è¦å…ˆä½¿ç”¨ `WordPiece` åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„æ¨¡å‹ï¼‰ï¼š

```python
tokenizer.model = models.WordPiece(unk_token="[UNK]")
tokenizer.train(["wikitext-2.txt"], trainer=trainer)
```

åœ¨è¿™ä¸¤ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬éƒ½å¯ä»¥é€šè¿‡è°ƒç”¨ `encode()` æ–¹æ³•æ¥æµ‹è¯•è¯å…ƒåˆ†æå™¨ 

```python
encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)
```

```python out
['let', "'", 's', 'test', 'this', 'tok', '##eni', '##zer', '.']
```

æ‰€å¾—åˆ°çš„ `encoding` æ˜¯ä¸€ä¸ª `Encoding` å¯¹è±¡ï¼Œå®ƒåŒ…å«è¯å…ƒåˆ†æå™¨çš„æ‰€æœ‰å¿…è¦å±æ€§ï¼š `ids` ã€ `type_ids` ã€ `tokens` ã€ `offsets` ã€ `attention_mask` ã€ `special_tokens_mask` å’Œ `overflowing` ã€‚

è¯å…ƒåˆ†æå™¨ç®¡é“çš„æœ€åä¸€æ­¥æ˜¯åå¤„ç†ã€‚æˆ‘ä»¬éœ€è¦åœ¨å¼€å¤´æ·»åŠ  `[CLS]` è¯å…ƒï¼Œç„¶ååœ¨ç»“æŸæ—¶ï¼ˆæˆ–åœ¨æ¯å¥è¯åï¼Œå¦‚æœæˆ‘ä»¬æœ‰ä¸€å¯¹å¥å­ï¼‰æ·»åŠ  `[SEP]` è¯å…ƒã€‚æˆ‘ä»¬å°†ä½¿ç”¨ `TemplateProcessor` æ¥å®Œæˆè¿™ä¸ªä»»åŠ¡ï¼Œä½†é¦–å…ˆæˆ‘ä»¬éœ€è¦çŸ¥é“è¯æ±‡è¡¨ä¸­ `[CLS]` å’Œ `[SEP]` è¯å…ƒçš„ IDï¼š

```python
cls_token_id = tokenizer.token_to_id("[CLS]")
sep_token_id = tokenizer.token_to_id("[SEP]")
print(cls_token_id, sep_token_id)
```

```python out
(2, 3)
```

ç¼–å†™ `TemplateProcessor` çš„æ¨¡æ¿æ—¶ï¼Œæˆ‘ä»¬å¿…é¡»æŒ‡å®šå¦‚ä½•å¤„ç†å•ä¸ªå¥å­å’Œä¸€å¯¹å¥å­ã€‚å¯¹äºè¿™ä¸¤è€…ï¼Œæˆ‘ä»¬å†™ä¸‹æˆ‘ä»¬æƒ³ä½¿ç”¨çš„ç‰¹æ®Šè¯å…ƒï¼›ç¬¬ä¸€å¥ï¼ˆæˆ–å•å¥ï¼‰ç”¨ `$A` è¡¨ç¤ºï¼Œè€Œç¬¬äºŒå¥ï¼ˆå¦‚æœç¼–ç ä¸€å¯¹å¥å­ï¼‰ç”¨ `$B` è¡¨ç¤ºã€‚å¯¹äºè¿™äº›ï¼ˆç‰¹æ®Šè¯å…ƒå’Œå¥å­ï¼‰ï¼Œæˆ‘ä»¬è¿˜éœ€è¦åœ¨å†’å·åæŒ‡å®šç›¸åº”çš„è¯å…ƒç±»å‹ IDã€‚

å› æ­¤ï¼Œç»å…¸çš„ BERT æ¨¡æ¿å®šä¹‰å¦‚ä¸‹ï¼š

```python
tokenizer.post_processor = processors.TemplateProcessing(
    single=f"[CLS]:0 $A:0 [SEP]:0",
    pair=f"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1",
    special_tokens=[("[CLS]", cls_token_id), ("[SEP]", sep_token_id)],
)
```

è¯·æ³¨æ„ï¼Œæˆ‘ä»¬éœ€è¦ä¼ é€’ç‰¹æ®Šè¯å…ƒçš„ IDï¼Œè¿™æ ·è¯å…ƒåˆ†æå™¨æ‰èƒ½æ­£ç¡®åœ°å°†å®ƒä»¬è½¬æ¢ä¸ºå®ƒä»¬çš„ IDã€‚

æ·»åŠ ä¹‹åï¼Œæˆ‘ä»¬å†æ¬¡å¯¹ä¹‹å‰çš„ä¾‹å­è¿›è¡Œè¯å…ƒåŒ–ï¼š

```python
encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)
```

```python out
['[CLS]', 'let', "'", 's', 'test', 'this', 'tok', '##eni', '##zer', '.', '[SEP]']
```

åœ¨ä¸€å¯¹å¥å­ä¸­ï¼Œæˆ‘ä»¬ä¹Ÿå¾—åˆ°äº†æ­£ç¡®çš„ç»“æœï¼š

```python
encoding = tokenizer.encode("Let's test this tokenizer...", "on a pair of sentences.")
print(encoding.tokens)
print(encoding.type_ids)
```

```python out
['[CLS]', 'let', "'", 's', 'test', 'this', 'tok', '##eni', '##zer', '...', '[SEP]', 'on', 'a', 'pair', 'of', 'sentences', '.', '[SEP]']
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]
```

æˆ‘ä»¬å‡ ä¹ä»å¤´å¼€å§‹æ„å»ºäº†è¿™ä¸ªè¯å…ƒåˆ†æå™¨â€”â€”ä½†æ˜¯è¿˜æœ‰æœ€åä¸€æ­¥ï¼šæŒ‡å®šä¸€ä¸ªè§£ç å™¨ï¼š

```python
tokenizer.decoder = decoders.WordPiece(prefix="##")
```

è®©æˆ‘ä»¬åœ¨ä¹‹å‰çš„ `encoding` ä¸Šæµ‹è¯•ä¸€ä¸‹å®ƒï¼š

```python
tokenizer.decode(encoding.ids)
```

```python out
"let's test this tokenizer... on a pair of sentences."
```

å¾ˆå¥½ï¼æˆ‘ä»¬å¯ä»¥å°†è¯å…ƒåˆ†æå™¨ä¿å­˜åœ¨ä¸€ä¸ª JSON æ–‡ä»¶ä¸­ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```python
tokenizer.save("tokenizer.json")
```

ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ä¸€ä¸ª `Tokenizer` å¯¹è±¡ä¸­ä½¿ç”¨ `from_file()` æ–¹æ³•é‡æ–°åŠ è½½è¯¥æ–‡ä»¶ï¼š

```python
new_tokenizer = Tokenizer.from_file("tokenizer.json")
```

è¦åœ¨ğŸ¤— Transformers ä¸­ä½¿ç”¨è¿™ä¸ªè¯å…ƒåˆ†æå™¨ï¼Œæˆ‘ä»¬éœ€è¦å°†å®ƒå°è£…åœ¨ä¸€ä¸ª `PreTrainedTokenizerFast` ç±»ä¸­ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨é€šç”¨ç±»ï¼Œæˆ–è€…ï¼Œå¦‚æœæˆ‘ä»¬çš„è¯å…ƒåˆ†æå™¨å¯¹åº”äºä¸€ä¸ªç°æœ‰çš„æ¨¡å‹ï¼Œåˆ™å¯ä»¥ä½¿ç”¨è¯¥ç±»ï¼ˆä¾‹å¦‚è¿™é‡Œçš„ `BertTokenizerFast` ï¼‰ã€‚å¦‚æœä½ ä½¿ç”¨è¿™ä¸ªè¯¾ç¨‹æ¥æ„å»ºä¸€ä¸ªå…¨æ–°çš„è¯å…ƒåˆ†æå™¨ï¼Œåˆ™å¿…é¡»éœ€è¦ä½¿ç”¨é€šç±»ã€‚

è¦å°†æ„å»ºçš„åˆ†è¯å™¨å°è£…åœ¨ `PreTrainedTokenizerFast` ç±»ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥å°†æˆ‘ä»¬æ„å»ºçš„è¯å…ƒåˆ†æå™¨ä½œä¸º `tokenizer_object` ä¼ å…¥ï¼Œæˆ–è€…å°†æˆ‘ä»¬ä¿å­˜çš„åˆ†è¯å™¨æ–‡ä»¶ä½œä¸º `tokenizer_file` ä¼ å…¥ã€‚è¦è®°ä½çš„å…³é”®ä¸€ç‚¹æ˜¯ï¼Œæˆ‘ä»¬éœ€è¦æ‰‹åŠ¨è®¾ç½®æ‰€æœ‰çš„ç‰¹æ®Šè¯å…ƒï¼Œå› ä¸ºè¿™ä¸ªç±»ä¸èƒ½ä» `tokenizer` å¯¹è±¡æ¨æ–­å‡ºå“ªä¸ªç¬¦å·æ˜¯æ©ç ç¬¦å·ï¼Œ `[CLS]` ç¬¦å·ç­‰ï¼š


```python
from transformers import PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    # tokenizer_file="tokenizer.json", # ä¹Ÿå¯ä»¥ä»tokenizeræ–‡ä»¶ä¸­åŠ è½½
    unk_token="[UNK]",
    pad_token="[PAD]",
    cls_token="[CLS]",
    sep_token="[SEP]",
    mask_token="[MASK]",
)
```

å¦‚æœä½ ä½¿ç”¨çš„æ˜¯å…¶ä»–çš„è¯å…ƒåˆ†æå™¨ç±»ï¼ˆå¦‚ `BertTokenizerFast` ï¼‰ï¼Œä½ åªéœ€è¦æŒ‡å®šé‚£äº›ä¸é»˜è®¤å€¼ä¸åŒçš„ç‰¹æ®Šç¬¦å·ï¼ˆè¿™é‡Œæ²¡æœ‰ï¼‰ï¼š

```python
from transformers import BertTokenizerFast

wrapped_tokenizer = BertTokenizerFast(tokenizer_object=tokenizer)
```

ç„¶åï¼Œä½ å°±å¯ä»¥åƒä½¿ç”¨å…¶ä»–çš„ğŸ¤— Transformers è¯å…ƒåˆ†æå™¨ä¸€æ ·ä½¿ç”¨è¿™ä¸ªè¯å…ƒåˆ†æå™¨äº†ã€‚ä½ å¯ä»¥ä½¿ç”¨ `save_pretrained()` æ–¹æ³•æ¥ä¿å­˜å®ƒï¼Œæˆ–è€…ä½¿ç”¨ `push_to_hub()` æ–¹æ³•å°†å®ƒä¸Šä¼ åˆ° Hubã€‚

æ—¢ç„¶æˆ‘ä»¬å·²ç»çœ‹åˆ°äº†å¦‚ä½•æ„å»ºä¸€ä¸ª WordPiece åˆ†è¯å™¨ï¼Œé‚£ä¹ˆè®©æˆ‘ä»¬å¯¹ BPE åˆ†è¯å™¨åšåŒæ ·çš„äº‹æƒ…ã€‚è¿™æ¬¡æˆ‘ä»¬ä¼šå¿«ä¸€äº›ï¼Œå› ä¸ºä½ å·²ç»çŸ¥é“æ‰€æœ‰çš„æ­¥éª¤ï¼Œæˆ‘ä»¬ä¸»è¦å¼ºè°ƒå…¶ä¸­çš„åŒºåˆ«ã€‚

## ä»å¤´å¼€å§‹æ„å»º BPE è¯å…ƒåˆ†æå™¨ [[ä»å¤´å¼€å§‹æ„å»º BPE è¯å…ƒåˆ†æå™¨]]

ç°åœ¨è®©æˆ‘ä»¬æ„å»ºä¸€ä¸ª GPT-2 è¯å…ƒåˆ†æå™¨ã€‚ä¸ BERT è¯å…ƒåˆ†æå™¨ä¸€æ ·ï¼Œæˆ‘ä»¬é¦–å…ˆé€šè¿‡ BPE model åˆå§‹åŒ–ä¸€ä¸ª `Tokenizer` ï¼š

```python
tokenizer = Tokenizer(models.BPE())
```

åŒæ ·ï¼Œå¯¹äº BERTï¼Œå¦‚æœæˆ‘ä»¬æœ‰ä¸€ä¸ªè¯æ±‡è¡¨ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥ä½¿ç”¨è¿™ä¸ªè¯æ±‡è¡¨æ¥åˆå§‹åŒ–æ¨¡å‹ï¼ˆåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬éœ€è¦ä¼ å…¥ `vocab` å’Œ `merges` ï¼‰ï¼Œä½†æ˜¯å› ä¸ºæˆ‘ä»¬å°†ä»å¤´å¼€å§‹è®­ç»ƒï¼Œæ‰€ä»¥æˆ‘ä»¬ä¸éœ€è¦åšè¿™ä¸ªã€‚æˆ‘ä»¬ä¹Ÿä¸éœ€è¦æŒ‡å®š `unk_token` ï¼Œå› ä¸º GPT-2 ä½¿ç”¨å­—èŠ‚çº§ BPEï¼Œè¿™ä¸éœ€è¦å®ƒã€‚

GPT-2 ä¸ä½¿ç”¨ `normalizer` ï¼Œå› æ­¤æˆ‘ä»¬è·³è¿‡è¯¥æ­¥éª¤å¹¶ç›´æ¥è¿›å…¥é¢„åˆ†è¯ï¼š

```python
tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)
```
æˆ‘ä»¬åœ¨è¿™é‡Œç»™ `ByteLevel` æ·»åŠ çš„é€‰é¡¹æ˜¯ä¸åœ¨å¥å­çš„å¼€å§‹æ·»åŠ ç©ºæ ¼ï¼ˆé»˜è®¤ä¸º tureï¼‰ã€‚æˆ‘ä»¬å¯ä»¥çœ‹ä¸€ä¸‹ä¹‹å‰çš„ç¤ºä¾‹æ–‡æœ¬çš„é¢„åˆ†è¯ï¼š

```python
tokenizer.pre_tokenizer.pre_tokenize_str("Let's test pre-tokenization!")
```

```python out
[('Let', (0, 3)), ("'s", (3, 5)), ('Ä test', (5, 10)), ('Ä pre', (10, 14)), ('-', (14, 15)),
 ('tokenization', (15, 27)), ('!', (27, 28))]
```

æ¥ä¸‹æ¥æ˜¯éœ€è¦è®­ç»ƒçš„æ¨¡å‹ã€‚å¯¹äº GPT-2ï¼Œå”¯ä¸€çš„ç‰¹æ®Šç¬¦å·æ˜¯æ–‡æœ¬ç»“æŸç¬¦ï¼š

```python
trainer = trainers.BpeTrainer(vocab_size=25000, special_tokens=["<|endoftext|>"])
tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)
```

å°±åƒ `WordPieceTrainer` ä¸€æ ·ï¼Œé™¤äº† `vocab_size` å’Œ `special_tokens` ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥æŒ‡å®š `min_frequency` ï¼Œæˆ–è€…å¦‚æœæˆ‘ä»¬éœ€è¦æ·»åŠ ä¸€ä¸ªè¯å°¾åç¼€ï¼ˆå¦‚ `</w>` ï¼‰ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ `end_of_word_suffix` è®¾ç½®å®ƒã€‚

è¿™ä¸ªè¯å…ƒåˆ†æå™¨ä¹Ÿå¯ä»¥åœ¨æ–‡æœ¬æ–‡ä»¶ä¸Šè®­ç»ƒï¼š

```python
tokenizer.model = models.BPE()
tokenizer.train(["wikitext-2.txt"], trainer=trainer)
```

è®©æˆ‘ä»¬çœ‹ä¸€ä¸‹ç¤ºä¾‹æ–‡æœ¬çš„è¯å…ƒåŒ–åçš„ç»“æœï¼š

```python
encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)
```

```python out
['L', 'et', "'", 's', 'Ä test', 'Ä this', 'Ä to', 'ken', 'izer', '.']
```

æˆ‘ä»¬å¯¹ GPT-2 è¯å…ƒåˆ†æå™¨æ·»åŠ å­—èŠ‚çº§åå¤„ç†ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```python
tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)
```
 `trim_offsets = False` è¿™ä¸ªé€‰é¡¹å‘Šè¯‰ post-processorï¼Œæˆ‘ä»¬åº”è¯¥è®©é‚£äº›ä»¥'Ä 'å¼€å§‹çš„è¯å…ƒçš„åç§»é‡ä¿æŒä¸å˜ï¼šè¿™æ ·ï¼Œåç§»é‡çš„å¼€å§‹å°†æŒ‡å‘å•è¯å‰çš„ç©ºæ ¼ï¼Œè€Œä¸æ˜¯å•è¯çš„ç¬¬ä¸€ä¸ªå­—ç¬¦ï¼ˆå› ä¸ºç©ºæ ¼åœ¨æŠ€æœ¯ä¸Šæ˜¯è¯å…ƒçš„ä¸€éƒ¨åˆ†ï¼‰ã€‚è®©æˆ‘ä»¬çœ‹ä¸€ä¸‹æˆ‘ä»¬åˆšåˆšç¼–ç çš„æ–‡æœ¬çš„ç»“æœï¼Œå…¶ä¸­ `'Ä test'` æ˜¯ç´¢å¼• 4 çš„è¯å…ƒï¼š

```python
sentence = "Let's test this tokenizer."
encoding = tokenizer.encode(sentence)
start, end = encoding.offsets[4]
sentence[start:end]
```

```python out
' test'
```

æœ€åï¼Œæˆ‘ä»¬æ·»åŠ ä¸€ä¸ªå­—èŠ‚çº§è§£ç å™¨ï¼š

```python
tokenizer.decoder = decoders.ByteLevel()
```

æˆ‘ä»¬å¯ä»¥å†æ¬¡æ£€æŸ¥å®ƒæ˜¯å¦å·¥ä½œæ­£å¸¸ï¼š

```python
tokenizer.decode(encoding.ids)
```

```python out
"Let's test this tokenizer."
```

å¤ªå¥½äº†ï¼ç°åœ¨æˆ‘ä»¬å®Œæˆäº†ï¼Œæˆ‘ä»¬å¯ä»¥åƒä¹‹å‰ä¸€æ ·ä¿å­˜è¯å…ƒåˆ†æå™¨ï¼Œå¹¶ä¸”å¦‚æœæˆ‘ä»¬æƒ³åœ¨ğŸ¤— Transformers ä¸­ä½¿ç”¨å®ƒï¼Œå¯ä»¥å°†å®ƒå°è£…åœ¨ `PreTrainedTokenizerFast` ç±»æˆ–è€… `GPT2TokenizerFast` ç±»ä¸­ï¼š

```python
from transformers import PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    bos_token="<|endoftext|>",
    eos_token="<|endoftext|>",
)
```

æˆ–è€…ï¼š

```python
from transformers import GPT2TokenizerFast

wrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer)
```

ä½œä¸ºæœ€åä¸€ä¸ªç¤ºä¾‹ï¼Œæˆ‘ä»¬å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ä»é›¶å¼€å§‹æ„å»º Unigram è¯å…ƒåˆ†æå™¨ã€‚

## ä»é›¶å¼€å§‹æ„å»º Unigram è¯å…ƒåˆ†æå™¨ [[ä»é›¶å¼€å§‹æ„å»º Unigram è¯å…ƒåˆ†æå™¨]]

ç°åœ¨è®©æˆ‘ä»¬æ„å»ºä¸€ä¸ª XLNet è¯å…ƒåˆ†æå™¨ã€‚ä¸ä¹‹å‰çš„è¯å…ƒåˆ†æå™¨ä¸€æ ·ï¼Œæˆ‘ä»¬é¦–å…ˆä½¿ç”¨ Unigram model åˆå§‹åŒ–ä¸€ä¸ª `Tokenizer` ï¼š

```python
tokenizer = Tokenizer(models.Unigram())
```

åŒæ ·ï¼Œå¦‚æœæˆ‘ä»¬æœ‰è¯æ±‡è¡¨ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨è¯æ±‡è¡¨åˆå§‹åŒ–è¿™ä¸ªæ¨¡å‹ã€‚

å¯¹äºæ ‡å‡†åŒ–ï¼ŒXLNet è¿›è¡Œäº†ä¸€äº›æ›¿æ¢ï¼ˆæ¥è‡ª SentencePieceï¼‰ï¼š

```python
from tokenizers import Regex

tokenizer.normalizer = normalizers.Sequence(
    [
        normalizers.Replace("``", '"'),
        normalizers.Replace("''", '"'),
        normalizers.NFKD(),
        normalizers.StripAccents(),
        normalizers.Replace(Regex(" {2,}"), " "),
    ]
)
```

è¿™ä¼šå°†<code>``</code>å’Œ<code>''</code>æ›¿æ¢ä¸º<code>"</code>ï¼Œå°†ä»»ä½•è¿ç»­ä¸¤ä¸ªæˆ–æ›´å¤šçš„ç©ºæ ¼æ›¿æ¢ä¸ºä¸€ä¸ªç©ºæ ¼ï¼ŒåŒæ—¶è¿˜å°†å»æ‰å¾…åˆ†è¯æ–‡æœ¬ä¸­çš„é‡éŸ³ã€‚

ä»»ä½• SentencePiece åˆ†è¯å™¨ä½¿ç”¨çš„é¢„åˆ†è¯å™¨æ˜¯ `Metaspace` ï¼š

```python
tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()
```

æˆ‘ä»¬å¯ä»¥åƒä»¥å‰ä¸€æ ·æŸ¥çœ‹ç¤ºä¾‹æ–‡æœ¬çš„é¢„åˆ†è¯ï¼š

```python
tokenizer.pre_tokenizer.pre_tokenize_str("Let's test the pre-tokenizer!")
```

```python out
[("â–Let's", (0, 5)), ('â–test', (5, 10)), ('â–the', (10, 14)), ('â–pre-tokenizer!', (14, 29))]
```

æ¥ä¸‹æ¥æ˜¯éœ€è¦è®­ç»ƒçš„æ¨¡å‹ã€‚XLNet æœ‰ä¸å°‘ç‰¹æ®Šçš„è¯å…ƒï¼š

```python
special_tokens = ["<cls>", "<sep>", "<unk>", "<pad>", "<mask>", "<s>", "</s>"]
trainer = trainers.UnigramTrainer(
    vocab_size=25000, special_tokens=special_tokens, unk_token="<unk>"
)
tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)
```

å¯¹äº `UnigramTrainer` æ¥è¯´ï¼Œä¸€ä¸ªéå¸¸é‡è¦çš„å‚æ•°æ˜¯ `unk_token` ã€‚æˆ‘ä»¬ä¹Ÿå¯ä»¥ä¼ é€’ä¸€äº› Unigram ç®—æ³•ç‹¬æœ‰çš„å…¶ä»–å‚æ•°ï¼Œä¾‹å¦‚æˆ‘ä»¬åœ¨æ¯ä¸ªåˆ é™¤è¯å…ƒæ—¶çš„ `shrinking_factor` ï¼ˆé»˜è®¤ä¸º 0.75ï¼‰ï¼Œæˆ–è€…æŒ‡å®šè¯å…ƒçš„ `max_piece_length` æœ€å¤§é•¿åº¦ï¼ˆé»˜è®¤ä¸º 16ï¼‰ã€‚

è¿™ä¸ªè¯å…ƒåˆ†æå™¨ä¹Ÿå¯ä»¥åœ¨æ–‡æœ¬æ–‡ä»¶ä¸Šè®­ç»ƒï¼š

```python
tokenizer.model = models.Unigram()
tokenizer.train(["wikitext-2.txt"], trainer=trainer)
```

è®©æˆ‘ä»¬çœ‹ä¸€ä¸‹ç¤ºä¾‹æ–‡æœ¬çš„è¯å…ƒåŒ–åçš„ç»“æœï¼š

```python
encoding = tokenizer.encode("Let's test this tokenizer.")
print(encoding.tokens)
```

```python out
['â–Let', "'", 's', 'â–test', 'â–this', 'â–to', 'ken', 'izer', '.']
```

XLNet çš„ä¸€ä¸ªç‰¹ç‚¹æ˜¯å®ƒå°† `<cls>` è¯å…ƒæ”¾åœ¨å¥å­çš„æœ«å°¾ï¼Œè¯å…ƒç±»å‹ ID ä¸º 2ï¼ˆä»¥åŒºåˆ«äºå…¶ä»–è¯å…ƒï¼‰ã€‚å› æ­¤ï¼Œå®ƒåœ¨å·¦è¾¹å¡«å……ã€‚æˆ‘ä»¬å¯ä»¥åƒå¯¹å¾… BERT ä¸€æ ·ï¼Œç”¨æ¨¡æ¿å¤„ç†æ‰€æœ‰ç‰¹æ®Šè¯å…ƒå’Œè¯å…ƒç±»å‹ IDï¼Œä½†é¦–å…ˆæˆ‘ä»¬éœ€è¦è·å– `<cls>` å’Œ `<sep>` è¯å…ƒçš„ IDï¼š

```python
cls_token_id = tokenizer.token_to_id("<cls>")
sep_token_id = tokenizer.token_to_id("<sep>")
print(cls_token_id, sep_token_id)
```

```python out
0 1
```

æ¨¡æ¿å¦‚ä¸‹æ‰€ç¤ºï¼š

```python
tokenizer.post_processor = processors.TemplateProcessing(
    single="$A:0 <sep>:0 <cls>:2",
    pair="$A:0 <sep>:0 $B:1 <sep>:1 <cls>:2",
    special_tokens=[("<sep>", sep_token_id), ("<cls>", cls_token_id)],
)
```

æˆ‘ä»¬å¯ä»¥é€šè¿‡ç¼–ç ä¸€å¯¹å¥å­æ¥æµ‹è¯•å®ƒæ˜¯å¦æœ‰æ•ˆï¼š

```python
encoding = tokenizer.encode("Let's test this tokenizer...", "on a pair of sentences!")
print(encoding.tokens)
print(encoding.type_ids)
```

```python out
['â–Let', "'", 's', 'â–test', 'â–this', 'â–to', 'ken', 'izer', '.', '.', '.', '<sep>', 'â–', 'on', 'â–', 'a', 'â–pair', 
  'â–of', 'â–sentence', 's', '!', '<sep>', '<cls>']
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2]
```

æœ€åï¼Œæˆ‘ä»¬æ·»åŠ ä¸€ä¸ª `Metaspace` è§£ç å™¨ï¼š

```python
tokenizer.decoder = decoders.Metaspace()
```

æˆ‘ä»¬å®Œæˆäº†è¿™ä¸ªè¯å…ƒåˆ†æå™¨ï¼æˆ‘ä»¬å¯ä»¥åƒä»¥å‰ä¸€æ ·ä¿å­˜è¯å…ƒåˆ†æå™¨ï¼Œå¦‚æœæˆ‘ä»¬æƒ³åœ¨ ğŸ¤— Transformers ä¸­ä½¿ç”¨å®ƒï¼Œå¯ä»¥å°†å®ƒå°è£…åœ¨ `PreTrainedTokenizerFast` ç±»æˆ– `XLNetTokenizerFast` ç±»ä¸­ã€‚ä½¿ç”¨ `PreTrainedTokenizerFast` ç±»æ—¶éœ€è¦æ³¨æ„çš„ä¸€ç‚¹æ˜¯ï¼Œé™¤äº†ç‰¹æ®Šè¯å…ƒä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜éœ€è¦å‘Šè¯‰ğŸ¤— Transformers åº“åœ¨å·¦è¾¹å¡«å……ï¼š
```python
from transformers import PreTrainedTokenizerFast

wrapped_tokenizer = PreTrainedTokenizerFast(
    tokenizer_object=tokenizer,
    bos_token="<s>",
    eos_token="</s>",
    unk_token="<unk>",
    pad_token="<pad>",
    cls_token="<cls>",
    sep_token="<sep>",
    mask_token="<mask>",
    padding_side="left",
)
```

æˆ–è€…ï¼š

```python
from transformers import XLNetTokenizerFast

wrapped_tokenizer = XLNetTokenizerFast(tokenizer_object=tokenizer)
```

ç°åœ¨æ‚¨å·²ç»äº†è§£äº†å¦‚ä½•ä½¿ç”¨å„ç§æ¨¡å—æ¥æ„å»ºç°æœ‰çš„è¯å…ƒåˆ†æå™¨ï¼Œæ‚¨åº”è¯¥èƒ½å¤Ÿä½¿ç”¨ ğŸ¤— tokenizer åº“ç¼–å†™æ‚¨æƒ³è¦çš„ä»»ä½•è¯å…ƒåˆ†æå™¨ï¼Œå¹¶èƒ½å¤Ÿåœ¨ ğŸ¤— Transformers ä¸­ä½¿ç”¨å®ƒã€‚