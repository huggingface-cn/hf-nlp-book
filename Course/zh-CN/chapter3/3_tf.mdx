<FrameworkSwitchCourse {fw} />

# ä½¿ç”¨ Keras å¾®è°ƒä¸€ä¸ªæ¨¡å‹ [[ä½¿ç”¨ Keras å¾®è°ƒä¸€ä¸ªæ¨¡å‹]]

<CourseFloatingBanner chapter={3}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/zh-CN/chapter3/section3_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/zh-CN/chapter3/section3_tf.ipynb"},
]} />

ä¸€æ—¦å®Œæˆä¸Šä¸€èŠ‚ä¸­çš„æ‰€æœ‰æ•°æ®é¢„å¤„ç†å·¥ä½œåï¼Œä½ åªå‰©ä¸‹æœ€åçš„å‡ ä¸ªæ­¥éª¤æ¥è®­ç»ƒæ¨¡å‹ã€‚ ä½†æ˜¯è¯·æ³¨æ„ï¼Œ`model.fit()` å‘½ä»¤åœ¨ CPU ä¸Šè¿è¡Œä¼šéå¸¸ç¼“æ…¢ã€‚ å¦‚æœä½ æ²¡æœ‰GPUï¼Œä½ å¯ä»¥åœ¨ [Google Colab](https://colab.research.google.com) ä¸Šä½¿ç”¨å…è´¹çš„ GPU æˆ– TPU(éœ€è¦æ¢¯å­)ã€‚

ä¸‹é¢çš„ä»£ç ç¤ºä¾‹å‡è®¾ä½ å·²ç»æ‰§è¡Œäº†ä¸Šä¸€èŠ‚ä¸­çš„ä»£ç ç¤ºä¾‹ã€‚ ä¸‹é¢ä¸€ä¸ªç®€çŸ­çš„æ‘˜è¦ï¼ŒåŒ…å«äº†åœ¨å¼€å§‹å­¦ä¹ è¿™ä¸€èŠ‚ä¹‹å‰ä½ éœ€è¦æ‰§è¡Œçš„ä»£ç ï¼š

```py
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding
import numpy as np

raw_datasets = load_dataset("glue", "mrpc")
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)


def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)


tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)

data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="tf")

tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)

tf_validation_dataset = tokenized_datasets["validation"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=False,
    collate_fn=data_collator,
    batch_size=8,
)
```

## è®­ç»ƒæ¨¡å‹ [[è®­ç»ƒæ¨¡å‹]]

ä»ğŸ¤— Transformers å¯¼å…¥çš„ TensorFlow æ¨¡å‹å·²ç»æ˜¯ Keras æ¨¡å‹ã€‚ ä¸‹é¢çš„è§†é¢‘æ˜¯å¯¹ Keras çš„ç®€çŸ­ä»‹ç»ã€‚

<Youtube id="rnTGBy2ax1c"/>

è¿™æ„å‘³ç€ä¸€æ—¦æˆ‘ä»¬æœ‰äº†æ•°æ®ï¼Œåªéœ€è¦å¾ˆå°‘çš„å·¥ä½œå°±å¯ä»¥å¼€å§‹å¯¹å…¶è¿›è¡Œè®­ç»ƒã€‚

<Youtube id="AUozVp78dhk"/>

å’Œ[ç¬¬äºŒç« ](/course/chapter2)ä½¿ç”¨çš„æ–¹æ³•ä¸€æ ·, æˆ‘ä»¬å°†ä½¿ç”¨äºŒåˆ†ç±»çš„ `TFAutoModelForSequenceClassification`ç±»ï¼Œæœ‰ä¸¤ä¸ªæ ‡ç­¾: 

```py
from transformers import TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
```

ä½ ä¼šæ³¨æ„åˆ°ï¼Œä¸ [ç¬¬äºŒç« ](/course/chapter2) ä¸åŒçš„æ˜¯ï¼Œåœ¨å®ä¾‹åŒ–è¿™ä¸ªé¢„è®­ç»ƒçš„æ¨¡å‹åä¼šæ”¶åˆ°è­¦å‘Šã€‚ è¿™æ˜¯å› ä¸º BERT æ²¡æœ‰å¯¹å¥å­å¯¹çš„åˆ†ç±»è¿›è¡Œé¢„è®­ç»ƒï¼Œæ‰€ä»¥é¢„è®­ç»ƒæ¨¡å‹çš„ head å·²ç»è¢«ä¸¢å¼ƒï¼Œè€Œæ’å…¥äº†ä¸€ä¸ªé€‚åˆåºåˆ—åˆ†ç±»çš„æ–° headã€‚ è­¦å‘Šè¡¨æ˜ï¼Œè¿™äº›æƒé‡æ²¡æœ‰ä½¿ç”¨ï¼ˆå¯¹åº”äºä¸¢å¼ƒçš„é¢„è®­ç»ƒheadæƒé‡ï¼‰ï¼Œè€Œå…¶ä»–ä¸€äº›æƒé‡æ˜¯éšæœºåˆå§‹åŒ–çš„ï¼ˆå¯¹å¯¹åº”äºæ–°headçš„æƒé‡ï¼‰ã€‚ æœ€åå®ƒé¼“åŠ±ä½ è®­ç»ƒæ¨¡å‹ï¼Œè¿™æ­£æ˜¯æˆ‘ä»¬ç°åœ¨è¦åšçš„ã€‚

ä¸ºäº†åœ¨æˆ‘ä»¬çš„æ•°æ®é›†ä¸Šå¾®è°ƒæ¨¡å‹ï¼Œæˆ‘ä»¬åªéœ€è¦åœ¨æˆ‘ä»¬çš„æ¨¡å‹ä¸Šè°ƒç”¨ `compile()` æ–¹æ³•ï¼Œç„¶åå°†æˆ‘ä»¬çš„æ•°æ®ä¼ é€’ç»™ `fit()` æ–¹æ³•ã€‚ è¿™å°†å¯åŠ¨å¾®è°ƒè¿‡ç¨‹ï¼ˆåœ¨ GPU ä¸Šåº”è¯¥éœ€è¦å‡ åˆ†é’Ÿï¼‰å¹¶è¾“å‡ºè®­ç»ƒæŸå¤±ï¼Œä»¥åŠæ¯ä¸ª epoch ç»“æŸæ—¶çš„éªŒè¯æŸå¤±ã€‚

<Tip>

è¯·æ³¨æ„ğŸ¤— Transformers æ¨¡å‹å…·æœ‰å¤§å¤šæ•° Keras æ¨¡å‹æ‰€æ²¡æœ‰çš„ç‰¹æ®Šèƒ½åŠ›â€”â€”å®ƒä»¬å¯ä»¥è‡ªåŠ¨ä½¿ç”¨å†…éƒ¨è®¡ç®—çš„æŸå¤±ã€‚ å¦‚æœä½ æ²¡æœ‰åœ¨ `compile()` ä¸­è®¾ç½®æŸå¤±å‚æ•°ï¼Œå®ƒä»¬å°†é»˜è®¤ä½¿ç”¨å†…éƒ¨è®¡ç®—çš„æŸå¤±ã€‚ è¯·æ³¨æ„ï¼Œè¦ä½¿ç”¨å†…éƒ¨æŸå¤±ï¼Œä½ éœ€è¦å°†æ ‡ç­¾ä½œä¸ºè¾“å…¥çš„ä¸€éƒ¨åˆ†ä¼ é€’ï¼Œè€Œä¸æ˜¯ä½œä¸ºå•ç‹¬çš„æ ‡ç­¾ï¼ˆè¿™æ˜¯åœ¨ Keras æ¨¡å‹ä¸­ä½¿ç”¨æ ‡ç­¾çš„æ­£å¸¸æ–¹å¼ï¼‰ã€‚ ä½ å°†åœ¨è¯¾ç¨‹çš„ç¬¬ 2 éƒ¨åˆ†ä¸­çœ‹åˆ°è¿™æ–¹é¢çš„ç¤ºä¾‹ï¼Œå…¶ä¸­å®šä¹‰æ­£ç¡®çš„æŸå¤±å‡½æ•°å¯èƒ½ä¼šå¾ˆæ£˜æ‰‹ã€‚ ç„¶è€Œå¯¹äºåºåˆ—åˆ†ç±»æ¥è¯´ï¼Œæ ‡å‡†çš„ Keras æŸå¤±å‡½æ•°å¯ä»¥æ­£å¸¸è¿è¡Œï¼Œå› æ­¤æˆ‘ä»¬å°†åœ¨è¿™é‡Œä½¿ç”¨å®ƒã€‚

</Tip>

```py
from tensorflow.keras.losses import SparseCategoricalCrossentropy

model.compile(
    optimizer="adam",
    loss=SparseCategoricalCrossentropy(from_logits=True),
    metrics=["accuracy"],
)
model.fit(
    tf_train_dataset,
    validation_data=tf_validation_dataset,
)
```

<Tip warning={true}>

è¯·æ³¨æ„è¿™é‡Œæœ‰ä¸€ä¸ªéå¸¸å¸¸è§çš„é™·é˜±â€”â€”ä½ `å¯ä»¥`æŠŠæŸå¤±çš„åç§°ä½œä¸ºä¸€ä¸ªå­—ç¬¦ä¸²ä¼ é€’ç»™ Kerasï¼Œä½†é»˜è®¤æƒ…å†µä¸‹ï¼ŒKeras ä¼šè®¤ä¸ºä½ å·²ç»å¯¹è¾“å‡ºåº”ç”¨äº† softmaxã€‚ ç„¶è€Œï¼Œè®¸å¤šæ¨¡å‹åœ¨åº”ç”¨ softmax ä¹‹å‰å°±ä¼šè¾“å‡ºæ•°å€¼ï¼Œä¹Ÿè¢«ç§°ä¸º `logits`ã€‚ æˆ‘ä»¬éœ€è¦å‘Šè¯‰æŸå¤±å‡½æ•°ï¼Œè¿™å°±æ˜¯æˆ‘ä»¬çš„æ¨¡å‹æ‰€åšçš„æ˜¯å¦ç»è¿‡äº†softmaxï¼Œå”¯ä¸€çš„æ–¹æ³•æ˜¯ç›´æ¥è°ƒç”¨å®ƒï¼Œè€Œä¸æ˜¯ç”¨å­—ç¬¦ä¸²æ¥å‘½åã€‚

</Tip>


## æå‡è®­ç»ƒçš„æ•ˆæœ [[æå‡è®­ç»ƒçš„æ•ˆæœ]]

<Youtube id="cpzq6ESSM5c"/>

å¦‚æœä½ å°è¯•ä¸Šè¿°ä»£ç ï¼Œå®ƒçš„è¿è¡Œæ˜¯æ²¡æœ‰é—®é¢˜çš„ï¼Œä½†ä½ ä¼šå‘ç°æŸå¤±åªæ˜¯ç¼“æ…¢æˆ–é›¶æ˜Ÿåœ°ä¸‹é™ã€‚ ä¸»è¦åŸå› æ˜¯`å­¦ä¹ ç‡`ã€‚ ä¸æŸå¤±ä¸€æ ·ï¼Œå½“æˆ‘ä»¬æŠŠä¼˜åŒ–å™¨çš„åç§°ä½œä¸ºå­—ç¬¦ä¸²ä¼ é€’ç»™ Keras æ—¶ï¼ŒKeras ä¼šåˆå§‹åŒ–è¯¥ä¼˜åŒ–å™¨å…·æœ‰æ‰€æœ‰å‚æ•°çš„é»˜è®¤å€¼ï¼ŒåŒ…æ‹¬å­¦ä¹ ç‡ã€‚ ä¸è¿‡æ ¹æ®é•¿æœŸç»éªŒï¼Œæˆ‘ä»¬çŸ¥é“Transformer æ¨¡å‹å¾—ç›Šäºæ¯” Adam çš„é»˜è®¤å€¼ä½å¾—å¤šçš„å­¦ä¹ ç‡ï¼ˆå³1e-3ï¼‰ï¼Œä¹Ÿå†™æˆä¸º 10 çš„ -3 æ¬¡æ–¹ï¼Œæˆ– 0.001ã€‚ 5e-5 (0.00005) æ¯”é»˜è®¤å€¼å¤§çº¦ä½ 20 å€ï¼Œæ˜¯ä¸€ä¸ªæ›´å¥½çš„èµ·ç‚¹ã€‚

é™¤äº†é™ä½å­¦ä¹ ç‡ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜æœ‰ç¬¬äºŒä¸ªæŠ€å·§ï¼šæˆ‘ä»¬å¯ä»¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ…¢æ…¢é™ä½å­¦ä¹ ç‡ã€‚åœ¨æ–‡çŒ®ä¸­ï¼Œä½ æœ‰æ—¶ä¼šçœ‹åˆ°è¿™è¢«ç§°ä¸º `decaying` æˆ– `annealing`å­¦ä¹ ç‡ã€‚ åœ¨ Keras ä¸­ï¼Œæœ€å¥½çš„æ–¹æ³•æ˜¯ä½¿ç”¨ `learning rate scheduler`ã€‚ ä¸€ä¸ªå¥½ç”¨çš„è°ƒåº¦å™¨æ˜¯`PolynomialDecay`â€”â€”å°½ç®¡å®ƒçš„åå­—å«`PolynomialDecay`ï¼Œä½†åœ¨é»˜è®¤è®¾ç½®ä¸‹ï¼Œå®ƒåªæ˜¯ç®€å•å°†å­¦ä¹ ç‡ä»åˆå§‹å€¼çº¿æ€§è¡°å‡åˆ°æœ€ç»ˆå€¼ï¼Œè¿™æ­£æ˜¯æˆ‘ä»¬æƒ³è¦çš„ã€‚ä¸è¿‡ä¸ºäº†æ­£ç¡®ä½¿ç”¨è°ƒåº¦ç¨‹åºï¼Œæˆ‘ä»¬éœ€è¦å‘Šè¯‰å®ƒè®­ç»ƒçš„æ¬¡æ•°ã€‚ æˆ‘ä»¬å°†åœ¨ä¸‹é¢ä¸ºå…¶è®¡ç®—â€œnum_train_stepsâ€ã€‚

```py
from tensorflow.keras.optimizers.schedules import PolynomialDecay

batch_size = 8
num_epochs = 3
# è®­ç»ƒæ­¥æ•°æ˜¯æ•°æ®é›†ä¸­çš„æ ·æœ¬æ•°é™¤ä»¥batch sizeå†ä¹˜ä»¥ epochã€‚
# æ³¨æ„è¿™é‡Œçš„tf_train_datasetæ˜¯ä¸€ä¸ªè½¬åŒ–ä¸ºbatchåçš„ tf.data.Datasetï¼Œ
# ä¸æ˜¯åŸæ¥çš„ Hugging Face Datasetï¼Œæ‰€ä»¥å®ƒçš„ len() å·²ç»æ˜¯ num_samples // batch_sizeã€‚
num_train_steps = len(tf_train_dataset) * num_epochs
lr_scheduler = PolynomialDecay(
    initial_learning_rate=5e-5, end_learning_rate=0.0, decay_steps=num_train_steps
)
from tensorflow.keras.optimizers import Adam

opt = Adam(learning_rate=lr_scheduler)
```

<Tip>

ğŸ¤— Transformers åº“è¿˜æœ‰ä¸€ä¸ª `create_optimizer()` å‡½æ•°ï¼Œå®ƒå°†åˆ›å»ºä¸€ä¸ªå…·æœ‰å­¦ä¹ ç‡è¡°å‡çš„ `AdamW` ä¼˜åŒ–å™¨ã€‚ è¿™æ˜¯ä¸€ä¸ªä¾¿æ·çš„æ–¹å¼ï¼Œä½ å°†åœ¨æœ¬è¯¾ç¨‹çš„åç»­éƒ¨åˆ†ä¸­è¯¦ç»†äº†è§£ã€‚

</Tip>

ç°åœ¨æˆ‘ä»¬æœ‰äº†å…¨æ–°çš„ä¼˜åŒ–å™¨ï¼Œæˆ‘ä»¬å¯ä»¥å°è¯•ä½¿ç”¨å®ƒè¿›è¡Œè®­ç»ƒã€‚ é¦–å…ˆï¼Œè®©æˆ‘ä»¬é‡æ–°åŠ è½½æ¨¡å‹ï¼Œé‡æ–°è®¾ç½®åˆšåˆšè®­ç»ƒæ—¶çš„æƒé‡å˜åŒ–ï¼Œç„¶åæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ–°çš„ä¼˜åŒ–å™¨å¯¹å…¶è¿›è¡Œç¼–è¯‘ï¼š

```py
import tensorflow as tf

model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
model.compile(optimizer=opt, loss=loss, metrics=["accuracy"])
```

ç°åœ¨ï¼Œæˆ‘ä»¬å†æ¬¡è¿›è¡Œfitï¼š

```py
model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=3)
```

<Tip>

ğŸ’¡ å¦‚æœä½ æƒ³åœ¨è®­ç»ƒæœŸé—´è‡ªåŠ¨å°†æ¨¡å‹ä¸Šä¼ åˆ° Hubï¼Œä½ å¯ä»¥åœ¨ `model.fit()` æ–¹æ³•ä¸­ä¼ é€’ä¸€ä¸ª `PushToHubCallback`ã€‚ æˆ‘ä»¬å°†åœ¨ [ç¬¬å››ç« ](/course/chapter4/3) ä¸­è¿›ä¸€æ­¥äº†è§£è¿™ä¸ªé—®é¢˜ã€‚

</Tip>

## æ¨¡å‹é¢„æµ‹ [[æ¨¡å‹é¢„æµ‹]]

<Youtube id="nx10eh4CoOs"/>


è®­ç»ƒå’Œè§‚å¯ŸæŸå¤±çš„ä¸‹é™éƒ½æ˜¯éå¸¸å¥½ï¼Œä½†å¦‚æœæˆ‘ä»¬æƒ³ä»è®­ç»ƒåçš„æ¨¡å‹ä¸­å¾—åˆ°è¾“å‡ºï¼Œæˆ–è€…è®¡ç®—ä¸€äº›æŒ‡æ ‡ï¼Œæˆ–è€…åœ¨ç”Ÿäº§ä¸­ä½¿ç”¨æ¨¡å‹ï¼Œè¯¥æ€ä¹ˆåŠå‘¢ï¼Ÿä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨`predict()` æ–¹æ³•ã€‚ è¿™å°†è¿”å›æ¨¡å‹çš„è¾“å‡ºå¤´çš„`logits`æ•°å€¼ï¼Œæ¯ä¸ªç±»ä¸€ä¸ªã€‚

```py
preds = model.predict(tf_validation_dataset)["logits"]
```

æˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿ç”¨ logit è½¬æ¢ä¸ºæ¨¡å‹çš„ç±»åˆ«é¢„æµ‹ï¼Œæ–¹æ³•æ˜¯ä½¿ç”¨ argmax æ‰¾åˆ°æœ€é«˜çš„ logitï¼Œè¿™å¯¹åº”äºæœ€æœ‰å¯èƒ½çš„ç±»åˆ«ï¼š

```py
class_preds = np.argmax(preds, axis=1)
print(preds.shape, class_preds.shape)
```

```python out
(408, 2) (408,)
```

ç°åœ¨ï¼Œè®©æˆ‘ä»¬ä½¿ç”¨è¿™äº› `preds` æ¥è®¡ç®—ä¸€äº›æŒ‡æ ‡ï¼ æˆ‘ä»¬å¯ä»¥åƒåŠ è½½æ•°æ®é›†ä¸€æ ·è½»æ¾åœ°åŠ è½½ä¸ MRPC æ•°æ®é›†ç›¸å…³çš„æŒ‡æ ‡ï¼Œè¿™æ¬¡ä½¿ç”¨çš„æ˜¯ `evaluate.load()` å‡½æ•°ã€‚ è¿”å›çš„å¯¹è±¡æœ‰ä¸€ä¸ª `compute()` æ–¹æ³•ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å®ƒæ¥è¿›è¡Œåº¦é‡è®¡ç®—ï¼š

```py
import evaluate

metric = evaluate.load("glue", "mrpc")
metric.compute(predictions=class_preds, references=raw_datasets["validation"]["label"])
```

```python out
{'accuracy': 0.8578431372549019, 'f1': 0.8996539792387542}
```

ä½ è·å¾—çš„ç¡®åˆ‡ç»“æœå¯èƒ½ä¼šæœ‰æ‰€ä¸åŒï¼Œå› ä¸ºæ¨¡å‹å¤´çš„éšæœºåˆå§‹åŒ–å¯èƒ½ä¼šæ”¹å˜å®ƒè·å¾—çš„æŒ‡æ ‡ã€‚ åœ¨è¿™é‡Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ¨¡å‹åœ¨éªŒè¯é›†ä¸Šçš„å‡†ç¡®ç‡ä¸º 85.78%ï¼ŒF1 å¾—åˆ†ä¸º 89.97ã€‚ è¿™å°±æ˜¯ç”¨äºè¯„ä¼° GLUE åŸºå‡†çš„ MRPC æ•°æ®é›†ä¸Šçš„ç»“æœä¸¤ä¸ªæŒ‡æ ‡ã€‚ [BERT è®ºæ–‡](https://arxiv.org/pdf/1810.04805.pdf) ä¸­çš„è¡¨æ ¼æŠ¥å‘Šäº†åŸºæœ¬æ¨¡å‹çš„ F1 åˆ†æ•°ä¸º 88.9ã€‚ é‚£æ˜¯ `uncased` æ¨¡å‹ï¼Œè€Œæˆ‘ä»¬ç›®å‰ä½¿ç”¨çš„æ˜¯ `cased` æ¨¡å‹ï¼Œè¿™è§£é‡Šäº†ä¸ºä»€ä¹ˆæˆ‘ä»¬ä¼šè·å¾—æ›´å¥½çš„ç»“æœã€‚

å…³äºä½¿ç”¨ Keras API è¿›è¡Œå¾®è°ƒçš„ä»‹ç»åˆ°æ­¤ç»“æŸã€‚ åœ¨[ç¬¬ 7 ç« ](/course/chapter7)å°†ç»™å‡ºå¯¹å¤§å¤šæ•°å¸¸è§ NLP ä»»åŠ¡æ‰§è¡Œæ­¤æ“ä½œçš„ç¤ºä¾‹ã€‚å¦‚æœä½ æƒ³åœ¨ Keras API ä¸Šç£¨ç»ƒè‡ªå·±çš„æŠ€èƒ½ï¼Œè¯·å°è¯•ä½¿ç¬¬äºŒèŠ‚æ‰€ä½¿ç”¨çš„çš„æ•°æ®å¤„ç†åœ¨ GLUE SST-2 æ•°æ®é›†ä¸Šå¾®è°ƒæ¨¡å‹ã€‚