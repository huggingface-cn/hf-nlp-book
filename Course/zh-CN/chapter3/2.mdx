<FrameworkSwitchCourse {fw} />

# 处理数据 [[处理数据]]

{#if fw === 'pt'}

<CourseFloatingBanner chapter={3}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/zh-CN/chapter3/section2_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/zh-CN/chapter3/section2_pt.ipynb"},
]} />

{:else}

<CourseFloatingBanner chapter={3}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/zh-CN/chapter3/section2_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/zh-CN/chapter3/section2_tf.ipynb"},
]} />

{/if}

{#if fw === 'pt'}
这一小节我们将学习[第一小节](/course/chapter2)中提到的“如何使用模型中心（hub）的大型数据集”，下面是我们用模型中心的数据在PyTorch上训练句子分类器的一个例子：

```python
import torch
from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification

# Same as before
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    "This course is amazing!",
]
batch = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")

# This is new
batch["labels"] = torch.tensor([1, 1])

optimizer = AdamW(model.parameters())
loss = model(**batch).loss
loss.backward()
optimizer.step()
```

{:else}
这一小节我们将学习[第一小节](/course/chapter2)中提到的“如何使用模型中心（hub）的大型数据集”，下面是我们用模型中心的数据在TensorFlow上训练句子分类器的一个例子：

```python
import tensorflow as tf
import numpy as np
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

# 和之前一样
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    "This course is amazing!",
]
batch = dict(tokenizer(sequences, padding=True, truncation=True, return_tensors="tf"))

# This is new
model.compile(optimizer="adam", loss="sparse_categorical_crossentropy")
labels = tf.convert_to_tensor([1, 1])
model.train_on_batch(batch, labels)
```

{/if}

然而仅用两句话训练模型不会产生很好的效果，你需要准备一个更大的数据集才能得到更好的训练结果。

在本节中，我们以 MRPC（微软研究释义语料库）数据集为例，该数据集由威廉·多兰和克里斯·布罗克特在 [这篇文章](https://www.aclweb.org/anthology/I05-5002.pdf) 发布，由 5801 对句子组成，每个句子对带有一个标签来指示它们是否为同义（即两个句子的意思相同）。在本章选择该数据集的原因是它的数据体量小，容易对其进行训练。

## 从模型中心（Hub）加载数据集 [[从模型中心（Hub）加载数据集]]

{#if fw === 'pt'}
`<Youtube id="_BZearw7f0w"/>`
{:else}
`<Youtube id="W_gMJF0xomE"/>`
{/if}

模型中心（hub）不仅仅包含模型，还有许多别的语言的数据集。访问 [数据集](https://huggingface.co/datasets) 的链接即可进行浏览。我们建议你在完成本节的学习后阅读一下 [加载和处理新的数据集](https://huggingface.co/docs/datasets/loading_datasets.html#from-the-huggingface-hub) 这篇文章，这会让你对 huggingface 的数据集理解更加清晰。现在让我们使用 MRPC 数据集中的 [GLUE 基准测试数据集](https://gluebenchmark.com) ，它是构成 MRPC 数据集的 10 个数据集之一，作为一个用于衡量机器学习模型在 10 个不同文本分类任务中性能的学术基准。

🤗 Datasets 库提供了一条非常便捷的命令，可以在模型中心（hub）上下载和缓存数据集。你可以通过以下代码下载 MRPC 数据集：

```py
from datasets import load_dataset

raw_datasets = load_dataset("glue", "mrpc")
raw_datasets
```

```python
DatasetDict({
    train: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 408
    })
    test: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 1725
    })
})
```

现在我们获得了一个 `DatasetDict` 对象，这个对象包含训练集、验证集和测试集。每一个集合都包含 4 个列（ `sentence1` ， `sentence2` ， `label` 和 `idx` ）以及一个代表行数的变量（每个集合中的行的个数）。运行结果显示该训练集中有 3668 对句子，验证集中有 408 对，测试集中有 1725 对。

默认情况下，该命令在下载数据集并缓存到 `~/.cache/huggingface/datasets`。回想在第 2 章中可以通过设置 `HF_HOME` 环境变量来自定义缓存的文件夹。

我们可以访问该数据集中的每一个 `raw_train_dataset` 对象，例如使用字典：

```py
raw_train_dataset = raw_datasets["train"]
raw_train_dataset[0]
```

```python
{'idx': 0,
 'label': 1,
 'sentence1': 'Amrozi accused his brother , whom he called " the witness " , of deliberately distorting his evidence .',
 'sentence2': 'Referring to him as only " the witness " , Amrozi accused his brother of deliberately distorting his evidence .'}
```

现在可以看到标签已经是整数了，因此不需要对标签做任何预处理。如果想要知道哪个数字对应于哪个标签，我们可以查看 `raw_train_dataset` 的 `features` 。这告诉我们每列的类型：

```py
raw_train_dataset.features
```

```python
{'sentence1': Value(dtype='string', id=None),
 'sentence2': Value(dtype='string', id=None),
 'label': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], names_file=None, id=None),
 'idx': Value(dtype='int32', id=None)}
```

在上面的例子之中， `Label（标签）` 是一种 `ClassLabel（分类标签）` ，也就是使用整数建立起类别标签的映射关系。 `0` 对应于 `not_equivalent` ， `1` 对应于 `equivalent` 。

<Tip>

✏️ **试试看！** 查看训练集的第 15 行元素和验证集的 87 行元素。他们的标签是什么？

</Tip

## 预处理数据集 [[预处理数据集]]

{#if fw === 'pt'}
`<Youtube id="0u3ioSwev3s"/>`
{:else}
`<Youtube id="P-rZWqcB6CE"/>`
{/if}

为了预处理数据集，我们需要将文本转换为模型能够理解的数字。在 [第二章](/course/chapter2) 我们已经学习过。这是通过一个 Tokenizer 完成的，我们可以向 Tokenizer 输入一个句子或一个句子列表。以下代码表示直接标记每对句子中的所有第一句和所有第二句：

```py
from transformers import AutoTokenizer

checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
tokenized_sentences_1 = tokenizer(raw_datasets["train"]["sentence1"])
tokenized_sentences_2 = tokenizer(raw_datasets["train"]["sentence2"])
```

不过在将两句话传递给模型，预测这两句话是否是同义之前，我们需要给这两句话依次进行适当的预处理。 Tokenizer 不仅仅可以输入单个句子，还可以输入一组句子，并按照 BERT 模型所期望的输入进行处理：

```py
inputs = tokenizer("This is the first sentence.", "This is the second one.")
inputs
```

```python
{ 
  'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102],
  'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],
  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
}
```

我们在 [第二章](/course/chapter2) 讨论了 `输入词id(input_ids)` 和 `注意力遮罩(attention_mask)` ，但尚未讨论 `类型标记ID(token_type_ids)` 。在本例中， `类型标记ID(token_type_ids)` 的作用就是告诉模型输入的哪一部分是第一句，哪一部分是第二句。

<Tip>

✏️ ** 试试看！** 选取训练集中的第 15 个元素，将两句话分别标记为一对。结果和上方的例子有什么不同？

</Tip>

如果将 `input_ids` 中的 id 转换回文字：

```py
tokenizer.convert_ids_to_tokens(inputs["input_ids"])
```

将得到：

```python
['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']
```

所以我们看到模型需要输入的形式是 `[CLS] sentence1 [SEP] sentence2 [SEP]` 。所以当有两句话的时候， `类型标记ID(token_type_ids)` 的值是：

```python
['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']
[      0,      0,    0,     0,       0,          0,   0,       0,      1,    1,     1,        1,     1,   1,       1]
```

现在输入中 `[CLS] sentence1 [SEP]` 它们的类型标记 ID 均为 `0` ，而其他部分对应 `sentence2 [SEP]` ，所有的类型标记 ID 均为 `1` 。

请注意，如果选择其他的检查点，不一定具有 `类型标记ID(token_type_ids)` ，如果使用 DistilBERT 模型，就不会返回它该类型标记。只有当它在预训练期间使用过这一层，也就是模型在构建时依赖它们时，才会返回类型标签 ID。

使用类型标记 ID 对 BERT 进行预训练并且使用 [第一章](/course/chapter1) 的遮罩语言模型，还有一个额外的应用类型叫下一句预测。这项任务的目标是建立成对句子之间关系的模型。

在下一句预测任务中，会给模型输入成对的句子（带有随机遮罩的token），并要求预测第二个句子是否紧跟第一个句子。为了提高模型的泛化能力，数据集中一有一半句子对中的句子在原始文档中挨在一起，另一半句子对中的两个句子来自两个不同的文档。

一般来说无需要担心是否有 `类型标记ID(token_type_ids)` 。在你的标输入中：只要你对 Tokenizer 和模型使用相同的检查点，Tokenizer 就会知道向模型提供什么，运行会顺利进行。

现在我们已经了解了 Tokenizer 如何处理一对句子，我们可以用它来处理整个数据集：如 [第二章](/course/chapter2) ，我们可以给 Tokenizer 提供一对句子，第一个参数是它第一个句子的列表，第二个参数是第二个句子的列表。这也与我们在 [第二章](/course/chapter2) 中看到的填充和截断选项兼容。所以预处理训练数据集的一种方法是：

```py
tokenized_dataset = tokenizer(
    raw_datasets["train"]["sentence1"],
    raw_datasets["train"]["sentence2"],
    padding=True,
    truncation=True,
)
```

这种方法虽然有效，但它的缺点是需要返回字典（字典的键是 `输入词id(input_ids)` ， `注意力遮罩(attention_mask)` 和 `类型标记ID(token_type_ids)` ，字典的值是键所对应值的列表）。这意味着在转换过程中要有足够的内存来存储整个数据集才不会出错。不过来自🤗数据集库中的数据集是 [Apache Arrow](https://arrow.apache.org) 存储在磁盘上的文件，因此你只需将接下来要用的数据加载在内存中，这对内存容量的需求比较友好。

为了将数据保存为数据集，我们将使用 [Dataset.map()](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map) 方法，如果我们需要做更多的预处理而不仅仅是标记化，这里有一些额外的自定义的方法。 `map()` 方法的工作原理是在数据集的每个元素上应用一个函数。让我们定义一个标记输入的函数：

```py
def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)
```

该函数接收一个字典（与数据集的项类似）并返回一个包含 `输入词id(input_ids)` ， `注意力遮罩(attention_mask)` 和 `类型标记ID(token_type_ids)` 键的新字典。请注意，如果 `example` 字典所对应的值包含多个句子（每个键作为一个句子列表），那么它依然可以运行，就像前面的例子一样， `tokenizer` 标记器可以处理成对的句子列表，这样的话我们可以在调用 `map()` 时使用该选项 `batched=True` ，这将显著加快标记的速度。 `Tokenizer` 来自 [🤗 Tokenizers](https://github.com/huggingface/tokenizers) 库由 Rust 编写而成。这个标记器可以非常快，但前提是需要一次给它很多输入。

请注意，现在我们在标记函数中暂时没有使用 `padding` 参数。这是因为将所有的样本填充到最大长度并不高效。一个更好的做法是：在构建批处理时填充样本。这样我们只需要填充到该批处理中的最大长度，而不是整个数据集的最大长度。当输入长度不稳定时，这可以节省大量时间和处理能力！

下面是我们如何在所有数据集上一次性应用标记函数。我们在调用 `map` 时使用了 `batch =True` ，这样函数就可以同时应用到数据集的多个元素上，而不是分别应用到每个元素，这样可以更快进行预处理。

```py
tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
tokenized_datasets
```

🤗Datasets 库应用这种处理的方式是向数据集添加新的字段，每个字段对应预处理函数返回的字典中的每个键：

```python
DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 408
    })
    test: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 1725
    })
})
```

在使用预处理函数 `map()` 时，甚至可以通过传递 `num_proc` 参数使用并行处理。我们在这里没有这样做，因为🤗标记器库已经使用多线程来更快地标记样本，但是如果没有使用该库支持的快速标记器，使用 `num_proc` 可能会加快预处理。

我们的 `标记函数(tokenize_function)` 返回包含 `输入词id(input_ids)` ， `注意力遮罩(attention_mask)` 和 `类型标记ID(token_type_ids)` 键的字典，这三个字段被添加到数据集的标记结果中。请注意，如果预处理函数 `map()` 为现有键返回一个新值，我们也可以修改现有的字段。

我们需要做的最后一件事是，当我们把元素批在一起的时候，把所有示例填充到最长元素的长度——我们把这种技术称之为动态填充。

## 动态填充 [[动态填充]]

<Youtube id="7q5NyFT8REg"/>

{#if fw === 'pt'}
负责在批处理中将数据整理为一个 batch 的函数称为 `collate 函数`。这是一个可以在构建 `DataLoader` 时传递的一个参数，默认是一个将你的数据集转换为 PyTorch 张量并将它们拼接起来的函数（如果你的元素是列表、元组或字典，则会使用递归）。这在本例子中下是不可行的，因为我们的输入不会都是相同大小的。我们特意推迟了填充的时间，只在每个 batch 上进行必要的填充，以避免出现有大量填充的过长输入。这将大大加快训练速度，但请注意，如果你在 TPU 上训练，需要注意一个问题——TPU 喜欢固定的形状，即使这需要额外的填充。

{:else}

负责在批处理中将数据整理为一个 batch 的函数称为 `collate 函数`。默认的拼合函数只会将你的样本转换为 tf.Tensor 并将它们拼接起来（如果你的元素是列表、元组或字典，则会使用递归）。这在本例中是不可行的，因为我们的输入不是都是相同大小的。我们特意推迟了填充时间，只在每个 batch 上进行填充，以避免有太多填充的过长的输入。这将大大加快训练速度，但请注意，如果你在 TPU 上训练，需要注意一个问题——TPU 喜欢固定的形状，即使这需要额外的填充。

{/if}

为了解决句子长度统一的问题，我们必须定义一个 collate 函数，该函数会将每个 batch 句子填充到正确的长度。幸运的是，🤗transformer 库通过 `DataCollatorWithPadding` 为我们提供了这样一个函数。当你实例化它时，它需要一个标记器（用来知道使用哪种填充标记，以及模型期望填充是在输入的左边还是右边），并将做你需要的一切：

{#if fw === 'pt'}

```py
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

{:else}

```py
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="tf")
```

{/if}

为了测试这个新玩具，让我们从我们的训练集中抽取几个样本，想到这里，我们删除列 `idx` ， `sentence1` 和 `sentence2` ，因为不需要它们，而且包含字符串（我们不能用字符串创建张量），然后查看一个 batch 中每个条目的长度：

```py
samples = tokenized_datasets["train"][:8]
samples = {k: v for k, v in samples.items() if k not in ["idx", "sentence1", "sentence2"]}
[len(x) for x in samples["input_ids"]]
```

```python
[50, 59, 47, 67, 59, 50, 62, 32]
```

不出所料，我们得到了不同长度的样本，从 32 到 67。动态填充意味着该批样本都应该填充到长度为 67，这是该批中的最大长度。如果没有动态填充，所有的样本都必须填充到整个数据集中的最大长度，或者模型可以接受的最大长度。让我们再次检查 `data_collator` 是否正确地动态填充了这批样本：

```py:

```py
batch = data_collator(samples)
{k: v.shape for k, v in batch.items()}
```

{#if fw === 'tf'}

```python
{'attention_mask': TensorShape([8, 67]),
 'input_ids': TensorShape([8, 67]),
 'token_type_ids': TensorShape([8, 67]),
 'labels': TensorShape([8])}
```

{:else}

```python
{'attention_mask': torch.Size([8, 67]),
 'input_ids': torch.Size([8, 67]),
 'token_type_ids': torch.Size([8, 67]),
 'labels': torch.Size([8])}
```

看起来不错！现在，我们已经从原始文本转化为了模型可以处理的数据，我们准备好对其进行微调。

{/if}

<Tip>

✏️ ** 试试看！** 在 GLUE SST-2 数据集上重复预处理。它有点不同，因为它是由单句而不是成对的句子组成的，但是我们所做的其他事情看起来应该是一样的。另一个更难的挑战是尝试编写一个可用于任何 GLUE 任务的预处理函数。

</Tip>

{#if fw === 'tf'}

现在我们有了 dataset 和 data collator，我们需要将 dataset 批量地应用 data collator。我们可以手动加载批次并整理它们，但这需要大量工作，性能也有可能不好。相反，有一个简单的方法为这个问题提供高效的解决方案： `to_tf_dataset()` 。这将在你的数据集周围包裹一个 `tf.data.Dataset` ，这个方法带有一个可选的 data collator 功能。 `tf.data.Dataset` 是 Keras 可用于 `model.fit()` 的原生 TensorFlow 格式，因此这种方法会立即将🤗 Dataset 转换为可用于训练的格式。让我们看看它在我们的数据集上是如何使用的吧！

```py
tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)

tf_validation_dataset = tokenized_datasets["validation"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "token_type_ids"],
    label_cols=["labels"],
    shuffle=False,
    collate_fn=data_collator,
    batch_size=8,
)
```

就是这样！我们可以把这些数据集带入下一节，在经过所有艰苦的数据预处理工作之后，训练将变得非常简单和愉快。

{/if}
