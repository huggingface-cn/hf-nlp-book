<FrameworkSwitchCourse {fw} />

# 章节简介 [[章节简介]]

在 [第三章](/course/chapter3) ，你了解了如何微调文本分类模型。在本章中，我们将处理以下常见的 NLP 任务：

- 词元（token）分类
- 掩码语言建模（如 BERT）
- 文本摘要
- 翻译
- 因果语言建模预训练（如 GPT-2）
- 问答

{#if fw === 'pt'}

为此，你需要利用 [第三章](/course/chapter3) 中学到的 `Trainer` API 和 🤗 Accelerate 库、 [第五章](/course/chapter5) 中的 🤗 Datasets 库以及 [第六章](/course/chapter6) 中的 🤗 Tokenizers 库的所有知识。我们同样会将结果上传到模型中心，就像我们在 [第四章](/course/chapter4) 中所做的那样，所以这确实是将所学融会贯通的一章！

每个部分都可以独立阅读，并将向你展示如何使用 `Trainer` API 或按照你自己的训练循环训练模型，并采用 🤗 Accelerate 加速。你可以随意跳过任何一部分，专注于你最感兴趣的部分： `Trainer` API 非常适用于微调（fine-tuning）或训练你的模型，且无需担心底层发生的事情；而采用 `Accelerate` 的训练循环可以让你更轻松地自定义所需的任何结构。

{:else}

为此，你需要利用 [第三章](/course/chapter3) 中学到的有关 Keras API、 [第五章](/course/chapter5) 中的 🤗 Datasets 库以及 [第六章](/course/chapter6) 中的 🤗 Tokenizers 库的所有知识。我们同样会将结果上传到模型中心，就像我们在 [第四章](/course/chapter4) 中所做的那样，所以这确实是将所学融会贯通的一章！

这一章每个部分都可以独立阅读。

{/if}


<Tip>

如果你按顺序阅读这些部分，你会注意到它们在代码和描述上有许多相似之处。这种重复是有意为之的，让你可以随时钻研（或稍后再回看）任何你感兴趣的任务并找到一个完整的可运行示例。

</Tip>
