<FrameworkSwitchCourse {fw} />

# Token åˆ†ç±» [[Token åˆ†ç±»]]

{#if fw === 'pt'}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter7/section2_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter7/section2_pt.ipynb"},
]} />

{:else}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter7/section2_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter7/section2_tf.ipynb"},
]} />

{/if}

æˆ‘ä»¬å°†é¦–å…ˆæ¢è®¨çš„åº”ç”¨æ˜¯ Token åˆ†ç±»ã€‚è¿™ä¸ªé€šç”¨ä»»åŠ¡æ¶µç›–äº†æ‰€æœ‰å¯ä»¥è¡¨è¿°ä¸ºâ€œç»™å¥å­ä¸­çš„è¯æˆ–å­—è´´ä¸Šæ ‡ç­¾â€çš„é—®é¢˜ï¼Œä¾‹å¦‚ï¼š

- **å®ä½“å‘½åè¯†åˆ« ï¼ˆNERï¼‰**ï¼šæ‰¾å‡ºå¥å­ä¸­çš„å®ä½“ï¼ˆå¦‚äººç‰©ã€åœ°ç‚¹æˆ–ç»„ç»‡ï¼‰ã€‚è¿™å¯ä»¥é€šè¿‡ä¸ºæ¯ä¸ªå®ä½“æˆ–â€œæ— å®ä½“â€æŒ‡å®šä¸€ä¸ªç±»åˆ«çš„æ ‡ç­¾ã€‚
- **è¯æ€§æ ‡æ³¨ ï¼ˆPOSï¼‰**ï¼šå°†å¥å­ä¸­çš„æ¯ä¸ªå•è¯æ ‡è®°ä¸ºå¯¹åº”äºç‰¹å®šçš„è¯æ€§ï¼ˆå¦‚åè¯ã€åŠ¨è¯ã€å½¢å®¹è¯ç­‰ï¼‰ã€‚
- **åˆ†å—ï¼ˆchunkingï¼‰**ï¼šæ‰¾å‡ºå±äºåŒä¸€å®ä½“çš„ tokens ã€‚è¿™ä¸ªä»»åŠ¡ï¼ˆå¯ä»¥ä¸è¯æ€§æ ‡æ³¨æˆ–å‘½åå®ä½“è¯†åˆ«ç»“åˆï¼‰å¯ä»¥è¢«æè¿°ä¸ºå°†ä½äºå—å¼€å¤´çš„ token èµ‹äºˆä¸€ä¸ªæ ‡ç­¾ï¼ˆé€šå¸¸æ˜¯ â€œ`B-`â€ ï¼ˆBeginï¼‰ï¼‰ï¼Œå°†ä½äºå—å†…çš„ tokens èµ‹äºˆå¦ä¸€ä¸ªæ ‡ç­¾ï¼ˆé€šå¸¸æ˜¯ â€œ`I-`â€ï¼ˆinnerï¼‰ï¼‰ï¼Œå°†ä¸å±äºä»»ä½•å—çš„ tokens èµ‹äºˆç¬¬ä¸‰ä¸ªæ ‡ç­¾ï¼ˆé€šå¸¸æ˜¯ â€œ`O`â€ ï¼ˆouterï¼‰ï¼‰ã€‚
<Youtube id="wVHdVlPScxA"/>

å½“ç„¶ï¼Œè¿˜æœ‰å¾ˆå¤šå…¶ä»–ç±»å‹çš„ token åˆ†ç±»é—®é¢˜ï¼›è¿™äº›åªæ˜¯å‡ ä¸ªæœ‰ä»£è¡¨æ€§çš„ä¾‹å­ã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†åœ¨ NER ä»»åŠ¡ä¸Šå¾®è°ƒæ¨¡å‹ ï¼ˆBERTï¼‰ï¼Œç„¶åè¯¥æ¨¡å‹å°†èƒ½å¤Ÿè®¡ç®—å¦‚ä¸‹é¢„æµ‹ï¼š

<iframe src="https://course-demos-bert-finetuned-ner.hf.space" frameBorder="0" height="350" title="Gradio app" class="block dark:hidden container p-0 flex-grow space-iframe" allow="accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking" sandbox="allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"></iframe>

<a class="flex justify-center" href="/huggingface-course/bert-finetuned-ner">
<img class="block dark:hidden lg:w-3/5" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/model-eval-bert-finetuned-ner.png" alt="One-hot encoded labels for question answering."/>
<img class="hidden dark:block lg:w-3/5" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/model-eval-bert-finetuned-ner-dark.png" alt="One-hot encoded labels for question answering."/>
</a>

ä½ å¯ä»¥ [åœ¨è¿™é‡Œ](https://huggingface.co/huggingface-course/bert-finetuned-ner?text=My+name+is+Sylvain+and+I+work+at+Hugging+Face+in+Brooklyn) ã€‚æ‰¾åˆ°æˆ‘ä»¬å°†è®­ç»ƒå¹¶ä¸Šä¼ åˆ° Hub çš„æ¨¡å‹ï¼Œå¯ä»¥å°è¯•è¾“å…¥ä¸€äº›å¥å­çœ‹çœ‹æ¨¡å‹çš„é¢„æµ‹ç»“æœã€‚

## å‡†å¤‡æ•°æ® [[å‡†å¤‡æ•°æ®]]

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªé€‚åˆ token åˆ†ç±»çš„æ•°æ®é›†ã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ [CoNLL-2003 æ•°æ®é›†](https://huggingface.co/datasets/conll2003) ï¼Œè¯¥æ•°æ®é›†åŒ…å«äº†æ¥è‡ªè·¯é€ç¤¾çš„æ–°é—»æ•…äº‹ã€‚

<Tip>

ğŸ’¡ åªè¦ä½ çš„æ•°æ®é›†ç”±å¸¦æœ‰ç›¸åº”æ ‡ç­¾çš„åˆ†è¯æ–‡æœ¬ç»„æˆï¼Œä½ å°±èƒ½å¤Ÿå°†è¿™é‡Œæè¿°çš„æ•°æ®å¤„ç†è¿‡ç¨‹åº”ç”¨åˆ°ä½ è‡ªå·±çš„æ•°æ®é›†ã€‚å¦‚æœéœ€è¦å¤ä¹ å¦‚ä½•åœ¨ `Dataset` ä¸­åŠ è½½è‡ªå®šä¹‰æ•°æ®é›†ï¼Œè¯·å¤ä¹  [ç¬¬äº”ç« ](/course/chapter5) ã€‚

</Tip>

### CoNLL-2003 æ•°æ®é›† [[CoNLL-2003 æ•°æ®é›†]]

è¦åŠ è½½ CoNLL-2003 æ•°æ®é›†ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ğŸ¤— Datasets åº“çš„ `load_dataset()` æ–¹æ³•ï¼š

```py
from datasets import load_dataset

raw_datasets = load_dataset("conll2003")
```

è¿™å°†ä¸‹è½½å¹¶ç¼“å­˜æ•°æ®é›†ï¼Œå°±åƒå’Œæˆ‘ä»¬åœ¨ [ç¬¬ä¸‰ç« ](/course/chapter3) åŠ è½½ GLUE MRPC æ•°æ®é›†ä¸€æ ·ã€‚æŸ¥çœ‹è¿™ä¸ªå¯¹è±¡å¯ä»¥è®©æˆ‘ä»¬çœ‹åˆ°å­˜åœ¨å“ªäº›åˆ—ï¼Œä»¥åŠè®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†ä¹‹é—´æ˜¯å¦‚ä½•åˆ†å‰²çš„ï¼š

```py
raw_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['chunk_tags', 'id', 'ner_tags', 'pos_tags', 'tokens'],
        num_rows: 14041
    })
    validation: Dataset({
        features: ['chunk_tags', 'id', 'ner_tags', 'pos_tags', 'tokens'],
        num_rows: 3250
    })
    test: Dataset({
        features: ['chunk_tags', 'id', 'ner_tags', 'pos_tags', 'tokens'],
        num_rows: 3453
    })
})
```

çœ‹åˆ°æ•°æ®é›†åŒ…å«äº†æˆ‘ä»¬ä¹‹å‰æåˆ°çš„ä¸‰é¡¹ä»»åŠ¡çš„æ ‡ç­¾ï¼šå‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰ã€è¯æ€§æ ‡æ³¨ï¼ˆPOSï¼‰ä»¥åŠåˆ†å—ï¼ˆchunkingï¼‰ã€‚è¿™ä¸ªæ•°æ®é›†ä¸å…¶ä»–æ•°æ®é›†çš„ä¸€ä¸ªæ˜¾è‘—åŒºåˆ«åœ¨äºï¼Œè¾“å…¥æ–‡æœ¬å¹¶éä»¥å¥å­æˆ–æ–‡æ¡£çš„å½¢å¼å‘ˆç°ï¼Œè€Œæ˜¯ä»¥å•è¯åˆ—è¡¨çš„å½¢å¼ï¼ˆæœ€åä¸€åˆ—è¢«ç§°ä¸º `tokens` ï¼Œä¸è¿‡`tokens`åˆ—ä¿å­˜çš„è¿˜æ˜¯å•è¯ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œè¿™äº›é¢„å…ˆåˆ†è¯çš„è¾“å…¥ä»éœ€è¦ç»è¿‡ tokenizer è¿›è¡Œå­è¯åˆ†è¯å¤„ç†ï¼‰ã€‚

æˆ‘ä»¬æ¥çœ‹çœ‹è®­ç»ƒé›†çš„ç¬¬ä¸€ä¸ªå…ƒç´ ï¼š

```py
raw_datasets["train"][0]["tokens"]
```

```python out
['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']
```

ç”±äºæˆ‘ä»¬è¦è¿›è¡Œå‘½åå®ä½“è¯†åˆ«ï¼Œè®©æˆ‘ä»¬æŸ¥çœ‹ä¸€ä¸‹ NER æ ‡ç­¾ï¼š

```py
raw_datasets["train"][0]["ner_tags"]
```

```python out
[3, 0, 7, 0, 0, 0, 7, 0, 0]
```

è¿™äº›æ˜¯ä¸ºè®­ç»ƒå‡†å¤‡çš„æ•´æ•°ç±»åˆ«æ ‡ç­¾ï¼Œä½†å½“æˆ‘ä»¬æƒ³è¦æ£€æŸ¥æ•°æ®æ—¶ï¼Œå®ƒä»¬ä¸æ˜¯å¾ˆç›´è§‚ã€‚å°±åƒåœ¨æ–‡æœ¬åˆ†ç±»ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æŸ¥çœ‹æ•°æ®é›†çš„ `features` å±æ€§æ¥è®¿é—®è¿™äº›æ•´æ•°å’Œæ ‡ç­¾åç§°ä¹‹é—´çš„å¯¹åº”å…³ç³»ï¼š

```py
ner_feature = raw_datasets["train"].features["ner_tags"]
ner_feature
```

```python out
Sequence(feature=ClassLabel(num_classes=9, names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], names_file=None, id=None), length=-1, id=None)
```

å› æ­¤ï¼Œè¿™ä¸€åˆ—åŒ…å«çš„å…ƒç´ æ˜¯ `ClassLabels` çš„åºåˆ—ã€‚åºåˆ—å…ƒç´ çš„ç±»å‹åœ¨ `ner_feature` çš„ `feature` ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æŸ¥çœ‹è¯¥ `feature` çš„ `names` å±æ€§æ¥è®¿é—®æ ‡ç­¾åç§°çš„åˆ—è¡¨ï¼š

```py
label_names = ner_feature.feature.names
label_names
```

```python out
['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']
```

æˆ‘ä»¬åœ¨ [ç¬¬å…­ç« ](/course/chapter6/3) ï¼Œæ·±å…¥ç ”ç©¶ `token-classification` ç®¡é“æ—¶å·²ç»çœ‹åˆ°äº†è¿™äº›æ ‡ç­¾ æˆ‘ä»¬åœ¨è¿™é‡Œè¿›è¡Œä¸€ä¸ªå¿«é€Ÿçš„å›é¡¾ï¼š

- `O` è¡¨ç¤ºè¿™ä¸ªè¯ä¸å¯¹åº”ä»»ä½•å®ä½“ã€‚
- `B-PER` / `I-PER` æ„å‘³ç€è¿™ä¸ªè¯å¯¹åº”äºäººåå®ä½“çš„å¼€å¤´/å†…éƒ¨ã€‚
- `B-ORG` / `I-ORG` çš„æ„æ€æ˜¯è¿™ä¸ªè¯å¯¹åº”äºç»„ç»‡åç§°å®ä½“çš„å¼€å¤´/å†…éƒ¨ã€‚
- `B-LOC` / `I-LOC` æŒ‡çš„æ˜¯æ˜¯è¿™ä¸ªè¯å¯¹åº”äºåœ°åå®ä½“çš„å¼€å¤´/å†…éƒ¨ã€‚
- `B-MISC` / `I-MISC` è¡¨ç¤ºè¯¥è¯å¯¹åº”äºä¸€ä¸ªæ‚é¡¹å®ä½“çš„å¼€å¤´/å†…éƒ¨ã€‚

ç°åœ¨è§£ç æˆ‘ä»¬ä¹‹å‰çœ‹åˆ°çš„æ ‡ç­¾ï¼Œæˆ‘ä»¬å¾—åˆ°ä»¥ä¸‹çš„è¾“å‡ºï¼š

```python
words = raw_datasets["train"][0]["tokens"]
labels = raw_datasets["train"][0]["ner_tags"]
line1 = ""
line2 = ""
for word, label in zip(words, labels):
    full_label = label_names[label]
    max_length = max(len(word), len(full_label))
    line1 += word + " " * (max_length - len(word) + 1)
    line2 += full_label + " " * (max_length - len(full_label) + 1)

print(line1)
print(line2)
```

```python out
'EU    rejects German call to boycott British lamb .'
'B-ORG O       B-MISC O    O  O       B-MISC  O    O'
```

æˆ‘ä»¬è¿˜å¯ä»¥æŸ¥çœ‹è®­ç»ƒé›†ä¸­ç´¢å¼•ä¸º 4 çš„å…ƒç´ ï¼Œå®ƒæ˜¯ä¸€ä¸ªåŒæ—¶åŒ…å« `B-` å’Œ `I-` æ ‡ç­¾çš„ä¾‹å­ï¼š

```python out
'Germany \'s representative to the European Union \'s veterinary committee Werner Zwingmann said on Wednesday consumers should buy sheepmeat from countries other than Britain until the scientific advice was clearer .'
'B-LOC   O  O              O  O   B-ORG    I-ORG O  O          O         B-PER  I-PER     O    O  O         O         O      O   O         O    O         O     O    B-LOC   O     O   O          O      O   O       O'
```

æ­£å¦‚æˆ‘ä»¬åœ¨ä¸Šé¢çš„è¾“å‡ºä¸­æ‰€çœ‹åˆ°çš„ï¼Œè·¨è¶Šä¸¤ä¸ªå•è¯çš„å®ä½“ï¼Œå¦‚â€œEuropean Unionâ€å’Œâ€œWerner Zwingmannâ€ï¼Œæ¨¡å‹æŠŠç¬¬ä¸€ä¸ªå•è¯æ ‡æ³¨ä¸ºäº† B- æ ‡ç­¾ï¼Œä¸ºç¬¬äºŒä¸ªå•è¯æ ‡è®°ä¸ºäº† I- æ ‡ç­¾ã€‚

<Tip>

âœï¸ **è½®åˆ°ä½ äº†ï¼** ä½¿ç”¨è¯æ€§æ ‡æ³¨ ï¼ˆPOSï¼‰æˆ–åˆ†å—ï¼ˆchunkingï¼‰è¯†åˆ«åŒä¸€ä¸ªå¥å­ï¼ŒæŸ¥çœ‹è¾“å‡ºçš„ç»“æœã€‚

</Tip>

### å¤„ç†æ•°æ® [[å¤„ç†æ•°æ®]]

<Youtube id="iY2AZYdZAr0"/>

åƒå¾€å¸¸ä¸€æ ·ï¼Œæˆ‘ä»¬çš„æ–‡æœ¬éœ€è¦è½¬æ¢ä¸º Token IDï¼Œç„¶åæ¨¡å‹æ‰èƒ½ç†è§£å®ƒä»¬ã€‚æ­£å¦‚æˆ‘ä»¬åœ¨ [ç¬¬å…­ç« ](/course/chapter6/) æ‰€å­¦çš„é‚£æ ·ã€‚ä¸è¿‡åœ¨ tokens åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œä¸€ä¸ªå¾ˆå¤§çš„åŒºåˆ«æ˜¯æˆ‘ä»¬æœ‰ pre-tokenized çš„è¾“å…¥ã€‚å¹¸è¿çš„æ˜¯ï¼Œtokenizer API å¯ä»¥å¾ˆå®¹æ˜“åœ°å¤„ç†è¿™ä¸ªé—®é¢˜ï¼›æˆ‘ä»¬åªéœ€è¦é€šè¿‡ä¸€ä¸ªç‰¹æ®Šçš„æ ‡å¿—å‘Šè¯‰ tokenizerã€‚

é¦–å…ˆï¼Œè®©æˆ‘ä»¬åˆ›å»º `tokenizer` å¯¹è±¡ã€‚å¦‚å‰æ‰€è¿°ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ BERT é¢„è®­ç»ƒæ¨¡å‹ï¼Œå› æ­¤æˆ‘ä»¬å°†ä»ä¸‹è½½å¹¶ç¼“å­˜å…³è”çš„ tokenizer å¼€å§‹ï¼š

```python
from transformers import AutoTokenizer

model_checkpoint = "bert-base-cased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
```

ä½ å¯ä»¥æ›´æ¢æŠŠ `model_checkpoint` æ›´æ¢ä¸º [Hub](https://huggingface.co/models) ä¸Šä»»ä½•ä½ å–œæ¬¢çš„å…¶ä»–å‹å·ï¼Œæˆ–ä½¿ç”¨ä½ æœ¬åœ°ä¿å­˜çš„é¢„è®­ç»ƒæ¨¡å‹å’Œ tokenizerã€‚å”¯ä¸€çš„é™åˆ¶æ˜¯ tokenizer éœ€è¦ç”± ğŸ¤— Tokenizers åº“æ”¯æŒï¼Œå¹¶ä¸”æœ‰ä¸€ä¸ªâ€œå¿«é€Ÿâ€ç‰ˆæœ¬å¯ç”¨ã€‚ä½ å¯ä»¥åœ¨ [è¿™å¼ å¤§è¡¨](https://huggingface.co/transformers/#supported-frameworks) ä¸Šçœ‹åˆ°æ‰€æœ‰å¸¦æœ‰å¿«é€Ÿç‰ˆæœ¬çš„æ¶æ„ï¼Œæˆ–è€…æ£€æŸ¥ä½ å¯ä»¥é€šè¿‡æŸ¥çœ‹å®ƒ `is_fast` å±æ€§æ¥æ£€æµ‹æ­£åœ¨ä½¿ç”¨çš„ `tokenizer` å¯¹è±¡æ˜¯å¦ç”± ğŸ¤— Tokenizers æ”¯æŒï¼š

```py
tokenizer.is_fast
```

```python out
True
```

æˆ‘ä»¬å¯ä»¥åƒå¾€å¸¸ä¸€æ ·ä½¿ç”¨æˆ‘ä»¬çš„ `tokenizer` å¯¹é¢„å…ˆåˆ†è¯çš„è¾“å…¥è¿›è¡Œå­è¯åˆ†è¯ï¼Œåªéœ€é¢å¤–æ·»åŠ  `is_split_into_words=True` ï¼š

```py
inputs = tokenizer(raw_datasets["train"][0]["tokens"], is_split_into_words=True)
inputs.tokens()
```

```python out
['[CLS]', 'EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'la', '##mb', '.', '[SEP]']
```

å¦‚æˆ‘ä»¬æ‰€è§ï¼Œtokenizer æ·»åŠ äº†æ¨¡å‹ä½¿ç”¨çš„ç‰¹æ®Š tokensï¼ˆåœ¨å¼€å§‹çš„ `[CLS]` ï¼Œåœ¨ç»“æŸçš„ `[SEP]` ï¼‰ï¼Œå¹¶ä¸”å¤§éƒ¨åˆ†å•è¯ä¿æŒä¸å˜ã€‚ç„¶è€Œï¼Œå•è¯ `lamb` è¢«åˆ†è¯ä¸ºä¸¤ä¸ªå­è¯ï¼Œ `la` å’Œ `##mb` ã€‚è¿™å¯¼è‡´äº†è¾“å…¥å’Œæ ‡ç­¾ä¹‹é—´çš„ä¸åŒ¹é…ï¼šæ ‡ç­¾åˆ—è¡¨åªæœ‰ 9 ä¸ªå…ƒç´ ï¼Œè€Œæˆ‘ä»¬çš„è¾“å…¥ç°åœ¨æœ‰ 12 ä¸ª tokensã€‚è§£å†³ç‰¹æ®Š tokens çš„é—®é¢˜å¾ˆå®¹æ˜“ï¼ˆæˆ‘ä»¬çŸ¥é“å®ƒä»¬åœ¨å¼€å§‹å’Œç»“æŸçš„ä½ç½®ï¼‰ï¼Œä½†æˆ‘ä»¬è¿˜éœ€è¦ç¡®ä¿æˆ‘ä»¬å°†æ‰€æœ‰çš„æ ‡ç­¾ä¸æ­£ç¡®çš„è¯å¯¹é½ã€‚


å¹¸è¿çš„æ˜¯ï¼Œç”±äºæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯å¿«é€Ÿ tokens ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ğŸ¤— Tokenizers è¶…èƒ½åŠ›ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬å¯ä»¥è½»æ¾åœ°å°†æ¯ä¸ª tokenæ˜ å°„åˆ°å…¶ç›¸åº”çš„å•è¯ï¼ˆå¦‚ [ç¬¬å…­ç« ](/course/chapter6/3) ä¸­æ‰€å­¦ï¼‰ï¼š

```py
inputs.word_ids()
```

```python out
[None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 8, None]
```

åªéœ€è¦ä¸€ç‚¹ç‚¹å·¥ä½œï¼Œæˆ‘ä»¬å°±å¯ä»¥æ‰©å±•æˆ‘ä»¬çš„æ ‡ç­¾åˆ—è¡¨æ¥åŒ¹é… tokensã€‚æˆ‘ä»¬å°†æ·»åŠ çš„ç¬¬ä¸€æ¡è§„åˆ™æ˜¯ï¼Œç‰¹æ®Š tokens çš„æ ‡ç­¾æ˜¯ `-100` ã€‚è¿™æ˜¯å› ä¸ºé»˜è®¤æƒ…å†µä¸‹ï¼Œ `-100` ä¼šè¢«æˆ‘ä»¬çš„æŸå¤±å‡½æ•°ï¼ˆäº¤å‰ç†µï¼‰å¿½ç•¥ã€‚ç„¶åï¼Œæ¯ä¸ª token å¾—åˆ°çš„æ ‡ç­¾ä¸å…¶æ‰€åœ¨çš„è¯çš„å¼€å§‹çš„ token ç›¸åŒï¼Œå› ä¸ºå®ƒä»¬æ˜¯åŒä¸€å®ä½“çš„ä¸€éƒ¨åˆ†ã€‚å› æ­¤å¯¹äºè¯å†…éƒ¨ä½†ä¸åœ¨å¼€å§‹ä½ç½®çš„ tokensï¼Œæˆ‘ä»¬å°† `B-` æ›¿æ¢ä¸º `I-` ï¼ˆå› ä¸ºè¯¥ token ä¸æ˜¯å®ä½“çš„å¼€å§‹ï¼‰ï¼š

```python
def align_labels_with_tokens(labels, word_ids):
    new_labels = []
    current_word = None
    for word_id in word_ids:
        if word_id != current_word:
            # æ–°å•è¯çš„å¼€å§‹!
            current_word = word_id
            label = -100 if word_id is None else labels[word_id]
            new_labels.append(label)
        elif word_id is None:
            # ç‰¹æ®Šçš„token
            new_labels.append(-100)
        else:
            # ä¸å‰ä¸€ä¸ª tokens ç±»å‹ç›¸åŒçš„å•è¯
            label = labels[word_id]
            # å¦‚æœæ ‡ç­¾æ˜¯ B-XXX æˆ‘ä»¬å°†å…¶æ›´æ”¹ä¸º I-XXX
            if label % 2 == 1:
                label += 1
            new_labels.append(label)

    return new_labels
```

è®©æˆ‘ä»¬åœ¨æˆ‘ä»¬çš„ç¬¬ä¸€å¥è¯ä¸Šè¯•ä¸€è¯•ï¼š

```py
labels = raw_datasets["train"][0]["ner_tags"]
word_ids = inputs.word_ids()
print(labels)
print(align_labels_with_tokens(labels, word_ids))
```

```python out
[3, 0, 7, 0, 0, 0, 7, 0, 0]
[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100]
```

æ­£å¦‚æˆ‘ä»¬æ‰€çœ‹åˆ°çš„ï¼Œæˆ‘ä»¬çš„å‡½æ•°ä¸ºå¼€å¤´å’Œç»“å°¾çš„ä¸¤ä¸ªç‰¹æ®Š tokens æ·»åŠ äº† `-100` ï¼Œå¹¶ä¸ºåˆ‡åˆ†æˆä¸¤ä¸ª tokens çš„å•è¯æ·»åŠ äº†ä¸€ä¸ªæ–°çš„ `0` ã€‚

<Tip>

âœï¸ **è½®åˆ°ä½ äº†ï¼** æœ‰äº›ç ”ç©¶äººå‘˜æ›´å–œæ¬¢æ¯ä¸ªå•è¯åªåˆ†é…ä¸€ä¸ªæ ‡ç­¾ï¼Œå¯¹è¯¥å•è¯å…¶ä»–éƒ¨åˆ†åˆ†é… `-100` ã€‚è¿™æ˜¯ä¸ºäº†é¿å…é‚£äº›åˆ†è§£æˆè®¸å¤šå­å­è¯çš„é•¿å•è¯å¯¹æŸå¤±ä½œå‡ºè¿‡é‡çš„è´¡çŒ®ã€‚è¯·æŒ‰ç…§è¿™ä¸ªè§„åˆ™ï¼Œæ”¹å˜ä¹‹å‰çš„å‡½æ•°ï¼Œä½¿æ ‡ç­¾ä¸ inputs ID å¯¹é½ã€‚

</Tip>

ä¸ºäº†é¢„å¤„ç†æˆ‘ä»¬çš„æ•´ä¸ªæ•°æ®é›†ï¼Œæˆ‘ä»¬éœ€è¦å¯¹æ‰€æœ‰è¾“å…¥è¿›è¡Œ tokenizeï¼Œå¹¶åœ¨æ‰€æœ‰æ ‡ç­¾ä¸Šåº”ç”¨ `align_labels_with_tokens()` å‡½æ•°ã€‚ä¸ºäº†å……åˆ†åˆ©ç”¨æˆ‘ä»¬å¿«é€Ÿ tokenizer çš„é€Ÿåº¦ï¼Œæœ€å¥½æ˜¯åŒæ—¶å¯¹å¤§é‡æ–‡æœ¬ tokenizeï¼Œæ‰€ä»¥æˆ‘ä»¬å°†ç¼–å†™ä¸€ä¸ªå¤„ç†ä¸€ç»„ç¤ºä¾‹çš„å‡½æ•°ï¼Œå¹¶ä½¿ç”¨å¸¦æœ‰ `batched=True` é€‰é¡¹çš„ `Dataset.map()` æ–¹æ³•ã€‚ä¸æˆ‘ä»¬ä¹‹å‰çš„ç¤ºä¾‹å”¯ä¸€ä¸åŒçš„æ˜¯ï¼Œå½“ tokenizer çš„è¾“å…¥æ˜¯æ–‡æœ¬åˆ—è¡¨ï¼ˆæˆ–ç¤ºä¾‹ä¸­å•è¯çš„åˆ—è¡¨çš„åˆ—è¡¨ï¼‰æ—¶ï¼Œ `word_ids()` å‡½æ•°éœ€è¦è·å–æˆ‘ä»¬ç¤ºä¾‹ä¸­å•è¯ ID çš„ç´¢å¼•ï¼Œæ‰€ä»¥æˆ‘ä»¬ä¹Ÿæ·»åŠ äº†è¿™ä¸ªåŠŸèƒ½ï¼š

```py
def tokenize_and_align_labels(examples):
    tokenized_inputs = tokenizer(
        examples["tokens"], truncation=True, is_split_into_words=True
    )
    all_labels = examples["ner_tags"]
    new_labels = []
    for i, labels in enumerate(all_labels):
        word_ids = tokenized_inputs.word_ids(i)
        new_labels.append(align_labels_with_tokens(labels, word_ids))

    tokenized_inputs["labels"] = new_labels
    return tokenized_inputs
```

æ³¨æ„ï¼Œæˆ‘ä»¬è¿˜æ²¡æœ‰å¡«å……æˆ‘ä»¬çš„è¾“å…¥ï¼Œæˆ‘ä»¬å°†åœ¨ç¨åè¿›è¡Œï¼Œå³åœ¨ä½¿ç”¨æ•°æ®æ•´ç†å™¨åˆ›å»º batch æ—¶ã€‚

æˆ‘ä»¬ç°åœ¨å¯ä»¥ä¸€æ¬¡æ€§å°†æ‰€æœ‰é¢„å¤„ç†åº”ç”¨äºæ•°æ®é›†çš„å…¶ä»–éƒ¨åˆ†ï¼š

```py
tokenized_datasets = raw_datasets.map(
    tokenize_and_align_labels,
    batched=True,
    remove_columns=raw_datasets["train"].column_names,
)
```

æˆ‘ä»¬å·²ç»å®Œæˆäº†æœ€å›°éš¾çš„éƒ¨åˆ†ï¼ç°åœ¨æ•°æ®å·²ç»ç»è¿‡äº†é¢„å¤„ç†ï¼Œå®é™…çš„è®­ç»ƒè¿‡ç¨‹å°†ä¼šä¸æˆ‘ä»¬åœ¨ [ç¬¬ä¸‰ç« ](/course/chapter3) æ‰€åšçš„å¾ˆç›¸ä¼¼ã€‚

{#if fw === 'pt'}

## ä½¿ç”¨ Trainer API å¾®è°ƒæ¨¡å‹ [[ä½¿ç”¨ Trainer API å¾®è°ƒæ¨¡å‹]]

ä½¿ç”¨ `Trainer` çš„å®é™…ä»£ç ä¼šå’Œä»¥å‰ä¸€æ ·ï¼Œå”¯ä¸€çš„å˜åŒ–æ˜¯æ•°æ®å¦‚ä½•æ•´ç†æˆ batch ä»¥åŠè¯„ä¼°è®¡ç®—å‡½æ•°çš„å˜åŒ–ã€‚

{:else}

## ä½¿ç”¨ Keras å¾®è°ƒæ¨¡å‹ [[ä½¿ç”¨ Keras å¾®è°ƒæ¨¡å‹]]

ä½¿ç”¨ Keras çš„å®é™…ä»£ç å°†ä¸ä¹‹å‰éå¸¸ç›¸ä¼¼ï¼›å”¯ä¸€çš„å˜åŒ–æ˜¯æ•°æ®å¦‚ä½•æ•´ç†æˆ batch ä»¥åŠè¯„ä¼°è®¡ç®—å‡½æ•°çš„å˜åŒ–ã€‚

{/if}

### æ•´ç†æ•°æ® [[æ•´ç†æ•°æ®]]

æˆ‘ä»¬ä¸èƒ½åƒ [ç¬¬ä¸‰ç« ](/course/chapter3) é‚£æ ·åªä½¿ç”¨ä¸€ä¸ª `DataCollatorWithPadding` ï¼Œå› ä¸ºé‚£æ ·åªä¼šå¡«å……è¾“å…¥ï¼ˆinputs IDã€æ³¨æ„æ©ç å’Œ tokens ç±»å‹ IDï¼‰ã€‚åœ¨è¿™é‡Œæˆ‘ä»¬åº”è¯¥å¯¹æ ‡ç­¾ä¹Ÿä½¿ç”¨ä¸è¾“å…¥å®Œå…¨ç›¸åŒçš„æ–¹å¼å¡«å……ï¼Œä»¥ä¿è¯å®ƒä»¬çš„å¤§å°ç›¸åŒã€‚æˆ‘ä»¬å°†ä½¿ç”¨ `-100` è¿›è¡Œå¡«å……ï¼Œä»¥ä¾¿åœ¨æŸå¤±è®¡ç®—ä¸­å¿½ç•¥ç›¸åº”çš„é¢„æµ‹ã€‚

è¿™äº›å¯ä»¥ç”± [`DataCollatorForTokenClassification`](https://huggingface.co/transformers/main_classes/data_collator.html#datacollatorfortokenclassification) å®ç°ã€‚å®ƒæ˜¯ä¸€ä¸ªå¸¦æœ‰å¡«å……åŠŸèƒ½çš„æ•°æ®æ•´ç†å™¨ï¼Œä½¿ç”¨æ—¶åªéœ€è¦ä¼ å…¥ç”¨äºé¢„å¤„ç†è¾“å…¥çš„ `tokenizer` ï¼š

{#if fw === 'pt'}

```py
from transformers import DataCollatorForTokenClassification

data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)
```

{:else}

```py
from transformers import DataCollatorForTokenClassification

data_collator = DataCollatorForTokenClassification(
    tokenizer=tokenizer, return_tensors="tf"
)
```

{/if}

ä¸ºäº†åœ¨å‡ ä¸ªæ ·æœ¬ä¸Šæµ‹è¯•è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨è®­ç»ƒé›†ä¸­çš„å‡ ä¸ªç¤ºä¾‹ä¸Šè°ƒç”¨å®ƒï¼š

```py
batch = data_collator([tokenized_datasets["train"][i] for i in range(2)])
batch["labels"]
```

```python out
tensor([[-100,    3,    0,    7,    0,    0,    0,    7,    0,    0,    0, -100],
        [-100,    1,    2, -100, -100, -100, -100, -100, -100, -100, -100, -100]])
```

è®©æˆ‘ä»¬å°†å…¶ä¸æ•°æ®é›†ä¸­ç¬¬ä¸€ä¸ªå’Œç¬¬äºŒä¸ªå…ƒç´ çš„æ ‡ç­¾è¿›è¡Œæ¯”è¾ƒï¼š

```py
for i in range(2):
    print(tokenized_datasets["train"][i]["labels"])
```

```python out
[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, 0, -100]
[-100, 1, 2, -100]
```

{#if fw === 'pt'}

æ­£å¦‚æˆ‘ä»¬æ‰€çœ‹åˆ°çš„ï¼Œç¬¬äºŒç»„æ ‡ç­¾çš„é•¿åº¦å·²ç»ä½¿ç”¨ `-100` å¡«å……åˆ°ä¸ç¬¬ä¸€ç»„æ ‡ç­¾ç›¸åŒã€‚

{:else}

æˆ‘ä»¬çš„æ•°æ®æ•´ç†å™¨å·²å‡†å¤‡å°±ç»ªï¼ç°åœ¨è®©æˆ‘ä»¬ä½¿ç”¨ `to_tf_dataset()` æ–¹æ³•åˆ›å»ºä¸€ä¸ª `tf.data.Dataset` ã€‚ä½ è¿˜å¯ä»¥ä½¿ç”¨ `model.prepare_tf_dataset()` æ¥ä½¿ç”¨æ›´å°‘çš„æ¨¡æ¿ä»£ç æ¥å®Œæˆæ­¤æ“ä½œâ€”â€”ä½ å°†åœ¨æœ¬ç« çš„å…¶ä»–å°èŠ‚ä¸­çœ‹åˆ°è¿™ä¸€ç‚¹ã€‚
```py
tf_train_dataset = tokenized_datasets["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "labels", "token_type_ids"],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=16,
)

tf_eval_dataset = tokenized_datasets["validation"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "labels", "token_type_ids"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=16,
)
```

ä¸‹ä¸€ç«™ï¼šæ¨¡å‹æœ¬èº«ã€‚

{/if}

{#if fw === 'tf'}

### å®šä¹‰æ¨¡å‹

ç”±äºæˆ‘ä»¬æ­£åœ¨ç ”ç©¶ Token åˆ†ç±»é—®é¢˜ï¼Œå› æ­¤æˆ‘ä»¬å°†ä½¿ç”¨ `TFAutoModelForTokenClassification` ç±»ã€‚å®šä¹‰æ­¤æ¨¡å‹æ—¶è¦è®°å¾—ä¼ é€’æˆ‘ä»¬æ ‡ç­¾çš„æ•°é‡ï¼Œæœ€ç®€å•æ–¹æ³•æ˜¯å°†è¯¥æ•°å­—ä¼ é€’ç»™ `num_labels` å‚æ•°ï¼Œä½†æ˜¯å¦‚æœæˆ‘ä»¬æƒ³è¦ä¸€ä¸ªå°±åƒæˆ‘ä»¬åœ¨æœ¬èŠ‚å¼€å¤´çœ‹åˆ°çš„é‚£æ ·çš„æ¨ç†å°éƒ¨ä»¶ï¼Œæœ€å¥½è®¾ç½®æ­£ç¡®çš„æ ‡ç­¾å¯¹åº”å…³ç³»ã€‚

åº”è¯¥ç”±ä¸¤ä¸ªå­—å…¸ `id2label` å’Œ `label2id` æ¥è®¾ç½®å®ƒä»¬ï¼Œè¿™ä¸¤ä¸ªå­—å…¸åŒ…å«ä» ID åˆ°æ ‡ç­¾çš„æ˜ å°„ä»¥åŠåå‘çš„æ˜ å°„ï¼š

```py
id2label = {str(i): label for i, label in enumerate(label_names)}
label2id = {v: k for k, v in id2label.items()}
```

ç°åœ¨æˆ‘ä»¬åªéœ€å°†å®ƒä»¬ä¼ é€’ç»™ `TFAutoModelForTokenClassification.from_pretrained()` æ–¹æ³•ï¼Œå®ƒä»¬å°±ä¼šè¢«ä¿å­˜åœ¨æ¨¡å‹çš„é…ç½®ä¸­ï¼Œç„¶åè¢«æ­£ç¡®åœ°ä¿å­˜å’Œä¸Šä¼ åˆ° Hubï¼š

```py
from transformers import TFAutoModelForTokenClassification

model = TFAutoModelForTokenClassification.from_pretrained(
    model_checkpoint,
    id2label=id2label,
    label2id=label2id,
)
```

å°±åƒæˆ‘ä»¬åœ¨ [ç¬¬ä¸‰ç« ](/course/chapter3) ä¸­å®šä¹‰ `TFAutoModelForTokenClassification` ç±»ä¸€æ · åˆ›å»ºæ¨¡å‹ä¼šå‘å‡ºä¸€ä¸ªè­¦å‘Šï¼Œæç¤ºä¸€äº›æƒé‡æœªè¢«ä½¿ç”¨ï¼ˆæ¥è‡ªé¢„è®­ç»ƒå¤´çš„æƒé‡ï¼‰å’Œä¸€äº›å…¶ä»–æƒé‡è¢«éšæœºåˆå§‹åŒ–ï¼ˆæ¥è‡ªæ–° Token åˆ†ç±»å¤´çš„æƒé‡ï¼‰ï¼Œæˆ‘ä»¬å°†è¦è®­ç»ƒè¿™äº›æƒé‡ã€‚æˆ‘ä»¬å°†åœ¨ä¸€ä¼šå„¿è¿›è¡Œè®­ç»ƒï¼Œä½†é¦–å…ˆè®©æˆ‘ä»¬ç¡®è®¤ä¸€ä¸‹æˆ‘ä»¬çš„æ¨¡å‹æ˜¯å¦å…·æœ‰æ­£ç¡®çš„æ ‡ç­¾æ•°é‡ï¼š

```python
model.config.num_labels
```

```python out
9
```

<Tip warning={true}>

âš ï¸ å¦‚æœä½ çš„æ¨¡å‹çš„æ ‡ç­¾æ•°é‡é”™è¯¯ï¼Œé‚£ä¹ˆåœ¨åé¢è°ƒç”¨ `model.fit()` æ—¶ï¼Œä½ ä¼šå¾—åˆ°ä¸€ä¸ªæ™¦æ¶©çš„é”™è¯¯ã€‚è¿™å¯èƒ½ä¼šä»¤äººçƒ¦æ¼ï¼Œæ‰€ä»¥ç¡®ä¿ä½ åšäº†è¿™ä¸ªæ£€æŸ¥ï¼Œç¡®è®¤ç¡®è®¤ä½ çš„æ ‡ç­¾æ•°é‡æ˜¯æ­£ç¡®ã€‚

</Tip>

### å¾®è°ƒæ¨¡å‹

ç°åœ¨ï¼Œæˆ‘ä»¬å·²å‡†å¤‡å¥½è®­ç»ƒæ¨¡å‹äº†ï¼ä¸è¿‡ï¼Œæˆ‘ä»¬é¦–å…ˆè¦åšä¸¤ä»¶äº‹ï¼šåº”è¯¥ç™»å½•åˆ° Hugging Face å¹¶å®šä¹‰æˆ‘ä»¬çš„è®­ç»ƒè¶…å‚æ•°ã€‚å¦‚æœä½ åœ¨ notebook ä¸Šå·¥ä½œï¼Œæœ‰ä¸€ä¸ªä¾¿æ·å‡½æ•°å¯ä»¥å¸®åŠ©ä½ åšåˆ°è¿™ä¸€ç‚¹ï¼š

```python
from huggingface_hub import notebook_login

notebook_login()
```

è¿™å°†æ˜¾ç¤ºä¸€ä¸ªå°éƒ¨ä»¶ï¼Œä½ å¯ä»¥åœ¨å…¶ä¸­è¾“å…¥ä½ çš„ Hugging Face è´¦å·å’Œå¯†ç ã€‚

å¦‚æœä½ ä¸æ˜¯åœ¨ notebook ä¸Šå·¥ä½œï¼Œåªéœ€åœ¨ç»ˆç«¯ä¸­è¾“å…¥ä»¥ä¸‹è¡Œï¼š

```bash
huggingface-cli login
```

ç™»å½•åï¼Œæˆ‘ä»¬å¯ä»¥å‡†å¤‡ç¼–è¯‘æˆ‘ä»¬æ¨¡å‹æ‰€éœ€è¦çš„æ‰€æœ‰é…ç½®ã€‚ğŸ¤— Transformers æä¾›äº†ä¸€ä¸ªä¾¿æ·çš„ `create_optimizer()` å‡½æ•°ï¼Œå®ƒå°†ç»™ä½ ä¸€ä¸ªå¸¦æœ‰é€‚å½“çš„æƒé‡è¡°å‡å’Œå­¦ä¹ ç‡è¡°å‡è®¾ç½®çš„ `AdamW` ä¼˜åŒ–å™¨ï¼Œä¸å†…ç½®çš„ `Adam` ä¼˜åŒ–å™¨ç›¸ä¼¼ï¼Œä¸è¿‡è¿™ä¸¤ä¸ªä¼˜åŒ–æŠ€å·§éƒ½å°†æé«˜æ¨¡å‹çš„æ€§èƒ½ï¼š

```python
from transformers import create_optimizer
import tensorflow as tf

# åœ¨æ··åˆç²¾åº¦ float16 ä¸­è¿›è¡Œè®­ç»ƒ
tf.keras.mixed_precision.set_global_policy("mixed_float16")

# è®­ç»ƒæ­¥æ•°æ˜¯æ•°æ®é›†ä¸­çš„æ ·æœ¬æ•°é‡ï¼Œé™¤ä»¥ batch å¤§å°ï¼Œç„¶åä¹˜ä»¥æ€»çš„ epoch æ•°ã€‚
# æ³¨æ„è¿™é‡Œçš„ tf_train_dataset æ˜¯ batch å½¢å¼çš„ tf.data.Datasetï¼Œ
# è€Œä¸æ˜¯åŸå§‹çš„ Hugging Face Dataset ï¼Œæ‰€ä»¥ len() æ±‚å–å®ƒçš„é•¿åº¦å·²ç»æ˜¯ num_samples // batch_sizeã€‚
num_epochs = 3
num_train_steps = len(tf_train_dataset) * num_epochs

optimizer, schedule = create_optimizer(
    init_lr=2e-5,
    num_warmup_steps=0,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)
model.compile(optimizer=optimizer)
```

è¯·æ³¨æ„ï¼Œæˆ‘ä»¬æ²¡æœ‰ç»™ `compile()` æä¾› `loss` å‚æ•°ã€‚è¿™æ˜¯å› ä¸ºæ¨¡å‹å®é™…ä¸Šå¯ä»¥å†…éƒ¨è®¡ç®—æŸå¤± â€”â€” å¦‚æœä½ ç¼–è¯‘æ—¶æ²¡æœ‰æä¾›æŸå¤±çš„è®¡ç®—æ–¹æ³•å¹¶åœ¨è¾“å…¥å­—å…¸ä¸­æä¾›æ ‡ç­¾ï¼ˆå°±åƒæˆ‘ä»¬åœ¨æ•°æ®é›†ä¸­æ‰€åšçš„é‚£æ ·ï¼‰ï¼Œé‚£ä¹ˆæ¨¡å‹å°†ä½¿ç”¨å†…éƒ¨é»˜è®¤çš„ `loss` è®¡ç®—æ–¹æ³•è¿›è¡Œè®­ç»ƒï¼Œè¿™å–å†³äºä½ é€‰æ‹©çš„ä»»åŠ¡å’Œæ¨¡å‹ç±»å‹ã€‚

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å®šä¹‰ä¸€ä¸ª `PushToHubCallback` å›è°ƒå‡½æ•°ï¼Œä»¥ä¾¿åœ¨è®­ç»ƒæœŸé—´å°†æ¨¡å‹ä¸Šä¼ åˆ° Hubï¼Œå¹¶ä½¿ç”¨è¯¥å›è°ƒæ¥æ‹Ÿåˆæ¨¡å‹ï¼š

```python
from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(output_dir="bert-finetuned-ner", tokenizer=tokenizer)

model.fit(
    tf_train_dataset,
    validation_data=tf_eval_dataset,
    callbacks=[callback],
    epochs=num_epochs,
)
```

ä½ å¯ä»¥ä½¿ç”¨ `hub_model_id` å‚æ•°æŒ‡å®šä½ æƒ³è¦æ¨é€çš„ä»“åº“çš„å…¨åï¼ˆç‰¹åˆ«æ˜¯ï¼Œå¦‚æœä½ éœ€è¦æ¨é€ç»™æŸä¸ªç»„ç»‡ï¼Œå°±å¿…é¡»ä½¿ç”¨è¿™ä¸ªå‚æ•°ï¼‰ã€‚ä¾‹å¦‚ï¼Œå½“æˆ‘ä»¬å°†æ¨¡å‹æ¨é€åˆ° [`huggingface-course` ç»„ç»‡](https://huggingface.co/huggingface-course) æ—¶ï¼Œæˆ‘ä»¬æ·»åŠ äº† `hub_model_id="huggingface-course/bert-finetuned-ner"` ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œä½¿ç”¨çš„ä»“åº“å°†ä¿å­˜åœ¨ä½ çš„è´¦æˆ·ä¹‹å†…ï¼Œå¹¶ä»¥ä½ è®¾ç½®çš„è¾“å‡ºç›®å½•å‘½åï¼Œä¾‹å¦‚ `"cool_huggingface_user/bert-finetuned-ner"` ã€‚

<Tip>

ğŸ’¡ å¦‚æœä½ ä½¿ç”¨çš„è¾“å‡ºç›®å½•å·²ç»å­˜åœ¨ï¼Œé‚£ä¹ˆå®ƒéœ€è¦æ˜¯ä½ æƒ³æ¨é€åˆ° hub çš„ä»“åº“çš„æœ¬åœ°å…‹éš†ã€‚å¦‚æœä¸æ˜¯ï¼Œä½ åœ¨è°ƒç”¨ `model.fit()` æ—¶å°†æ”¶åˆ°ä¸€ä¸ªé”™è¯¯ï¼Œå¹¶éœ€è¦è®¾ç½®ä¸€ä¸ªæ–°çš„åå­—ã€‚

</Tip>

è¯·æ³¨æ„ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ¯æ¬¡ä¿å­˜æ¨¡å‹æ—¶ï¼ˆè¿™é‡Œæ˜¯æ¯ä¸ª epoochï¼‰ï¼Œå®ƒéƒ½ä¼šåœ¨åå°ä¸Šä¼ åˆ° Hubã€‚è¿™æ ·ï¼Œå¦‚æœ‰å¿…è¦ï¼Œä½ å°†èƒ½å¤Ÿåœ¨å¦ä¸€å°æœºå™¨ä¸Šç»§ç»­ä½ çš„è®­ç»ƒã€‚

åˆ°æ­¤é˜¶æ®µï¼Œä½ å¯ä»¥åœ¨æ¨¡å‹ Hub ä¸Šä½¿ç”¨æ¨ç†å°éƒ¨ä»¶æ¥æµ‹è¯•ä½ çš„æ¨¡å‹å¹¶ä¸ä½ çš„æœ‹å‹åˆ†äº«ã€‚ä½ å·²ç»æˆåŠŸåœ°åœ¨ä¸€ä¸ª tokens åˆ†ç±»ä»»åŠ¡ä¸Šå¾®è°ƒäº†ä¸€ä¸ªæ¨¡å‹â€”â€”æ­å–œä½ ï¼ä½†æˆ‘ä»¬çš„æ¨¡å‹çœŸçš„å¥½å—ï¼Ÿæˆ‘ä»¬åº”è¯¥æ‰¾å‡ºä¸€äº›æŒ‡æ ‡æ¥å¯¹æ¨¡å‹è¿›è¯„ä¼°ã€‚

{/if}

### è¯„ä¼°æŒ‡æ ‡ [[è¯„ä¼°æŒ‡æ ‡]]

{#if fw === 'pt'}

è¦è®© `Trainer` åœ¨æ¯ä¸ªå‘¨æœŸè®¡ç®—ä¸€ä¸ªæŒ‡æ ‡ï¼Œæˆ‘ä»¬éœ€è¦å®šä¹‰ä¸€ä¸ª `compute_metrics()` å‡½æ•°ï¼Œè¯¥å‡½æ•°çš„è¾“å…¥æ˜¯é¢„æµ‹å€¼å’Œæ ‡ç­¾çš„æ•°ç»„ï¼Œå¹¶è¿”å›å¸¦æœ‰æŒ‡æ ‡åç§°å’Œå€¼çš„å­—å…¸ã€‚

ç”¨äºè¯„ä¼° Token åˆ†ç±»é¢„æµ‹çš„ä¼ ç»Ÿæ¡†æ¶æ˜¯ [*seqeval*](https://github.com/chakki-works/seqeval) ã€‚è¦ä½¿ç”¨æ­¤æŒ‡æ ‡ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦å®‰è£… seqeval åº“ï¼š

```py
!pip install seqeval
```

ç„¶åæˆ‘ä»¬å¯ä»¥é€šè¿‡ `evaluate.load()` å‡½æ•°åŠ è½½å®ƒï¼Œå°±åƒæˆ‘ä»¬åœ¨ [ç¬¬ä¸‰ç« ](/course/chapter3) ä¸­æ‰€åšçš„é‚£æ ·ï¼š

{:else}

ç”¨äºè¯„ä¼° Token åˆ†ç±»é¢„æµ‹çš„ä¼ ç»Ÿæ¡†æ¶æ˜¯ [*seqeval*](https://github.com/chakki-works/seqeval) ã€‚è¦ä½¿ç”¨æ­¤æŒ‡æ ‡ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦å®‰è£… seqeval åº“ï¼š

```py
!pip install seqeval
```

ç„¶åæˆ‘ä»¬å¯ä»¥é€šè¿‡ `evaluate.load()` å‡½æ•°åŠ è½½å®ƒï¼Œå°±åƒæˆ‘ä»¬åœ¨ [ç¬¬ä¸‰ç« ](/course/chapter3) ä¸­æ‰€åšçš„é‚£æ ·ï¼š

{/if}

```py
import evaluate

metric = evaluate.load("seqeval")
```

è¿™ä¸ªæŒ‡æ ‡å¹¶ä¸åƒæ ‡å‡†çš„ç²¾åº¦é‚£æ ·ï¼šå®ƒéœ€è¦å­—ç¬¦ä¸²å½¢å¼çš„æ ‡ç­¾åˆ—è¡¨è€Œä¸æ˜¯æ•´æ•°ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦åœ¨å°†å®ƒä»¬ä¼ é€’ç»™æŒ‡æ ‡ä¹‹å‰è§£ç é¢„æµ‹å€¼å’Œæ ‡ç­¾ã€‚è®©æˆ‘ä»¬çœ‹çœ‹å®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å°†è·å–æˆ‘ä»¬ç¬¬ä¸€ä¸ªè®­ç»ƒæ ·æœ¬çš„æ ‡ç­¾ï¼š

```py
labels = raw_datasets["train"][0]["ner_tags"]
labels = [label_names[i] for i in labels]
labels
```

```python out
['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']
```

ç„¶åæˆ‘ä»¬å¯ä»¥é€šè¿‡æ›´æ”¹ç´¢å¼• 2 å¤„çš„å€¼æ¥ä¸ºè¿™äº›æ ‡ç­¾åˆ›å»ºå‡çš„é¢„æµ‹å€¼ï¼š

```py
predictions = labels.copy()
predictions[2] = "O"
metric.compute(predictions=[predictions], references=[labels])
```

è¯·æ³¨æ„ï¼Œè¯¥æŒ‡æ ‡çš„è¾“å…¥æ˜¯é¢„æµ‹åˆ—è¡¨ï¼ˆä¸ä»…ä»…æ˜¯ä¸€ä¸ªï¼‰å’Œæ ‡ç­¾åˆ—è¡¨ã€‚è¿™æ˜¯è¾“å‡ºï¼š

```python out
{'MISC': {'precision': 1.0, 'recall': 0.5, 'f1': 0.67, 'number': 2},
 'ORG': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},
 'overall_precision': 1.0,
 'overall_recall': 0.67,
 'overall_f1': 0.8,
 'overall_accuracy': 0.89}
```

{#if fw === 'pt'}

å®ƒè¿”å›å¾ˆå¤šä¿¡æ¯ï¼æˆ‘ä»¬è·å¾—æ¯ä¸ªå•ç‹¬å®ä½“ä»¥åŠæ•´ä½“çš„å‡†ç¡®ç‡ã€å¬å›ç‡å’Œ F1 åˆ†æ•°ã€‚å¯¹äºæˆ‘ä»¬çš„åº¦é‡è®¡ç®—ï¼Œæˆ‘ä»¬å°†åªä¿ç•™æ€»åˆ†ï¼Œä½†æ˜¯ä½ å¯ä»¥è‡ªç”±åœ°è°ƒæ•´ `compute_metrics()` å‡½æ•°è¿”å›ä½ æƒ³è¦æŸ¥çœ‹çš„æ‰€æœ‰æŒ‡æ ‡ã€‚
`compute_metrics()` å‡½æ•°é¦–å…ˆå– logits çš„ argmaxï¼Œå°†å®ƒä»¬è½¬æ¢ä¸ºé¢„æµ‹å€¼ï¼ˆé€šå¸¸æƒ…å†µä¸‹ï¼Œlogits å’Œæ¦‚ç‡çš„é¡ºåºæ˜¯ç›¸åŒï¼Œæ‰€ä»¥æˆ‘ä»¬ä¸éœ€è¦ä½¿ç”¨ softmaxï¼‰ã€‚ç„¶åæˆ‘ä»¬éœ€è¦å°†æ ‡ç­¾å’Œé¢„æµ‹å€¼éƒ½ä»æ•´æ•°è½¬æ¢ä¸ºå­—ç¬¦ä¸²ã€‚æˆ‘ä»¬åˆ é™¤æ‰€æœ‰æ ‡ç­¾ä¸º `-100` çš„å€¼ï¼Œç„¶åå°†ç»“æœä¼ é€’ç»™ `metric.compute()` æ–¹æ³•ï¼š

```py
import numpy as np


def compute_metrics(eval_preds):
    logits, labels = eval_preds
    predictions = np.argmax(logits, axis=-1)

    # åˆ é™¤å¿½ç•¥çš„ç´¢å¼•(ç‰¹æ®Š tokens )å¹¶è½¬æ¢ä¸ºæ ‡ç­¾
    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]
    true_predictions = [
        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)
    return {
        "precision": all_metrics["overall_precision"],
        "recall": all_metrics["overall_recall"],
        "f1": all_metrics["overall_f1"],
        "accuracy": all_metrics["overall_accuracy"],
    }
```

ç°åœ¨å·²ç»å®Œæˆäº†ï¼Œæˆ‘ä»¬å‡ ä¹å¯ä»¥å¼€å§‹å®šä¹‰æˆ‘ä»¬çš„ `Trainer` äº†ã€‚æˆ‘ä»¬åªéœ€è¦ä¸€ä¸ª `model` å¾®è°ƒï¼

{:else}

å®ƒè¿”å›äº†å¤§é‡çš„ä¿¡æ¯ï¼æˆ‘ä»¬è·å¾—æ¯ä¸ªå•ç‹¬å®ä½“ä»¥åŠæ•´ä½“çš„å‡†ç¡®ç‡ã€å¬å›ç‡å’Œ F1 åˆ†æ•°ã€‚ç°åœ¨ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å¦‚æœæˆ‘ä»¬å°è¯•ä½¿ç”¨æˆ‘ä»¬å®é™…çš„æ¨¡å‹é¢„æµ‹æ¥è®¡ç®—ä¸€äº›çœŸå®çš„åˆ†æ•°ä¼šå‘ç”Ÿä»€ä¹ˆã€‚

TensorFlow ä¸å–œæ¬¢æŠŠæˆ‘ä»¬çš„é¢„æµ‹æ‹¼æ¥åœ¨ä¸€èµ·ï¼Œå› ä¸ºè¿™æ ·ä¼šå¯¼è‡´åºåˆ—é•¿åº¦ä¸ç»Ÿä¸€ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬ä¸èƒ½ä»…ä»…ç›´æ¥ä½¿ç”¨ `model.predict()` â€”â€” ä½†è¿™å¹¶ä¸èƒ½é˜»æ­¢æˆ‘ä»¬ã€‚æˆ‘ä»¬å°†é€æ‰¹è·å–ä¸€äº›é¢„æµ‹å¹¶åœ¨è¿›è¡Œçš„è¿‡ç¨‹ä¸­å°†å®ƒä»¬æ‹¼æ¥æˆä¸€ä¸ªå¤§çš„åˆ—è¡¨ï¼Œåˆ é™¤è¡¨ç¤º masking/padding çš„ `-100`  tokens ï¼Œç„¶ååœ¨æœ€åçš„åˆ—è¡¨ä¸Šè®¡ç®—åº¦é‡å€¼ï¼š

```py
import numpy as np

all_predictions = []
all_labels = []
for batch in tf_eval_dataset:
    logits = model.predict_on_batch(batch)["logits"]
    labels = batch["labels"]
    predictions = np.argmax(logits, axis=-1)
    for prediction, label in zip(predictions, labels):
        for predicted_idx, label_idx in zip(prediction, label):
            if label_idx == -100:
                continue
            all_predictions.append(label_names[predicted_idx])
            all_labels.append(label_names[label_idx])
metric.compute(predictions=[all_predictions], references=[all_labels])
```


```python out
{'LOC': {'precision': 0.91, 'recall': 0.92, 'f1': 0.91, 'number': 1668},
 'MISC': {'precision': 0.70, 'recall': 0.79, 'f1': 0.74, 'number': 702},
 'ORG': {'precision': 0.85, 'recall': 0.90, 'f1': 0.88, 'number': 1661},
 'PER': {'precision': 0.95, 'recall': 0.95, 'f1': 0.95, 'number': 1617},
 'overall_precision': 0.87,
 'overall_recall': 0.91,
 'overall_f1': 0.89,
 'overall_accuracy': 0.97}
```

ä¸æˆ‘ä»¬çš„æ¨¡å‹ç›¸æ¯”ï¼Œä½ çš„æ¨¡å‹çš„è¡¨ç°å¦‚ä½•ï¼Ÿå¦‚æœä½ è·å¾—ç±»ä¼¼çš„æ•°å­—ï¼Œé‚£ä¹ˆä½ çš„è®­ç»ƒå°±æˆåŠŸäº†ï¼

{/if}

{#if fw === 'pt'}

### å®šä¹‰æ¨¡å‹

ç”±äºæˆ‘ä»¬æ­£åœ¨ç ”ç©¶ Token åˆ†ç±»é—®é¢˜ï¼Œå› æ­¤æˆ‘ä»¬å°†ä½¿ç”¨ `AutoModelForTokenClassification` ç±»ã€‚å®šä¹‰æ­¤æ¨¡å‹æ—¶è¦è®°å¾—ä¼ é€’æˆ‘ä»¬æ ‡ç­¾çš„æ•°é‡ï¼Œæœ€ç®€å•æ–¹æ³•æ˜¯å°†è¯¥æ•°å­—ä¼ é€’ç»™ `num_labels` å‚æ•°ï¼Œä½†æ˜¯å¦‚æœæˆ‘ä»¬æƒ³è¦ä¸€ä¸ªå°±åƒæˆ‘ä»¬åœ¨æœ¬èŠ‚å¼€å¤´çœ‹åˆ°çš„é‚£æ ·çš„æ¨ç†å°éƒ¨ä»¶ï¼Œæœ€å¥½è®¾ç½®æ­£ç¡®çš„æ ‡ç­¾å¯¹åº”å…³ç³»ã€‚

åº”è¯¥ç”±ä¸¤ä¸ªå­—å…¸ `id2label` å’Œ `label2id` æ¥è®¾ç½®å®ƒä»¬ï¼Œè¿™ä¸¤ä¸ªå­—å…¸åŒ…å«ä» ID åˆ°æ ‡ç­¾çš„æ˜ å°„ä»¥åŠåå‘çš„æ˜ å°„ï¼š

```py
id2label = {str(i): label for i, label in enumerate(label_names)}
label2id = {v: k for k, v in id2label.items()}
```

ç°åœ¨æˆ‘ä»¬åªéœ€å°†å®ƒä»¬ä¼ é€’ç»™ `AutoModelForTokenClassification.from_pretrained()` æ–¹æ³•ï¼Œå®ƒä»¬å°±ä¼šè¢«ä¿å­˜åœ¨æ¨¡å‹çš„é…ç½®ä¸­ï¼Œç„¶åè¢«æ­£ç¡®åœ°ä¿å­˜å’Œä¸Šä¼ åˆ° Hubï¼š

```py
from transformers import AutoModelForTokenClassification

model = AutoModelForTokenClassification.from_pretrained(
    model_checkpoint,
    id2label=id2label,
    label2id=label2id,
)
```

å°±åƒæˆ‘ä»¬åœ¨ [ç¬¬ä¸‰ç« ](/course/chapter3) ä¸­å®šä¹‰ `AutoModelForSequenceClassification` ç±»ä¸€æ · åˆ›å»ºæ¨¡å‹ä¼šå‘å‡ºä¸€ä¸ªè­¦å‘Šï¼Œæç¤ºä¸€äº›æƒé‡æœªè¢«ä½¿ç”¨ï¼ˆæ¥è‡ªé¢„è®­ç»ƒå¤´çš„æƒé‡ï¼‰å’Œä¸€äº›å…¶ä»–æƒé‡è¢«éšæœºåˆå§‹åŒ–ï¼ˆæ¥è‡ªæ–° Token åˆ†ç±»å¤´çš„æƒé‡ï¼‰ï¼Œæˆ‘ä»¬å°†è¦è®­ç»ƒè¿™äº›æƒé‡ã€‚æˆ‘ä»¬å°†åœ¨ä¸€ä¼šå„¿è¿›è¡Œè®­ç»ƒï¼Œä½†é¦–å…ˆè®©æˆ‘ä»¬ç¡®è®¤ä¸€ä¸‹æˆ‘ä»¬çš„æ¨¡å‹æ˜¯å¦å…·æœ‰æ­£ç¡®çš„æ ‡ç­¾æ•°é‡ï¼š

```python
model.config.num_labels
```

```python out
9
```

<Tip warning={true}>

âš ï¸ å¦‚æœä½ çš„æ¨¡å‹çš„æ ‡ç­¾æ•°é‡é”™è¯¯ï¼Œé‚£ä¹ˆåœ¨åé¢è°ƒç”¨ `Trainer.train()` æ—¶ï¼Œä½ ä¼šå¾—åˆ°ä¸€ä¸ªæ™¦æ¶©çš„é”™è¯¯ï¼ˆç±»ä¼¼äºâ€œCUDA errorï¼šdevice-side assert triggeredâ€ï¼‰ã€‚è¿™å¯èƒ½ä¼šä»¤äººçƒ¦æ¼ï¼Œæ‰€ä»¥ç¡®ä¿ä½ åšäº†è¿™ä¸ªæ£€æŸ¥ï¼Œç¡®è®¤ä½ çš„æ ‡ç­¾æ•°é‡æ˜¯æ­£ç¡®ã€‚

</Tip>

### å¾®è°ƒæ¨¡å‹

æˆ‘ä»¬ç°åœ¨å‡†å¤‡å¥½è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹äº†ï¼åœ¨å®šä¹‰æˆ‘ä»¬çš„ `Trainer` ä¹‹å‰ï¼Œæˆ‘ä»¬åªéœ€è¦åšæœ€åä¸¤ä»¶äº‹ï¼šç™»å½• Hugging Face å¹¶å®šä¹‰æˆ‘ä»¬çš„è®­ç»ƒå‚æ•°ã€‚å¦‚æœä½ åœ¨ notebook ä¸Šå·¥ä½œï¼Œæœ‰ä¸€ä¸ªæ–¹ä¾¿çš„åŠŸèƒ½å¯ä»¥å¸®åŠ©ä½ ï¼š

```python
from huggingface_hub import notebook_login

notebook_login()
```
ç°åœ¨ï¼Œæˆ‘ä»¬å·²å‡†å¤‡å¥½è®­ç»ƒæ¨¡å‹äº†ï¼ä¸è¿‡ï¼Œæˆ‘ä»¬é¦–å…ˆè¦åšä¸¤ä»¶äº‹ï¼šåº”è¯¥ç™»å½•åˆ° Hugging Face å¹¶å®šä¹‰æˆ‘ä»¬çš„è®­ç»ƒè¶…å‚æ•°ã€‚å¦‚æœä½ åœ¨ notebook ä¸Šå·¥ä½œï¼Œæœ‰ä¸€ä¸ªä¾¿åˆ©å‡½æ•°å¯ä»¥å¸®åŠ©ä½ åšåˆ°è¿™ä¸€ç‚¹ï¼š

```bash
huggingface-cli login
```

ç™»å½•åï¼Œæˆ‘ä»¬å°±å¯ä»¥å®šä¹‰æˆ‘ä»¬çš„ `TrainingArguments` ï¼š

```python
from transformers import TrainingArguments

args = TrainingArguments(
    "bert-finetuned-ner",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
    push_to_hub=True,
)
```

ä½ å·²ç»å¯¹å¤§å¤šæ•°å†…å®¹æœ‰æ‰€äº†è§£äº†ï¼šæˆ‘ä»¬è®¾ç½®äº†ä¸€äº›è¶…å‚æ•°ï¼ˆå¦‚å­¦ä¹ ç‡ã€è®­ç»ƒçš„è½®æ•°å’Œæƒé‡è¡°å‡ï¼‰ï¼Œå¹¶æŒ‡å®š `push_to_hub=True` ï¼Œä»¥è¡¨ç¤ºæˆ‘ä»¬å¸Œæœ›åœ¨æ¯ä¸ªè®­ç»ƒè½®æ¬¡ç»“æŸæ—¶ä¿å­˜å¹¶è¯„ä¼°æ¨¡å‹ï¼Œå¹¶å¸Œæœ›å°†ç»“æœä¸Šä¼ åˆ°æ¨¡å‹ä¸­å¿ƒã€‚æ³¨æ„ï¼Œä½ å¯ä»¥é€šè¿‡ `hub_model_id` å‚æ•°æŒ‡å®šä½ æƒ³æ¨é€çš„ä»“åº“çš„åç§°ï¼ˆç‰¹åˆ«æ˜¯ï¼Œå¦‚æœä½ éœ€è¦æ¨é€ç»™æŸä¸ªç»„ç»‡ï¼Œå°±å¿…é¡»ä½¿ç”¨è¿™ä¸ªå‚æ•°ï¼‰ã€‚ä¾‹å¦‚ï¼Œå½“æˆ‘ä»¬å°†æ¨¡å‹æ¨é€åˆ° [`huggingface-course` ç»„ç»‡](https://huggingface.co/huggingface-course) æ—¶ï¼Œæˆ‘ä»¬åœ¨ `TrainingArguments` ä¸­æ·»åŠ äº† `hub_model_id="huggingface-course/bert-finetuned-ner"` ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œä½¿ç”¨çš„ä»“åº“å°†ä¿å­˜åœ¨ä½ çš„è´¦æˆ·ä¹‹å†…ï¼Œå¹¶ä»¥ä½ è®¾ç½®çš„è¾“å‡ºç›®å½•å‘½åï¼Œæ‰€ä»¥åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œä»“åº“çš„åœ°å€æ˜¯ `"sgugger/bert-finetuned-ner"` ã€‚

<Tip>

ğŸ’¡ å¦‚æœä½ ä½¿ç”¨çš„è¾“å‡ºç›®å½•å·²ç»å­˜åœ¨ï¼Œé‚£ä¹ˆå®ƒéœ€è¦æ˜¯ä½ æƒ³æ¨é€åˆ° hub çš„ä»“åº“çš„æœ¬åœ°å…‹éš†ã€‚å¦‚æœä¸æ˜¯ï¼Œä½ å°†åœ¨å£°æ˜ `Trainer` æ—¶é‡åˆ°é”™è¯¯ï¼Œå¹¶éœ€è¦è®¾ç½®ä¸€ä¸ªæ–°çš„åå­—ã€‚

</Tip>

æœ€åï¼Œæˆ‘ä»¬å°†æ‰€æœ‰å†…å®¹ä¼ é€’ç»™ `Trainer` å¹¶å¯åŠ¨è®­ç»ƒï¼š

```python
from transformers import Trainer

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    tokenizer=tokenizer,
)
trainer.train()
```

è¯·æ³¨æ„ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ¯æ¬¡ä¿å­˜æ¨¡å‹æ—¶ï¼ˆè¿™é‡Œæ˜¯æ¯ä¸ª epoochï¼‰ï¼Œå®ƒéƒ½ä¼šåœ¨åå°ä¸Šä¼ åˆ° Hubã€‚è¿™æ ·ï¼Œå¦‚æœ‰å¿…è¦ï¼Œä½ å°†èƒ½å¤Ÿåœ¨å¦ä¸€å°æœºå™¨ä¸Šç»§ç»­ä½ çš„è®­ç»ƒã€‚

è®­ç»ƒå®Œæˆåï¼Œæˆ‘ä»¬ä½¿ç”¨ `push_to_hub()` ä¸Šä¼ æ¨¡å‹çš„æœ€æ–°ç‰ˆæœ¬

```py
trainer.push_to_hub(commit_message="Training complete")
```

å¦‚æœä½ æƒ³æ£€æŸ¥ä¸€ä¸‹æ˜¯å¦ä¸Šä¼ æˆåŠŸï¼Œè¿™ä¸ªå‘½ä»¤ä¼šè¿”å›åˆšåˆšæ‰§è¡Œçš„æäº¤çš„ URLï¼š

```python out
'https://huggingface.co/sgugger/bert-finetuned-ner/commit/26ab21e5b1568f9afeccdaed2d8715f571d786ed'
```

åŒæ—¶ `Trainer` è¿˜åˆ›å»ºå¹¶ä¸Šä¼ äº†ä¸€å¼ åŒ…å«æ‰€æœ‰è¯„ä¼°ç»“æœçš„æ¨¡å‹å¡ã€‚åˆ°æ­¤é˜¶æ®µï¼Œä½ å¯ä»¥åœ¨æ¨¡å‹ Hub ä¸Šä½¿ç”¨æ¨ç†å°éƒ¨ä»¶æ¥æµ‹è¯•ä½ çš„æ¨¡å‹å¹¶ä¸ä½ çš„æœ‹å‹åˆ†äº«ã€‚ä½ å·²ç»æˆåŠŸåœ°åœ¨ä¸€ä¸ª tokens åˆ†ç±»ä»»åŠ¡ä¸Šå¾®è°ƒäº†ä¸€ä¸ªæ¨¡å‹â€”â€”æ­å–œä½ ï¼

å¦‚æœä½ æƒ³æ›´æ·±å…¥åœ°äº†è§£è®­ç»ƒå¾ªç¯ï¼Œæˆ‘ä»¬ç°åœ¨å°†å‘ä½ å±•ç¤ºå¦‚ä½•ä½¿ç”¨ ğŸ¤— Accelerate åšåŒæ ·çš„äº‹æƒ…ã€‚

## è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ [[è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯]]

ç°åœ¨æˆ‘ä»¬æ¥çœ‹çœ‹å®Œæ•´çš„è®­ç»ƒå¾ªç¯ï¼Œè¿™æ ·ä½ å°±å¯ä»¥è½»æ¾åœ°å®šåˆ¶ä½ éœ€è¦çš„éƒ¨åˆ†ã€‚å®ƒä¸æˆ‘ä»¬åœ¨ [ç¬¬ä¸‰ç« ](/course/chapter3/4) ä¸­æ‰€åšçš„å†…å®¹å¾ˆç›¸ä¼¼ï¼Œä½†å¯¹è¯„ä¼°éƒ¨åˆ†æœ‰ä¸€äº›æ”¹åŠ¨ã€‚

### åšå¥½è®­ç»ƒå‰çš„å‡†å¤‡ [[åšå¥½è®­ç»ƒå‰çš„å‡†å¤‡]]

é¦–å…ˆæˆ‘ä»¬éœ€è¦ä¸ºæˆ‘ä»¬çš„æ•°æ®é›†æ„å»º `DataLoader` ã€‚æˆ‘ä»¬å°† `data_collator` è¾“å…¥ `collate_fn` å‚æ•°å¹¶æ‰“ä¹±è®­ç»ƒé›†ï¼Œä½†ä¸æ‰“ä¹±éªŒè¯é›†ï¼š

```py
from torch.utils.data import DataLoader

train_dataloader = DataLoader(
    tokenized_datasets["train"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=8,
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], collate_fn=data_collator, batch_size=8
)
```

æ¥ä¸‹æ¥æˆ‘ä»¬é‡æ–°å®ä¾‹åŒ–æˆ‘ä»¬çš„æ¨¡å‹ï¼Œä»¥ç¡®ä¿æˆ‘ä»¬ä¸æ˜¯ç»§ç»­ä¹‹å‰çš„å¾®è°ƒï¼Œè€Œæ˜¯é‡æ–°å¼€å§‹ä» BERT é¢„è®­ç»ƒæ¨¡å‹ï¼š

```py
model = AutoModelForTokenClassification.from_pretrained(
    model_checkpoint,
    id2label=id2label,
    label2id=label2id,
)
```

ç„¶åæˆ‘ä»¬éœ€è¦ä¸€ä¸ªä¼˜åŒ–å™¨ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ç»å…¸ `AdamW` ï¼Œå®ƒç±»ä¼¼äº `Adam` ï¼Œä½†åœ¨æƒé‡è¡°å‡çš„æ–¹å¼ä¸Šè¿›è¡Œäº†æ”¹è¿›ï¼š

```py
from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr=2e-5)
```

Once we have all those objects, we can send them to the `accelerator.prepare()` method:

```py
from accelerate import Accelerator

accelerator = Accelerator()
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)
```

<Tip>

ğŸš¨ å¦‚æœä½ æ­£åœ¨ TPU ä¸Šè®­ç»ƒï¼Œä½ éœ€è¦å°†ä¸Šé¢å•å…ƒæ ¼å¼€å§‹çš„æ‰€æœ‰ä»£ç ç§»åŠ¨åˆ°ä¸€ä¸ªä¸“é—¨çš„è®­ç»ƒå‡½æ•°ä¸­ã€‚æ›´å¤šè¯¦æƒ…è¯·è§ [ç¬¬ä¸‰ç« ](/course/chapter3) ã€‚

</Tip>

ç°åœ¨æˆ‘ä»¬å·²ç»å°†æˆ‘ä»¬çš„ `train_dataloader` ä¼ é€’ç»™äº† `accelerator.prepare()` ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `len()` æ¥è®¡ç®—è®­ç»ƒæ­¥éª¤çš„æ•°é‡ã€‚è¯·è®°ä½ï¼Œæˆ‘ä»¬åº”è¯¥åœ¨å‡†å¤‡å¥½ `dataloader` åå†ä½¿ç”¨ `len()` ï¼Œå› ä¸ºæ”¹åŠ¨ `dataloader` ä¼šæ”¹å˜å…¶é•¿åº¦ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªä»å­¦ä¹ ç‡è¡°å‡åˆ° 0 çš„ç»å…¸çº¿æ€§å­¦ä¹ ç‡è°ƒåº¦ï¼š

```py
from transformers import get_scheduler

num_train_epochs = 3
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
```

æœ€åï¼Œä¸ºäº†å°†æˆ‘ä»¬çš„æ¨¡å‹æ¨é€åˆ° Hubï¼Œæˆ‘ä»¬éœ€è¦åœ¨ä¸€ä¸ªå·¥ä½œæ–‡ä»¶å¤¹ä¸­åˆ›å»ºä¸€ä¸ª `Repository` å¯¹è±¡ã€‚å¦‚æœä½ è¿˜æ²¡æœ‰ç™»å½•çš„è¯ï¼Œé¦–å…ˆéœ€è¦ç™»å½•åˆ° Hugging Faceï¼Œæˆ‘ä»¬å°†æ ¹æ®æˆ‘ä»¬æƒ³ç»™æˆ‘ä»¬çš„æ¨¡å‹çš„æ¨¡å‹ ID æ¥ç¡®å®šä»“åº“åç§°ï¼ˆä½ å¯ä»¥ä½¿ç”¨ä½ å–œæ¬¢çš„åå­—æ›¿æ¢ `repo_name` ï¼›å®ƒåªéœ€è¦åŒ…å«ä½ çš„ç”¨æˆ·åï¼Œå¯ä»¥ä½¿ç”¨ `get_full_repo_name()` å‡½æ•°çš„æŸ¥çœ‹ç›®å‰çš„ repo_nameï¼‰ï¼š

```py
from huggingface_hub import Repository, get_full_repo_name

model_name = "bert-finetuned-ner-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name
```

```python out
'sgugger/bert-finetuned-ner-accelerate'
```

ç„¶åæˆ‘ä»¬å¯ä»¥å°†è¯¥ä»“åº“å…‹éš†åˆ°æœ¬åœ°æ–‡ä»¶å¤¹ä¸­ã€‚å¦‚æœå®ƒå·²ç»å­˜åœ¨ï¼Œè¿™ä¸ªæœ¬åœ°æ–‡ä»¶å¤¹åº”è¯¥æ˜¯æˆ‘ä»¬æ­£åœ¨ä½¿ç”¨çš„ä»“åº“çš„ç°æœ‰å…‹éš†ï¼š

```py
output_dir = "bert-finetuned-ner-accelerate"
repo = Repository(output_dir, clone_from=repo_name)
```

æˆ‘ä»¬ç°åœ¨å¯ä»¥é€šè¿‡è°ƒç”¨ `repo.push_to_hub()` æ–¹æ³•ä¸Šä¼ ä¿å­˜åœ¨ `output_dir` ä¸­çš„æ‰€æœ‰å†…å®¹ã€‚è¿™å°†å¸®åŠ©æˆ‘ä»¬åœ¨æ¯ä¸ªè®­ç»ƒå‘¨æœŸç»“æŸæ—¶ä¸Šä¼ ä¸­é—´æ¨¡å‹ã€‚

### è®­ç»ƒå¾ªç¯ [[è®­ç»ƒå¾ªç¯]]

æˆ‘ä»¬ç°åœ¨å‡†å¤‡ç¼–å†™å®Œæ•´çš„è®­ç»ƒå¾ªç¯ã€‚ä¸ºäº†ç®€åŒ–å…¶è¯„ä¼°éƒ¨åˆ†ï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ª `postprocess()` å‡½æ•°ï¼Œè¯¥å‡½æ•°æ¥æ”¶æ¨¡å‹çš„é¢„æµ‹å’Œæ ‡ç­¾ï¼Œå¹¶å°†å®ƒä»¬è½¬æ¢ä¸ºå­—ç¬¦ä¸²åˆ—è¡¨ï¼Œä¹Ÿå°±æ˜¯æˆ‘ä»¬çš„ `metric` ï¼ˆè¯„ä¼°å‡½æ•°ï¼‰å¯¹è±¡éœ€è¦çš„è¾“å…¥æ ¼å¼ã€‚

```py
def postprocess(predictions, labels):
    predictions = predictions.detach().cpu().clone().numpy()
    labels = labels.detach().cpu().clone().numpy()

    # åˆ é™¤å¿½ç•¥çš„ç´¢å¼•(ç‰¹æ®Š tokens )å¹¶è½¬æ¢ä¸ºæ ‡ç­¾
    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]
    true_predictions = [
        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    return true_labels, true_predictions
```

ç„¶åæˆ‘ä»¬å¯ä»¥ç¼–å†™è®­ç»ƒå¾ªç¯ã€‚åœ¨å®šä¹‰ä¸€ä¸ªè¿›åº¦æ¡æ¥è·Ÿè¸ªè®­ç»ƒçš„è¿›è¡Œåï¼Œå¾ªç¯åˆ†ä¸ºä¸‰ä¸ªéƒ¨åˆ†ï¼š

- è®­ç»ƒæœ¬èº«ï¼Œè¿™æ˜¯ç»å…¸çš„è¿­ä»£è¿‡ç¨‹ï¼Œå³åœ¨ `train_dataloader` ä¸Šè¿›è¡Œè¿­ä»£ï¼Œåœ¨æ¨¡å‹ä¸Šå‰å‘ä¼ æ’­ï¼Œç„¶ååå‘ä¼ é€’å’Œä¼˜åŒ–å‚æ•°
- è¯„ä¼°ï¼Œåœ¨è·å–æ¨¡å‹åœ¨ä¸€ä¸ª batch ä¸Šçš„è¾“å‡ºä¹‹åï¼Œè¿™é‡Œæœ‰ä¸€ä¸ªéœ€è¦æ³¨æ„çš„åœ°æ–¹ï¼šç”±äºä¸¤ä¸ªè¿›ç¨‹å¯èƒ½å·²å°†è¾“å…¥å’Œæ ‡ç­¾å¡«å……åˆ°ä¸åŒçš„å½¢çŠ¶ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨ `accelerator.pad_across_processes()` ä½¿é¢„æµ‹å’Œæ ‡ç­¾åœ¨è°ƒç”¨ `gather()` æ–¹æ³•ä¹‹å‰å…·æœ‰ç›¸åŒçš„å½¢çŠ¶ã€‚å¦‚æœæˆ‘ä»¬ä¸è¿™æ ·åšï¼Œè¯„ä¼°å¾ªç¯å°†ä¼šå‡ºé”™æˆ–æ— é™æœŸæŒ‚èµ·ã€‚ç„¶åæˆ‘ä»¬å°†ç»“æœå‘é€åˆ° `metric.add_batch()` ï¼Œå¹¶åœ¨è¯„ä¼°å¾ªç¯ç»“æŸæ—¶è°ƒç”¨ `metric.compute()` ã€‚
- ä¿å­˜å’Œä¸Šä¼ ï¼Œé¦–å…ˆä¿å­˜æ¨¡å‹å’Œ tokenizer ç„¶åè°ƒç”¨ `repo.push_to_hub()` ã€‚æ³¨æ„ï¼Œæˆ‘ä»¬ä½¿ç”¨å‚æ•° `blocking=False` æ¥å‘Šè¯‰ ğŸ¤— Hub åº“åœ¨ä¸€ä¸ªå¼‚æ­¥è¿›ç¨‹ä¸­æ¨é€ã€‚è¿™æ ·ï¼Œåœ¨è®­ç»ƒæ—¶ï¼Œè¿™ä¸ªæŒ‡ä»¤åœ¨åå°å°†æ¨¡å‹å’Œ tokenizer æ¨é€åˆ° hubã€‚

ä»¥ä¸‹æ˜¯å®Œæ•´çš„è®­ç»ƒå¾ªç¯ä»£ç ï¼š

```py
from tqdm.auto import tqdm
import torch

progress_bar = tqdm(range(num_training_steps))

for epoch in range(num_train_epochs):
    # è®­ç»ƒ
    model.train()
    for batch in train_dataloader:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

    # è¯„ä¼°
    model.eval()
    for batch in eval_dataloader:
        with torch.no_grad():
            outputs = model(**batch)

        predictions = outputs.logits.argmax(dim=-1)
        labels = batch["labels"]

        # å¡«å……æ¨¡å‹çš„é¢„æµ‹å’Œæ ‡ç­¾åæ‰èƒ½è°ƒç”¨ gathere()
        predictions = accelerator.pad_across_processes(predictions, dim=1, pad_index=-100)
        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)

        predictions_gathered = accelerator.gather(predictions)
        labels_gathered = accelerator.gather(labels)

        true_predictions, true_labels = postprocess(predictions_gathered, labels_gathered)
        metric.add_batch(predictions=true_predictions, references=true_labels)

    results = metric.compute()
    print(
        f"epoch {epoch}:",
        {
            key: results[f"overall_{key}"]
            for key in ["precision", "recall", "f1", "accuracy"]
        },
    )

    # ä¿å­˜å¹¶ä¸Šä¼ 
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
    if accelerator.is_main_process:
        tokenizer.save_pretrained(output_dir)
        repo.push_to_hub(
            commit_message=f"Training in progress epoch {epoch}", blocking=False
        )
```

æœè¿™æ˜¯ä½ ç¬¬ä¸€æ¬¡çœ‹åˆ°ç”¨ ğŸ¤— Accelerate ä¿å­˜çš„æ¨¡å‹ï¼Œè®©æˆ‘ä»¬èŠ±ç‚¹æ—¶é—´æ¥äº†è§£ä¸€ä¸‹è¿™ä¸ªè¿‡ç¨‹ä¸­çš„ä¸‰è¡Œä»£ç ï¼š

```py
accelerator.wait_for_everyone()
unwrapped_model = accelerator.unwrap_model(model)
unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
```

ç¬¬ä¸€è¡Œæ˜¯ä¸è¨€è‡ªæ˜çš„ï¼šå®ƒå‘Šè¯‰æ‰€æœ‰çš„è¿›ç¨‹ç­‰å¾…ï¼Œç›´åˆ°æ‰€æœ‰çš„è¿›ç¨‹éƒ½å¤„äºé‚£ä¸ªé˜¶æ®µå†ç»§ç»­ï¼ˆé˜»å¡ï¼‰ã€‚è¿™æ˜¯ä¸ºäº†ç¡®ä¿åœ¨ä¿å­˜ä¹‹å‰ï¼Œæˆ‘ä»¬åœ¨æ¯ä¸ªè¿›ç¨‹ä¸­éƒ½æœ‰ç›¸åŒçš„æ¨¡å‹ã€‚ç„¶åè·å– `unwrapped_model` ï¼Œå®ƒå°±æ˜¯æˆ‘ä»¬å®šä¹‰çš„åŸºæœ¬æ¨¡å‹ã€‚ `accelerator.prepare()` æ–¹æ³•ä¼šä¸ºäº†åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­å·¥ä½œè€Œå¯¹æ¨¡å‹è¿›è¡Œäº†ä¸€äº›ä¿®æ”¹ï¼Œæ‰€ä»¥å®ƒä¸å†æœ‰ `save_pretraining()` æ–¹æ³•ï¼› ä½¿ç”¨`accelerator.unwrap_model()` æ–¹æ³•å¯ä»¥æ’¤é”€å¯¹æ¨¡å‹çš„æ›´æ”¹ã€‚æœ€åï¼Œæˆ‘ä»¬è°ƒç”¨ `save_pretraining()` ï¼Œå¹¶æŒ‡å®š `accelerator.save()` ä½œä¸º `save_function` è€Œä¸æ˜¯ `torch.save()` ã€‚

å®Œæˆè¿™äº›æ“ä½œåï¼Œä½ åº”è¯¥æ‹¥æœ‰ä¸€ä¸ªä¸ `Trainer` è®­ç»ƒå‡ºçš„æ¨¡å‹ç»“æœç›¸å½“ç±»ä¼¼çš„æ¨¡å‹ã€‚ä½ å¯ä»¥åœ¨ [huggingface-course/bert-finetuned-ner-accelerate](https://huggingface.co/huggingface-course/bert-finetuned-ner-accelerate) æŸ¥çœ‹æˆ‘ä»¬ä½¿ç”¨è¿™äº›ä»£ç è®­ç»ƒçš„æ¨¡å‹ã€‚å¦‚æœä½ æƒ³åœ¨è®­ç»ƒå¾ªç¯ä¸­æµ‹è¯•ä»»ä½•è°ƒæ•´ï¼Œä½ å¯ä»¥ç›´æ¥é€šè¿‡ç¼–è¾‘ä¸Šé¢æ˜¾ç¤ºçš„ä»£ç æ¥å®ç°å®ƒä»¬ï¼

{/if}

## ä½¿ç”¨å¾®è°ƒæ¨¡å‹ [[ä½¿ç”¨å¾®è°ƒæ¨¡å‹]]

æˆ‘ä»¬å·²ç»å‘ä½ å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨æˆ‘ä»¬åœ¨æ¨¡å‹ä¸­å¿ƒå¾®è°ƒçš„æ¨¡å‹å’Œæ¨ç†å°éƒ¨ä»¶ã€‚åœ¨æœ¬åœ°ä½¿ç”¨ `pipeline` æ¥ä½¿ç”¨å®ƒï¼Œä½ åªéœ€è¦æŒ‡å®šæ­£ç¡®çš„æ¨¡å‹æ ‡ç­¾ï¼š

```py
from transformers import pipeline

# å°†æ­¤æ›¿æ¢ä¸ºä½ è‡ªå·±çš„ checkpoint
model_checkpoint = "huggingface-course/bert-finetuned-ner"
token_classifier = pipeline(
    "token-classification", model=model_checkpoint, aggregation_strategy="simple"
)
token_classifier("My name is Sylvain and I work at Hugging Face in Brooklyn.")
```

```python out
[{'entity_group': 'PER', 'score': 0.9988506, 'word': 'Sylvain', 'start': 11, 'end': 18},
 {'entity_group': 'ORG', 'score': 0.9647625, 'word': 'Hugging Face', 'start': 33, 'end': 45},
 {'entity_group': 'LOC', 'score': 0.9986118, 'word': 'Brooklyn', 'start': 49, 'end': 57}]
```

å¤ªæ£’äº†ï¼æˆ‘ä»¬çš„æ¨¡å‹ä¸æ­¤ç®¡é“çš„é»˜è®¤æ¨¡å‹ä¸€æ ·æœ‰æ•ˆï¼
